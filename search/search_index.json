{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyTorch \u4e2d\u6587\u6587\u6863 &amp; \u6559\u7a0b","text":"<p>PyTorch \u662f\u4e00\u4e2a\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60, \u5e76\u4e14\u4f7f\u7528 GPU \u548c CPU \u6765\u4f18\u5316\u7684 tensor library (\u5f20\u91cf\u5e93) </p> \u6b63\u5728\u7ffb\u8bd1: 2.0 \u4e2d\u6587\u7248\u672c \u6700\u65b0 \u82f1\u6587\u6559\u7a0b \u6700\u65b0 \u82f1\u6587\u6587\u6863 1.4 \u4e2d\u6587\u7248\u672c 1.0 \u4e2d\u6587\u7248\u672c 0.3 \u4e2d\u6587\u7248\u672c 0.2 \u4e2d\u6587\u7248\u672c"},{"location":"#_1","title":"\u4ecb\u7ecd","text":"<p>\u6b22\u8fce\u4efb\u4f55\u4eba\u53c2\u4e0e\u548c\u5b8c\u5584\uff1a\u4e00\u4e2a\u4eba\u53ef\u4ee5\u8d70\u7684\u5f88\u5feb\uff0c\u4f46\u662f\u4e00\u7fa4\u4eba\u5374\u53ef\u4ee5\u8d70\u7684\u66f4\u8fdc\u3002</p> <p>\u7ec4\u7ec7\u6784\u5efa[\u7f51\u7ad9]</p> <ul> <li>GitHub Pages(\u56fd\u5916): https://pytorch.apachecn.org</li> <li>Gitee Pages(\u56fd\u5185): https://apachecn.gitee.io/pytorch-doc-zh</li> </ul> <p>\u7b2c\u4e09\u65b9\u7ad9\u957f[\u7f51\u7ad9]</p> <ul> <li>pytorch \u4e2d\u6587\u6587\u6863: https://www.bookstack.cn/search/result?wd=pytorch</li> <li>\u5730\u5740A: xxx (\u6b22\u8fce\u7559\u8a00\uff0c\u6211\u4eec\u5b8c\u5584\u8865\u5145)</li> </ul> <p>\u5176\u4ed6\u8865\u5145</p> <ul> <li>ApacheCN \u5b66\u4e60\u8d44\u6e90</li> <li>ApacheCN \u4e2d\u6587\u7ffb\u8bd1\u7ec4 713436582</li> </ul> <p>\u7248\u672c\u7279\u6027</p> <ul> <li>PyTorch V1.2 \u65b0\u7279\u6027</li> <li>PyTorch V1.3 \u65b0\u7279\u6027</li> <li>PyTorch V1.4 \u65b0\u7279\u6027</li> </ul> <p>PyTorch \u5b98\u65b9\u5165\u53e3</p> <ul> <li>\u4e2d\u6587\u6587\u6863: https://pytorch.org/resources</li> </ul> <p></p>"},{"location":"#_2","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u4e3a\u4e86\u4e0d\u65ad\u6539\u8fdb\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6211\u4eec\u7279\u6b64\u542f\u52a8\u4e86\u3010\u7ffb\u8bd1\u3001\u6821\u5bf9\u3001\u7b14\u8bb0\u6574\u7406\u6d3b\u52a8\u3011\uff0c\u5f00\u8bbe\u4e86\u591a\u4e2a\u6821\u5bf9\u9879\u76ee\u3002\u8d21\u732e\u8005\u6821\u5bf9\u4e00\u7ae0\u4e4b\u540e\u53ef\u4ee5\u9886\u53d6\u5343\u5b572\\~4\u5143\u7684\u5956\u52b1\u3002\u8fdb\u884c\u4e2d\u7684\u6821\u5bf9\u6d3b\u52a8\u8bf7\u89c1\u6d3b\u52a8\u5217\u8868\u3002\u66f4\u591a\u8be6\u60c5\u8bf7\u8054\u7cfb\u98de\u9f99\uff08Q562826179\uff0cV:wizardforcel\uff09\u3002</p>"},{"location":"#docx","title":"DOCX\uff1a\u5f00\u653e\u5171\u4eab\u79d1\u7814\u8bb0\u5f55\u884c\u52a8\u5021\u8bae","text":"<p>\u6211\u4eec\u79ef\u6781\u54cd\u5e94\u79d1\u7814\u5f00\u6e90\u8ba1\u5212\uff08DOCX\uff09\u3002\u5982\u4eca\u5f00\u6e90\u4e0d\u4ec5\u4ec5\u662f\u5f00\u653e\u6e90\u7801\uff0c\u8fd8\u5305\u62ec\u6570\u636e\u96c6\u3001\u6a21\u578b\u3001\u6559\u7a0b\u548c\u5b9e\u9a8c\u8bb0\u5f55\u3002\u6211\u4eec\u4e5f\u5728\u63a2\u8ba8\u5176\u5b83\u7c7b\u522b\u7684\u5f00\u6e90\u65b9\u6848\u548c\u534f\u8bae\u3002</p> <p>\u5e0c\u671b\u5927\u5bb6\u4e86\u89e3\u8fd9\u4e2a\u5021\u8bae\uff0c\u628a\u8fd9\u4e2a\u5021\u8bae\u4e0e\u81ea\u5df1\u7684\u5174\u8da3\u70b9\u7ed3\u5408\uff0c\u505a\u70b9\u529b\u6240\u80fd\u53ca\u7684\u4e8b\u60c5\u3002\u6bcf\u4e2a\u4eba\u7684\u5fae\u5c0f\u7684\u8d21\u732e\uff0c\u6c47\u805a\u5728\u4e00\u8d77\u5c31\u662f\u6574\u4e2a\u5f00\u6e90\u751f\u6001\u3002</p>"},{"location":"#_3","title":"\u9879\u76ee\u770b\u677f","text":"<p>\u9879\u76ee PyTorch 1.4 \u770b\u677f</p> <ul> <li>\u8d1f\u8d23\u4eba: \u8bb0\u5f97\u66f4\u65b0\u548c\u4f18\u5316</li> <li>\u5730\u5740: https://github.com/apachecn/pytorch-doc-zh/projects/2</li> </ul> <p>\u9879\u76ee PyTorch 1.0 \u770b\u677f</p> <ul> <li>\u8d1f\u8d23\u4eba: \u8bb0\u5f97\u66f4\u65b0\u548c\u4f18\u5316</li> <li>\u5730\u5740: https://github.com/apachecn/pytorch-doc-zh/projects/1</li> </ul>"},{"location":"#_4","title":"\u5efa\u8bae\u53cd\u9988","text":"<ul> <li>\u5728\u6211\u4eec\u7684 apachecn/pytorch-doc-zh github \u4e0a\u63d0 issue.</li> <li>\u53d1\u90ae\u4ef6\u5230 Email: <code>apachecn@163.com</code>.</li> <li>\u5728\u6211\u4eec\u7684 QQ\u7fa4: \u52a0\u5165\u65b9\u5f0f \u4e2d\u8054\u7cfb\u7fa4\u4e3b/\u7ba1\u7406\u5458\u5373\u53ef.</li> </ul>"},{"location":"#_5","title":"\u5173\u4e8e\u8f6c\u8f7d","text":"<ul> <li>\u6700\u8fd1\u6709\u5f88\u591a\u4eba\u8054\u7cfb\u6211\u4eec\uff0c\u5173\u4e8e\u5185\u5bb9\u6388\u6743\u95ee\u9898\uff01</li> <li>\u5f00\u6e90\u662f\u6307\u77e5\u8bc6\u5e94\u8be5\u91cd\u5728\u4f20\u64ad\u548c\u8fed\u4ee3\uff08\u800c\u4e0d\u662f\u7981\u6b62\u522b\u4eba\u8f6c\u8f7d\uff09</li> <li>\u4e0d\u7136\u4f60TM\u5728GitHub\u5f00\u6e90\uff0c\u7136\u540e\u53c8\u8bf4\u4e0d\u8ba9\u8f6c\u8f7d\uff0c\u4f60TM\u6709\u75c5\u5427\uff01</li> <li>\u7981\u6b62\u5546\u4e1a\u5316\uff0c\u7b26\u5408\u534f\u8bae\u89c4\u8303\uff0c\u5907\u6ce8\u5730\u5740\u6765\u6e90\uff0c\u91cd\u70b9: \u4e0d\u9700\u8981\u53d1\u90ae\u4ef6\u7ed9\u6211\u4eec\u7533\u8bf7</li> </ul>"},{"location":"#_6","title":"\u8d5e\u52a9\u6211\u4eec","text":""},{"location":"contrib/","title":"\u8d21\u732e\u8005","text":""},{"location":"contrib/#_1","title":"\u9879\u76ee\u8d1f\u8d23\u4eba","text":"<p>\u683c\u5f0f: GitHub + QQ</p> <p>\u7b2c4\u671f 1.2/1.4 (2019-09-17)</p> <ul> <li>\u7247\u523b: 529815144</li> <li>Alex: 1272296763</li> <li>Holly: 514397511</li> <li>N!no: 1352899627</li> <li>qiwei_ji: 1390867192</li> </ul> <p>\u7b2c3\u671f 1.0 (2019-06-10)</p> <ul> <li>FontTian: 2404846224</li> <li>Smile: 240485545</li> <li>Ir1dXD: 1953959092</li> <li>Kai He: 254108879</li> <li>gongel: 1324522527</li> <li>cluster: 859287553</li> <li>sunxia233: 871171307</li> <li>kunwuz: 514397511</li> </ul> <p>\u7b2c2\u671f 1.0 (2019-01-22)</p> <ul> <li>\u98de\u9f99: 562826179</li> <li>\u7247\u523b: 529815144</li> <li>\u54b8\u9c7c: 1034616238</li> </ul> <p>\u7b2c1\u671f 0.3 (2018-04-04)</p> <ul> <li>\u90a3\u4f0a\u62b9\u5fae\u7b11: 1042658081</li> <li>\u98de\u9f99: 562826179</li> <li>\u7247\u523b: 529815144</li> <li>\u54b8\u9c7c: 1034616238</li> <li>Twinkle: 1097078987</li> </ul> <p>-- \u8d1f\u8d23\u4eba\u8981\u6c42: (\u6b22\u8fce\u4e00\u8d77\u4e3a <code>Pytorch \u4e2d\u6587\u7248\u672c</code> \u505a\u8d21\u732e)</p> <ul> <li>\u70ed\u7231\u5f00\u6e90\uff0c\u559c\u6b22\u88c5\u903c</li> <li>\u957f\u671f\u4f7f\u7528 PyTorch(\u81f3\u5c111\u5e74)</li> <li>\u80fd\u591f\u6709\u65f6\u95f4\u53ca\u65f6\u4f18\u5316\u9875\u9762bug\u548c\u7528\u6237issues</li> <li>\u7531\u4e8e\u4f1a\u4e0d\u5b9a\u671f\u548c PyTorch \u5b98\u65b9 \u8fdb\u884cissues or email \u4ea4\u6d41\uff0c\u6240\u4ee5\u66f4\u8981\u79ef\u6781\u4e3b\u52a8</li> <li>\u8bd5\u7528\u671f: 2\u4e2a\u6708</li> <li>\u6b22\u8fce\u8054\u7cfb: \u7247\u523b 529815144</li> </ul>"},{"location":"1.0/","title":"PyTorch 1.0 \u4e2d\u6587\u6587\u6863 &amp; \u6559\u7a0b","text":"<p>PyTorch \u662f\u4e00\u4e2a\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60, \u5e76\u4e14\u4f7f\u7528 GPU \u548c CPU \u6765\u4f18\u5316\u7684 tensor library (\u5f20\u91cf\u5e93) </p> 1.0 \u4e2d\u6587\u7248\u672c \u6700\u65b0 \u82f1\u6587\u6559\u7a0b \u6700\u65b0 \u82f1\u6587\u6587\u6863 0.4 \u4e2d\u6587\u7248\u672c 0.3 \u4e2d\u6587\u7248\u672c 0.2 \u4e2d\u6587\u7248\u672c <p></p> <p>\u6b22\u8fce\u4efb\u4f55\u4eba\u53c2\u4e0e\u548c\u5b8c\u5584\uff1a\u4e00\u4e2a\u4eba\u53ef\u4ee5\u8d70\u7684\u5f88\u5feb\uff0c\u4f46\u662f\u4e00\u7fa4\u4eba\u5374\u53ef\u4ee5\u8d70\u7684\u66f4\u8fdc\u3002</p> <ul> <li>\u5728\u7ebf\u9605\u8bfb</li> <li>ApacheCN \u5b66\u4e60\u8d44\u6e90</li> <li>PyTorch \u4e2d\u6587\u7ffb\u8bd1\u7ec4 | ApacheCN 713436582</li> </ul>"},{"location":"1.0/#_1","title":"\u76ee\u5f55\u7ed3\u6784","text":"<ul> <li>Introduction</li> <li>\u4e2d\u6587\u6559\u7a0b<ul> <li>\u8d77\u6b65<ul> <li>PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8<ul> <li>\u4ec0\u4e48\u662f PyTorch\uff1f</li> <li>Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc</li> <li>\u795e\u7ecf\u7f51\u7edc</li> <li>\u8bad\u7ec3\u5206\u7c7b\u5668</li> <li>\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c\u5904\u7406</li> </ul> </li> <li>\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b</li> <li>\u7528\u4f8b\u5b50\u5b66\u4e60 PyTorch</li> <li>\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b</li> <li>\u6df7\u5408\u524d\u7aef\u7684 seq2seq \u6a21\u578b\u90e8\u7f72</li> <li>Saving and Loading Models</li> <li>What is torch.nn really?</li> </ul> </li> <li>\u56fe\u50cf<ul> <li>Torchvision \u6a21\u578b\u5fae\u8c03</li> <li>\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u6559\u7a0b</li> <li>\u4f7f\u7528 PyTorch \u8fdb\u884c\u56fe\u50cf\u98ce\u683c\u8f6c\u6362</li> <li>\u5bf9\u6297\u6027\u793a\u4f8b\u751f\u6210</li> <li>\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u4f20\u8f93\u5230 Caffe2 \u548c\u79fb\u52a8\u7aef</li> </ul> </li> <li>\u6587\u672c<ul> <li>\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b</li> <li>\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684 RNN \u7f51\u7edc\u751f\u6210\u59d3\u6c0f</li> <li>\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684 RNN \u7f51\u7edc\u8fdb\u884c\u59d3\u6c0f\u5206\u7c7b</li> <li>Deep Learning for NLP with Pytorch<ul> <li>PyTorch \u4ecb\u7ecd</li> <li>\u4f7f\u7528 PyTorch \u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60</li> <li>Word Embeddings: Encoding Lexical Semantics</li> <li>\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc</li> <li>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</li> </ul> </li> <li>\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684 seq2seq \u795e\u7ecf\u7f51\u7edc\u7ffb\u8bd1</li> </ul> </li> <li>\u751f\u6210<ul> <li>DCGAN Tutorial</li> </ul> </li> <li>\u5f3a\u5316\u5b66\u4e60<ul> <li>Reinforcement Learning (DQN) Tutorial</li> </ul> </li> <li>\u6269\u5c55 PyTorch<ul> <li>\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55</li> <li>Custom C++   and CUDA Extensions</li> <li>Extending TorchScript with Custom C++   Operators</li> </ul> </li> <li>\u751f\u4ea7\u6027\u4f7f\u7528<ul> <li>Writing Distributed Applications with PyTorch</li> <li>\u4f7f\u7528 Amazon AWS \u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>ONNX \u73b0\u573a\u6f14\u793a\u6559\u7a0b</li> <li>\u5728 C++ \u4e2d\u52a0\u8f7d PYTORCH \u6a21\u578b</li> </ul> </li> <li>\u5176\u5b83\u8bed\u8a00\u4e2d\u7684 PyTorch<ul> <li>\u4f7f\u7528 PyTorch C++ \u524d\u7aef</li> </ul> </li> </ul> </li> <li>\u4e2d\u6587\u6587\u6863<ul> <li>\u6ce8\u89e3<ul> <li>\u81ea\u52a8\u6c42\u5bfc\u673a\u5236</li> <li>\u5e7f\u64ad\u8bed\u4e49</li> <li>CUDA \u8bed\u4e49</li> <li>Extending PyTorch</li> <li>Frequently Asked Questions</li> <li>Multiprocessing best practices</li> <li>Reproducibility</li> <li>Serialization semantics</li> <li>Windows FAQ</li> </ul> </li> <li>\u5305\u53c2\u8003<ul> <li>torch<ul> <li>Tensors</li> <li>Random sampling</li> <li>Serialization, Parallelism, Utilities</li> <li>Math operations<ul> <li>Pointwise Ops</li> <li>Reduction Ops</li> <li>Comparison Ops</li> <li>Spectral Ops</li> <li>Other Operations</li> <li>BLAS and LAPACK Operations</li> </ul> </li> </ul> </li> <li>torch.Tensor</li> <li>Tensor Attributes</li> <li>\u6570\u636e\u7c7b\u578b\u4fe1\u606f</li> <li>torch.sparse</li> <li>torch.cuda</li> <li>torch.Storage</li> <li>torch.nn</li> <li>torch.nn.functional</li> <li>torch.nn.init</li> <li>torch.optim</li> <li>Automatic differentiation package - torch.autograd</li> <li>Distributed communication package - torch.distributed</li> <li>Probability distributions - torch.distributions</li> <li>Torch Script</li> <li>\u591a\u8fdb\u7a0b\u5305 - torch.multiprocessing</li> <li>torch.utils.bottleneck</li> <li>torch.utils.checkpoint</li> <li>torch.utils.cpp_extension</li> <li>torch.utils.data</li> <li>torch.utils.dlpack</li> <li>torch.hub</li> <li>torch.utils.model_zoo</li> <li>torch.onnx</li> <li>Distributed communication package (deprecated) - torch.distributed.deprecated</li> </ul> </li> <li>torchvision \u53c2\u8003<ul> <li>torchvision.datasets</li> <li>torchvision.models</li> <li>torchvision.transforms</li> <li>torchvision.utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"1.0/ONNXLive/","title":"ONNX \u73b0\u573a\u6f14\u793a\u6559\u7a0b","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d </p> <p>\u672c\u6559\u7a0b\u5c06\u5411\u60a8\u5c55\u793a\u5982\u4f55\u4f7f\u7528ONNX\u5c06\u5df2\u4ecePyTorch\u5bfc\u51fa\u7684\u795e\u7ecf\u6a21\u578b\u4f20\u8f93\u6a21\u578b\u8f6c\u6362\u4e3aApple CoreML\u683c\u5f0f\u3002\u8fd9\u5c06\u5141\u8bb8\u60a8\u5728Apple\u8bbe\u5907\u4e0a\u8f7b\u677e\u8fd0\u884c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4ece\u6444\u50cf\u673a\u76f4\u64ad\u6f14\u793a\u3002  </p>"},{"location":"1.0/ONNXLive/#onnx_1","title":"\u4ec0\u4e48\u662fONNX","text":"<p>ONNX(\u5f00\u653e\u5f0f\u795e\u7ecf\u7f51\u7edc\u4ea4\u6362\uff09\u662f\u4e00\u79cd\u8868\u793a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u653e\u683c\u5f0f\u3002\u501f\u52a9ONNX\uff0cAI\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u66f4\u8f7b\u677e\u5730\u5728\u6700\u5148\u8fdb\u7684\u5de5\u5177\u4e4b\u95f4\u79fb\u52a8\u6a21\u578b\uff0c\u5e76\u9009\u62e9\u6700\u9002\u5408\u5b83\u4eec\u7684\u7ec4\u5408\u3002ONNX\u7531\u5408\u4f5c\u4f19\u4f34\u793e\u533a\u5f00\u53d1\u548c\u652f\u6301\u3002 \u60a8\u53ef\u4ee5\u8bbf\u95ee onnx.ai\uff0c\u4e86\u89e3\u6709\u5173ONNX\u7684\u66f4\u591a\u4fe1\u606f\u4ee5\u53ca\u652f\u6301\u7684\u5de5\u5177\u3002  </p>"},{"location":"1.0/ONNXLive/#_1","title":"\u6559\u7a0b\u9884\u89c8","text":"<p>\u672c\u6559\u7a0b\u5c06\u5e26\u4f60\u8d70\u8fc7\u5982\u4e0b\u4e3b\u89814\u6b65\uff1a  </p> <ol> <li>\u4e0b\u8f7d(\u6216\u8bad\u7ec3\uff09Pytorch\u98ce\u683c\u88c5\u6362\u6a21\u578b</li> <li>\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u81f3ONNX\u6a21\u578b</li> <li>\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u81f3CoreML\u6a21\u578b</li> <li>\u5728\u652f\u6301\u98ce\u683c\u8f6c\u6362iOS App\u4e2d\u8fd0\u884cCoreML\u6a21\u578b </li> </ol>"},{"location":"1.0/ONNXLive/#_2","title":"\u73af\u5883\u51c6\u5907","text":"<p>\u6211\u4eec\u5c06\u5728\u865a\u62df\u73af\u5883\u5de5\u4f5c\uff0c\u4ee5\u907f\u514d\u4e0e\u60a8\u7684\u672c\u5730\u73af\u5883\u51b2\u7a81\u3002\u5728\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528Python 3.6\uff0c\u4f46\u5176\u4ed6\u7248\u672c\u4e5f\u5e94\u8be5\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c\u3002  </p> <pre><code>python3.6 -m venv venv\nsource ./venv/bin/activate\n\n</code></pre> <p>\u6211\u4eec\u9700\u8981\u5b89\u88c5Pytorch\u548c onnx-&gt;coreml \u8f6c\u6362\u5668\uff1a  </p> <pre><code>pip install torchvision onnx-coreml\n\n</code></pre> <p>\u5982\u679c\u8981\u5728iPhone\u4e0a\u8fd0\u884ciOS\u6837\u5f0f\u4f20\u8f93\u5e94\u7528\u7a0b\u5e8f\uff0c\u8fd8\u9700\u8981\u5b89\u88c5XCode\u3002\u60a8\u4e5f\u53ef\u4ee5\u5728Linux\u4e2d\u8f6c\u6362\u6a21\u578b\uff0c\u4f46\u8981\u8fd0\u884ciOS\u5e94\u7528\u7a0b\u5e8f\u672c\u8eab\uff0c\u60a8\u5c06\u9700\u8981\u4e00\u53f0Mac\u3002</p>"},{"location":"1.0/ONNXLive/#pytorch","title":"\u4e0b\u8f7d(\u6216\u8bad\u7ec3\uff09Pytorch\u98ce\u683c\u88c5\u6362\u6a21\u578b","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4e0epytorch\u4e00\u8d77\u53d1\u5e03\u7684\u6837\u5f0f\u4f20\u8f93\u6a21\u578b\uff0c\u5730\u5740\u4e3ahttps://github.com/pytorch/examples/tree/master/fast_neural_style\u3002\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u5176\u4ed6PyTorch\u6216ONNX\u6a21\u578b\uff0c\u8bf7\u968f\u610f\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002  </p> <p>\u8fd9\u4e9b\u6a21\u578b\u7528\u4e8e\u5728\u9759\u6001\u56fe\u50cf\u4e0a\u5e94\u7528\u6837\u5f0f\u4f20\u8f93\uff0c\u5e76\u4e14\u5b9e\u9645\u4e0a\u6ca1\u6709\u9488\u5bf9\u89c6\u9891\u8fdb\u884c\u4f18\u5316\u4ee5\u83b7\u5f97\u8db3\u591f\u5feb\u7684\u901f\u5ea6\u3002\u4f46\u662f\uff0c\u5982\u679c\u6211\u4eec\u5c06\u5206\u8fa8\u7387\u964d\u4f4e\u5230\u8db3\u591f\u4f4e\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u89c6\u9891\u3002  </p> <p>\u6211\u4eec\u5148\u4e0b\u8f7d\u6a21\u578b\uff1a</p> <pre><code>git clone https://github.com/pytorch/examples\ncd examples/fast_neural_style\n</code></pre> <p>\u5982\u679c\u60a8\u60f3\u81ea\u5df1\u8bad\u7ec3\u6a21\u578b\uff0c\u60a8\u521a\u521a\u514b\u9686\u4e0b\u8f7d\u7684\u7684pytorch/examples\u5b58\u50a8\u5e93\u6709\u66f4\u591a\u5173\u4e8e\u5982\u4f55\u6267\u884c\u6b64\u64cd\u4f5c\u7684\u4fe1\u606f\u3002\u76ee\u524d\uff0c\u6211\u4eec\u53ea\u9700\u4f7f\u7528\u5b58\u50a8\u5e93\u63d0\u4f9b\u7684\u811a\u672c\u4e0b\u8f7d\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\uff1a  </p> <pre><code>./download_saved_models.sh\n\n</code></pre> <p>\u6b64\u811a\u672c\u4e0b\u8f7d\u9884\u5148\u8bad\u7ec3\u7684PyTorch\u6a21\u578b\u5e76\u5c06\u5b83\u4eec\u653e\u5165saved_models\u6587\u4ef6\u5939\u4e2d\u3002 \u4f60\u7684\u76ee\u5f55\u4e2d\u73b0\u5728\u5e94\u8be5\u67094\u4e2a\u6587\u4ef6\uff0ccandy.pth\uff0cmosaic.pth\uff0crain_princess.pth\u548cudnie.pth\u3002  </p>"},{"location":"1.0/ONNXLive/#pytorchonnx","title":"\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u81f3ONNX\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u5df2\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684PyTorch\u6a21\u578b\u4f5c\u4e3asaved_models\u6587\u4ef6\u5939\u4e2d\u7684.pth\u6587\u4ef6\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3aONNX\u683c\u5f0f\u3002\u6a21\u578b\u5b9a\u4e49\u5728\u6211\u4eec\u4e4b\u524d\u514b\u9686\u7684pytorch/examples\u5b58\u50a8\u5e93\u4e2d\uff0c\u901a\u8fc7\u51e0\u884cpython\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u5bfc\u51fa\u5230ONNX\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5c06\u8c03\u7528torch.onnx._export\u800c\u4e0d\u662f\u5b9e\u9645\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5c06PyTorch\u4f5c\u4e3aapi\u63d0\u4f9b\uff0c\u4ee5\u76f4\u63a5\u4ecePyTorch\u5bfc\u51faONNX\u683c\u5f0f\u7684\u6a21\u578b\u3002\u4f46\u662f\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u751a\u81f3\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff0c\u56e0\u4e3a\u811a\u672c\u5df2\u7ecf\u5b58\u5728Neural_style / neural_style.py\uff0c\u5b83\u5c06\u4e3a\u6211\u4eec\u6267\u884c\u6b64\u64cd\u4f5c\u3002\u5982\u679c\u8981\u5c06\u5176\u5e94\u7528\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u67e5\u770b\u8be5\u811a\u672c\u3002  </p> <p>\u4ecePyTorch\u5bfc\u51faONNX\u683c\u5f0f\u672c\u8d28\u4e0a\u662f\u8ffd\u8e2a\u60a8\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u56e0\u6b64\u8fd9\u4e2aapi\u8c03\u7528\u5c06\u5728\u5185\u90e8\u8fd0\u884c\u7f51\u7edc\u201c\u865a\u62df\u6570\u636e\u201d\u4ee5\u751f\u6210\u56fe\u5f62\u3002\u4e3a\u6b64\uff0c\u5b83\u9700\u8981\u8f93\u5165\u56fe\u50cf\u6765\u5e94\u7528\u6837\u5f0f\u8f6c\u79fb\uff0c\u5176\u53ef\u4ee5\u7b80\u5355\u5730\u662f\u7a7a\u767d\u56fe\u50cf\u3002\u4f46\u662f\uff0c\u6b64\u56fe\u50cf\u7684\u50cf\u7d20\u5927\u5c0f\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u5c06\u662f\u5bfc\u51fa\u7684\u6837\u5f0f\u4f20\u8f93\u6a21\u578b\u7684\u5927\u5c0f\u3002\u4e3a\u4e86\u83b7\u5f97\u826f\u597d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u5c06\u4f7f\u7528250x540\u7684\u5206\u8fa8\u7387\u3002\u5982\u679c\u60a8\u4e0d\u592a\u5173\u5fc3FPS\uff0c\u53ef\u4ee5\u968f\u610f\u91c7\u53d6\u66f4\u5927\u7684\u5206\u8fa8\u7387\uff0c\u66f4\u591a\u5173\u4e8e\u98ce\u683c\u8f6c\u79fb\u8d28\u91cf\u3002    </p> <p>\u8ba9\u6211\u4eec\u4f7f\u7528ImageMagick\u521b\u5efa\u6211\u4eec\u60f3\u8981\u7684\u5206\u8fa8\u7387\u7684\u7a7a\u767d\u56fe\u50cf\uff1a   </p> <pre><code>convert -size 250x540 xc:white png24:dummy.jpg\n\n</code></pre> <p>\u7136\u540e\u7528\u5b83\u6765\u5bfc\u51faPyTorch\u6a21\u578b\u7528\u5b83\u6765\u5bfc\u51faPyTorch\u6a21\u578b\uff1a  </p> <pre><code>python ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/candy.pth --cuda 0 --export_onnx ./saved_models/candy.onnx\npython ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/udnie.pth --cuda 0 --export_onnx ./saved_models/udnie.onnx\npython ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/rain_princess.pth --cuda 0 --export_onnx ./saved_models/rain_princess.onnx\npython ./neural_style/neural_style.py eval --content-image dummy.jpg --output-image dummy-out.jpg --model ./saved_models/mosaic.pth --cuda 0 --export_onnx ./saved_models/mosaic.onnx\n\n</code></pre> <p>\u4f60\u5e94\u8be5\u5f97\u52304\u4e2a\u6587\u4ef6\uff0c<code>candy.onnx</code>\uff0c<code>mosaic.onnx</code>\uff0c<code>rain_princess.onnx</code>\u548c<code>udnie.onnx</code>\uff0c\u7531\u76f8\u5e94\u7684<code>.pth</code>\u6587\u4ef6\u521b\u5efa\u3002  </p>"},{"location":"1.0/ONNXLive/#onnxcoreml","title":"\u5c06ONNX\u6a21\u578b\u8f6c\u6362\u81f3CoreML\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u6709\u4e86ONNX\u6a21\u578b\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3aCoreML\u6a21\u578b\uff0c\u4ee5\u4fbf\u5728Apple\u8bbe\u5907\u4e0a\u8fd0\u884c\u5b83\u4eec\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4f7f\u7528\u4e4b\u524d\u5b89\u88c5\u7684onnx-coreml\u8f6c\u6362\u5668\u3002\u8f6c\u6362\u5668\u9644\u5e26\u4e00\u4e2aconvert-onnx-to-coreml\u811a\u672c\uff0c\u4e0a\u9762\u7684\u5b89\u88c5\u6b65\u9aa4\u6dfb\u52a0\u5230\u6211\u4eec\u7684\u8def\u5f84\u4e2d\u3002\u9057\u61be\u7684\u662f\uff0c\u8fd9\u5bf9\u6211\u4eec\u4e0d\u8d77\u4f5c\u7528\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u5c06\u7f51\u7edc\u7684\u8f93\u5165\u548c\u8f93\u51fa\u6807\u8bb0\u4e3a\u56fe\u50cf\uff0c\u5e76\u4e14\u867d\u7136\u8fd9\u662f\u8f6c\u6362\u5668\u652f\u6301\u7684\uff0c\u4f46\u53ea\u6709\u5728\u4ecepython\u8c03\u7528\u8f6c\u6362\u5668\u65f6\u624d\u652f\u6301\u5b83\u3002  </p> <p>\u901a\u8fc7\u67e5\u770b\u6837\u5f0f\u4f20\u8f93\u6a21\u578b(\u4f8b\u5982\u5728\u50cfNetron\u8fd9\u6837\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6253\u5f00.onnx\u6587\u4ef6\uff09\uff0c\u6211\u4eec\u770b\u5230\u8f93\u5165\u540d\u4e3a'0'\uff0c\u8f93\u51fa\u540d\u4e3a'186'\u3002\u8fd9\u4e9b\u53ea\u662fPyTorch\u5206\u914d\u7684\u6570\u5b57ID\u3002\u6211\u4eec\u9700\u8981\u5c06\u5b83\u4eec\u6807\u8bb0\u4e3a\u56fe\u50cf\u3002  </p> <p>\u6240\u4ee5\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2apython\u5c0f\u6587\u4ef6\u5e76\u5c06\u5176\u547d\u540d\u4e3aonnx_to_coreml.py\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528touch\u547d\u4ee4\u521b\u5efa\uff0c\u5e76\u4f7f\u7528\u60a8\u559c\u6b22\u7684\u7f16\u8f91\u5668\u8fdb\u884c\u7f16\u8f91\uff0c\u4ee5\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\u884c\u3002  </p> <pre><code>import sys\nfrom onnx import onnx_pb\nfrom onnx_coreml import convert\n\nmodel_in = sys.argv[1]\nmodel_out = sys.argv[2]\n\nmodel_file = open(model_in, 'rb')\nmodel_proto = onnx_pb.ModelProto()\nmodel_proto.ParseFromString(model_file.read())\ncoreml_model = convert(model_proto, image_input_names=['0'], image_output_names=['186'])\ncoreml_model.save(model_out)\n\n</code></pre> <p>\u73b0\u5728\u6765\u8fd0\u884c:</p> <pre><code>python onnx_to_coreml.py ./saved_models/candy.onnx ./saved_models/candy.mlmodel\npython onnx_to_coreml.py ./saved_models/udnie.onnx ./saved_models/udnie.mlmodel\npython onnx_to_coreml.py ./saved_models/rain_princess.onnx ./saved_models/rain_princess.mlmodel\npython onnx_to_coreml.py ./saved_models/mosaic.onnx ./saved_models/mosaic.mlmodel\n\n</code></pre> <p>\u73b0\u5728\uff0c\u60a8\u7684saved_models\u76ee\u5f55\u4e2d\u5e94\u8be5\u67094\u4e2aCoreML\u6a21\u578b\uff1acandy.mlmodel\uff0cmosaic.mlmodel\uff0crain_princess.mlmodel\u548cudnie.mlmodel\u3002  </p>"},{"location":"1.0/ONNXLive/#ios-appcoreml","title":"\u5728\u652f\u6301\u98ce\u683c\u8f6c\u6362iOS App\u4e2d\u8fd0\u884cCoreML\u6a21\u578b","text":"<p>\u6b64\u5b58\u50a8\u5e93(\u5373\u60a8\u5f53\u524d\u6b63\u5728\u9605\u8bfbREADME.md\u7684\u5b58\u50a8\u5e93\uff09\u5305\u542b\u4e00\u4e2aiOS\u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u4ee5\u5728\u624b\u673a\u6444\u50cf\u5934\u7684\u5b9e\u65f6\u6444\u50cf\u5934\u6d41\u4e0a\u8fd0\u884cCoreML\u6837\u5f0f\u4f20\u8f93\u6a21\u578b\u3002  </p> <pre><code>git clone https://github.com/onnx/tutorials\n\n</code></pre> <p>\u5e76\u5728XCode\u4e2d\u6253\u5f00tutorials/examples/CoreML/NNXLive/ONNXLive.xcodeproj\u9879\u76ee\u3002\u6211\u4eec\u5efa\u8bae\u4f7f\u7528XCode 9.3\u548ciPhone X\u3002\u5728\u65e7\u8bbe\u5907\u6216XCode\u7248\u672c\u4e0a\u53ef\u80fd\u4f1a\u51fa\u73b0\u95ee\u9898\u3002   </p> <p>\u5728Models/\u6587\u4ef6\u5939\u4e2d\uff0c\u9879\u76ee\u5305\u542b\u4e00\u4e9b.mlmodel\u6587\u4ef6\u3002\u6211\u4eec\u5c06\u7528\u6211\u4eec\u521a\u521a\u521b\u5efa\u7684\u6a21\u578b\u66ff\u6362\u5b83\u4eec\u3002  </p> <p>\u7136\u540e\u4f60\u5728iPhone\u4e0a\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u5c31\u53ef\u4ee5\u4e86\u3002\u70b9\u51fb\u5c4f\u5e55\u53ef\u5207\u6362\u6a21\u578b\u3002  </p>"},{"location":"1.0/ONNXLive/#_3","title":"\u7ed3\u8bba","text":"<p>\u6211\u4eec\u5e0c\u671b\u672c\u6559\u7a0b\u80fd\u591f\u6982\u8ff0ONNX\u7684\u5185\u5bb9\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b83\u6765\u5728\u6846\u67b6\u4e4b\u95f4\u8f6c\u6362\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u795e\u7ecf\u98ce\u683c\u7684\u4f20\u8f93\u6a21\u578b\u4ecePyTorch\u8f6c\u79fb\u5230CoreML\u3002  </p> <p>\u60a8\u53ef\u4ee5\u968f\u610f\u5c1d\u8bd5\u8fd9\u4e9b\u6b65\u9aa4\u5e76\u5728\u81ea\u5df1\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u5982\u679c\u60a8\u9047\u5230\u4efb\u4f55\u95ee\u9898\u6216\u60f3\u8981\u63d0\u4f9b\u53cd\u9988\uff0c\u8bf7\u544a\u8bc9\u6211\u4eec\u3002\u6211\u4eec\u503e\u542c\u4f60\u7684\u60f3\u6cd5\u3002</p>"},{"location":"1.0/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Introduction</li> <li>\u4e2d\u6587\u6559\u7a0b<ul> <li>\u8d77\u6b65<ul> <li>PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8<ul> <li>\u4ec0\u4e48\u662f PyTorch\uff1f</li> <li>Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc</li> <li>\u795e\u7ecf\u7f51\u7edc</li> <li>\u8bad\u7ec3\u5206\u7c7b\u5668</li> <li>\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c\u5904\u7406</li> </ul> </li> <li>\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b</li> <li>\u7528\u4f8b\u5b50\u5b66\u4e60 PyTorch</li> <li>\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b</li> <li>\u6df7\u5408\u524d\u7aef\u7684 seq2seq \u6a21\u578b\u90e8\u7f72</li> <li>Saving and Loading Models</li> <li>What is torch.nn really?</li> </ul> </li> <li>\u56fe\u50cf<ul> <li>Torchvision \u6a21\u578b\u5fae\u8c03</li> <li>\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u6559\u7a0b</li> <li>\u4f7f\u7528 PyTorch \u8fdb\u884c\u56fe\u50cf\u98ce\u683c\u8f6c\u6362</li> <li>\u5bf9\u6297\u6027\u793a\u4f8b\u751f\u6210</li> <li>\u4f7f\u7528 ONNX \u5c06\u6a21\u578b\u4ece PyTorch \u4f20\u8f93\u5230 Caffe2 \u548c\u79fb\u52a8\u7aef</li> </ul> </li> <li>\u6587\u672c<ul> <li>\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b</li> <li>\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684 RNN \u7f51\u7edc\u751f\u6210\u59d3\u6c0f</li> <li>\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684 RNN \u7f51\u7edc\u8fdb\u884c\u59d3\u6c0f\u5206\u7c7b</li> <li>Deep Learning for NLP with Pytorch<ul> <li>PyTorch \u4ecb\u7ecd</li> <li>\u4f7f\u7528 PyTorch \u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60</li> <li>Word Embeddings: Encoding Lexical Semantics</li> <li>\u5e8f\u5217\u6a21\u578b\u548c LSTM \u7f51\u7edc</li> <li>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</li> </ul> </li> <li>\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684 seq2seq \u795e\u7ecf\u7f51\u7edc\u7ffb\u8bd1</li> </ul> </li> <li>\u751f\u6210<ul> <li>DCGAN Tutorial</li> </ul> </li> <li>\u5f3a\u5316\u5b66\u4e60<ul> <li>Reinforcement Learning (DQN) Tutorial</li> </ul> </li> <li>\u6269\u5c55 PyTorch<ul> <li>\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55</li> <li>Custom C++   and CUDA Extensions</li> <li>Extending TorchScript with Custom C++   Operators</li> </ul> </li> <li>\u751f\u4ea7\u6027\u4f7f\u7528<ul> <li>Writing Distributed Applications with PyTorch</li> <li>\u4f7f\u7528 Amazon AWS \u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>ONNX \u73b0\u573a\u6f14\u793a\u6559\u7a0b</li> <li>\u5728 C++ \u4e2d\u52a0\u8f7d PYTORCH \u6a21\u578b</li> </ul> </li> <li>\u5176\u5b83\u8bed\u8a00\u4e2d\u7684 PyTorch<ul> <li>\u4f7f\u7528 PyTorch C++ \u524d\u7aef</li> </ul> </li> </ul> </li> <li>\u4e2d\u6587\u6587\u6863<ul> <li>\u6ce8\u89e3<ul> <li>\u81ea\u52a8\u6c42\u5bfc\u673a\u5236</li> <li>\u5e7f\u64ad\u8bed\u4e49</li> <li>CUDA \u8bed\u4e49</li> <li>Extending PyTorch</li> <li>Frequently Asked Questions</li> <li>Multiprocessing best practices</li> <li>Reproducibility</li> <li>Serialization semantics</li> <li>Windows FAQ</li> </ul> </li> <li>\u5305\u53c2\u8003<ul> <li>torch<ul> <li>Tensors</li> <li>Random sampling</li> <li>Serialization, Parallelism, Utilities</li> <li>Math operations<ul> <li>Pointwise Ops</li> <li>Reduction Ops</li> <li>Comparison Ops</li> <li>Spectral Ops</li> <li>Other Operations</li> <li>BLAS and LAPACK Operations</li> </ul> </li> </ul> </li> <li>torch.Tensor</li> <li>Tensor Attributes</li> <li>\u6570\u636e\u7c7b\u578b\u4fe1\u606f</li> <li>torch.sparse</li> <li>torch.cuda</li> <li>torch.Storage</li> <li>torch.nn</li> <li>torch.nn.functional</li> <li>torch.nn.init</li> <li>torch.optim</li> <li>Automatic differentiation package - torch.autograd</li> <li>Distributed communication package - torch.distributed</li> <li>Probability distributions - torch.distributions</li> <li>Torch Script</li> <li>\u591a\u8fdb\u7a0b\u5305 - torch.multiprocessing</li> <li>torch.utils.bottleneck</li> <li>torch.utils.checkpoint</li> <li>torch.utils.cpp_extension</li> <li>torch.utils.data</li> <li>torch.utils.dlpack</li> <li>torch.hub</li> <li>torch.utils.model_zoo</li> <li>torch.onnx</li> <li>Distributed communication package (deprecated) - torch.distributed.deprecated</li> </ul> </li> <li>torchvision \u53c2\u8003<ul> <li>torchvision.datasets</li> <li>torchvision.models</li> <li>torchvision.transforms</li> <li>torchvision.utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"1.0/autograd/","title":"Automatic differentiation package - torch.autograd","text":"<p>\u8bd1\u8005\uff1agfjiangly</p> <p><code>torch.autograd</code> \u63d0\u4f9b\u7c7b\u548c\u51fd\u6570\uff0c\u5b9e\u73b0\u4efb\u610f\u6807\u91cf\u503c\u51fd\u6570\u7684\u81ea\u52a8\u5fae\u5206\u3002 \u5b83\u8981\u6c42\u5bf9\u5df2\u6709\u4ee3\u7801\u7684\u6700\u5c0f\u6539\u53d8---\u4f60\u4ec5\u9700\u8981\u7528<code>requires_grad=True</code>\u5173\u952e\u5b57\u4e3a\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684\u58f0\u660e<code>Tensor</code>\u3002</p> <pre><code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)\n</code></pre> <p>\u8ba1\u7b97\u88ab\u7ed9\u5f20\u91cf\u5173\u4e8e\u56fe\u7684\u53f6\u8282\u70b9\u7684\u68af\u5ea6\u548c\u3002</p> <p>\u56fe\u4f7f\u7528\u94fe\u5f0f\u6cd5\u5219\u5fae\u5206\u3002\u5982\u4f55\u4efb\u4f55<code>tensors</code>\u662f\u975e\u6807\u91cf(\u4f8b\u5982\u4ed6\u4eec\u7684\u6570\u636e\u4e0d\u6b62\u4e00\u4e2a\u5143\u7d20\uff09\u5e76\u4e14\u8981\u6c42\u68af\u5ea6\uff0c\u51fd\u6570\u8981\u989d\u5916\u6307\u51fa<code>grad_tensors</code>\u3002\u5b83\u5e94\u662f\u4e00\u4e2a\u5339\u914d\u957f\u5ea6\u7684\u5e8f\u5217\uff0c\u5305\u542b\u53ef\u5fae\u51fd\u6570\u5173\u4e8e\u76f8\u5e94\u5f20\u91cf\u7684\u68af\u5ea6(<code>None</code>\u662f\u4e00\u4e2a\u5bf9\u6240\u6709\u5f20\u91cf\u53ef\u63a5\u53d7\u7684\u503c\uff0c\u4e0d\u9700\u8981\u68af\u5ea6\u5f20\u91cf\uff09\u3002</p> <p>\u6b64\u51fd\u6570\u5728\u53f6\u8282\u70b9\u7d2f\u79ef\u68af\u5ea6 - \u4f60\u53ef\u80fd\u9700\u8981\u5728\u8c03\u7528\u524d\u628a\u5b83\u521d\u59cb\u5316\u4e3a0.</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensors (Tensor\u5e8f\u5217) \u2013 \u8ba1\u7b97\u5bfc\u6570\u7684\u5f20\u91cf\u3002</li> <li>grad_tensors (Tensor _\u6216 None\u5e8f\u5217_) \u2013 \u5173\u4e8e\u76f8\u5e94\u5f20\u91cf\u6bcf\u4e2a\u5143\u7d20\u7684\u68af\u5ea6\u3002\u6807\u91cf\u5f20\u91cf\u6216\u4e0d\u9700\u8981\u68af\u5ea6\u7684\u53ef\u7528None\u6307\u5b9a\u3002\u5982\u679cNone\u5bf9\u6240\u6709grad_tensors\u53ef\u63a5\u53d7\uff0c\u5219\u6b64\u53c2\u6570\u53ef\u9009\u3002</li> <li>retain_graph (bool, \u53ef\u9009) \u2013 \u5982\u679cFalse\uff0c\u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u7684\u56fe\u5c06\u88ab\u91ca\u653e\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u5c06\u6b64\u9009\u9879\u8bbe\u7f6e\u4e3a\u771f\uff0c\u800c\u4e14\u901a\u5e38\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002\u9ed8\u8ba4\u4e3acreate_graph\u503c\u3002</li> <li>create_graph (bool, \u53ef\u9009) \u2013 \u5982\u679cTrue\uff0c\u5219\u6784\u9020\u5bfc\u6570\u56fe\uff0c\u4ee5\u4fbf\u8ba1\u7b97\u66f4\u9ad8\u9636\u5bfc\u6570\uff0c\u9ed8\u8ba4False\u3002</li> </ul> <pre><code>torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n</code></pre> <p>\u8ba1\u7b97\u548c\u8fd4\u56de\u8f93\u51fa\u5173\u4e8e\u8f93\u5165\u7684\u68af\u5ea6\u548c\u3002</p> <p><code>grad_outputs</code> \u5e94\u662f\u957f\u5ea6\u5339\u914d\u8f93\u51fa\u7684\u5e8f\u5217\uff0c\u5305\u542b\u5173\u4e8e\u8f93\u51fa\u6bcf\u4e2a\u5143\u7d20\u7684\u9884\u8ba1\u7b97\u68af\u5ea6\u3002\u5982\u679c\u4e00\u4e2a\u8f93\u51fa\u4e0d\u8981\u6c42\u68af\u5ea6\uff0c\u5219\u68af\u5ea6\u662f<code>None</code>\u3002</p> <p>\u5982\u679c<code>only_inputs</code>\u662f<code>True</code>\uff0c\u6b64\u51fd\u6570\u5c06\u4ec5\u8fd4\u56de\u5173\u4e8e\u6307\u5b9a\u8f93\u5165\u7684\u68af\u5ea6list\u3002\u5982\u679c\u6b64\u53c2\u6570\u662f<code>False</code>\uff0c\u5219\u5173\u4e8e\u5176\u4f59\u5168\u90e8\u53f6\u5b50\u7684\u68af\u5ea6\u4ecd\u88ab\u8ba1\u7b97\uff0c\u5e76\u4e14\u5c06\u7d2f\u52a0\u5230<code>.grad</code>\u5c5e\u6027\u4e2d\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>outputs (Tensor\u5e8f\u5217) \u2013 \u53ef\u5fae\u51fd\u6570\u8f93\u51fa</li> <li>inputs (Tensor\u5e8f\u5217) \u2013 \u5173\u4e8e\u5c06\u8fd4\u56de\u68af\u5ea6\u7684\u8f93\u5165(\u4e0d\u7d2f\u52a0\u5230<code>.grad</code>)\u3002</li> <li>grad_outputs (Tensor\u5e8f\u5217) \u2013 \u5173\u4e8e\u6bcf\u4e2a\u8f93\u5165\u7684\u68af\u5ea6\u3002\u6807\u91cf\u5f20\u91cf\u6216\u4e0d\u9700\u8981\u68af\u5ea6\u7684\u53ef\u7528None\u6307\u5b9a\u3002\u5982\u679cNone\u5bf9\u6240\u6709grad_tensors\u53ef\u63a5\u53d7\uff0c\u5219\u6b64\u53c2\u6570\u53ef\u9009\u3002\u9ed8\u8ba4\uff1a<code>None</code>\u3002</li> <li>retain_graph (bool, \u53ef\u9009) \u2013 \u5982\u679c<code>False</code>\uff0c\u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u7684\u56fe\u5c06\u88ab\u91ca\u653e\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u5c06\u6b64\u9009\u9879\u8bbe\u7f6e\u4e3a\u771f\uff0c\u800c\u4e14\u901a\u5e38\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002\u9ed8\u8ba4\u4e3a<code>create_graph</code>\u503c\u3002</li> <li>create_graph (bool, \u53ef\u9009) \u2013 \u5982\u679c<code>True</code>\uff0c\u5219\u6784\u9020\u5bfc\u6570\u56fe\uff0c\u4ee5\u4fbf\u8ba1\u7b97\u66f4\u9ad8\u9636\u5bfc\u6570\uff0c\u9ed8\u8ba4<code>False</code>\u3002</li> <li>allow_unused (bool, \u53ef\u9009) \u2013 \u5982\u679c<code>False</code>, \u5f53\u8ba1\u7b97\u8f93\u51fa\u51fa\u9519\u65f6\u6307\u660e\u4e0d\u4f7f\u7528\u7684\u8f93\u5165 (\u56e0\u6b64\u5b83\u4eec\u7684\u68af\u5ea6\u4e00\u76f4\u662f0)\u3002 \u9ed8\u8ba4<code>False</code>\u3002</li> </ul>"},{"location":"1.0/autograd/#_1","title":"\u5c40\u90e8\u7981\u7528\u68af\u5ea6\u8ba1\u7b97","text":"<pre><code>class torch.autograd.no_grad\n</code></pre> <p>\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <p>\u5f53\u4f60\u786e\u8ba4\u4e0d\u4f1a\u8c03\u7528 <code>Tensor.backward()</code>\uff0c\u5bf9\u4e8e\u63a8\u65ad\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\u662f\u6709\u7528\u7684\u3002\u5b83\u5c06\u51cf\u5c11\u8ba1\u7b97\u7684\u5185\u5b58\u6d88\u8017\uff0c\u5426\u5219\u4f1a\u6709<code>requires_grad=True</code>\u3002\u5728\u8fd9\u4e2a\u6a21\u5f0f\u4e2d\uff0c\u6bcf\u4e2a\u8ba1\u7b97\u7ed3\u679c\u5c06\u5bfc\u81f4<code>requires_grad=False</code>, \u5373\u4fbf\u8f93\u5165\u6709<code>requires_grad=True</code>\u3002</p> <p>\u51fd\u6570\u8fd8\u53ef\u4f5c\u4e3a\u88c5\u9970\u5668\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...   y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n&gt;&gt;&gt; @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n&gt;&gt;&gt; z = doubler(x)\n&gt;&gt;&gt; z.requires_grad\nFalse\n\n</code></pre> <pre><code>class torch.autograd.enable_grad\n</code></pre> <p>\u4f7f\u80fd\u68af\u5ea6\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <p>\u5728\u4e00\u4e2a<code>no_grad</code>\u4e0a\u4e0b\u6587\u4e2d\u4f7f\u80fd\u68af\u5ea6\u8ba1\u7b97\u3002\u5728<code>no_grad</code>\u5916\u90e8\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u65e0\u5f71\u54cd</p> <p>\u51fd\u6570\u8fd8\u53ef\u4f5c\u4e3a\u88c5\u9970\u5668\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n&gt;&gt;&gt; y.backward()\n&gt;&gt;&gt; x.grad\n&gt;&gt;&gt; @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n&gt;&gt;&gt; with torch.no_grad():\n...     z = doubler(x)\n&gt;&gt;&gt; z.requires_grad\nTrue\n\n</code></pre> <pre><code>class torch.autograd.set_grad_enabled(mode)\n</code></pre> <p>\u8bbe\u7f6e\u68af\u5ea6\u8ba1\u7b97\u6253\u5f00\u6216\u5173\u95ed\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <p><code>set_grad_enabled</code>\u5c06\u57fa\u4e8e\u5b83\u7684\u53c2\u6570<code>mode</code>\u4f7f\u7528\u6216\u7981\u7528\u68af\u5ea6\u3002\u5b83\u4e5f\u80fd\u4f5c\u4e3a\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u6216\u51fd\u6570\u4f7f\u7528\u3002</p> \u53c2\u6570: mode (bool) \u2013 \u6807\u8bb0\u662f\u5426\u4f7f\u80fd\u68af\u5ea6(True\uff09\uff0c\u6216\u4f7f\u80fd(False\uff09\u3002\u8fd9\u80fd\u88ab\u7528\u5728\u6709\u6761\u4ef6\u7684\u4f7f\u80fd\u68af\u5ea6\u3002 <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; is_train = False\n&gt;&gt;&gt; with torch.set_grad_enabled(is_train):\n...   y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n&gt;&gt;&gt; torch.set_grad_enabled(True)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n&gt;&gt;&gt; torch.set_grad_enabled(False)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n</code></pre>"},{"location":"1.0/autograd/#tensors","title":"\u5173\u4e8eTensors\u7684\u539f\u4f4d\u64cd\u4f5c","text":"<p>\u5728autograd\u4e2d\u652f\u6301\u539f\u4f4d\u64cd\u4f5c\u662f\u4e00\u4ef6\u5f88\u96be\u7684\u4e8b\uff0c\u5e76\u4e14\u6211\u4eec\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4e0d\u9f13\u52b1\u4f7f\u7528\u5b83\u4eec\u3002Autograd\u79ef\u6781\u7684\u7f13\u51b2\u533a\u91ca\u653e\u548c\u91cd\u7528\u4f7f\u5176\u975e\u5e38\u9ad8\u6548\uff0c\u5b9e\u9645\u4e0a\u539f\u4f4d\u64cd\u4f5c\u4f1a\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u60c5\u51b5\u975e\u5e38\u5c11\u3002\u4f60\u53ef\u80fd\u6c38\u8fdc\u4e0d\u4f1a\u4f7f\u7528\u5b83\u4eec\uff0c\u9664\u975e\u6b63\u5728\u5f88\u5927\u7684\u5185\u5b58\u538b\u529b\u4e0b\u64cd\u4f5c\u3002</p>"},{"location":"1.0/autograd/#_2","title":"\u5c31\u5730\u6b63\u786e\u6027\u68c0\u67e5","text":"<p>\u5168\u90e8\u7684Tensor\u4fdd\u6301\u8ffd\u8e2a\u5e94\u7528\u5230\u5b83\u4eec\u8eab\u4e0a\u7684\u539f\u4f4d\u64cd\u4f5c\uff0c\u5e76\u4e14\u5982\u679c\u5b9e\u73b0\u68c0\u6d4b\u5230\u5728\u4efb\u4f55\u4e00\u4e2a\u51fd\u6570\u4e2d\uff0c\u4e00\u4e2atensor\u4e3a\u53cd\u5411\u4f20\u64ad\u4fdd\u5b58\uff0c\u4f46\u662f\u968f\u540e\u88ab\u539f\u4f4d\u4fee\u6539\uff0c\u4e00\u65e6\u53cd\u5411\u4f20\u64ad\u5f00\u59cb\u5c06\u629b\u51fa\u4e00\u4e2a\u9519\u8bef\u3002\u6b64\u8bbe\u8ba1\u786e\u4fdd\u5982\u679c\u4f60\u6b63\u5728\u4f7f\u7528\u539f\u4f4d\u64cd\u4f5c\u51fd\u6570\u5e76\u4e14\u6ca1\u6709\u770b\u5230\u4efb\u4f55\u9519\u8bef\uff0c\u4f60\u53ef\u4ee5\u786e\u4fdd\u8ba1\u7b97\u7684\u68af\u5ea6\u662f\u6b63\u786e\u7684\u3002</p>"},{"location":"1.0/autograd/#variable","title":"Variable (\u5f03\u7528)","text":"<p>\u8b66\u544a</p> <p>Variable API\u5df2\u7ecf\u88ab\u5f03\u7528\u3002\u5bf9\u5f20\u91cf\u4f7f\u7528\u81ea\u52a8\u6c42\u5bfc\u4e0d\u518d\u9700\u8981Variable\u3002Autograd\u81ea\u52a8\u652f\u6301<code>requires_grad</code>\u53c2\u6570\u8bbe\u7f6e\u6210<code>True</code>\u7684\u5f20\u91cf\u3002\u4ee5\u4e0b\u662f\u6709\u5173\u66f4\u6539\u5185\u5bb9\u7684\u5feb\u901f\u6307\u5357\uff1a</p> <ul> <li><code>Variable(tensor)</code> \u548c<code>Variable(tensor, requires_grad)</code>\u4ecd\u7136\u548c\u9884\u671f\u4e00\u6837\u5de5\u4f5c\uff0c\u4f46\u5b83\u4eec\u8fd4\u56deTensors\u4ee3\u66ffVariables\u3002</li> <li><code>var.data</code> \u548c <code>tensor.data</code>\u662f\u4e00\u56de\u4e8b\u3002</li> <li>\u65b9\u6cd5\u5982<code>var.backward(), var.detach(), var.register_hook()</code>\u73b0\u5728\u5728tensors\u4e0a\u4f7f\u7528\u76f8\u540c\u7684\u540d\u5b57\u8d77\u4f5c\u7528\u3002</li> </ul> <p>\u6b64\u5916\uff0c\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u8bf8\u5982<code>torch.randn()</code>, <code>torch.zeros()</code>, <code>torch.ones()</code>\u7b49\u5de5\u5382\u65b9\u6cd5\u521b\u5efarequires_grad=True\u7684\u5f20\u91cf\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <p><code>autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)</code></p>"},{"location":"1.0/autograd/#_3","title":"\u5f20\u91cf\u81ea\u52a8\u6c42\u5bfc\u51fd\u6570","text":"<pre><code>class torch.Tensor\n</code></pre> <pre><code>backward(gradient=None, retain_graph=None, create_graph=False)\n</code></pre> <p>\u8ba1\u7b97\u5f53\u524d\u5f20\u91cf\u5173\u4e8e\u56fe\u53f6\u8282\u70b9\u7684\u68af\u5ea6\u3002</p> <p>\u56fe\u4f7f\u7528\u94fe\u5f0f\u53cd\u5219\u5fae\u5206\u3002\u5982\u679c\u5f20\u91cf\u662f\u975e\u6807\u91cf\u5e76\u4e14\u8981\u6c42\u68af\u5ea6\uff0c\u51fd\u6570\u989d\u5916\u8981\u6c42\u6307\u68af\u5ea6\u3002\u5b83\u5e94\u662f\u4e00\u4e2a\u5339\u914d\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u5f20\u91cf\uff0c\u542b\u6709\u53ef\u5fae\u51fd\u6570\u5173\u4e8e\u5b83\u672c\u8eab\u7684\u68af\u5ea6\u3002</p> <p>\u6b64\u51fd\u6570\u5728\u53f6\u8282\u70b9\u7d2f\u52a0\u68af\u5ea6-\u4f60\u53ef\u80fd\u9700\u8981\u5728\u8c03\u7528\u524d\u5c06\u5b83\u521d\u59cb\u5316\u4e3a0\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>gradient (Tensor \u6216 None) \u2013 \u5173\u4e8e\u5f20\u91cf\u7684\u68af\u5ea6\u3002\u5982\u679c\u5b83\u662f\u4e00\u4e2a\u5f20\u91cf\uff0c\u5b83\u5c06\u88ab\u81ea\u52a8\u8f6c\u6362\u6210\u4e0d\u8981\u6c42\u68af\u5ea6\u7684\u5f20\u91cf\uff0c\u9664\u975e<code>create_graph</code>\u662f<code>True</code>\u3002\u6807\u91cf\u5f20\u91cf\u6216\u4e0d\u9700\u8981\u68af\u5ea6\u7684\u53ef\u7528<code>None</code>\u6307\u5b9a\u3002\u5982\u679cNone\u5bf9\u6240\u6709grad_tensors\u53ef\u63a5\u53d7\uff0c\u5219\u6b64\u53c2\u6570\u53ef\u9009\u3002</li> <li>retain_graph (bool, \u53ef\u9009) \u2013 \u5982\u679c<code>False</code>\uff0c\u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u7684\u56fe\u5c06\u88ab\u91ca\u653e\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u4e0d\u9700\u8981\u5c06\u6b64\u9009\u9879\u8bbe\u7f6e\u4e3a\u771f\uff0c\u800c\u4e14\u901a\u5e38\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u95ee\u9898\u3002\u9ed8\u8ba4\u4e3a<code>create_graph</code>\u503c\u3002</li> <li>create_graph (bool, \u53ef\u9009) \u2013 \u5982\u679c<code>True</code>\uff0c\u5219\u6784\u9020\u5bfc\u6570\u56fe\uff0c\u4ee5\u4fbf\u8ba1\u7b97\u66f4\u9ad8\u9636\u5bfc\u6570\uff0c\u9ed8\u8ba4<code>False</code>\u3002</li> </ul> <pre><code>detach()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684Tensor\uff0c\u4ece\u5f53\u524d\u56fe\u4e2d\u5206\u79bb\u51fa\u6765\u3002</p> <p>\u7ed3\u679c\u4e0d\u8981\u6c42\u68af\u5ea6\u3002</p> <p>\u6ce8\u610f</p> <p>\u8fd4\u56de\u7684\u5f20\u91cf\u4e0e\u539f\u59cb\u5f20\u91cf\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u3002\u5173\u4e8e\u5b83\u4eec\u4e2d\u4efb\u4e00\u4e2a\u539f\u4f4d\u4fee\u6539\u5c06\u88ab\u770b\u89c1\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u6b63\u786e\u6027\u68c0\u67e5\u4e2d\u89e6\u53d1\u9519\u8bef\u3002</p> <pre><code>detach_()\n</code></pre> <p>\u4ece\u521b\u5efa\u5b83\u7684\u56fe\u4e2d\u5206\u79bb\u5f20\u91cf\uff0c\u4f7f\u5176\u6210\u4e3a\u53f6\u3002\u4e0d\u80fd\u5c31\u5730\u5206\u79bb\u89c6\u56fe\u3002</p> <pre><code>grad\n</code></pre> <p>\u6b64\u5c5e\u6027\u9ed8\u8ba4<code>None</code>\uff0c\u5e76\u4e14\u8c03\u7528<code>backward()</code>\u8ba1\u7b97\u81ea\u8eab\u68af\u5ea6\u65f6\u7b2c\u4e00\u65f6\u95f4\u6210\u4e3a\u4e00\u4e2aTensor\u3002\u6b64\u5c5e\u6027\u5c06\u542b\u8ba1\u7b97\u7684\u68af\u5ea6\uff0c\u4ee5\u540e\u8c03\u7528<code>backward()</code>\u5c06\u7d2f\u52a0\u63d0\u5230\u5230\u81ea\u8eab\u3002</p> <pre><code>is_leaf\n</code></pre> <p>\u6309\u60ef\u4f8b\uff0c\u6240\u6709<code>requires_grad</code>=False\u7684\u5f20\u91cf\u5c06\u662f\u53f6\u8282\u70b9\u5f20\u91cf</p> <p>\u5982\u679c\u5f20\u91cf\u662f\u7531\u7528\u6237\u521b\u5efa\uff0c<code>requires_grad</code>\u7684\u5f20\u91cf\u4e5f\u662f\u53f6\u8282\u70b9\u5f20\u91cf\u3002\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u4e0d\u662f\u4e00\u4e2a\u64cd\u4f5c\u7684\u7ed3\u679c\uff0c\u5e76\u4e14<code>grad_fn</code>\u662f<code>None</code>\u3002</p> <p>\u4ec5\u53f6\u8282\u70b9\u5f20\u91cf\u5728\u8c03\u7528<code>backward()</code>\u65f6\u586b\u5145\u5b83\u4eec\u7684<code>grad</code>\u3002\u4e3a\u5f97\u5230\u4ece\u975e\u53f6\u8282\u70b9\u5f20\u91cf\u586b\u5145\u7684\u68af\u5ea6\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528<code>retain_grad()</code>.</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)\n&gt;&gt;&gt; a.is_leaf\nTrue\n&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()\n&gt;&gt;&gt; b.is_leaf\nFalse\n# b \u662f\u7531cpu Tensor\u6295\u5165cuda Tensor\u7684\u64cd\u4f5c\u521b\u5efa\u7684\n&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2\n&gt;&gt;&gt; c.is_leaf\nFalse\n# c \u662f\u7531\u52a0\u64cd\u4f5c\u521b\u5efa\u7684\n&gt;&gt;&gt; d = torch.rand(10).cuda()\n&gt;&gt;&gt; d.is_leaf\nTrue\n# d \u4e0d\u8981\u6c42\u68af\u5ea6\uff0c\u6240\u4ee5\u6ca1\u6709\u521b\u5efa\u5b83\u7684\u64cd\u4f5c (\u88ab\u81ea\u52a8\u6c42\u5bfc\u5f15\u64ce\u8ffd\u8e2a)\n&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()\n&gt;&gt;&gt; e.is_leaf\nTrue\n# e \u8981\u6c42\u68af\u5ea6\u5e76\u4e14\u6ca1\u6709\u521b\u5efa\u5b83\u7684\u64cd\u4f5c\n&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt; f.is_leaf\nTrue\n# f \u8981\u6c42\u68af\u5ea6\u5e76\u4e14\u6ca1\u6709\u521b\u5efa\u5b83\u7684\u64cd\u4f5c\n\n</code></pre> <pre><code>register_hook(hook)\n</code></pre> <p>\u6ce8\u518c\u4e00\u4e2a\u53cd\u5411\u94a9\u5b50</p> <p>\u6b64\u94a9\u5b50\u6bcf\u6b21\u5728\u5bf9\u5e94\u5f20\u91cf\u68af\u5ea6\u88ab\u8ba1\u7b97\u65f6\u8c03\u7528\u3002\u6b64\u94a9\u5b50\u5e94\u6709\u4e0b\u9762\u9c9c\u660e\u7279\u5f81\uff1a</p> <pre><code>hook(grad) -&gt; Tensor or None\n\n</code></pre> <p>\u6b64\u94a9\u5b50\u4e0d\u5e94\u8be5\u4fee\u6539\u5b83\u7684\u53c2\u6570\uff0c\u4f46\u5b83\u80fd\u53ef\u9009\u5730\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7528\u4e8e\u66ff\u4ee3 <code>grad</code>\u7684\u68af\u5ea6\u3002</p> <p>\u6b64\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u53e5\u67c4\uff0c\u5176\u53e5\u67c4\u65b9\u6cd5\u4e3a<code>handle.remove()</code>\uff0c\u7528\u4e8e\u4ece\u6a21\u5757\u4e2d\u5220\u9664\u94a9\u5b50\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)\n&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))\n&gt;&gt;&gt; v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n&gt;&gt;&gt; h.remove()  # removes the hook\n\n</code></pre> <pre><code>requires_grad\n</code></pre> <p>\u5982\u679c\u68af\u5ea6\u9700\u8981\u4e3a\u6b64\u5f20\u91cf\u8ba1\u7b97\u5219\u662f<code>True</code>\uff0c\u5426\u5219\u4e3a<code>False</code></p> <p>\u6ce8\u610f</p> <p>\u4e8b\u5b9e\u662f\u68af\u5ea6\u9700\u8981\u4e3a\u6b64\u5f20\u91cf\u8ba1\u7b97\u4e0d\u610f\u5473\u7740<code>grad</code>\u5c5e\u6027\u5c06\u88ab\u586b\u5145\uff0c\u66f4\u591a\u7ec6\u8282\u89c1<code>is_leaf</code>\u3002</p> <pre><code>retain_grad()\n</code></pre> <p>\u4e3a\u975e\u53f6\u8282\u70b9\u5f20\u91cf\u4f7f\u80fd<code>.grad</code>\u5c5e\u6027</p>"},{"location":"1.0/autograd/#function","title":"Function","text":"<pre><code>class torch.autograd.Function\n</code></pre> <p>\u8bb0\u5f55\u64cd\u4f5c\u5386\u53f2\uff0c\u5b9a\u4e49\u53ef\u5fae\u64cd\u4f5c\u516c\u5f0f\u3002</p> <p>\u5728Tensor\u4e0a\u6267\u884c\u7684\u6bcf\u4e2a\u64cd\u4f5c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u51fd\u6570\u5bf9\u8c61\uff0c\u6267\u884c\u8ba1\u7b97\uff0c\u8bb0\u5f55\u5b83\u53d1\u751f\u7684\u3002\u5386\u53f2\u8bb0\u5f55\u4ee5\u51fd\u6570\u7684DAG\u5f62\u5f0f\u4fdd\u7559\uff0cDAG\u7684\u8fb9\u8868\u793a\u6570\u636e\u7684\u4f9d\u8d56\u6027(<code>input &amp;lt;- output</code>\uff09\u3002\u7136\u540e\uff0c\u5f53backward\u88ab\u8c03\u7528\uff0c\u56fe\u6309\u62d3\u6251\u987a\u5e8f\u88ab\u5904\u7406\uff0c\u901a\u8fc7\u8c03\u7528\u6bcf\u4e2a<code>Function</code>\u5bf9\u8c61\u7684backward()\u65b9\u6cd5\uff0c\u5e76\u4e14\u4f20\u9012\u68af\u5ea6\u7ed9\u4e0b\u4e00\u4e2a<code>Function</code>\u3002</p> <p>\u901a\u5e38\uff0c\u7528\u6237\u4e0e\u51fd\u6570\u4ea4\u4e92\u7684\u552f\u4e00\u65b9\u5f0f\u662f\u901a\u8fc7\u521b\u5efa\u5b50\u7c7b\u548c\u5b9a\u4e49\u65b0\u64cd\u4f5c\u3002\u8fd9\u662f\u4e00\u79cd\u88ab\u63a8\u8350\u7684\u6269\u5c55<code>torch.autograd</code>\u7684\u65b9\u5f0f\u3002</p> <p>\u6bcf\u4e2a\u51fd\u6570\u5bf9\u8c61\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21(\u5728\u6b63\u5411\u4f20\u9012\u4e2d\uff09\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n\n</code></pre> <pre><code>static backward(ctx, *grad_outputs)\n</code></pre> <p>\u5b9a\u4e49\u4e00\u4e2a\u516c\u5f0f\u8ba1\u7b97\u64cd\u4f5c\u5bfc\u6570\u3002</p> <p>\u6b64\u51fd\u6570\u88ab\u6240\u6709\u5b50\u7c7b\u91cd\u8f7d\u3002</p> <p>\u5b83\u5fc5\u987b\u63a5\u53d7\u4e00\u4e2a\u4e0a\u4e0b\u6587<code>ctx</code>\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u968f\u540e\u662f<code>forward()</code>\u8fd4\u56de\u7684\u5927\u91cf\u8f93\u51fa\uff0c\u5e76\u4e14\u5b83\u5e94\u8fd4\u56de\u5c3d\u53ef\u80fd\u591a\u7684\u5f20\u91cf\uff0c\u4f5c\u4e3a<code>forward()</code>\u51fd\u6570\u8f93\u5165\u3002\u6bcf\u4e2a\u53c2\u6570\u662f\u5173\u4e8e\u88ab\u7ed9\u8f93\u51fa\u7684\u68af\u5ea6\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fd4\u56de\u503c\u662f\u5173\u4e8e\u76f8\u5e94\u8f93\u5165\u7684\u68af\u5ea6\u3002</p> <p><code>ctx</code>\u4e0a\u4e0b\u6587\u53ef\u7528\u4e8e\u6062\u590d\u4fdd\u5b58\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u7684\u68af\u5ea6\u3002\u5b83\u6709\u4e00\u4e2a<code>ctx.needs_input_grad</code>\u5c5e\u6027\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u4ee3\u8868\u6bcf\u4e2a\u8f93\u5165\u662f\u5426\u9700\u8981\u68af\u5ea6\u7684\u5e03\u5c14\u5143\u7ec4\u3002</p> <pre><code>static forward(ctx, *args, **kwargs)\n</code></pre> <p>\u6267\u884c\u64cd\u4f5c\u3002</p> <p>\u6b64\u51fd\u6570\u88ab\u6240\u6709\u5b50\u7c7b\u91cd\u8f7d\u3002</p> <p>\u5b83\u5fc5\u987b\u63a5\u53d7\u4e00\u4e2a\u4e0a\u4e0b\u6587<code>ctx</code>\u4f5c\u4e3a\u7b2c\u4e00\u4e2a\u53c2\u6570\uff0c\u968f\u540e\u662f\u4efb\u610f\u6570\u91cf\u7684\u53c2\u6570(tensor\u6216\u5176\u5b83\u7c7b\u578b\uff09\u3002</p> <p>\u6b64\u4e0a\u4e0b\u6587\u53ef\u88ab\u7528\u6765\u5b58\u50a8\u5f20\u91cf\uff0c\u968f\u540e\u53ef\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u53d6\u51fa\u3002</p>"},{"location":"1.0/autograd/#_4","title":"\u6570\u503c\u68af\u5ea6\u68c0\u67e5","text":"<pre><code>torch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True)\n</code></pre> <p>\u901a\u8fc7\u5c0f\u7684\u6709\u9650\u5dee\u5206\u4e0e\u5173\u4e8e\u6d6e\u70b9\u7c7b\u578b\u4e14<code>requires_grad=True</code>\u7684\u8f93\u5165\u5f20\u91cf\u6765\u68c0\u67e5\u8ba1\u7b97\u7684\u68af\u5ea6\u3002</p> <p>\u5728\u6570\u7ec4\u68af\u5ea6\u548c\u5206\u6790\u68af\u5ea6\u4e4b\u95f4\u68c0\u67e5\u4f7f\u7528<code>allclose()</code>\u3002</p> <p>\u6ce8\u610f</p> <p>\u9ed8\u8ba4\u503c\u4e3a\u53cc\u7cbe\u5ea6\u8f93\u5165\u8bbe\u8ba1\u3002\u5982\u679c\u8f93\u5165\u6b20\u7cbe\u5ea6\u6b64\u68c0\u67e5\u6709\u53ef\u80fd\u5931\u8d25\uff0c\u4f8b\u5982\uff0c<code>FloatTensor</code>\u3002</p> <p>\u8b66\u544a</p> <p>\u5982\u679c\u5728\u8f93\u5165\u4e2d\u4efb\u4f55\u88ab\u68c0\u67e5\u7684\u5f20\u91cf\u6709\u91cd\u53e0\u7684\u5185\u5b58\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6307\u5411\u76f8\u540c\u5185\u5b58\u5730\u5740\u7684\u4e0d\u540c\u5207\u7247(\u4f8b\u5982\uff0c\u4ecetorch.expand()\uff09\uff0c\u6b64\u68c0\u67e5\u5c06\u6709\u53ef\u80fd\u5931\u8d25\uff0c\u56e0\u4e3a\u5728\u8fd9\u4e2a\u7d22\u5f15\u901a\u8fc7\u70b9\u6270\u52a8\u8ba1\u7b97\u7684\u6570\u503c\u68af\u5ea6\u5c06\u6539\u53d8\u5728\u5168\u90e8\u5176\u5b83\u7d22\u5f15\u5904\u5171\u4eab\u5185\u5b58\u5730\u5740\u7684\u503c\u3002</p> <p>\u53c2\u6570</p> <ul> <li>func (function) \u2013 \u4e00\u4e2aPython\u51fd\u6570\uff0c\u8f93\u5165\u662f\u5f20\u91cf\uff0c\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\u6216\u5f20\u91cf\u5143\u7ec4</li> <li>inputs (\u5f20\u91cf\u5143\u7ec4 or Tensor) \u2013 func\u51fd\u6570\u8f93\u5165</li> <li>eps (float, \u53ef\u9009) \u2013 \u6709\u9650\u5dee\u5206\u7684\u6270\u52a8</li> <li>atol (float, \u53ef\u9009) \u2013 \u7edd\u5bf9\u5bb9\u5dee</li> <li>rtol (float, \u53ef\u9009) \u2013 \u76f8\u5bf9\u5bb9\u5dee</li> <li>raise_exception (bool, \u53ef\u9009) \u2013 \u6307\u793a\u5982\u679c\u68c0\u67e5\u5931\u8d25\u662f\u5426\u629b\u51fa\u4e00\u4e2a\u5f02\u5e38\u3002\u6b64\u5f02\u5e38\u7ed9\u51fa\u5173\u4e8e\u5931\u8d25\u7684\u786e\u5207\u6027\u8d28\u7684\u66f4\u591a\u4fe1\u606f\u3002\u8fd9\u5728\u68af\u5ea6\u68c0\u67e5\u8c03\u8bd5\u65f6\u662f\u6709\u7528\u7684\u3002</li> </ul> \u8fd4\u56de: \u5982\u679c\u6240\u6709\u5dee\u90fd\u6ee1\u8db3\u5168\u90e8\u95ed\u5408\u6761\u4ef6\uff0c\u5219\u4e3aTrue <pre><code>torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True)\n</code></pre> <p>\u901a\u8fc7\u5c0f\u7684\u6709\u9650\u5dee\u5206\u4e0e\u5173\u4e8e\u5728\u8f93\u5165\u4e2d\u5f20\u91cf\u7684\u5206\u6790\u68af\u5ea6\uff0c\u68c0\u67e5\u5df2\u8ba1\u7b97\u68af\u5ea6\u7684\u68af\u5ea6\uff0c\u5e76\u4e14\u5728requires_grad=True \u60c5\u51b5\u4e0b\uff0cgrad_outputs\u662f\u6d6e\u70b9\u7c7b\u578b\u3002</p> <p>\u6b64\u51fd\u6570\u68c0\u67e5\u901a\u8fc7\u8ba1\u7b97\u5230\u7ed9\u5b9agrad_outputs\u7684\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\u662f\u5426\u6b63\u786e\u3002</p> <p>\u5728\u6570\u503c\u68af\u5ea6\u548c\u5206\u6790\u68af\u5ea6\u4e4b\u95f4\u4f7f\u7528<code>allclose()</code>\u68c0\u67e5\u3002</p> <p>\u6ce8\u610f</p> <p>\u9ed8\u8ba4\u503c\u4e3a\u53cc\u7cbe\u5ea6\u8f93\u5165\u8bbe\u8ba1\u3002\u5982\u679c\u8f93\u5165\u6b20\u7cbe\u5ea6\u6b64\u68c0\u67e5\u6709\u53ef\u80fd\u5931\u8d25\uff0c\u4f8b\u5982\uff0c<code>FloatTensor</code>\u3002</p> <p>\u8b66\u544a</p> <p>\u5982\u679c\u5728\u8f93\u5165\u4e2d\u4efb\u4f55\u88ab\u68c0\u67e5\u7684\u5f20\u91cf\u6709\u91cd\u53e0\u7684\u5185\u5b58\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6307\u5411\u76f8\u540c\u5185\u5b58\u5730\u5740\u7684\u4e0d\u540c\u5207\u7247(\u4f8b\u5982\uff0c\u4ece<code>torch.expand()</code>\uff09\uff0c\u6b64\u68c0\u67e5\u5c06\u6709\u53ef\u80fd\u5931\u8d25\uff0c\u56e0\u4e3a\u5728\u8fd9\u4e2a\u7d22\u5f15\u901a\u8fc7\u70b9\u6270\u52a8\u8ba1\u7b97\u7684\u6570\u503c\u68af\u5ea6\u5c06\u6539\u53d8\u5728\u5168\u90e8\u5176\u5b83\u7d22\u5f15\u5904\u5171\u4eab\u5185\u5b58\u5730\u5740\u7684\u503c\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>func (function) \u2013 \u4e00\u4e2aPython\u51fd\u6570\uff0c\u8f93\u5165\u662f\u5f20\u91cf\uff0c\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\u6216\u5f20\u91cf\u5143\u7ec4</li> <li>inputs (\u5f20\u91cf\u5143\u7ec4 or Tensor) \u2013 func\u51fd\u6570\u8f93\u5165</li> <li>grad_outputs (tuple of Tensor or Tensor, \u53ef\u9009) \u2013 The gradients with respect to the function's outputs.</li> <li>eps (float, \u53ef\u9009) \u2013 \u6709\u9650\u5dee\u5206\u7684\u6270\u52a8</li> <li>atol (float, \u53ef\u9009) \u2013 \u7edd\u5bf9\u5bb9\u5dee</li> <li>rtol (float, \u53ef\u9009) \u2013 \u76f8\u5bf9\u5bb9\u5dee</li> <li>gen_non_contig_grad_outputs (bool, \u53ef\u9009) \u2013 \u5982\u679c grad_outputs \u662f None \u5e76\u4e14 gen_non_contig_grad_outputs \u662f True\uff0c\u968f\u673a\u751f\u6210\u7684\u68af\u5ea6\u8f93\u51fa\u662f\u4e0d\u8fde\u7eed\u7684</li> <li>raise_exception (bool, \u53ef\u9009) \u2013  \u6307\u793a\u5982\u679c\u68c0\u67e5\u5931\u8d25\u662f\u5426\u629b\u51fa\u4e00\u4e2a\u5f02\u5e38\u3002\u6b64\u5f02\u5e38\u7ed9\u51fa\u5173\u4e8e\u5931\u8d25\u7684\u786e\u5207\u6027\u8d28\u7684\u66f4\u591a\u4fe1\u606f\u3002\u8fd9\u5728\u68af\u5ea6\u68c0\u67e5\u8c03\u8bd5\u65f6\u662f\u6709\u7528\u7684\u3002</li> </ul> \u8fd4\u56de: \u5982\u679c\u6240\u6709\u5dee\u90fd\u6ee1\u8db3\u5168\u90e8\u95ed\u5408\u6761\u4ef6\uff0c\u5219\u4e3aTrue"},{"location":"1.0/autograd/#profiler","title":"Profiler","text":"<p>Autograd \u5305\u542b\u4e00\u4e2a\u4e8b\u4ef6\u63a2\u67e5\u5668\uff0c\u8ba9\u4f60\u6d1e\u5bdf\u5728\u4f60\u7684\u6a21\u578b\u4e2d\u4e0d\u540c\u64cd\u4f5c\u7684\u4ee3\u4ef7-CPU\u548cGPU\u4e2d\u90fd\u6709\u3002\u73b0\u5728\u6709\u4e24\u79cd\u6a21\u5f0f\u5b9e\u73b0-CPU-\u4ec5\u4f7f\u7528<code>profile</code>\uff0c\u548c\u4f7f\u7528<code>emit_nvtx</code>\u7684nvprof(\u6ce8\u518cCPU\u548cGPU\u6d3b\u52a8\uff09</p> <pre><code>class torch.autograd.profiler.profile(enabled=True, use_cuda=False)\n</code></pre> <p>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u7ba1\u7406autograd\u4e8b\u4ef6\u63a2\u67e5\u5668\u72b6\u6001\u548c\u4fdd\u6301\u4e00\u4efd\u6c47\u603b\u7ed3\u679c\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>enabled (bool, \u53ef\u9009) \u2013 \u8bbe\u7f6e\u6210False \u8ba9\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e00\u4e2a no-op. \u9ed8\u8ba4\uff1a<code>True</code>\u3002</li> <li>use_cuda (bool, \u53ef\u9009) \u2013 \u4f7f\u7528cudaEvent API\u4e5f\u53ef\u4ee5\u542f\u7528CUDA\u4e8b\u4ef6\u7684\u8ba1\u65f6\u3002 \u6bcf\u4e2a\u5f20\u91cf\u64cd\u4f5c\u589e\u52a0\u5927\u7ea64us\u7684\u5f00\u9500\u3002\u9ed8\u8ba4:  <code>False</code></li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)\n&gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:\n...     y = x ** 2\n...     y.backward()\n&gt;&gt;&gt; # \u6ce8\u610f\uff1a\u4e3a\u7b80\u6d01\u8d77\u89c1\uff0c\u5220\u9664\u4e86\u4e00\u4e9b\u5217\n... print(prof)\n-------------------------------------  ---------------  ---------------\nName                                          CPU time        CUDA time\n-------------------------------------  ---------------  ---------------\nPowConstant                                  142.036us          0.000us\nN5torch8autograd9GraphRootE                   63.524us          0.000us\nPowConstantBackward                          184.228us          0.000us\nMulConstant                                   50.288us          0.000us\nPowConstant                                   28.439us          0.000us\nMul                                           20.154us          0.000us\nN5torch8autograd14AccumulateGradE             13.790us          0.000us\nN5torch8autograd5CloneE                        4.088us          0.000us\n\n</code></pre> <pre><code>export_chrome_trace(path)\n</code></pre> <p>\u5c06EventList\u5bfc\u51fa\u4e3aChrome\u8ddf\u8e2a\u5de5\u5177\u6587\u4ef6\u3002</p> <p>\u68c0\u67e5\u70b9\u968f\u540e\u88ab\u52a0\u8f7d\u548c\u68c0\u67e5\u5728<code>chrome://tracing URL</code>\u3002</p> \u53c2\u6570: path (str) \u2013 \u5c06\u5199\u5165\u8ddf\u8e2a\u7684\u8def\u5f84\u3002 <pre><code>key_averages()\n</code></pre> <p>\u5e73\u5747\u952e\u4e0a\u7684\u6240\u6709\u51fd\u6570\u4e8b\u4ef6.</p> \u8fd4\u56de: \u4e00\u4e2a\u5305\u542bFunctionEventAvg\u5bf9\u8c61\u7684EventList\u3002 <pre><code>table(sort_by=None)\n</code></pre> <p>\u5c06EventList\u6253\u5370\u4e3a\u683c\u5f0f\u826f\u597d\u7684\u8868\u3002</p> \u53c2\u6570: sort_by (str, optional) \u2013 \u7528\u6765\u6392\u5e8f\u4e8b\u4ef6\u7684\u5c5e\u6027\u3002\u9ed8\u8ba4\u4ee5\u5b83\u4eec\u88ab\u6ce8\u518c\u65f6\u987a\u5e8f\u6253\u5370\u3002 \u5408\u6cd5\u7684\u5173\u952e\u5b57\u5305\u62ec\uff1a<code>cpu_time</code>, <code>cuda_time</code>, <code>cpu_time_total</code>, <code>cuda_time_total</code>, <code>count</code>\u3002 \u8fd4\u56de: \u4e00\u4e2a\u5305\u542b\u8868\u683c\u7684\u5b57\u7b26\u4e32\u3002 --- --- <pre><code>total_average()\n</code></pre> <p>\u5e73\u5747\u5316\u5168\u90e8\u4e8b\u4ef6\u3002</p> \u8fd4\u56de: \u4e00\u4e2aFunctionEventAvg\u4e8b\u4ef6\u3002 <pre><code>class torch.autograd.profiler.emit_nvtx(enabled=True)\n</code></pre> <p>\u8ba9\u6bcf\u4e2a\u81ea\u52a8\u6c42\u5bfc\u64cd\u4f5c\u53d1\u51fa\u5728\u4e00\u4e2aNVTX\u8303\u56f4\u5185\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <p>\u5f53\u5728nvprof\u4e0b\u8fd0\u884c\u7a0b\u5e8f\u662f\u6709\u7528\u7684\uff1a</p> <pre><code>nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;\n\n</code></pre> <p>\u4e0d\u5e78\u5730\uff0c\u6ca1\u6709\u529e\u6cd5\u5f3a\u5236nvprof\u5c06\u5b83\u6536\u96c6\u7684\u6570\u636e\u8f93\u51fa\u5230\u78c1\u76d8\uff0c\u6240\u4ee5\u5bf9\u4e8eCUDA\u5206\u6790\uff0c\u5fc5\u987b\u4f7f\u7528\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u6765\u58f0\u660envprof\u8ddf\u8e2a\u5e76\u7b49\u5f85\u8fdb\u7a0b\u5728\u68c0\u67e5\u4e4b\u524d\u9000\u51fa\u3002\u7136\u540e\uff0c\u53ef\u4f7f\u7528NVIDIA\u53ef\u89c6\u5316Profiler(nvvp)\u6765\u663e\u793a\u65f6\u95f4\u7ebf\uff0c\u6216<code>torch.autograd.profiler.load_nvprof()</code>\u53ef\u52a0\u8f7d\u7ed3\u679c\u4ee5\u4f9b\u68c0\u67e5\uff0c\u4f8b\u5982\uff1a\u5728Python REPL\u4e2d\u3002</p> \u53c2\u6570: enabled (bool, \u53ef\u9009) \u2013 \u8bbe\u7f6e\u6210False \u8ba9\u6b64\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e00\u4e2a no-op. \u9ed8\u8ba4\uff1a<code>True</code>\u3002 <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n\n</code></pre> <p>Forward-backward correlation</p> <p>\u5728Nvidia Visual Profiler\u4e2d\u67e5\u770b\u4f7f\u7528emit_nvtx\u521b\u5efa\u7684\u914d\u7f6e\u6587\u4ef6\u65f6\uff0c\u5c06\u6bcf\u4e2a\u53cd\u5411\u4f20\u9012\u64cd\u4f5c\u4e0e\u76f8\u5e94\u7684\u524d\u5411\u4f20\u9012\u64cd\u4f5c\u76f8\u5173\u8054\u53ef\u80fd\u5f88\u56f0\u96be\u3002 \u4e3a\u4e86\u7b80\u5316\u6b64\u4efb\u52a1\uff0cemit_nvtx\u5c06\u5e8f\u5217\u53f7\u4fe1\u606f\u9644\u52a0\u5230\u5b83\u751f\u6210\u7684\u8303\u56f4\u3002</p> <p>\u5728\u524d\u5411\u4f20\u9012\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u51fd\u6570\u8303\u56f4\u90fd\u7528<code>seq=&amp;lt;N&amp;gt;</code>\u8fdb\u884c\u4fee\u9970\u3002 <code>seq</code>\u662f\u4e00\u4e2a\u8fd0\u884c\u8ba1\u6570\u5668\uff0c\u6bcf\u6b21\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u53cd\u5411Function\u5bf9\u8c61\u65f6\u4f1a\u9012\u589e\uff0c\u5e76\u5bf9\u524d\u5411\u4e0d\u53ef\u89c1\u3002 \u56e0\u6b64\uff0c\u4e0e\u6bcf\u4e2a\u524d\u5411\u51fd\u6570\u8303\u56f4\u76f8\u5173\u8054\u7684<code>seq=&amp;lt;N&amp;gt;</code>\u6ce8\u91ca\u544a\u8bc9\u4f60\uff0c\u5982\u679c\u6b64\u524d\u5411\u51fd\u6570\u521b\u5efa\u4e86\u53cd\u5411\u7684Function\u5bf9\u8c61\uff0c\u5219\u53cd\u5411\u5bf9\u8c61\u5c06\u6536\u5230\u5e8f\u5217\u53f7N\u3002\u5728\u53cd\u5411\u4f20\u9012\u8fc7\u7a0b\uff0c\u9876\u5c42\u8303\u56f4\u5305\u88c5\u6bcf\u4e2aC++\u53cd\u5411\u51fd\u6570\u7684<code>apply()</code>\u8c03\u7528\u90fd\u7528\u4e0d\u53ef\u89c1\u7684<code>stashed seq=&amp;lt;M&amp;gt;</code>\u8fdb\u884c\u4fee\u9970\u3002 M\u662f\u521b\u5efa\u53cd\u5411\u5bf9\u8c61\u7684\u5e8f\u5217\u53f7\u3002 \u901a\u8fc7\u6bd4\u8f83\u5728\u53cd\u5411\u4e0d\u53ef\u89c1\u7684\u5e8f\u5217\u53f7\u548c\u5728\u524d\u5411\u7684\u5e8f\u5217\u53f7\uff0c\u4f60\u53ef\u4ee5\u8ddf\u8e2a\u54ea\u4e2a\u524d\u5411\u64cd\u4f5c\u521b\u5efa\u4e86\u6bcf\u4e2a\u53cd\u5411\u51fd\u6570\u3002</p> <p>\u5728\u53cd\u5411\u4f20\u9012\u671f\u95f4\u6267\u884c\u7684\u4efb\u4f55\u51fd\u6570\u4e5f\u7528<code>seq=&amp;lt;N&amp;gt;</code>\u8fdb\u884c\u4fee\u9970\u3002 \u5728\u9ed8\u8ba4\u53cd\u5411(\u4f7f\u7528<code>create_graph=False</code>\uff09\u65f6\uff0c\u6b64\u4fe1\u606f\u65e0\u5173\u7d27\u8981\uff0c\u4e8b\u5b9e\u4e0a\uff0c\u5bf9\u4e8e\u6240\u6709\u6b64\u7c7b\u51fd\u6570\uff0c<code>N</code>\u53ef\u80fd\u53ea\u662f0\u3002 \u4f5c\u4e3a\u5c06\u8fd9\u4e9bFunction\u5bf9\u8c61\u4e0e\u65e9\u671f\u7684\u5411\u524d\u4f20\u9012\u76f8\u5173\u8054\u7684\u65b9\u6cd5\uff0c\u53ea\u6709\u4e0e\u53cd\u5411Function\u5bf9\u8c61\u7684<code>apply()</code>\u65b9\u6cd5\u5173\u8054\u7684\u9876\u7ea7\u8303\u56f4\u624d\u6709\u7528\u3002</p> <p>Double-backward</p> <p>\u53e6\u4e00\u65b9\u9762\uff0c\u5982\u679c\u6b63\u5728\u8fdb\u884c<code>create_graph=True</code>\u7684\u53cd\u5411\u4f20\u9012(\u6362\u53e5\u8bdd\u8bf4\uff0c\u5982\u679c\u4f60\u8bbe\u7f6e\u4e3adouble-backward\uff09\uff0c\u5219\u5728\u53cd\u5411\u671f\u95f4\u6267\u884c\u6bcf\u4e2a\u88ab\u7ed9\u4e00\u4e2a\u975e\u96f6\uff0c\u6709\u7528\u7684<code>seq=&amp;lt;N&amp;gt;</code>\u7684\u51fd\u6570\u3002 \u8fd9\u4e9b\u51fd\u6570\u672c\u8eab\u53ef\u4ee5\u7a0d\u540e\u5728double-backward\u671f\u95f4\u521b\u5efaFunction\u5bf9\u8c61\u6765\u6267\u884c\uff0c\u5c31\u50cf\u5728\u524d\u5411\u4f20\u9012\u4e2d\u539f\u59cb\u51fd\u6570\u6240\u505a\u7684\u4e00\u6837\u3002 \u53cd\u5411\u548cdouble-backward\u7684\u5173\u7cfb\u5728\u6982\u5ff5\u4e0a\u4e0e\u524d\u5411\u548c\u53cd\u5411\u7684\u5173\u7cfb\u76f8\u540c\uff1a\u51fd\u6570\u4ecd\u7136\u53d1\u51fa\u5f53\u524d\u5e8f\u5217\u53f7\u6807\u8bb0\u7684\u8303\u56f4\uff0c\u5b83\u4eec\u521b\u5efa\u7684Function\u5bf9\u8c61\u4ecd\u7136\u5b58\u50a8\u90a3\u4e9b\u5e8f\u5217\u53f7\uff0c\u5e76\u4e14\u5728\u6700\u7ec8\u7684double-backward\u671f\u95f4 \u5411\u540e\uff0cFunction\u5bf9\u8c61\u7684<code>apply()</code>\u8303\u56f4\u4ecd\u7136\u7528 <code>stashed seq</code>\u6570\u5b57\u6807\u8bb0\uff0c\u53ef\u4ee5\u4e0e\u53cd\u5411\u4f20\u9012\u4e2d\u7684<code>seq</code>\u6570\u5b57\u8fdb\u884c\u6bd4\u8f83\u3002</p> <pre><code>torch.autograd.profiler.load_nvprof(path)\n</code></pre> <p>\u6253\u5f00\u4e00\u4e2anvprof\u8ddf\u8e2a\u6587\u4ef6\u5e76\u4e14\u89e3\u6790autograd\u6ce8\u91ca\u3002</p> \u53c2\u6570: path (str) \u2013 nvprof\u8ddf\u8e2a\u8def\u5f84"},{"location":"1.0/autograd/#_5","title":"\u5f02\u5e38\u68c0\u6d4b","text":"<pre><code>class torch.autograd.detect_anomaly\n</code></pre> <p>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u4e3a\u81ea\u52a8\u6c42\u5bfc\u5f15\u64ce\u4f7f\u80fd\u5f02\u5e38\u68c0\u6d4b\u3002</p> <p>\u8fd9\u505a\u4e86\u4e24\u4ef6\u4e8b\uff1a- \u5728\u542f\u7528\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\u524d\u5411\u4f20\u9012\u5c06\u5141\u8bb8\u53cd\u5411\u4f20\u9012\u6253\u5370\u521b\u5efa\u5931\u8d25\u7684\u53cd\u5411\u51fd\u6570\u7684\u524d\u5411\u64cd\u4f5c\u8ddf\u8e2a\u3002 - \u4efb\u4f55\u751f\u6210\u201cnan\u201d\u503c\u7684\u53cd\u5411\u8ba1\u7b97\u90fd\u4f1a\u5f15\u53d1\u9519\u8bef\u3002</p> <p>\u793a\u4f8b</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import autograd\n&gt;&gt;&gt; class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n&gt;&gt;&gt; def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n&gt;&gt;&gt; inp = torch.rand(10, 10, requires_grad=True)\n&gt;&gt;&gt; out = run_fn(inp)\n&gt;&gt;&gt; out.backward()\n Traceback (most recent call last):\n File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n allow_unreachable=True)  # allow_unreachable flag\n File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n return self._forward_cls.backward(self, *args)\n File \"&lt;stdin&gt;\", line 8, in backward\n RuntimeError: Some error in backward\n&gt;&gt;&gt; with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n Traceback of forward call that caused the error:\n File \"tmp.py\", line 53, in &lt;module&gt;\n out = run_fn(inp)\n File \"tmp.py\", line 44, in run_fn\n out = MyFunc.apply(a)\n Traceback (most recent call last):\n File \"&lt;stdin&gt;\", line 4, in &lt;module&gt;\n File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n torch.autograd.backward(self, gradient, retain_graph, create_graph)\n File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n allow_unreachable=True)  # allow_unreachable flag\n File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n return self._forward_cls.backward(self, *args)\n File \"&lt;stdin&gt;\", line 8, in backward\n RuntimeError: Some error in backward\n\n</code></pre> <pre><code>class torch.autograd.set_detect_anomaly(mode)\n</code></pre> <p>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u4e3a\u81ea\u52a8\u6c42\u5bfc\u5f15\u64ce\u8bbe\u7f6e\u5f02\u5e38\u68c0\u6d4b\u5f00\u6216\u5173\u3002</p> <p><code>set_detect_anomaly</code>\u5c06\u57fa\u4e8e\u5b83\u7684\u53c2\u6570<code>mode</code>\u4f7f\u80fd\u6216\u7981\u7528\u81ea\u52a8\u6c42\u5bfc\u5f02\u5e38\u68c0\u6d4b\u3002\u5b83\u4e5f\u80fd\u4f5c\u4e3a\u4e00\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u6216\u51fd\u6570\u4f7f\u7528\u3002</p> <p>\u5f02\u5e38\u68c0\u6d4b\u884c\u4e3a\u7ec6\u8282\u89c1\u4e0a\u9762<code>detect_anomaly</code>\u3002</p> \u53c2\u6570: mode (bool) \u2013 \u6807\u8bb0\u662f\u5426\u4f7f\u80fd\u5f02\u5e38\u68c0\u6d4b(<code>True</code>\uff09\uff0c\u6216\u7981\u7528(<code>False</code>\uff09\u3002"},{"location":"1.0/aws_distributed_training_tutorial/","title":"PyTorch 1.0 \u4f7f\u7528 Amazon AWS \u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u8bd1\u8005\uff1ayportne13</p> <p>\u4f5c\u8005: Nathan Inkawhich</p> <p>\u7f16\u8f91: Teng Li</p> <p>\u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d\u6211\u4eec\u4f1a\u5c55\u793a\u5982\u4f55\u4f7f\u7528 Amazon AWS \u7684\u4e24\u4e2a\u591a\u8defGPU\u8282\u70b9\u6765\u8bbe\u7f6e\uff0c\u7f16\u5199\u548c\u8fd0\u884c PyTorch 1.0 \u5206\u5e03\u5f0f\u8bad\u7ec3\u7a0b\u5e8f\u3002\u9996\u5148\u6211\u4eec\u4f1a\u4ecb\u7ecd AWS \u8bbe\u7f6e, \u7136\u540e\u662f PyTorch \u73af\u5883\u914d\u7f6e, \u6700\u540e\u662f\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4ee3\u7801\u3002\u4f60\u4f1a\u53d1\u73b0\u60f3\u6539\u6210\u5206\u5e03\u5f0f\u5e94\u7528\u4f60\u53ea\u9700\u8981\u5bf9\u4f60\u76ee\u524d\u5199\u7684\u8bad\u7ec3\u7a0b\u5e8f\u505a\u5f88\u5c11\u7684\u4ee3\u7801\u6539\u52a8, \u7edd\u5927\u591a\u6570\u5de5\u4f5c\u90fd\u53ea\u662f\u4e00\u6b21\u6027\u7684\u73af\u5883\u914d\u7f6e\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#amazon-aws","title":"Amazon AWS \u8bbe\u7f6e","text":"<p>\u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d\u6211\u4eec\u4f1a\u5728\u4e24\u4e2a\u591a\u8def GPU \u8282\u70b9\u4e0a\u8fd0\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u5728\u8fd9\u4e00\u8282\u4e2d\u6211\u4eec\u9996\u5148\u4f1a\u5c55\u793a\u5982\u4f55\u521b\u5efa\u8282\u70b9\uff0c\u7136\u540e\u662f\u8bbe\u7f6e\u5b89\u5168\u7ec4(security group)\u6765\u8ba9\u8282\u70b9\u4e4b\u95f4\u80fd\u591f\u901a\u4fe1\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#_1","title":"\u521b\u5efa\u8282\u70b9","text":"<p>\u5728 Amazon AWS \u4e0a\u521b\u5efa\u4e00\u4e2a\u5b9e\u4f8b\u9700\u8981\u4e03\u4e2a\u6b65\u9aa4\u3002\u9996\u5148\uff0c\u767b\u5f55\u5e76\u9009\u62e9 Launch Instance.</p> <p>1: \u9009\u62e9 Amazon Machine Image (AMI) - \u6211\u4eec\u9009\u62e9 <code>Deep Learning AMI (Ubuntu) Version 14.0</code>\u3002 \u6b63\u5982\u4ecb\u7ecd\u4e2d\u6240\u8bf4\u7684\uff0c\u8fd9\u4e2a\u5b9e\u4f8b\u5b89\u88c5\u4e86\u8bb8\u591a\u6d41\u884c\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5e76\u5df2\u7ecf\u914d\u7f6e\u597d\u4e86 CUDA, cuDNN \u548c NCCL\u3002 \u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb\u3002</p> <p>2: \u9009\u62e9\u4e00\u4e2a\u5b9e\u4f8b\u7c7b\u578b - \u9009\u62e9GPU\u8ba1\u7b97\u5355\u5143 <code>p2.8xlarge</code>\u3002 \u6ce8\u610f\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u4ef7\u683c\u4e0d\u540c\uff0c\u8fd9\u4e2a\u5b9e\u4f8b\u4e3a\u6bcf\u4e2a\u8282\u70b9\u63d0\u4f9b 8 \u4e2a NVIDIA Tesla K80 GPU\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u9002\u5408\u591a\u8def GPU \u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u67b6\u6784\u3002</p> <p>3: \u8bbe\u7f6e\u5b9e\u4f8b\u7684\u7ec6\u8282 - \u552f\u4e00\u9700\u8981\u8bbe\u7f6e\u7684\u5c31\u662f\u628a Number of instances \u52a0\u5230 2\u3002\u5176\u4ed6\u7684\u90fd\u53ef\u4ee5\u4fdd\u7559\u9ed8\u8ba4\u8bbe\u7f6e\u3002</p> <p>4: \u589e\u52a0\u5b58\u50a8\u7a7a\u95f4 - \u6ce8\u610f, \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd9\u4e9b\u8282\u70b9\u5e76\u6ca1\u6709\u5f88\u5927\u7684\u5b58\u50a8\u7a7a\u95f4 (\u53ea\u6709 75 GB)\u3002\u5bf9\u4e8e\u8fd9\u4e2a\u6559\u7a0b, \u6211\u4eec\u53ea\u4f7f\u7528 STL-10 \u6570\u636e\u96c6, \u5b58\u50a8\u7a7a\u95f4\u662f\u5b8c\u5168\u591f\u7528\u7684\u3002\u4f46\u5982\u679c\u4f60\u60f3\u8981\u8bad\u7ec3\u4e00\u4e2a\u5927\u7684\u6570\u636e\u96c6\u6bd4\u5982 ImageNet , \u4f60\u9700\u8981\u6839\u636e\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6a21\u578b\u53bb\u589e\u52a0\u5b58\u50a8\u7a7a\u95f4\u3002</p> <p>5: \u52a0 Tags - \u8fd9\u4e00\u6b65\u6ca1\u6709\u4ec0\u4e48\u9700\u8981\u8bbe\u7f6e\u7684\uff0c\u76f4\u63a5\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002</p> <p>6: \u8bbe\u7f6e\u5b89\u5168\u7ec4(Security Group) - \u8fd9\u4e00\u6b65\u5f88\u91cd\u8981\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u540c\u4e00\u5b89\u5168\u7ec4\u7684\u4e24\u4e2a\u8282\u70b9\u65e0\u6cd5\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u901a\u4fe1\u3002 \u8fd9\u91cc\u6211\u4eec\u60f3\u8981\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u7ec4\u5e76\u5c06\u4e24\u4e2a\u8282\u70b9\u52a0\u5165\u7ec4\u5185\u3002 \u4f46\u662f\u6211\u4eec\u6ca1\u6cd5\u5728\u8fd9\u4e00\u6b65\u5b8c\u6210\u8fd9\u4e00\u8bbe\u7f6e\u3002\u8bb0\u4f4f\u4f60\u8bbe\u7f6e\u7684\u65b0\u7684\u5b89\u5168\u7ec4\u540d(\u4f8b\u5982 launch-wizard-12)\u7136\u540e\u8fdb\u5165\u4e0b\u4e00\u6b65\u6b65\u9aa47\u3002</p> <p>7: \u786e\u8ba4\u5b9e\u4f8b\u542f\u52a8 - \u63a5\u4e0b\u6765\uff0c\u786e\u8ba4\u4f8b\u7a0b\u5e76\u542f\u52a8\u5b83\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd9\u4f1a\u81ea\u52a8\u5f00\u59cb\u4e24\u4e2a\u5b9e\u4f8b\u7684\u521d\u59cb\u5316\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u9762\u677f\u76d1\u89c6\u521d\u59cb\u5316\u7684\u8fdb\u7a0b\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#_2","title":"\u8bbe\u7f6e\u5b89\u5168\u7ec4","text":"<p>\u6211\u4eec\u521a\u624d\u5728\u521b\u5efa\u5b9e\u4f8b\u7684\u65f6\u5019\u6ca1\u529e\u6cd5\u6b63\u786e\u8bbe\u7f6e\u5b89\u5168\u7ec4\u3002\u5f53\u4f60\u542f\u52a8\u597d\u5b9e\u4f8b\u540e\uff0c\u5728 EC2 \u7684\u63a7\u5236\u9762\u677f\u9009\u62e9 Network &amp; Security &gt; Security Groups \u9009\u9879\u3002 \u8fd9\u5c06\u663e\u793a\u4f60\u6709\u6743\u9650\u8bbf\u95ee\u7684\u5b89\u5168\u7ec4\u5217\u8868\u3002 \u9009\u62e9\u4f60\u5728\u7b2c\u516d\u6b65\u521b\u5efa\u7684\u65b0\u7684\u5b89\u5168\u7ec4(\u4e5f\u5c31\u662f launch-wizard-12), \u4f1a\u5f39\u51fa\u9009\u9879 Description, Inbound, Outbound, and Tags\u3002 \u9996\u5148\uff0c\u9009\u62e9 Inbound \u7684 Edit \u9009\u9879\u6dfb\u52a0\u89c4\u5219\u4ee5\u5141\u8bb8\u6765\u81ea launch-wizard-12 \u5b89\u5168\u7ec4\u5185\u6e90(\u201cSources\u201d)\u7684\u6240\u6709\u6d41\u91cf(\u201cAll Traffic\u201d)\u3002 \u7136\u540e\u9009\u62e9 Outbound \u9009\u9879\u5e76\u505a\u540c\u6837\u7684\u5de5\u4f5c\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u6709\u6548\u5730\u5141\u8bb8\u4e86 launch-wizard-12 \u5b89\u5168\u7ec4\u6240\u6709\u7c7b\u578b\u7684\u5165\u7ad9\u548c\u51fa\u7ad9\u6d41\u91cf(Inbound and Outbound traffic)\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#_3","title":"\u5fc5\u8981\u7684\u4fe1\u606f","text":"<p>\u7ee7\u7eed\u4e0b\u4e00\u6b65\u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u627e\u5230\u5e76\u8bb0\u4f4f\u8282\u70b9\u7684IP\u5730\u5740\u3002 \u5728 EC2 \u7684\u63a7\u5236\u9762\u677f\u627e\u5230\u4f60\u6b63\u5728\u8fd0\u884c\u7684\u5b9e\u4f8b\u3002 \u8bb0\u4e0b\u5b9e\u4f8b\u7684 IPv4 Public IP \u548c Private IPs\u3002 \u5728\u4e4b\u540e\u7684\u6587\u6863\u4e2d\u6211\u4eec\u4f1a\u628a\u8fd9\u4e9b\u79f0\u4e3a node0-publicIP, node0-privateIP, node1-publicIP \u548c node1-privateIP\u3002 \u5176\u4e2d public IP \u5730\u5740\u7528\u6765 SSH \u767b\u5f55,  private IP \u7528\u6765\u8282\u70b9\u95f4\u901a\u4fe1\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#_4","title":"\u73af\u5883\u914d\u7f6e","text":"<p>\u4e0b\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\u662f\u8bbe\u7f6e\u5404\u4e2a\u8282\u70b9\u3002 \u4e0d\u5e78\u7684\u662f\uff0c\u6211\u4eec\u4e0d\u80fd\u540c\u65f6\u8bbe\u7f6e\u4e24\u4e2a\u8282\u70b9, \u6240\u4ee5\u8fd9\u4e00\u6b65\u5fc5\u987b\u6bcf\u4e2a\u8282\u70b9\u5206\u522b\u505a\u4e00\u904d\u3002 \u7136\u800c\uff0c\u8fd9\u662f\u4e00\u6b21\u6027\u7684\u8bbe\u7f6e\uff0c\u4e00\u65e6\u4f60\u7684\u8282\u70b9\u8bbe\u7f6e\u6b63\u5e38\u4f60\u5c31\u4e0d\u9700\u8981\u518d\u4e3a\u4f60\u672a\u6765\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u9879\u76ee\u91cd\u65b0\u8bbe\u7f6e\u4e86\u3002</p> <p>\u7b2c\u4e00\u6b65\uff0c\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u4e00\u4e2a\u5e26 python 3.6 \u548c numpy \u7684 conda \u73af\u5883\u3002 \u521b\u5efa\u5b8c\u6210\u540e\u6fc0\u6d3b\u73af\u5883\u3002</p> <pre><code>$ conda create -n nightly_pt python=3.6 numpy\n$ source activate nightly_pt\n\n</code></pre> <p>\u4e0b\u4e00\u6b65\uff0c\u6211\u4eec\u4f7f\u7528 pip \u5728 conda \u73af\u5883\u4e2d\u6bcf\u65e5\u7f16\u8bd1 (nightly build) \u652f\u6301 Cuda 9.0 \u7684 PyTorch \u3002</p> <pre><code>$ pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html\n\n</code></pre> <p>\u6211\u4eec\u8fd8\u9700\u8981\u5b89\u88c5 torchvision \u6765\u4f7f\u7528 torchvision \u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002\u8fd9\u6b21\u6211\u4eec\u9700\u8981\u4ece\u6e90\u4ee3\u7801\u6784\u5efa torchvision \u56e0\u4e3a\u4f7f\u7528 pip \u5b89\u88c5\u4f1a\u9ed8\u8ba4\u5b89\u88c5\u8001\u7248\u672c\u7684 PyTorch \u3002</p> <pre><code>$ cd\n$ git clone https://github.com/pytorch/vision.git\n$ cd vision\n$ python setup.py install\n\n</code></pre> <p>\u6700\u540e, \u4e00\u6b65\u5f88\u91cd\u8981\u7684\u6b65\u9aa4\u662f\u4e3a NCCL \u8bbe\u7f6e\u7f51\u7edc\u63a5\u53e3\u540d\u3002\u8fd9\u6b65\u901a\u8fc7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf <code>NCCL_SOCKET_IFNAME</code> \u6765\u5b9e\u73b0\u3002 \u4e3a\u4e86\u83b7\u5f97\u6b63\u786e\u7684\u540d\u5b57\uff0c\u5728\u8282\u70b9\u4e0a\u6267\u884c <code>ifconfig</code> \u547d\u4ee4\u5e76\u770b\u548c\u8282\u70b9\u5bf9\u5e94\u7684 privateIP (\u4f8b\u5982 ens3)\u63a5\u53e3\u540d\u5b57\u3002 \u7136\u540e\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u5982\u4e0b</p> <pre><code>$ export NCCL_SOCKET_IFNAME=ens3\n\n</code></pre> <p>\u8bb0\u4f4f\uff0c\u5bf9\u6240\u6709\u8282\u70b9\u90fd\u6267\u884c\u8fd9\u4e2a\u64cd\u4f5c\u3002 \u4f60\u4e5f\u8bb8\u8fd8\u9700\u8981\u8003\u8651\u5bf9 .bashrc \u6dfb\u52a0 NCCL_SOCKET_IFNAME\u3002 \u6ce8\u610f\u5230\u6211\u4eec\u6ca1\u6709\u5728\u8282\u70b9\u95f4\u8bbe\u7f6e\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u3002 \u56e0\u6b64\uff0c\u6bcf\u4e2a\u8282\u70b9\u90fd\u9700\u8981\u590d\u5236\u4e00\u4efd\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002 \u60f3\u8981\u4e86\u89e3\u66f4\u591a\u6709\u5173\u8bbe\u7f6e\u8282\u70b9\u95f4\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u53c2\u8003\u8fd9\u91cc.</p>"},{"location":"1.0/aws_distributed_training_tutorial/#_5","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3\u4ee3\u7801","text":"<p>\u5b9e\u4f8b\u5f00\u59cb\u8fd0\u884c\uff0c\u73af\u5883\u914d\u7f6e\u597d\u4e86\u4ee5\u540e\u6211\u4eec\u53ef\u4ee5\u5f00\u59cb\u51c6\u5907\u8bad\u7ec3\u4ee3\u7801\u4e86\u3002\u7edd\u5927\u591a\u6570\u4ee3\u7801\u662f\u4ece PyTorch ImageNet Example \u6765\u7684\uff0c\u8fd9\u4e9b\u4ee3\u7801\u540c\u6837\u652f\u6301\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u4ee5\u8fd9\u4e2a\u4ee3\u7801\u4e3a\u57fa\u7840\u4f60\u53ef\u4ee5\u642d\u81ea\u5df1\u7684\u8bad\u7ec3\u4ee3\u7801\u56e0\u4e3a\u5b83\u6709\u6807\u51c6\u7684\u8bad\u7ec3\u5faa\u73af\uff0c\u9a8c\u8bc1\u5faa\u73af\u548c\u51c6\u786e\u7387\u8ffd\u8e2a\u51fd\u6570\u3002\u7136\u800c\uff0c\u4f60\u4f1a\u6ce8\u610f\u5230\u4e3a\u4e86\u7b80\u6d01\u8d77\u89c1\u53c2\u6570\u89e3\u6790\u548c\u5176\u4ed6\u975e\u5fc5\u987b\u7684\u51fd\u6570\u88ab\u53bb\u6389\u4e86\u3002</p> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\u6211\u4eec\u4f1a\u4f7f\u7528 torchvision.models.resnet18 \u6a21\u578b\u5e76\u5c06\u4f1a\u5728 torchvision.datasets.STL10 \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5b83\u3002 \u4e3a\u4e86\u89e3\u51b3 STL-10 \u548c Resnet18 \u7ef4\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898, \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a\u53d8\u6362\u628a\u56fe\u7247\u7684\u5c3a\u5bf8\u6539\u4e3a 224x224\u3002 \u6ce8\u610f\u5230\uff0c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u4ee3\u7801\uff0c\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u9009\u62e9\u662f\u6b63\u4ea4(orthogonal)\u7684, \u4f60\u53ef\u4ee5\u9009\u62e9\u4efb\u4f55\u4f60\u60f3\u7528\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u64cd\u4f5c\u7684\u6b65\u9aa4\u662f\u4e00\u6837\u7684\u3002 \u8ba9\u6211\u4eec\u9996\u5148\u64cd\u4f5c import \u548c\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\u3002\u7136\u540e\u6211\u4eec\u4f1a\u5b9a\u4e49 train \u548c test \u51fd\u6570\uff0c\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u4ece ImageNet Example \u4f8b\u7a0b\u4e2d\u5927\u91cf\u590d\u5236\u51fa\u6765\u3002 \u7ed3\u5c3e\u90e8\u5206\uff0c\u6211\u4eec\u4f1a\u642d\u5efa\u4ee3\u7801\u7684 main \u90e8\u5206\u6765\u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002 \u6700\u540e\u6211\u4eec\u4f1a\u8ba8\u8bba\u5982\u4f55\u8ba9\u4ee3\u7801\u8fd0\u884c\u8d77\u6765\u3002</p>"},{"location":"1.0/aws_distributed_training_tutorial/#imports","title":"Imports","text":"<p>\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7279\u522b\u9700\u8981 import \u7684\u4e1c\u897f\u662f torch.nn.parallel, torch.distributed, torch.utils.data.distributed \u548c torch.multiprocessing\u3002\u540c\u65f6\u9700\u8981\u628a\u591a\u8fdb\u7a0b\u7684 start \u65b9\u6cd5(multiprocessing start method) \u8bbe\u7f6e\u4e3a spawn \u6216 forkserver (\u4ec5\u652f\u6301 Python 3), \u56e0\u4e3a\u9ed8\u8ba4\u662f fork \u4f1a\u5bfc\u81f4\u4f7f\u7528\u591a\u8fdb\u7a0b\u52a0\u8f7d\u6570\u636e\u65f6\u9501\u6b7b\u3002</p> <pre><code>import time\nimport sys\nimport torch\n\nif __name__ == '__main__':\n    torch.multiprocessing.set_start_method('spawn')\n\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.distributed as dist\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom torch.multiprocessing import Pool, Process\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_6","title":"\u8f85\u52a9\u51fd\u6570","text":"<p>\u6211\u4eec\u8fd8\u9700\u8981\u5b9a\u4e49\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\u548c\u7c7b\u6765\u4f7f\u8bad\u7ec3\u66f4\u7b80\u5355\u3002 <code>AverageMeter</code> \u7c7b\u8ffd\u8e2a\u8bad\u7ec3\u7684\u72b6\u6001\u6bd4\u5982\u51c6\u786e\u7387\u548c\u5faa\u73af\u6b21\u6570\u3002<code>accuracy</code> \u51fd\u6570\u8ba1\u7b97\u5e76\u8fd4\u8fd8\u6a21\u578b\u7684 top-k \u51c6\u786e\u7387\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u8ddf\u8e2a\u5b66\u4e60\u8fdb\u7a0b\u3002 \u8fd9\u4e24\u4e2a\u90fd\u662f\u4e3a\u4e86\u8bad\u7ec3\u65b9\u4fbf\u800c\u4e0d\u662f\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7279\u522b\u8bbe\u5b9a\u7684\u3002</p> <pre><code>class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_7","title":"\u8bad\u7ec3\u51fd\u6570","text":"<p>\u4e3a\u4e86\u7b80\u5316 main \u5faa\u73af\uff0c\u6700\u597d\u628a\u4e00\u6b65 training epoch \u653e\u8fdb <code>train</code> \u51fd\u6570\u4e2d\u3002 \u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u8bad\u7ec3\u4e00\u4e2a epoch \u7684 train_loader \u7684\u8f93\u5165\u6a21\u578b\u3002 \u552f\u4e00\u9700\u8981\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7279\u522b\u8c03\u6574\u7684\u662f\u5728\u524d\u5411\u4f20\u64ad\u524d\u5c06\u6570\u636e\u548c\u6807\u7b7e\u5f20\u91cf\u7684 non_blocking \u5c5e\u6027\u8bbe\u7f6e\u4e3a <code>True</code>\u3002 \u8fd9\u5141\u8bb8\u5f02\u6b65 GPU \u590d\u5236\u6570\u636e\u4e5f\u5c31\u662f\u8bf4\u8ba1\u7b97\u548c\u6570\u636e\u4f20\u8f93\u53ef\u4ee5\u540c\u65f6\u8fdb\u884c\u3002 \u8fd9\u4e2a\u51fd\u6570\u540c\u65f6\u4e5f\u8f93\u51fa\u8bad\u7ec3\u72b6\u6001\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5728\u6574\u4e2a epoch \u4e2d\u8ddf\u8e2a\u8fdb\u5c55\u3002</p> <p>\u53e6\u4e00\u4e2a\u9700\u8981\u5b9a\u4e49\u7684\u51fd\u6570\u662f <code>adjust_learning_rate</code>, \u8fd9\u4e2a\u51fd\u6570\u4ee5\u4e00\u4e2a\u56fa\u5b9a\u7684\u65b9\u5f0f\u8c03\u4f4e\u5b66\u4e60\u7387\u3002\u8fd9\u4e5f\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u8bad\u7ec3\u51fd\u6570\uff0c\u6709\u52a9\u4e8e\u8bad\u7ec3\u51c6\u786e\u7684\u6a21\u578b\u3002</p> <pre><code>def train(train_loader, model, criterion, optimizer, epoch):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode \u8f6c\u5230\u8bad\u7ec3\u6a21\u5f0f\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n\n        # measure data loading time \u8ba1\u7b97\u52a0\u8f7d\u6570\u636e\u7684\u65f6\u95f4\n        data_time.update(time.time() - end)\n\n        # Create non_blocking tensors for distributed training \u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u521b\u5efa non_blocking \u5f20\u91cf\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output \u8ba1\u7b97\u8f93\u51fa\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss \u8ba1\u7b97\u51c6\u786e\u7387\u5e76\u8bb0\u5f55 loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradients in a backward pass \u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u8ba1\u7b97\u68af\u5ea6\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Call step of optimizer to update model params \u8c03\u7528\u4e00\u4e2a optimizer \u6b65\u9aa4\u6765\u66f4\u65b0\u6a21\u578b\u53c2\u6570\n        optimizer.step()\n\n        # measure elapsed time \u8ba1\u7b97\u82b1\u8d39\u7684\u65f6\u95f4\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 10 == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n\ndef adjust_learning_rate(initial_lr, optimizer, epoch):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    lr = initial_lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_8","title":"\u9a8c\u8bc1\u51fd\u6570","text":"<p>\u4e3a\u4e86\u8fdb\u4e00\u6b65\u7b80\u5316 main \u5faa\u73af\u548c\u8ffd\u8e2a\u8fdb\u7a0b\u6211\u4eec\u53ef\u4ee5\u628a\u9a8c\u8bc1\u8fc7\u7a0b\u653e\u8fdb\u547d\u540d\u4e3a <code>validate</code> \u7684\u51fd\u6570\u4e2d\u3002 \u8fd9\u4e2a\u51fd\u6570\u5bf9\u8f93\u5165\u7684\u9a8c\u8bc1\u96c6\u6570\u636e\u5728\u8f93\u5165\u6a21\u578b\u4e0a\u6267\u884c\u4e00\u4e2a\u5b8c\u6574\u7684\u9a8c\u8bc1\u6b65\u9aa4\u5e76\u8fd4\u8fd8\u9a8c\u8bc1\u96c6\u5bf9\u8be5\u6a21\u578b\u7684 top-1 \u51c6\u786e\u7387\u3002 \u548c\u521a\u624d\u4e00\u6837\uff0c\u4f60\u4f1a\u6ce8\u610f\u5230\u8fd9\u91cc\u552f\u4e00\u9700\u8981\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7279\u522b\u8bbe\u7f6e\u7684\u7279\u6027\u4f9d\u7136\u662f\u5728\u4f20\u9012\u8fdb\u6a21\u578b\u524d\u5c06\u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\u503c\u8bbe\u5b9a <code>non_blocking=True</code>\u3002</p> <pre><code>def validate(val_loader, model, criterion):\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode \u8f6c\u5230\u9a8c\u8bc1\u6a21\u5f0f\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (input, target) in enumerate(val_loader):\n\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n\n            # compute output \u8ba1\u7b97\u8f93\u51fa\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss \u8ba1\u7b97\u51c6\u786e\u7387\u5e76\u8bb0\u5f55 loss\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n\n            # measure elapsed time \u8ba1\u7b97\u82b1\u8d39\u65f6\u95f4\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 100 == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses,\n                       top1=top1, top5=top5))\n\n        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n              .format(top1=top1, top5=top5))\n\n    return top1.avg\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_9","title":"\u8f93\u5165","text":"<p>\u968f\u7740\u8f85\u52a9\u51fd\u6570\u7684\u51fa\u73b0\uff0c\u6211\u4eec\u8fdb\u5165\u4e86\u6709\u8da3\u7684\u90e8\u5206\u3002\u8fd9\u91cc\u6211\u4eec\u5c06\u4f1a\u5b9a\u4e49\u7a0b\u5e8f\u7684\u8f93\u5165\u90e8\u5206\u3002\u4e00\u4e9b\u8f93\u5165\u7684\u53c2\u6570\u662f\u6807\u51c6\u7684\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u5165\u6bd4\u5982 batch size \u548c\u8bad\u7ec3\u7684 epoch \u6570, \u800c\u6709\u4e9b\u5219\u662f\u6211\u4eec\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4efb\u52a1\u7279\u522b\u9700\u8981\u7684\u3002\u9700\u8981\u7684\u8f93\u5165\u53c2\u6570\u662f\uff1a</p> <ul> <li>batch_size - \u5206\u5e03\u5f0f\u8bad\u7ec3\u7ec4\u4e2d_\u5355\u4e00_\u8fdb\u7a0b\u7684 batch size\u3002 \u6574\u4e2a\u5206\u5e03\u5f0f\u6a21\u578b\u603b\u7684 batch size \u662f batch_size*world_size</li> <li>workers - \u6bcf\u4e2a\u8fdb\u7a0b\u4e2d\u6570\u636e\u52a0\u8f7d\u4f7f\u7528\u7684\u5de5\u4f5c\u8fdb\u7a0b\u6570</li> <li>num_epochs - \u603b\u7684\u8bad\u7ec3\u7684 epoch \u6570</li> <li>starting_lr - \u5f00\u59cb\u8bad\u7ec3\u65f6\u7684\u5b66\u4e60\u7387</li> <li>world_size - \u5206\u5e03\u5f0f\u8bad\u7ec3\u73af\u5883\u7684\u8fdb\u7a0b\u6570</li> <li>dist_backend - \u5206\u5e03\u5f0f\u8bad\u7ec3\u901a\u4fe1\u4f7f\u7528\u7684\u540e\u7aef\u6846\u67b6 (\u4e5f\u5c31\u662f NCCL, Gloo, MPI \u7b49)\u3002 \u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u4e86\u591a\u4e2a\u591a\u8def GPU \u8282\u70b9\u56e0\u6b64\u63a8\u8350 NCCL\u3002</li> <li>dist_url - \u786e\u5b9a\u8fdb\u7a0b\u7ec4\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u7684 URL\u3002 \u8fd9\u53ef\u80fd\u5305\u542b IP \u5730\u5740\u548c rank0 \u8fdb\u7a0b\u7684\u7aef\u53e3\u6216\u8005\u662f\u4e00\u4e2a\u5728\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u4e2d\u7684 non-existant \u6587\u4ef6\u3002 \u8fd9\u91cc\u7531\u4e8e\u6211\u4eec\u6ca1\u6709\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u56e0\u6b64\u662f\u5305\u542b node0-privateIP \u548c\u8981\u4f7f\u7528\u7684 node0 \u7684\u7aef\u53e3\u7684 url\u3002</li> </ul> <pre><code>print(\"Collect Inputs...\")\n\n# Batch Size for training and testing \u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684 batch size\nbatch_size = 32\n\n# Number of additional worker processes for dataloading \u6570\u636e\u52a0\u8f7d\u7684\u989d\u5916\u5de5\u4f5c\u8fdb\u7a0b\u6570\nworkers = 2\n\n# Number of epochs to train for \u8bad\u7ec3\u7684 epoch \u6570\nnum_epochs = 2\n\n# Starting Learning Rate \u521d\u59cb\u5b66\u4e60\u7387\nstarting_lr = 0.1\n\n# Number of distributed processes \u5206\u5e03\u5f0f\u8fdb\u7a0b\u6570\nworld_size = 4\n\n# Distributed backend type \u5206\u5e03\u5f0f\u540e\u7aef\u7c7b\u578b\ndist_backend = 'nccl'\n\n# Url used to setup distributed training \u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684 url\ndist_url = \"tcp://172.31.22.234:23456\"\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_10","title":"\u521d\u59cb\u5316\u8fdb\u7a0b\u7ec4","text":"<p>\u5728\u4f7f\u7528 PyTorch \u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u6709\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u90e8\u5206\u662f\u6b63\u786e\u8bbe\u7f6e\u8fdb\u7a0b\u7ec4, \u4e5f\u5c31\u662f\u521d\u59cb\u5316 <code>torch.distributed</code> \u5305\u7684\u7b2c\u4e00\u6b65\u3002\u4e3a\u4e86\u5b8c\u6210\u8fd9\u4e00\u6b65\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528 <code>torch.distributed.init_process_group</code> \u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u51e0\u4e2a\u8f93\u5165\u53c2\u6570\u3002\u9996\u5148\uff0c\u9700\u8981\u8f93\u5165 backend \u53c2\u6570\uff0c\u8fd9\u4e2a\u53c2\u6570\u63cf\u8ff0\u4e86\u9700\u8981\u4ec0\u4e48\u540e\u7aef(\u4e5f\u5c31\u662f NCCL, Gloo, MPI \u7b49)\u3002 \u8f93\u5165\u53c2\u6570 init_method \u540c\u65f6\u4e5f\u662f\u5305\u542b rank0 \u5730\u5740\u548c\u7aef\u53e3\u7684 url \u6216\u662f\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u4e0a\u7684 non-existant \u6587\u4ef6\u8def\u5f84\u3002\u6ce8\u610f\uff0c\u4e3a\u4e86\u4f7f\u7528\u6587\u4ef6\u7684 init_method, \u6240\u6709\u673a\u5668\u5fc5\u987b\u6709\u8bbf\u95ee\u6587\u4ef6\u7684\u6743\u9650\uff0c\u548c\u4f7f\u7528 url \u65b9\u6cd5\u7c7b\u4f3c\uff0c\u6240\u6709\u673a\u5668\u5fc5\u987b\u8981\u80fd\u591f\u8054\u7f51\u901a\u4fe1\u6240\u4ee5\u786e\u4fdd\u9632\u706b\u5899\u548c\u7f51\u7edc\u8bbe\u7f6e\u6b63\u786e\u3002 init_process_group \u51fd\u6570\u4e5f\u63a5\u53d7 rank \u548c world_size \u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u8868\u660e\u4e86\u8fdb\u7a0b\u8fd0\u884c\u65f6\u7684\u7f16\u53f7\u5e76\u5206\u522b\u5c55\u793a\u4e86\u96c6\u7fa4\u5185\u7684\u8fdb\u7a0b\u6570\u3002init_method \u4e5f\u53ef\u4ee5\u662f \u201cenv://\u201d\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0crank0 \u673a\u5668\u7684\u5730\u5740\u548c\u7aef\u53e3\u5c06\u4f1a\u5206\u522b\u4ece\u4ee5\u4e0b\u73af\u5883\u53d8\u91cf\u4e2d\u8bfb\u51fa\u6765\uff1aMASTER_ADDR, MASTER_PORT\u3002 \u5982\u679c rank \u548c world_size \u53c2\u6570\u6ca1\u6709\u5728 init_process_group \u51fd\u6570\u4e2d\u8868\u793a\u51fa\u6765\uff0c\u4ed6\u4eec\u90fd\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u73af\u5883\u53d8\u91cf\u4e2d\u5206\u522b\u8bfb\u51fa\u6765\uff1aRANK, WORLD_SIZE\u3002</p> <p>\u53e6\u4e00\u4e2a\u91cd\u8981\u6b65\u9aa4\uff0c\u5c24\u5176\u662f\u5f53\u4e00\u4e2a\u8282\u70b9\u4f7f\u7528\u591a\u8def gpu \u7684\u65f6\u5019\uff0c\u5c31\u662f\u8bbe\u7f6e\u8fdb\u7a0b\u7684 local_rank\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4f60\u6709\u4e24\u4e2a\u8282\u70b9\uff0c\u6bcf\u4e2a\u8282\u70b9\u67098\u4e2a GPU \u5e76\u4e14\u4f60\u5e0c\u671b\u4f7f\u7528\u6240\u6709 GPU \u6765\u8bad\u7ec3\u90a3\u4e48\u8bbe\u7f6e \\(\\(world\\_size=16\\)\\) \u8fd9\u6837\u6bcf\u4e2a\u8282\u70b9\u90fd\u4f1a\u6709\u4e00\u4e2a\u672c\u5730\u7f16\u53f7\u4e3a 0-7 \u7684\u8fdb\u7a0b\u3002 \u8fd9\u4e2a\u672c\u5730\u7f16\u53f7(local_rank) \u662f\u7528\u6765\u4e3a\u8fdb\u7a0b\u914d\u7f6e\u8bbe\u5907 (\u4e5f\u5c31\u662f\u6240\u4f7f\u7528\u7684 GPU ) \u5e76\u4e14\u4e4b\u540e\u7528\u6765\u521b\u5efa\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\u6a21\u578b\u65f6\u914d\u7f6e\u8bbe\u5907\u3002 \u5728\u8fd9\u6837\u7684\u5047\u5b9a\u73af\u5883\u4e0b\u540c\u6837\u63a8\u8350\u4f7f\u7528 NCCL \u540e\u7aef\u56e0\u4e3a NCCL \u66f4\u9002\u5408\u591a\u8def gpu \u8282\u70b9\u3002</p> <pre><code>print(\"Initialize Process Group...\")\n# Initialize Process Group \u521d\u59cb\u5316\u8fdb\u7a0b\u7ec4\n# v1 - init with url  \u4f7f\u7528 url \u521d\u59cb\u5316\ndist.init_process_group(backend=dist_backend, init_method=dist_url, rank=int(sys.argv[1]), world_size=world_size)\n# v2 - init with file \u4f7f\u7528\u6587\u4ef6\u521d\u59cb\u5316\n# dist.init_process_group(backend=\"nccl\", init_method=\"file:///home/ubuntu/pt-distributed-tutorial/trainfile\", rank=int(sys.argv[1]), world_size=world_size)\n# v3 - init with environment variables \u4f7f\u7528\u73af\u5883\u53d8\u91cf\u521d\u59cb\u5316\n# dist.init_process_group(backend=\"nccl\", init_method=\"env://\", rank=int(sys.argv[1]), world_size=world_size)\n\n# Establish Local Rank and set device on this node \u8bbe\u7f6e\u8282\u70b9\u7684\u672c\u5730\u5316\u7f16\u53f7\u548c\u8bbe\u5907\nlocal_rank = int(sys.argv[2])\ndp_device_ids = [local_rank]\ntorch.cuda.set_device(local_rank)\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_11","title":"\u521d\u59cb\u5316\u6a21\u578b","text":"<p>\u4e0b\u4e00\u4e2a\u4e3b\u8981\u6b65\u9aa4\u662f\u521d\u59cb\u5316\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u91cc\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528 <code>torchvision.models</code> \u4e2d\u7684 resnet18 \u6a21\u578b\u4f46\u662f\u4f60\u53ef\u4ee5\u9009\u7528\u4efb\u4f55\u4e00\u79cd\u6a21\u578b\u3002\u9996\u5148\uff0c\u6211\u4eec\u521d\u59cb\u5316\u6a21\u578b\u5e76\u5c06\u5b83\u653e\u8fdb\u663e\u5b58\u4e2d\u3002\u7136\u540e\uff0c\u6211\u4eec\u521b\u5efa\u6a21\u578b <code>DistributedDataParallel</code>, \u5b83\u8d1f\u8d23\u5206\u914d\u6570\u636e\u8fdb\u51fa\u6a21\u578b\uff0c\u8fd9\u5bf9\u5206\u5e03\u5f0f\u8bad\u7ec3\u5f88\u91cd\u8981\u3002 <code>DistributedDataParallel</code> \u6a21\u5757\u540c\u65f6\u4e5f\u8ba1\u7b97\u6574\u4f53\u7684\u5e73\u5747\u68af\u5ea6, \u8fd9\u6837\u6211\u4eec\u5c31\u4e0d\u9700\u8981\u5728\u8bad\u7ec3\u6b65\u9aa4\u8ba1\u7b97\u5e73\u5747\u68af\u5ea6\u3002</p> <p>\u8fd8\u8981\u6ce8\u610f\u5230\u8fd9\u662f\u4e00\u4e2a\u963b\u585e\u51fd\u6570 (blocking function), \u4e5f\u5c31\u662f\u7a0b\u5e8f\u6267\u884c\u65f6\u4f1a\u5728\u8fd9\u4e2a\u51fd\u6570\u7b49\u5f85\u76f4\u5230 world_size \u8fdb\u7a0b\u52a0\u5165\u8fdb\u7a0b\u7ec4\u3002 \u540c\u65f6\u6ce8\u610f\u5230\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u8bbe\u5907 ids \u8868\u4ee5\u53c2\u6570\u7684\u5f62\u5f0f\u4f20\u9012\uff0c\u8fd9\u4e2a\u53c2\u6570\u8fd8\u5305\u542b\u4e86\u6211\u4eec\u6b63\u5728\u4f7f\u7528\u7684\u672c\u5730\u7f16\u53f7 (\u4e5f\u5c31\u662f GPU)\u3002 \u6700\u540e\uff0c\u6211\u4eec\u8bbe\u5b9a\u4e86\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u7684 loss function \u548c optimizer\u3002</p> <pre><code>print(\"Initialize Model...\")\n# Construct Model \u6784\u5efa\u6a21\u578b\nmodel = models.resnet18(pretrained=False).cuda()\n# Make model DistributedDataParallel  \nmodel = torch.nn.parallel.DistributedDataParallel(model, device_ids=dp_device_ids, output_device=local_rank)\n\n# define loss function (criterion) and optimizer \u5b9a\u4e49 loss \u51fd\u6570\u548c optimizer \ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = torch.optim.SGD(model.parameters(), starting_lr, momentum=0.9, weight_decay=1e-4)\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#dataloader","title":"\u521d\u59cb\u5316\u6570\u636e\u52a0\u8f7d\u5668 (dataloader)","text":"<p>\u51c6\u5907\u8bad\u7ec3\u7684\u6700\u540e\u4e00\u6b65\u662f\u786e\u8ba4\u4f7f\u7528\u4ec0\u4e48\u6570\u636e\u96c6\u3002 \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 torchvision.datasets.STL10 \u4e2d\u7684 STL-10 dataset\u3002 STL10 \u6570\u636e\u96c6\u662f\u4e00\u4e2a 10 \u5206\u7c7b 96x96px \u5f69\u8272\u56fe\u7247\u96c6\u3002\u4e3a\u4e86\u5728\u6211\u4eec\u7684\u6a21\u578b\u4e2d\u4f7f\u7528\u5b83\uff0c\u6211\u4eec\u5728\u4e00\u4e2a\u53d8\u6362\u4e2d\u628a\u56fe\u7247\u7684\u5c3a\u5bf8\u8c03\u6574\u4e3a 224x224px\u3002 \u5728\u8fd9\u8282\u4e2d\u7279\u522b\u9700\u8981\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u51c6\u5907\u7684\u4e1c\u897f\u662f\u4e3a\u8bad\u7ec3\u96c6\u4f7f\u7528 <code>DistributedSampler</code>\uff0c\u8fd9\u662f\u8bbe\u8ba1\u6765\u4e0e <code>DistributedDataParallel</code> \u6a21\u578b\u76f8\u7ed3\u5408\u7684\u3002 \u8fd9\u4e2a\u5bf9\u8c61\u63a7\u5236\u8fdb\u5165\u5206\u5e03\u5f0f\u73af\u5883\u7684\u6570\u636e\u96c6\u4ee5\u786e\u4fdd\u6a21\u578b\u4e0d\u662f\u5bf9\u540c\u4e00\u4e2a\u5b50\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u4ee5\u8fbe\u5230\u8bad\u7ec3\u76ee\u6807\u3002\u6700\u540e\uff0c\u6211\u4eec\u521b\u5efa <code>DataLoader</code> \u8d1f\u8d23\u5411\u6a21\u578b\u5582\u6570\u636e\u3002</p> <p>\u5982\u679c\u4f60\u7684\u8282\u70b9\u4e0a\u6ca1\u6709 STL-10 \u6570\u636e\u96c6\u90a3\u4e48\u5b83\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u5230\u8282\u70b9\u4e0a\u3002\u5982\u679c\u4f60\u60f3\u8981\u4f7f\u7528\u4f60\u81ea\u5df1\u7684\u6570\u636e\u96c6\u90a3\u4e48\u4e0b\u8f7d\u4f60\u7684\u6570\u636e\u96c6\uff0c\u642d\u5efa\u4f60\u81ea\u5df1\u7684\u6570\u636e\u64cd\u4f5c\u51fd\u6570\u548c\u52a0\u8f7d\u5668\u3002</p> <pre><code>print(\"Initialize Dataloaders...\")\n# Define the transform for the data. Notice, we must resize to 224x224 with this dataset and model. \u5b9a\u4e49\u6570\u636e\u7684\u53d8\u6362\u3002\u5c3a\u5bf8\u8f6c\u4e3a224x224\ntransform = transforms.Compose(\n    [transforms.Resize(224),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Initialize Datasets. STL10 will automatically download if not present \u521d\u59cb\u5316\u6570\u636e\u96c6\u3002\u5982\u679c\u6ca1\u6709STL10\u6570\u636e\u96c6\u5219\u4f1a\u81ea\u52a8\u4e0b\u8f7d\ntrainset = datasets.STL10(root='./data', split='train', download=True, transform=transform)\nvalset = datasets.STL10(root='./data', split='test', download=True, transform=transform)\n\n# Create DistributedSampler to handle distributing the dataset across nodes when training \u521b\u5efa\u5206\u5e03\u5f0f\u91c7\u6837\u5668\u6765\u63a7\u5236\u8bad\u7ec3\u4e2d\u8282\u70b9\u95f4\u7684\u6570\u636e\u5206\u53d1\n# This can only be called after torch.distributed.init_process_group is called \u8fd9\u4e2a\u53ea\u80fd\u5728 torch.distributed.init_process_group \u88ab\u8c03\u7528\u540e\u8c03\u7528\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n\n# Create the Dataloaders to feed data to the training and validation steps \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5728\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6b65\u9aa4\u4e2d\u5582\u6570\u636e\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(train_sampler is None), num_workers=workers, pin_memory=False, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=False)\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_12","title":"\u8bad\u7ec3\u5faa\u73af","text":"<p>\u6700\u540e\u4e00\u6b65\u662f\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\u3002\u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u7edd\u5927\u591a\u6570\u5de5\u4f5c\u4e86\uff0c\u8fd9\u4e00\u6b65\u4e0d\u662f\u7279\u522b\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u505a\u7684\u3002 \u552f\u4e00\u7684\u7ec6\u8282\u662f\u5728 <code>DistributedSampler</code> \u4e2d\u8bb0\u5f55\u76ee\u524d\u7684 epoch \u6570\uff0c \u56e0\u4e3a\u91c7\u6837\u5668\u662f\u6839\u636e epoch \u6765\u51b3\u5b9a\u5982\u4f55\u6253\u4e71\u5206\u914d\u6570\u636e\u8fdb\u5404\u4e2a\u8fdb\u7a0b\u3002 \u66f4\u65b0\u91c7\u6837\u5668\u540e\uff0c\u5faa\u73af\u6267\u884c\u4e00\u4e2a\u5b8c\u6574\u7684 epoch\uff0c \u4e00\u4e2a\u5b8c\u6574\u7684\u9a8c\u8bc1\u6b65\u9aa4\u7136\u540e\u6253\u5370\u76ee\u524d\u6a21\u578b\u7684\u8868\u73b0\u5e76\u548c\u76ee\u524d\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5bf9\u6bd4\u3002 \u5728\u8bad\u7ec3\u4e86 num_epochs \u540e, \u5faa\u73af\u9000\u51fa\uff0c\u6559\u7a0b\u7ed3\u675f\u3002\u6ce8\u610f\uff0c\u56e0\u4e3a\u8fd9\u53ea\u662f\u4e2a\u4f8b\u7a0b\uff0c\u6211\u4eec\u6ca1\u6709\u4fdd\u5b58\u6a21\u578b\uff0c\u4f46\u5982\u679c\u60f3\u8981\u8bad\u7ec3\u7ed3\u675f\u540e\u4fdd\u5b58\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u8bf7\u770b\u8fd9\u91cc\u3002</p> <pre><code>best_prec1 = 0\n\nfor epoch in range(num_epochs):\n    # Set epoch count for DistributedSampler \u4e3a\u5206\u5e03\u5f0f\u91c7\u6837\u5668\u8bbe\u7f6e epoch \u6570\n    train_sampler.set_epoch(epoch)\n\n    # Adjust learning rate according to schedule \u8c03\u6574\u5b66\u4e60\u7387\n    adjust_learning_rate(starting_lr, optimizer, epoch)\n\n    # train for one epoch \u8bad\u7ec31\u4e2a epoch\n    print(\"\\nBegin Training Epoch {}\".format(epoch+1))\n    train(train_loader, model, criterion, optimizer, epoch)\n\n    # evaluate on validation set \u5728\u9a8c\u8bc1\u96c6\u4e0a\u9a8c\u8bc1\n    print(\"Begin Validation @ Epoch {}\".format(epoch+1))\n    prec1 = validate(val_loader, model, criterion)\n\n    # remember best prec@1 and save checkpoint if desired \u4fdd\u5b58\u6700\u4f73\u7684prec@1\uff0c\u5982\u679c\u9700\u8981\u7684\u8bdd\u4fdd\u5b58\u68c0\u67e5\u70b9\n    # is_best = prec1 &gt; best_prec1\n    best_prec1 = max(prec1, best_prec1)\n\n    print(\"Epoch Summary: \")\n    print(\"\\tEpoch Accuracy: {}\".format(prec1))\n    print(\"\\tBest Accuracy: {}\".format(best_prec1))\n\n</code></pre>"},{"location":"1.0/aws_distributed_training_tutorial/#_13","title":"\u8fd0\u884c\u4ee3\u7801","text":"<p>\u548c\u5176\u4ed6 PyTorch \u6559\u7a0b\u4e0d\u4e00\u6837, \u8fd9\u4e2a\u4ee3\u7801\u4e5f\u8bb8\u4e0d\u80fd\u76f4\u63a5\u4ee5 notebook \u7684\u5f62\u5f0f\u6267\u884c\u3002 \u4e3a\u4e86\u8fd0\u884c\u5b83\u9700\u8981\u4ee5 .py \u5f62\u5f0f\u4e0b\u8f7d\u8fd9\u4efd\u6587\u4ef6(\u6216\u8005\u4f7f\u7528\u8fd9\u4e2a\u6765\u8f6c\u6362\u5b83)\u7136\u540e\u590d\u5236\u5230\u5404\u4e2a\u8282\u70b9\u4e0a\u3002 \u806a\u660e\u7684\u8bfb\u8005\u4e5f\u8bb8\u6ce8\u610f\u5230\u4e86\u6211\u4eec\u5199\u6b7b\u4e86(\u786c\u7f16\u7801\uff0chardcode) node0-privateIP \u548c \\(\\(world\\_size=4\\)\\) \u4f46\u628a rank \u548c local_rank \u4ee5 arg[1] \u548c arg[2] \u547d\u4ee4\u884c\u53c2\u6570\u7684\u5f62\u5f0f\u5206\u522b\u8f93\u5165\u3002 \u4e0a\u4f20\u540e\u5bf9\u6bcf\u4e2a\u8282\u70b9\u5206\u522b\u6253\u5f00\u4e24\u4e2a ssh \u7ec8\u7aef\u3002</p> <ul> <li>\u5bf9 node0 \u7684\u7b2c\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fd0\u884c <code>$ python main.py 0 0</code></li> <li>\u5bf9 node0 \u7684\u7b2c\u4e8c\u4e2a\u7ec8\u7aef\uff0c\u8fd0\u884c <code>$ python main.py 1 1</code></li> <li>\u5bf9 node1 \u7684\u7b2c\u4e00\u4e2a\u7ec8\u7aef\uff0c\u8fd0\u884c <code>$ python main.py 2 0</code></li> <li>\u5bf9 node1 \u7684\u7b2c\u4e8c\u4e2a\u7ec8\u7aef\uff0c\u8fd0\u884c <code>$ python main.py 3 1</code></li> </ul> <p>\u7a0b\u5e8f\u4f1a\u5f00\u59cb\u8fd0\u884c\u5e76\u7b49\u5f85\u76f4\u5230\u56db\u4e2a\u8fdb\u7a0b\u90fd\u52a0\u5165\u8fdb\u7a0b\u7ec4\u540e\u6253\u5370 \u201cInitialize Model\u2026\u201d \u3002 \u6ce8\u610f\u5230\u7b2c\u4e00\u4e2a\u53c2\u6570\u4e0d\u80fd\u91cd\u590d\u56e0\u4e3a\u8fd9\u662f\u8fdb\u7a0b\u7684\u5168\u5c40\u7f16\u53f7(\u552f\u4e00\u7684)\u3002 \u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u91cd\u590d\u56e0\u4e3a\u8fd9\u662f\u8282\u70b9\u4e0a\u8fdb\u7a0b\u7684\u672c\u5730\u7f16\u53f7\u3002 \u5982\u679c\u4f60\u5bf9\u6bcf\u4e2a\u8282\u70b9\u8fd0\u884c <code>nvidia-smi</code>\uff0c\u4f60\u4f1a\u770b\u89c1\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6709\u4e24\u4e2a\u8fdb\u7a0b\uff0c\u4e00\u4e2a\u8fd0\u884c\u5728 GPU0 \u4e0a\uff0c\u53e6\u4e00\u4e2a\u8fd0\u884c\u5728 GPU1 \u4e0a\u3002</p> <p>\u6211\u4eec\u73b0\u5728\u5df2\u7ecf\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8303\u4f8b\uff01 \u5e0c\u671b\u4f60\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u6559\u7a0b\u5b66\u4f1a\u5982\u4f55\u5728\u4f60\u81ea\u5df1\u7684\u6570\u636e\u96c6\u4e0a\u642d\u5efa\u4f60\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u5373\u4f7f\u4f60\u4e0d\u662f\u4f7f\u7528\u540c\u6837\u7684\u5206\u5e03\u5f0f\u73af\u5883\u3002 \u5982\u679c\u4f60\u5728\u4f7f\u7528 AWS\uff0c\u5207\u8bb0\u5728\u4f60\u4e0d\u4f7f\u7528\u65f6\u5173\u6389\u4f60\u7684\u8282\u70b9\u4e0d\u7136\u6708\u672b\u4f60\u4f1a\u53d1\u73b0\u4f60\u8981\u4ea4\u597d\u591a\u94b1\u3002</p> <p>\u63a5\u4e0b\u6765\u770b\u4ec0\u4e48</p> <ul> <li>\u770b\u770b launcher utility \u4ee5\u4e86\u89e3\u53e6\u4e00\u79cd\u542f\u52a8\u8fd0\u884c\u7684\u65b9\u5f0f</li> <li>\u770b\u770b torch.multiprocessing.spawn utility \u4ee5\u4e86\u89e3\u53e6\u4e00\u79cd\u7b80\u5355\u7684\u542f\u52a8\u591a\u8def\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7684\u65b9\u5f0f\u3002 PyTorch ImageNet Example \u5df2\u7ecf\u5b9e\u73b0\u5e76\u53ef\u4ee5\u6f14\u793a\u5982\u4f55\u4f7f\u7528\u5b83\u3002</li> <li>\u5982\u679c\u53ef\u80fd\uff0c\u8bf7\u8bbe\u7f6e\u4e00\u4e2aNFS\uff0c\u8fd9\u6837\u4f60\u53ea\u9700\u8981\u4e00\u4e2a\u6570\u636e\u96c6\u526f\u672c</li> </ul>"},{"location":"1.0/blitz_autograd_tutorial/","title":"Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>PyTorch\u4e2d\uff0c\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u6838\u5fc3\u662f <code>autograd</code> \u5305\u3002\u5148\u7b80\u5355\u4ecb\u7ecd\u4e00\u4e0b\u8fd9\u4e2a\u5305\uff0c\u7136\u540e\u8bad\u7ec3\u6211\u4eec\u7684\u7b2c\u4e00\u4e2a\u7684\u795e\u7ecf\u7f51\u7edc\u3002</p> <p><code>autograd</code> \u5305\u4e3a\u5f20\u91cf\u4e0a\u7684\u6240\u6709\u64cd\u4f5c\u63d0\u4f9b\u4e86\u81ea\u52a8\u6c42\u5bfc\u673a\u5236\u3002\u5b83\u662f\u4e00\u4e2a\u5728\u8fd0\u884c\u65f6\u5b9a\u4e49(define-by-run\uff09\u7684\u6846\u67b6\uff0c\u8fd9\u610f\u5473\u7740\u53cd\u5411\u4f20\u64ad\u662f\u6839\u636e\u4ee3\u7801\u5982\u4f55\u8fd0\u884c\u6765\u51b3\u5b9a\u7684\uff0c\u5e76\u4e14\u6bcf\u6b21\u8fed\u4ee3\u53ef\u4ee5\u662f\u4e0d\u540c\u7684.</p> <p>\u8ba9\u6211\u4eec\u7528\u4e00\u4e9b\u7b80\u5355\u7684\u4f8b\u5b50\u6765\u770b\u770b\u5427\u3002</p>"},{"location":"1.0/blitz_autograd_tutorial/#_1","title":"\u5f20\u91cf","text":"<p><code>torch.Tensor</code> \u662f\u8fd9\u4e2a\u5305\u7684\u6838\u5fc3\u7c7b\u3002\u5982\u679c\u8bbe\u7f6e\u5b83\u7684\u5c5e\u6027 <code>.requires_grad</code> \u4e3a <code>True</code>\uff0c\u90a3\u4e48\u5b83\u5c06\u4f1a\u8ffd\u8e2a\u5bf9\u4e8e\u8be5\u5f20\u91cf\u7684\u6240\u6709\u64cd\u4f5c\u3002\u5f53\u5b8c\u6210\u8ba1\u7b97\u540e\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 <code>.backward()</code>\uff0c\u6765\u81ea\u52a8\u8ba1\u7b97\u6240\u6709\u7684\u68af\u5ea6\u3002\u8fd9\u4e2a\u5f20\u91cf\u7684\u6240\u6709\u68af\u5ea6\u5c06\u4f1a\u81ea\u52a8\u7d2f\u52a0\u5230<code>.grad</code>\u5c5e\u6027.</p> <p>\u8981\u963b\u6b62\u4e00\u4e2a\u5f20\u91cf\u88ab\u8ddf\u8e2a\u5386\u53f2\uff0c\u53ef\u4ee5\u8c03\u7528 <code>.detach()</code> \u65b9\u6cd5\u5c06\u5176\u4e0e\u8ba1\u7b97\u5386\u53f2\u5206\u79bb\uff0c\u5e76\u963b\u6b62\u5b83\u672a\u6765\u7684\u8ba1\u7b97\u8bb0\u5f55\u88ab\u8ddf\u8e2a\u3002</p> <p>\u4e3a\u4e86\u9632\u6b62\u8ddf\u8e2a\u5386\u53f2\u8bb0\u5f55(\u548c\u4f7f\u7528\u5185\u5b58\uff09\uff0c\u53ef\u4ee5\u5c06\u4ee3\u7801\u5757\u5305\u88c5\u5728 <code>with torch.no_grad():</code> \u4e2d\u3002\u5728\u8bc4\u4f30\u6a21\u578b\u65f6\u7279\u522b\u6709\u7528\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5177\u6709 <code>requires_grad = True</code> \u7684\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\uff0c\u4f46\u662f\u6211\u4eec\u4e0d\u9700\u8981\u5728\u6b64\u8fc7\u7a0b\u4e2d\u5bf9\u4ed6\u4eec\u8fdb\u884c\u68af\u5ea6\u8ba1\u7b97\u3002</p> <p>\u8fd8\u6709\u4e00\u4e2a\u7c7b\u5bf9\u4e8eautograd\u7684\u5b9e\u73b0\u975e\u5e38\u91cd\u8981\uff1a<code>Function</code>\u3002</p> <p><code>Tensor</code> \u548c <code>Function</code> \u4e92\u76f8\u8fde\u63a5\u751f\u6210\u4e86\u4e00\u4e2a\u65e0\u5708\u56fe(acyclic graph)\uff0c\u5b83\u7f16\u7801\u4e86\u5b8c\u6574\u7684\u8ba1\u7b97\u5386\u53f2\u3002\u6bcf\u4e2a\u5f20\u91cf\u90fd\u6709\u4e00\u4e2a <code>.grad_fn</code> \u5c5e\u6027\uff0c\u8be5\u5c5e\u6027\u5f15\u7528\u4e86\u521b\u5efa <code>Tensor</code> \u81ea\u8eab\u7684<code>Function</code>(\u9664\u975e\u8fd9\u4e2a\u5f20\u91cf\u662f\u7528\u6237\u624b\u52a8\u521b\u5efa\u7684\uff0c\u5373\u8fd9\u4e2a\u5f20\u91cf\u7684 <code>grad_fn</code> \u662f <code>None</code> )\u3002</p> <p>\u5982\u679c\u9700\u8981\u8ba1\u7b97\u5bfc\u6570\uff0c\u53ef\u4ee5\u5728 <code>Tensor</code> \u4e0a\u8c03\u7528 <code>.backward()</code>\u3002\u5982\u679c <code>Tensor</code> \u662f\u4e00\u4e2a\u6807\u91cf(\u5373\u5b83\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u7684\u6570\u636e\uff09\uff0c\u5219\u4e0d\u9700\u8981\u4e3a <code>backward()</code> \u6307\u5b9a\u4efb\u4f55\u53c2\u6570\uff0c\u4f46\u662f\u5982\u679c\u5b83\u6709\u66f4\u591a\u7684\u5143\u7d20\uff0c\u5219\u9700\u8981\u6307\u5b9a\u4e00\u4e2a <code>gradient</code> \u53c2\u6570\uff0c\u8be5\u53c2\u6570\u662f\u5f62\u72b6\u5339\u914d\u7684\u5f20\u91cf\u3002</p> <pre><code>import torch\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u5e76\u8bbe\u7f6e<code>requires_grad=True</code>\u7528\u6765\u8ffd\u8e2a\u5176\u8ba1\u7b97\u5386\u53f2</p> <pre><code>x = torch.ones(2, 2, requires_grad=True)\nprint(x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n</code></pre> <p>\u5bf9\u8fd9\u4e2a\u5f20\u91cf\u505a\u4e00\u6b21\u8fd0\u7b97\uff1a</p> <pre><code>y = x + 2\nprint(y)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[3., 3.],\n        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p><code>y</code>\u662f\u8ba1\u7b97\u7684\u7ed3\u679c\uff0c\u6240\u4ee5\u5b83\u6709<code>grad_fn</code>\u5c5e\u6027\u3002</p> <pre><code>print(y.grad_fn)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>&lt;AddBackward0 object at 0x7f1b248453c8&gt;\n</code></pre> <p>\u5bf9y\u8fdb\u884c\u66f4\u591a\u64cd\u4f5c</p> <pre><code>z = y * y * 3\nout = z.mean()\n\nprint(z, out)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[27., 27.],\n        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)\n</code></pre> <p><code>.requires_grad_(...)</code> \u539f\u5730\u6539\u53d8\u4e86\u73b0\u6709\u5f20\u91cf\u7684 <code>requires_grad</code> \u6807\u5fd7\u3002\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u7684\u8bdd\uff0c\u9ed8\u8ba4\u8f93\u5165\u7684\u8fd9\u4e2a\u6807\u5fd7\u662f <code>False</code>\u3002</p> <pre><code>a = torch.randn(2, 2)\na = ((a * 3) / (a - 1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb = (a * a).sum()\nprint(b.grad_fn)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>False\nTrue\n&lt;SumBackward0 object at 0x7f1b24845f98&gt;\n</code></pre>"},{"location":"1.0/blitz_autograd_tutorial/#_2","title":"\u68af\u5ea6","text":"<p>\u73b0\u5728\u5f00\u59cb\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u56e0\u4e3a <code>out</code> \u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u56e0\u6b64 <code>out.backward()</code> \u548c <code>out.backward(torch.tensor(1.))</code> \u7b49\u4ef7\u3002</p> <pre><code>out.backward()\n</code></pre> <p>\u8f93\u51fa\u5bfc\u6570 <code>d(out)/dx</code></p> <pre><code>print(x.grad)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])\n</code></pre> <p>\u6211\u4eec\u7684\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u6570\u53d6\u503c\u5168\u90e8\u4e3a<code>4.5</code>\u7684\u77e9\u9635\u3002</p> <p>\u8ba9\u6211\u4eec\u6765\u8c03\u7528 <code>out</code> \u5f20\u91cf <code>o</code>\u3002</p> <p>\u5c31\u53ef\u4ee5\u5f97\u5230 \\(\\(o = \\frac{1}{4}\\sum_i z_i\\)\\)\uff0c\\(\\(z_i = 3(x_i+2)^2\\)\\) \u548c \\(\\(z_i\\bigr\\rvert_{x_i=1} = 27\\)\\) \u56e0\u6b64, \\(\\(\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)\\)\\)\uff0c\u56e0\u800c \\(\\(\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5\\)\\)\u3002</p> <p>\u6570\u5b66\u4e0a\uff0c\u82e5\u6709\u5411\u91cf\u503c\u51fd\u6570 \\(\\(\\vec{y}=f(\\vec{x})\\)\\)\uff0c\u90a3\u4e48 \\(\\(\\vec{y}\\)\\) \u76f8\u5bf9\u4e8e \\(\\(\\vec{x}\\)\\) \u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u96c5\u53ef\u6bd4\u77e9\u9635\uff1a</p> \\[ J=\\left(\\begin{array}{ccc}    \\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\    \\vdots &amp; \\ddots &amp; \\vdots\\\\    \\frac{\\partial y_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}    \\end{array}\\right) \\] <p>\u901a\u5e38\u6765\u8bf4\uff0c<code>torch.autograd</code> \u662f\u8ba1\u7b97\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u7684\u4e00\u4e2a\u201c\u5f15\u64ce\u201d\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u7ed9\u5b9a\u4efb\u610f\u5411\u91cf \\(\\(v=\\left(\\begin{array}{cccc} v_{1} &amp; v_{2} &amp; \\cdots &amp; v_{m}\\end{array}\\right)^{T}\\)\\)\uff0c\u8ba1\u7b97\u4e58\u79ef \\(\\(v^{T}\\cdot J\\)\\)\u3002\u5982\u679c \\(\\(v\\)\\) \u6070\u597d\u662f\u4e00\u4e2a\u6807\u91cf\u51fd\u6570 \\(\\(l=g\\left(\\vec{y}\\right)\\)\\) \u7684\u5bfc\u6570\uff0c\u5373 \\(\\(v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} &amp; \\cdots &amp; \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\)\\)\uff0c\u90a3\u4e48\u6839\u636e\u94fe\u5f0f\u6cd5\u5219\uff0c\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u5e94\u8be5\u662f \\(\\(l\\)\\) \u5bf9 \\(\\(\\vec{x}\\)\\) \u7684\u5bfc\u6570\uff1a</p> \\[ J^{T}\\cdot v=\\left(\\begin{array}{ccc}    \\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\    \\vdots &amp; \\ddots &amp; \\vdots\\\\    \\frac{\\partial y_{1}}{\\partial x_{n}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}    \\end{array}\\right)\\left(\\begin{array}{c}    \\frac{\\partial l}{\\partial y_{1}}\\\\    \\vdots\\\\    \\frac{\\partial l}{\\partial y_{m}}    \\end{array}\\right)=\\left(\\begin{array}{c}    \\frac{\\partial l}{\\partial x_{1}}\\\\    \\vdots\\\\    \\frac{\\partial l}{\\partial x_{n}}    \\end{array}\\right) \\] <p>(\u6ce8\u610f\uff1a\u884c\u5411\u91cf\u7684$$ v^{T}\\cdot J\\(\\(\u4e5f\u53ef\u4ee5\u88ab\u89c6\u4f5c\u5217\u5411\u91cf\u7684\\)\\)J^{T}\\cdot v$$)</p> <p>\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u7684\u8fd9\u4e00\u7279\u6027\u4f7f\u5f97\u5c06\u5916\u90e8\u68af\u5ea6\u8f93\u5165\u5230\u5177\u6709\u975e\u6807\u91cf\u8f93\u51fa\u7684\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u65b9\u4fbf\u3002</p> <p>\u73b0\u5728\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\u7684\u4f8b\u5b50:</p> <pre><code>x = torch.randn(3, requires_grad=True)\n\ny = x * 2\nwhile y.data.norm() &lt; 1000:\n    y = y * 2\n\nprint(y)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([-278.6740,  935.4016,  439.6572], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>y</code> \u4e0d\u518d\u662f\u6807\u91cf\u3002<code>torch.autograd</code> \u4e0d\u80fd\u76f4\u63a5\u8ba1\u7b97\u5b8c\u6574\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u4f46\u662f\u5982\u679c\u6211\u4eec\u53ea\u60f3\u8981\u96c5\u53ef\u6bd4\u5411\u91cf\u79ef\uff0c\u53ea\u9700\u5c06\u8fd9\u4e2a\u5411\u91cf\u4f5c\u4e3a\u53c2\u6570\u4f20\u7ed9 <code>backward</code>\uff1a</p> <pre><code>v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\ny.backward(v)\n\nprint(x.grad)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([4.0960e+02, 4.0960e+03, 4.0960e-01])\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5c06\u4ee3\u7801\u5757\u5305\u88c5\u5728 <code>with torch.no_grad():</code> \u4e2d\uff0c\u6765\u963b\u6b62autograd\u8ddf\u8e2a\u8bbe\u7f6e\u4e86 <code>.requires_grad=True</code> \u7684\u5f20\u91cf\u7684\u5386\u53f2\u8bb0\u5f55\u3002</p> <pre><code>print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n    print((x ** 2).requires_grad)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>True\nTrue\nFalse\n</code></pre> <p>\u540e\u7eed\u9605\u8bfb\uff1a</p> <p><code>autograd</code> \u548c <code>Function</code> \u7684\u6587\u6863\u89c1\uff1ahttps://pytorch.org/docs/autograd</p>"},{"location":"1.0/blitz_cifar10_tutorial/","title":"\u8bad\u7ec3\u5206\u7c7b\u5668","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u4ee5\u53ca\u770b\u5230\u4e86\u5982\u4f55\u5b9a\u4e49\u7f51\u7edc\uff0c\u8ba1\u7b97\u635f\u5931\uff0c\u5e76\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd\u3002\u6240\u4ee5\u4f60\u73b0\u5728\u53ef\u80fd\u4f1a\u60f3,</p>"},{"location":"1.0/blitz_cifar10_tutorial/#_2","title":"\u6570\u636e\u5e94\u8be5\u600e\u4e48\u529e\u5462\uff1f","text":"<p>\u901a\u5e38\u6765\u8bf4\uff0c\u5f53\u5fc5\u987b\u5904\u7406\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\u6216\u89c6\u9891\u6570\u636e\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528python\u6807\u51c6\u5e93\u5c06\u6570\u636e\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u91cc\u3002\u7136\u540e\u5c06\u8fd9\u4e2a\u6570\u7ec4\u8f6c\u5316\u6210<code>torch.*Tensor</code>\u3002</p> <ul> <li>\u5bf9\u4e8e\u56fe\u7247\uff0c\u6709Pillow\uff0cOpenCV\u7b49\u5305\u53ef\u4ee5\u4f7f\u7528</li> <li>\u5bf9\u4e8e\u97f3\u9891\uff0c\u6709scipy\u548clibrosa\u7b49\u5305\u53ef\u4ee5\u4f7f\u7528</li> <li>\u5bf9\u4e8e\u6587\u672c\uff0c\u4e0d\u7ba1\u662f\u539f\u751fpython\u7684\u6216\u8005\u662f\u57fa\u4e8eCython\u7684\u6587\u672c\uff0c\u53ef\u4ee5\u4f7f\u7528NLTK\u548cSpaCy</li> </ul> <p>\u7279\u522b\u5bf9\u4e8e\u89c6\u89c9\u65b9\u9762\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\uff0c\u540d\u5b57\u53eb<code>torchvision</code>\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u9488\u5bf9Imagenet\u3001CIFAR10\u3001MNIST\u7b49\u5e38\u7528\u6570\u636e\u96c6\u7684\u6570\u636e\u52a0\u8f7d\u5668(data loaders\uff09\uff0c\u8fd8\u6709\u5bf9\u56fe\u7247\u6570\u636e\u53d8\u6362\u7684\u64cd\u4f5c\uff0c\u5373<code>torchvision.datasets</code>\u548c<code>torch.utils.data.DataLoader</code>\u3002</p> <p>\u8fd9\u63d0\u4f9b\u4e86\u6781\u5927\u7684\u4fbf\u5229\uff0c\u53ef\u4ee5\u907f\u514d\u7f16\u5199\u6837\u677f\u4ee3\u7801\u3002</p> <p>\u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CIFAR10\u6570\u636e\u96c6\uff0c\u5b83\u6709\u5982\u4e0b\u7684\u5206\u7c7b\uff1a\u201c\u98de\u673a\u201d\uff0c\u201c\u6c7d\u8f66\u201d\uff0c\u201c\u9e1f\u201d\uff0c\u201c\u732b\u201d\uff0c\u201c\u9e7f\u201d\uff0c\u201c\u72d7\u201d\uff0c\u201c\u9752\u86d9\u201d\uff0c\u201c\u9a6c\u201d\uff0c\u201c\u8239\u201d\uff0c\u201c\u5361\u8f66\u201d\u7b49\u3002\u5728CIFAR-10\u91cc\u9762\u7684\u56fe\u7247\u6570\u636e\u5927\u5c0f\u662f3x32x32\uff0c\u5373\u4e09\u901a\u9053\u5f69\u8272\u56fe\uff0c\u56fe\u7247\u5927\u5c0f\u662f32x32\u50cf\u7d20\u3002</p> <p></p>"},{"location":"1.0/blitz_cifar10_tutorial/#_3","title":"\u8bad\u7ec3\u4e00\u4e2a\u56fe\u7247\u5206\u7c7b\u5668","text":"<p>\u6211\u4eec\u5c06\u6309\u987a\u5e8f\u505a\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ol> <li>\u901a\u8fc7<code>torchvision</code>\u52a0\u8f7dCIFAR10\u91cc\u9762\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316</li> <li>\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc</li> <li>\u5b9a\u4e49\u635f\u5931\u51fd\u6570</li> <li>\u5229\u7528\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\u7f51\u7edc</li> <li>\u5229\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc</li> </ol>"},{"location":"1.0/blitz_cifar10_tutorial/#1cifar10","title":"1.\u52a0\u8f7d\u5e76\u6807\u51c6\u5316CIFAR10","text":"<p>\u4f7f\u7528<code>torchvision</code>\u52a0\u8f7dCIFAR10\u8d85\u7ea7\u7b80\u5355\u3002</p> <pre><code>import torch\nimport torchvision\nimport torchvision.transforms as transforms\n</code></pre> <p>torchvision\u6570\u636e\u96c6\u52a0\u8f7d\u5b8c\u540e\u7684\u8f93\u51fa\u662f\u8303\u56f4\u5728[0, 1]\u4e4b\u95f4\u7684PILImage\u3002\u6211\u4eec\u5c06\u5176\u6807\u51c6\u5316\u4e3a\u8303\u56f4\u5728[-1, 1]\u4e4b\u95f4\u7684\u5f20\u91cf\u3002</p> <pre><code>transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\nFiles already downloaded and verified\n\n</code></pre> <p>\u4e50\u8da3\u6240\u81f4\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u53ef\u89c6\u5316\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u3002</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# \u8f93\u51fa\u56fe\u50cf\u7684\u51fd\u6570\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# \u968f\u673a\u83b7\u53d6\u8bad\u7ec3\u56fe\u7247\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# \u663e\u793a\u56fe\u7247\nimshow(torchvision.utils.make_grid(images))\n# \u6253\u5370\u56fe\u7247\u6807\u7b7e\nprint(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n\n</code></pre> <p></p> <p>\u8f93\u51fa\uff1a</p> <pre><code>horse horse horse   car\n</code></pre>"},{"location":"1.0/blitz_cifar10_tutorial/#2","title":"2.\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","text":"<p>\u5c06\u4e4b\u524d\u795e\u7ecf\u7f51\u7edc\u7ae0\u8282\u5b9a\u4e49\u7684\u795e\u7ecf\u7f51\u7edc\u62ff\u8fc7\u6765\uff0c\u5e76\u5c06\u5176\u4fee\u6539\u6210\u8f93\u5165\u4e3a3\u901a\u9053\u56fe\u50cf(\u66ff\u4ee3\u539f\u6765\u5b9a\u4e49\u7684\u5355\u901a\u9053\u56fe\u50cf\uff09\u3002</p> <pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\n</code></pre>"},{"location":"1.0/blitz_cifar10_tutorial/#3","title":"3.\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668","text":"<p>\u6211\u4eec\u4f7f\u7528\u5206\u7c7b\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d(\u4f7f\u7528momentum\uff09\u3002</p> <pre><code>import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n</code></pre>"},{"location":"1.0/blitz_cifar10_tutorial/#4","title":"4.\u8bad\u7ec3\u7f51\u7edc","text":"<p>\u4e8b\u60c5\u5f00\u59cb\u53d8\u5f97\u6709\u8da3\u4e86\u3002\u6211\u4eec\u53ea\u9700\u8981\u904d\u5386\u6211\u4eec\u7684\u6570\u636e\u8fed\u4ee3\u5668\uff0c\u5e76\u5c06\u8f93\u5165\u201c\u5582\u201d\u7ed9\u7f51\u7edc\u548c\u4f18\u5316\u51fd\u6570\u3002</p> <pre><code>for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>[1,  2000] loss: 2.182\n[1,  4000] loss: 1.819\n[1,  6000] loss: 1.648\n[1,  8000] loss: 1.569\n[1, 10000] loss: 1.511\n[1, 12000] loss: 1.473\n[2,  2000] loss: 1.414\n[2,  4000] loss: 1.365\n[2,  6000] loss: 1.358\n[2,  8000] loss: 1.322\n[2, 10000] loss: 1.298\n[2, 12000] loss: 1.282\nFinished Training\n</code></pre>"},{"location":"1.0/blitz_cifar10_tutorial/#5","title":"5.\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u6d4b\u8bd5\u7f51\u7edc","text":"<p>\u6211\u4eec\u5df2\u7ecf\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u4e862\u904d\u7f51\u7edc\u3002\u4f46\u662f\u6211\u4eec\u9700\u8981\u68c0\u67e5\u7f51\u7edc\u662f\u5426\u5b66\u5230\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002</p> <p>\u6211\u4eec\u5c06\u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u6807\u7b7e\u6765\u68c0\u67e5\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u548c\u6b63\u786e\u6837\u672c\u8fdb\u884c(ground-truth\uff09\u5bf9\u6bd4\u3002\u5982\u679c\u9884\u6d4b\u662f\u6b63\u786e\u7684\uff0c\u6211\u4eec\u5c06\u6837\u672c\u6dfb\u52a0\u5230\u6b63\u786e\u9884\u6d4b\u7684\u5217\u8868\u4e2d\u3002</p> <p>ok\uff0c\u7b2c\u4e00\u6b65\u3002\u8ba9\u6211\u4eec\u663e\u793a\u6d4b\u8bd5\u96c6\u4e2d\u7684\u56fe\u50cf\u6765\u719f\u6089\u4e00\u4e0b\u3002</p> <pre><code>dataiter = iter(testloader)\nimages, labels = dataiter.next()\n\n# \u8f93\u51fa\u56fe\u7247\nimshow(torchvision.utils.make_grid(images))\nprint('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n</code></pre> <p></p> <pre><code>GroundTruth:    cat  ship  ship plane\n</code></pre> <p>ok\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u770b\u795e\u7ecf\u7f51\u7edc\u8ba4\u4e3a\u4e0a\u9762\u7684\u4f8b\u5b50\u662f:</p> <pre><code>outputs = net(images)\n</code></pre> <p>\u8f93\u51fa\u662f10\u4e2a\u7c7b\u522b\u7684\u91cf\u503c\u3002\u4e00\u4e2a\u7c7b\u7684\u503c\u8d8a\u9ad8\uff0c\u7f51\u7edc\u5c31\u8d8a\u8ba4\u4e3a\u8fd9\u4e2a\u56fe\u50cf\u5c5e\u4e8e\u8fd9\u4e2a\u7279\u5b9a\u7684\u7c7b\u3002\u8ba9\u6211\u4eec\u5f97\u5230\u6700\u9ad8\u91cf\u503c\u7684\u4e0b\u6807/\u7d22\u5f15\uff1b</p> <pre><code>_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Predicted:    dog  ship  ship plane\n</code></pre> <p>\u7ed3\u679c\u8fd8\u4e0d\u9519\u3002</p> <p>\u8ba9\u6211\u4eec\u770b\u770b\u7f51\u7edc\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7684\u600e\u4e48\u6837\u3002</p> <pre><code>correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct / total))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Accuracy of the network on the 10000 test images: 55 %\n</code></pre> <p>\u8fd9\u6bd4\u968f\u673a\u9009\u53d6(\u5373\u4ece10\u4e2a\u7c7b\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u7c7b\uff0c\u6b63\u786e\u7387\u662f10%\uff09\u8981\u597d\u5f88\u591a\u3002\u770b\u6765\u7f51\u7edc\u786e\u5b9e\u5b66\u5230\u4e86\u4e00\u4e9b\u4e1c\u897f\u3002</p> <p>\u90a3\u4e48\u54ea\u4e9b\u662f\u8868\u73b0\u597d\u7684\u7c7b\u5462\uff1f\u54ea\u4e9b\u662f\u8868\u73b0\u7684\u5dee\u7684\u7c7b\u5462\uff1f</p> <pre><code>class_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] / class_total[i]))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Accuracy of plane : 70 %\nAccuracy of   car : 70 %\nAccuracy of  bird : 28 %\nAccuracy of   cat : 25 %\nAccuracy of  deer : 37 %\nAccuracy of   dog : 60 %\nAccuracy of  frog : 66 %\nAccuracy of horse : 62 %\nAccuracy of  ship : 69 %\nAccuracy of truck : 61 %\n</code></pre> <p>ok\uff0c\u63a5\u4e0b\u6765\u5462\uff1f</p> <p>\u600e\u4e48\u5728GPU\u4e0a\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\u5462\uff1f</p>"},{"location":"1.0/blitz_cifar10_tutorial/#gpu","title":"\u5728GPU\u4e0a\u8bad\u7ec3","text":"<p>\u4e0e\u5c06\u4e00\u4e2a\u5f20\u91cf\u4f20\u9012\u7ed9GPU\u4e00\u6837\uff0c\u53ef\u4ee5\u8fd9\u6837\u5c06\u795e\u7ecf\u7f51\u7edc\u8f6c\u79fb\u5230GPU\u4e0a\u3002 </p> <p>\u5982\u679c\u6211\u4eec\u6709cuda\u53ef\u7528\u7684\u8bdd\uff0c\u8ba9\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u7b2c\u4e00\u4e2a\u8bbe\u5907\u4e3a\u53ef\u89c1cuda\u8bbe\u5907\uff1a</p> <pre><code>device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>cuda:0\n</code></pre> <p>\u672c\u8282\u7684\u5176\u4f59\u90e8\u5206\u5047\u8bbe<code>device</code>\u662fCUDA\u3002</p> <p>\u7136\u540e\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u9012\u5f52\u904d\u5386\u6240\u6709\u6a21\u5757\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u6362\u4e3aCUDA\u5f20\u91cf\uff1a</p> <pre><code>net.to(device)\n</code></pre> <p>\u8bf7\u8bb0\u4f4f\uff0c\u6211\u4eec\u4e0d\u5f97\u4e0d\u5c06\u8f93\u5165\u548c\u76ee\u6807\u5728\u6bcf\u4e00\u6b65\u90fd\u9001\u5165GPU\uff1a</p> <pre><code>inputs, labels = inputs.to(device), labels.to(device)\n</code></pre> <p>\u4e3a\u4ec0\u4e48\u6211\u4eec\u611f\u53d7\u4e0d\u5230\u4e0eCPU\u76f8\u6bd4\u7684\u5de8\u5927\u52a0\u901f\uff1f\u56e0\u4e3a\u6211\u4eec\u7684\u7f51\u7edc\u5b9e\u5728\u662f\u592a\u5c0f\u4e86\u3002</p> <p>\u5c1d\u8bd5\u4e00\u4e0b\uff1a\u52a0\u5bbd\u4f60\u7684\u7f51\u7edc(\u6ce8\u610f\u7b2c\u4e00\u4e2a<code>nn.Conv2d</code>\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u548c\u7b2c\u4e8c\u4e2a<code>nn.Conv2d</code>\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u8981\u76f8\u540c\uff09\uff0c\u770b\u770b\u80fd\u83b7\u5f97\u591a\u5c11\u52a0\u901f\u3002</p> <p>\u5df2\u5b9e\u73b0\u7684\u76ee\u6807\uff1a</p> <ul> <li>\u5728\u66f4\u9ad8\u5c42\u6b21\u4e0a\u7406\u89e3PyTorch\u7684Tensor\u5e93\u548c\u795e\u7ecf\u7f51\u7edc</li> <li>\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u505a\u56fe\u7247\u5206\u7c7b</li> </ul>"},{"location":"1.0/blitz_cifar10_tutorial/#gpu_1","title":"\u5728\u591aGPU\u4e0a\u8bad\u7ec3","text":"<p>\u5982\u679c\u5e0c\u671b\u4f7f\u7528\u60a8\u6240\u6709GPU\u83b7\u5f97\u66f4\u5927\u7684\u52a0\u901f\uff0c\u8bf7\u67e5\u770bOptional: Data Parallelism\u3002</p>"},{"location":"1.0/blitz_cifar10_tutorial/#_4","title":"\u63a5\u4e0b\u6765\u8981\u505a\u4ec0\u4e48\uff1f","text":"<ul> <li>Train neural nets to play video games</li> <li>Train a state-of-the-art ResNet network on imagenet</li> <li>Train a face generator using Generative Adversarial Networks</li> <li>Train a word-level language model using Recurrent LSTM networks</li> <li>More examples</li> <li>More tutorials</li> <li>Discuss PyTorch on the Forums</li> <li>Chat with other users on Slack</li> </ul>"},{"location":"1.0/blitz_data_parallel_tutorial/","title":"\u53ef\u9009: \u6570\u636e\u5e76\u884c\u5904\u7406","text":"<p>\u4f5c\u8005: Sung Kim Jenny Kang</p> <p>\u8bd1\u8005: bat67</p> <p>\u6821\u5bf9\u8005: FontTian \u7247\u523b</p> <p>\u5728\u8fd9\u4e2a\u6559\u7a0b\u91cc\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u6570\u636e\u5e76\u884c(<code>DataParallel</code>\uff09\u6765\u4f7f\u7528\u591aGPU\u3002</p> <p>PyTorch\u975e\u5e38\u5bb9\u6613\u7684\u5c31\u53ef\u4ee5\u4f7f\u7528GPU\uff0c\u53ef\u4ee5\u7528\u5982\u4e0b\u65b9\u5f0f\u628a\u4e00\u4e2a\u6a21\u578b\u653e\u5230GPU\u4e0a: </p> <pre><code>device = torch.device(\"cuda: 0\")\nmodel.to(device)\n</code></pre> <p>\u7136\u540e\u53ef\u4ee5\u590d\u5236\u6240\u6709\u7684\u5f20\u91cf\u5230GPU\u4e0a: </p> <pre><code>mytensor = my_tensor.to(device)\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c\u8c03\u7528<code>my_tensor.to(device)</code>\u8fd4\u56de\u4e00\u4e2aGPU\u4e0a\u7684<code>my_tensor</code>\u526f\u672c\uff0c\u800c\u4e0d\u662f\u91cd\u5199<code>my_tensor</code>\u3002\u6211\u4eec\u9700\u8981\u628a\u5b83\u8d4b\u503c\u7ed9\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u5e76\u5728GPU\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u5f20\u91cf\u3002</p> <p>\u5728\u591aGPU\u4e0a\u6267\u884c\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u662f\u81ea\u7136\u800c\u7136\u7684\u4e8b\u3002\u7136\u800c\uff0cPyTorch\u9ed8\u8ba4\u5c06\u53ea\u662f\u7528\u4e00\u4e2aGPU\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528<code>DataParallel</code>\u8ba9\u6a21\u578b\u5e76\u884c\u8fd0\u884c\u6765\u8f7b\u6613\u7684\u8ba9\u4f60\u7684\u64cd\u4f5c\u5728\u591a\u4e2aGPU\u4e0a\u8fd0\u884c\u3002</p> <pre><code>model = nn.DataParallel(model)\n</code></pre> <p>\u8fd9\u662f\u8fd9\u7bc7\u6559\u7a0b\u80cc\u540e\u7684\u6838\u5fc3\uff0c\u6211\u4eec\u63a5\u4e0b\u6765\u5c06\u66f4\u8be6\u7ec6\u7684\u4ecb\u7ecd\u5b83\u3002</p>"},{"location":"1.0/blitz_data_parallel_tutorial/#_2","title":"\u5bfc\u5165\u548c\u53c2\u6570","text":"<p>\u5bfc\u5165PyTorch\u6a21\u5757\u548c\u5b9a\u4e49\u53c2\u6570\u3002</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Parameters \u548c DataLoaders\ninput_size = 5\noutput_size = 2\n\nbatch_size = 30\ndata_size = 100\n</code></pre> <p>\u8bbe\u5907(Device\uff09: </p> <pre><code>device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_3","title":"\u865a\u62df\u6570\u636e\u96c6","text":"<p>\u8981\u5236\u4f5c\u4e00\u4e2a\u865a\u62df(\u968f\u673a\uff09\u6570\u636e\u96c6\uff0c\u53ea\u9700\u5b9e\u73b0<code>__getitem__</code>\u3002</p> <pre><code>class RandomDataset(Dataset):\n\n    def __init__(self, size, length):\n        self.len = length\n        self.data = torch.randn(length, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n\nrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n                         batch_size=batch_size, shuffle=True)\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_4","title":"\u7b80\u5355\u6a21\u578b","text":"<p>\u4f5c\u4e3a\u6f14\u793a\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u6267\u884c\u4e00\u4e2a\u7ebf\u6027\u64cd\u4f5c\uff0c\u7136\u540e\u5f97\u5230\u7ed3\u679c\u3002\u7136\u800c\uff0c\u4f60\u80fd\u5728\u4efb\u4f55\u6a21\u578b(CNN\uff0cRNN\uff0cCapsule Net\u7b49\uff09\u4e0a\u4f7f\u7528<code>DataParallel</code>\u3002</p> <p>\u6211\u4eec\u5728\u6a21\u578b\u5185\u90e8\u653e\u7f6e\u4e86\u4e00\u6761\u6253\u5370\u8bed\u53e5\u6765\u68c0\u6d4b\u8f93\u5165\u548c\u8f93\u51fa\u5411\u91cf\u7684\u5927\u5c0f\u3002\u8bf7\u6ce8\u610f\u6279\u7b49\u7ea7\u4e3a0\u65f6\u6253\u5370\u7684\u5185\u5bb9\u3002</p> <pre><code>class Model(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, output_size):\n        super(Model, self).__init__()\n        self.fc = nn.Linear(input_size, output_size)\n\n    def forward(self, input):\n        output = self.fc(input)\n        print(\"\\tIn Model: input size\", input.size(),\n              \"output size\", output.size())\n\n        return output\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_5","title":"\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c","text":"<p>\u8fd9\u662f\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u90e8\u5206\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8b\u548c\u68c0\u6d4b\u6211\u4eec\u662f\u5426\u6709\u591a\u4e2aGPU\u3002\u5982\u679c\u6211\u4eec\u6709\u591a\u4e2aGPU\uff0c\u6211\u4eec\u4f7f\u7528<code>nn.DataParallel</code>\u6765\u5305\u88c5\u6211\u4eec\u7684\u6a21\u578b\u3002\u7136\u540e\u901a\u8fc7<code>model.to(device)</code>\u628a\u6a21\u578b\u653e\u5230GPU\u4e0a\u3002</p> <pre><code>model = Model(input_size, output_size)\nif torch.cuda.device_count() &gt; 1: \n  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n  # dim = 0 [30, xxx] -&gt; [10, ...], [10, ...], [10, ...] on 3 GPUs\n  model = nn.DataParallel(model)\n\nmodel.to(device)\n</code></pre> <p>\u8f93\u51fa: </p> <pre><code>Let's use 2 GPUs!\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_6","title":"\u8fd0\u884c\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u770b\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\u3002</p> <pre><code>for data in rand_loader: \n    input = data.to(device)\n    output = model(input)\n    print(\"Outside: input size\", input.size(),\n          \"output_size\", output.size())\n</code></pre> <p>\u8f93\u51fa: </p> <pre><code>In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n        In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n        In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_7","title":"\u7ed3\u679c","text":"<p>\u5f53\u6211\u4eec\u5bf930\u4e2a\u8f93\u5165\u548c\u8f93\u51fa\u8fdb\u884c\u6279\u5904\u7406\u65f6\uff0c\u6211\u4eec\u548c\u671f\u671b\u7684\u4e00\u6837\u5f97\u523030\u4e2a\u8f93\u5165\u548c30\u4e2a\u8f93\u51fa\uff0c\u4f46\u662f\u82e5\u6709\u591a\u4e2aGPU\uff0c\u4f1a\u5f97\u5230\u5982\u4e0b\u7684\u7ed3\u679c\u3002</p>"},{"location":"1.0/blitz_data_parallel_tutorial/#2gpu","title":"2\u4e2aGPU","text":"<p>\u82e5\u67092\u4e2aGPU\uff0c\u5c06\u770b\u5230: </p> <pre><code>Let's use 2 GPUs!\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n    In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n    In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#3gpu","title":"3\u4e2aGPU","text":"<p>\u82e5\u67093\u4e2aGPU\uff0c\u5c06\u770b\u5230: </p> <pre><code>Let's use 3 GPUs!\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#8gpu","title":"8\u4e2aGPU","text":"<p>\u82e5\u67098\u4e2aGPU\uff0c\u5c06\u770b\u5230: </p> <pre><code>Let's use 8 GPUs!\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\nOutside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n    In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\nOutside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n</code></pre>"},{"location":"1.0/blitz_data_parallel_tutorial/#_8","title":"\u603b\u7ed3","text":"<p><code>DataParallel</code>\u81ea\u52a8\u7684\u5212\u5206\u6570\u636e\uff0c\u5e76\u5c06\u4f5c\u4e1a\u53d1\u9001\u5230\u591a\u4e2aGPU\u4e0a\u7684\u591a\u4e2a\u6a21\u578b\u3002<code>DataParallel</code>\u4f1a\u5728\u6bcf\u4e2a\u6a21\u578b\u5b8c\u6210\u4f5c\u4e1a\u540e\uff0c\u6536\u96c6\u4e0e\u5408\u5e76\u7ed3\u679c\u7136\u540e\u8fd4\u56de\u7ed9\u4f60\u3002</p> <p>\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html</p>"},{"location":"1.0/blitz_neural_networks_tutorial/","title":"\u795e\u7ecf\u7f51\u7edc","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u53ef\u4ee5\u4f7f\u7528<code>torch.nn</code>\u5305\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc.</p> <p>\u6211\u4eec\u5df2\u7ecf\u4ecb\u7ecd\u4e86<code>autograd</code>\uff0c<code>nn</code>\u5305\u5219\u4f9d\u8d56\u4e8e<code>autograd</code>\u5305\u6765\u5b9a\u4e49\u6a21\u578b\u5e76\u5bf9\u5b83\u4eec\u6c42\u5bfc\u3002\u4e00\u4e2a<code>nn.Module</code>\u5305\u542b\u5404\u4e2a\u5c42\u548c\u4e00\u4e2a<code>forward(input)</code>\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de<code>output</code>\u3002</p> <p>\u4f8b\u5982\uff0c\u4e0b\u9762\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5bf9\u6570\u5b57\u8fdb\u884c\u5206\u7c7b\uff1a</p> <p></p> <p>\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc(feed-forward network\uff09\u3002\u5b83\u63a5\u53d7\u4e00\u4e2a\u8f93\u5165\uff0c\u7136\u540e\u5c06\u5b83\u9001\u5165\u4e0b\u4e00\u5c42\uff0c\u4e00\u5c42\u63a5\u4e00\u5c42\u7684\u4f20\u9012\uff0c\u6700\u540e\u7ed9\u51fa\u8f93\u51fa\u3002</p> <p>\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u5178\u578b\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\uff1a</p> <ul> <li>\u5b9a\u4e49\u5305\u542b\u4e00\u4e9b\u53ef\u5b66\u4e60\u53c2\u6570(\u6216\u8005\u53eb\u6743\u91cd\uff09\u7684\u795e\u7ecf\u7f51\u7edc</li> <li>\u5728\u8f93\u5165\u6570\u636e\u96c6\u4e0a\u8fed\u4ee3</li> <li>\u901a\u8fc7\u7f51\u7edc\u5904\u7406\u8f93\u5165</li> <li>\u8ba1\u7b97\u635f\u5931(\u8f93\u51fa\u548c\u6b63\u786e\u7b54\u6848\u7684\u8ddd\u79bb\uff09</li> <li>\u5c06\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u7ed9\u7f51\u7edc\u7684\u53c2\u6570</li> <li>\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u4e00\u822c\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u89c4\u5219\uff1a<code>weight = weight - learning_rate * gradient</code></li> </ul>"},{"location":"1.0/blitz_neural_networks_tutorial/#_2","title":"\u5b9a\u4e49\u7f51\u7edc","text":"<p>\u8ba9\u6211\u4eec\u5b9a\u4e49\u8fd9\u6837\u4e00\u4e2a\u7f51\u7edc\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # \u8f93\u5165\u56fe\u50cfchannel\uff1a1\uff1b\u8f93\u51fachannel\uff1a6\uff1b5x5\u5377\u79ef\u6838\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # 2x2 Max pooling\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # \u5982\u679c\u662f\u65b9\u9635,\u5219\u53ef\u4ee5\u53ea\u4f7f\u7528\u4e00\u4e2a\u6570\u5b57\u8fdb\u884c\u5b9a\u4e49\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # \u9664\u53bb\u6279\u5904\u7406\u7ef4\u5ea6\u7684\u5176\u4ed6\u6240\u6709\u7ef4\u5ea6\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\nnet = Net()\nprint(net)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Net(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n</code></pre> <p>\u6211\u4eec\u53ea\u9700\u8981\u5b9a\u4e49 <code>forward</code> \u51fd\u6570\uff0c<code>backward</code>\u51fd\u6570\u4f1a\u5728\u4f7f\u7528<code>autograd</code>\u65f6\u81ea\u52a8\u5b9a\u4e49\uff0c<code>backward</code>\u51fd\u6570\u7528\u6765\u8ba1\u7b97\u5bfc\u6570\u3002\u53ef\u4ee5\u5728 <code>forward</code> \u51fd\u6570\u4e2d\u4f7f\u7528\u4efb\u4f55\u9488\u5bf9\u5f20\u91cf\u7684\u64cd\u4f5c\u548c\u8ba1\u7b97\u3002</p> <p>\u4e00\u4e2a\u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7<code>net.parameters()</code>\u8fd4\u56de</p> <pre><code>params = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>10\ntorch.Size([6, 1, 5, 5])\n</code></pre> <p>\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e00\u4e2a\u968f\u673a\u768432x32\u7684\u8f93\u5165\u3002\u6ce8\u610f:\u8fd9\u4e2a\u7f51\u7edc(LeNet\uff09\u7684\u671f\u5f85\u8f93\u5165\u662f32x32\u3002\u5982\u679c\u4f7f\u7528MNIST\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\uff0c\u8981\u628a\u56fe\u7247\u5927\u5c0f\u91cd\u65b0\u8c03\u6574\u523032x32\u3002</p> <pre><code>input = torch.randn(1, 1, 32, 32)\nout = net(input)\nprint(out)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 0.0399, -0.0856,  0.0668,  0.0915,  0.0453, -0.0680, -0.1024,  0.0493,\n         -0.1043, -0.1267]], grad_fn=&lt;AddmmBackward&gt;)\n</code></pre> <p>\u6e05\u96f6\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u7f13\u5b58\uff0c\u7136\u540e\u8fdb\u884c\u968f\u673a\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\uff1a</p> <pre><code>net.zero_grad()\nout.backward(torch.randn(1, 10))\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p><code>torch.nn</code>\u53ea\u652f\u6301\u5c0f\u6279\u91cf\u5904\u7406(mini-batches\uff09\u3002\u6574\u4e2a<code>torch.nn</code>\u5305\u53ea\u652f\u6301\u5c0f\u6279\u91cf\u6837\u672c\u7684\u8f93\u5165\uff0c\u4e0d\u652f\u6301\u5355\u4e2a\u6837\u672c\u3002</p> <p>\u6bd4\u5982\uff0c<code>nn.Conv2d</code> \u63a5\u53d7\u4e00\u4e2a4\u7ef4\u7684\u5f20\u91cf\uff0c\u5373<code>nSamples x nChannels x Height x Width</code></p> <p>\u5982\u679c\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u6837\u672c\uff0c\u53ea\u9700\u8981\u4f7f\u7528<code>input.unsqueeze(0)</code>\u6765\u6dfb\u52a0\u4e00\u4e2a\u201c\u5047\u7684\u201d\u6279\u5927\u5c0f\u7ef4\u5ea6\u3002</p> <p>\u5728\u7ee7\u7eed\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u56de\u987e\u4e00\u4e0b\u5230\u76ee\u524d\u4e3a\u6b62\u770b\u5230\u7684\u6240\u6709\u7c7b\u3002</p> <p>\u590d\u4e60\uff1a</p> <ul> <li> <p><code>torch.Tensor</code> - \u4e00\u4e2a\u591a\u7ef4\u6570\u7ec4\uff0c\u652f\u6301\u8bf8\u5982<code>backward()</code>\u7b49\u7684\u81ea\u52a8\u6c42\u5bfc\u64cd\u4f5c\uff0c\u540c\u65f6\u4e5f\u4fdd\u5b58\u4e86\u5f20\u91cf\u7684\u68af\u5ea6\u3002</p> </li> <li> <p><code>nn.Module</code> - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u3002\u662f\u4e00\u79cd\u65b9\u4fbf\u5c01\u88c5\u53c2\u6570\u7684\u65b9\u5f0f\uff0c\u5177\u6709\u5c06\u53c2\u6570\u79fb\u52a8\u5230GPU\u3001\u5bfc\u51fa\u3001\u52a0\u8f7d\u7b49\u529f\u80fd\u3002</p> </li> <li> <p><code>nn.Parameter</code> - \u5f20\u91cf\u7684\u4e00\u79cd\uff0c\u5f53\u5b83\u4f5c\u4e3a\u4e00\u4e2a\u5c5e\u6027\u5206\u914d\u7ed9\u4e00\u4e2a<code>Module</code>\u65f6\uff0c\u5b83\u4f1a\u88ab\u81ea\u52a8\u6ce8\u518c\u4e3a\u4e00\u4e2a\u53c2\u6570\u3002</p> </li> <li> <p><code>autograd.Function</code> - \u5b9e\u73b0\u4e86\u81ea\u52a8\u6c42\u5bfc\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7684\u5b9a\u4e49\uff0c\u6bcf\u4e2a<code>Tensor</code>\u81f3\u5c11\u521b\u5efa\u4e00\u4e2a<code>Function</code>\u8282\u70b9\uff0c\u8be5\u8282\u70b9\u8fde\u63a5\u5230\u521b\u5efa<code>Tensor</code>\u7684\u51fd\u6570\u5e76\u5bf9\u5176\u5386\u53f2\u8fdb\u884c\u7f16\u7801\u3002</p> </li> </ul> <p>\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\uff1a</p> <ul> <li>\u5b9a\u4e49\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc</li> <li>\u5904\u7406\u8f93\u5165\u8c03\u7528<code>backward</code></li> </ul> <p>\u8fd8\u5269\u4e0b\uff1a</p> <ul> <li>\u8ba1\u7b97\u635f\u5931</li> <li>\u66f4\u65b0\u7f51\u7edc\u6743\u91cd</li> </ul>"},{"location":"1.0/blitz_neural_networks_tutorial/#_3","title":"\u635f\u5931\u51fd\u6570","text":"<p>\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u63a5\u53d7\u4e00\u5bf9(output, target)\u4f5c\u4e3a\u8f93\u5165\uff0c\u8ba1\u7b97\u4e00\u4e2a\u503c\u6765\u4f30\u8ba1\u7f51\u7edc\u7684\u8f93\u51fa\u548c\u76ee\u6807\u503c\u76f8\u5dee\u591a\u5c11\u3002</p> <p>nn\u5305\u4e2d\u6709\u5f88\u591a\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u3002<code>nn.MSELoss</code>\u662f\u6bd4\u8f83\u7b80\u5355\u7684\u4e00\u79cd\uff0c\u5b83\u8ba1\u7b97\u8f93\u51fa\u548c\u76ee\u6807\u7684\u5747\u65b9\u8bef\u5dee(mean-squared error\uff09\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>output = net(input)\ntarget = torch.randn(10)  # \u672c\u4f8b\u5b50\u4e2d\u4f7f\u7528\u6a21\u62df\u6570\u636e\ntarget = target.view(1, -1)  # \u4f7f\u76ee\u6807\u503c\u4e0e\u6570\u636e\u503c\u5f62\u72b6\u4e00\u81f4\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(1.0263, grad_fn=&lt;MseLossBackward&gt;)\n</code></pre> <p>\u73b0\u5728\uff0c\u5982\u679c\u4f7f\u7528<code>loss</code>\u7684<code>.grad_fn</code>\u5c5e\u6027\u8ddf\u8e2a\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\uff0c\u4f1a\u770b\u5230\u8ba1\u7b97\u56fe\u5982\u4e0b\uff1a</p> <pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d\n      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear\n      -&gt; MSELoss\n      -&gt; loss\n</code></pre> <p>\u6240\u4ee5\uff0c\u5f53\u6211\u4eec\u8c03\u7528<code>loss.backward()</code>\uff0c\u6574\u5f20\u56fe\u5f00\u59cb\u5173\u4e8eloss\u5fae\u5206\uff0c\u56fe\u4e2d\u6240\u6709\u8bbe\u7f6e\u4e86<code>requires_grad=True</code>\u7684\u5f20\u91cf\u7684<code>.grad</code>\u5c5e\u6027\u7d2f\u79ef\u7740\u68af\u5ea6\u5f20\u91cf\u3002</p> <p>\u4e3a\u4e86\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u8ba9\u6211\u4eec\u5411\u540e\u8ddf\u8e2a\u51e0\u6b65\uff1a</p> <pre><code>print(loss.grad_fn)  # MSELoss\nprint(loss.grad_fn.next_functions[0][0])  # Linear\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>&lt;MseLossBackward object at 0x7f94c821fdd8&gt;\n&lt;AddmmBackward object at 0x7f94c821f6a0&gt;\n&lt;AccumulateGrad object at 0x7f94c821f6a0&gt;\n</code></pre>"},{"location":"1.0/blitz_neural_networks_tutorial/#_4","title":"\u53cd\u5411\u4f20\u64ad","text":"<p>\u6211\u4eec\u53ea\u9700\u8981\u8c03\u7528<code>loss.backward()</code>\u6765\u53cd\u5411\u4f20\u64ad\u6743\u91cd\u3002\u6211\u4eec\u9700\u8981\u6e05\u96f6\u73b0\u6709\u7684\u68af\u5ea6\uff0c\u5426\u5219\u68af\u5ea6\u5c06\u4f1a\u4e0e\u5df2\u6709\u7684\u68af\u5ea6\u7d2f\u52a0\u3002</p> <p>\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u8c03\u7528<code>loss.backward()</code>\uff0c\u5e76\u67e5\u770bconv1\u5c42\u7684\u504f\u7f6e(bias\uff09\u5728\u53cd\u5411\u4f20\u64ad\u524d\u540e\u7684\u68af\u5ea6\u3002</p> <pre><code>net.zero_grad()     # \u6e05\u96f6\u6240\u6709\u53c2\u6570(parameter\uff09\u7684\u68af\u5ea6\u7f13\u5b58\n\nprint('conv1.bias.grad before backward')\nprint(net.conv1.bias.grad)\n\nloss.backward()\n\nprint('conv1.bias.grad after backward')\nprint(net.conv1.bias.grad)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([ 0.0084,  0.0019, -0.0179, -0.0212,  0.0067, -0.0096])\n</code></pre> <p>\u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u89c1\u5230\u4e86\u5982\u4f55\u4f7f\u7528\u635f\u5931\u51fd\u6570\u3002</p> <p>\u7a0d\u540e\u9605\u8bfb</p> <p>\u795e\u7ecf\u7f51\u7edc\u5305\u5305\u542b\u4e86\u5404\u79cd\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e9b\u6a21\u5757\u548c\u635f\u5931\u51fd\u6570\u6784\u6210\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u6a21\u5757\u3002\u5b8c\u6574\u7684\u6587\u6863\u5217\u8868\u89c1\u8fd9\u91cc\u3002</p> <p>\u73b0\u5728\u552f\u4e00\u8981\u5b66\u4e60\u7684\u662f\uff1a</p> <ul> <li>\u66f4\u65b0\u7f51\u7edc\u7684\u6743\u91cd</li> </ul>"},{"location":"1.0/blitz_neural_networks_tutorial/#_5","title":"\u66f4\u65b0\u6743\u91cd","text":"<p>\u6700\u7b80\u5355\u7684\u66f4\u65b0\u89c4\u5219\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6cd5(SGD\uff09:</p> <p><code>weight = weight - learning_rate * gradient</code></p> <p>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684python\u4ee3\u7801\u6765\u5b9e\u73b0:</p> <pre><code>learning_rate = 0.01\nfor f in net.parameters():\n    f.data.sub_(f.grad.data * learning_rate)\n</code></pre> <p>\u7136\u800c\uff0c\u5728\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u5982SGD\u3001Nesterov-SGD\u3001Adam\u3001RMSProp\u7b49\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u8f83\u5c0f\u7684\u5305<code>torch.optim</code>\uff0c\u5b83\u5b9e\u73b0\u4e86\u6240\u6709\u7684\u8fd9\u4e9b\u65b9\u6cd5\u3002\u4f7f\u7528\u5b83\u5f88\u7b80\u5355\uff1a</p> <pre><code>import torch.optim as optim\n\n# \u521b\u5efa\u4f18\u5316\u5668(optimizer\uff09\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\n# \u5728\u8bad\u7ec3\u7684\u8fed\u4ee3\u4e2d\uff1a\noptimizer.zero_grad()   # \u6e05\u96f6\u68af\u5ea6\u7f13\u5b58\noutput = net(input)\nloss = criterion(output, target)\nloss.backward()\noptimizer.step()    # \u66f4\u65b0\u53c2\u6570\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p>\u89c2\u5bdf\u68af\u5ea6\u7f13\u5b58\u533a\u662f\u5982\u4f55\u4f7f\u7528<code>optimizer.zero_grad()</code>\u624b\u52a8\u6e05\u96f6\u7684\u3002\u8fd9\u662f\u56e0\u4e3a\u68af\u5ea6\u662f\u7d2f\u52a0\u7684\uff0c\u6b63\u5982\u524d\u9762\u53cd\u5411\u4f20\u64ad\u7ae0\u8282\u53d9\u8ff0\u7684\u90a3\u6837\u3002</p>"},{"location":"1.0/blitz_tensor_tutorial/","title":"\u4ec0\u4e48\u662fPyTorch\uff1f","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005\uff1a Soumith Chintala</p> <p>PyTorch\u662f\u4e00\u4e2a\u57fa\u4e8epython\u7684\u79d1\u5b66\u8ba1\u7b97\u5305\uff0c\u4e3b\u8981\u9488\u5bf9\u4e24\u7c7b\u4eba\u7fa4\uff1a</p> <ul> <li>\u4f5c\u4e3aNumPy\u7684\u66ff\u4ee3\u54c1\uff0c\u53ef\u4ee5\u5229\u7528GPU\u7684\u6027\u80fd\u8fdb\u884c\u8ba1\u7b97</li> <li>\u4f5c\u4e3a\u4e00\u4e2a\u9ad8\u7075\u6d3b\u6027\u3001\u901f\u5ea6\u5feb\u7684\u6df1\u5ea6\u5b66\u4e60\u5e73\u53f0</li> </ul>"},{"location":"1.0/blitz_tensor_tutorial/#_1","title":"\u5165\u95e8","text":""},{"location":"1.0/blitz_tensor_tutorial/#_2","title":"\u5f20\u91cf","text":"<p><code>Tensor</code>(\u5f20\u91cf\uff09\u7c7b\u4f3c\u4e8e<code>NumPy</code>\u7684<code>ndarray</code>\uff0c\u4f46\u8fd8\u53ef\u4ee5\u5728GPU\u4e0a\u4f7f\u7528\u6765\u52a0\u901f\u8ba1\u7b97\u3002</p> <pre><code>from __future__ import print_function\nimport torch\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u6ca1\u6709\u521d\u59cb\u5316\u76845*3\u77e9\u9635\uff1a</p> <pre><code>x = torch.empty(5, 3)\nprint(x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[2.2391e-19, 4.5869e-41, 1.4191e-17],\n        [4.5869e-41, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u968f\u673a\u521d\u59cb\u5316\u77e9\u9635\uff1a</p> <pre><code>x = torch.rand(5, 3)\nprint(x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[0.5307, 0.9752, 0.5376],\n        [0.2789, 0.7219, 0.1254],\n        [0.6700, 0.6100, 0.3484],\n        [0.0922, 0.0779, 0.2446],\n        [0.2967, 0.9481, 0.1311]])\n</code></pre> <p>\u6784\u9020\u4e00\u4e2a\u586b\u6ee1<code>0</code>\u4e14\u6570\u636e\u7c7b\u578b\u4e3a<code>long</code>\u7684\u77e9\u9635:</p> <pre><code>x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n</code></pre> <p>\u76f4\u63a5\u4ece\u6570\u636e\u6784\u9020\u5f20\u91cf\uff1a</p> <pre><code>x = torch.tensor([5.5, 3])\nprint(x)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([5.5000, 3.0000])\n</code></pre> <p>\u6216\u8005\u6839\u636e\u5df2\u6709\u7684tensor\u5efa\u7acb\u65b0\u7684tensor\u3002\u9664\u975e\u7528\u6237\u63d0\u4f9b\u65b0\u7684\u503c\uff0c\u5426\u5219\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u91cd\u7528\u8f93\u5165\u5f20\u91cf\u7684\u5c5e\u6027\uff0c\u4f8b\u5982dtype\u7b49\uff1a</p> <pre><code>x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)    # \u91cd\u8f7d dtype!\nprint(x)                                      # \u7ed3\u679csize\u4e00\u81f4\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[ 1.6040, -0.6769,  0.0555],\n        [ 0.6273,  0.7683, -0.2838],\n        [-0.7159, -0.5566, -0.2020],\n        [ 0.6266,  0.3566,  1.4497],\n        [-0.8092, -0.6741,  0.0406]])\n</code></pre> <p>\u83b7\u53d6\u5f20\u91cf\u7684\u5f62\u72b6\uff1a</p> <pre><code>print(x.size())\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>torch.Size([5, 3])\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p><code>torch.Size</code>\u672c\u8d28\u4e0a\u8fd8\u662f<code>tuple</code>\uff0c\u6240\u4ee5\u652f\u6301tuple\u7684\u4e00\u5207\u64cd\u4f5c\u3002</p>"},{"location":"1.0/blitz_tensor_tutorial/#_3","title":"\u8fd0\u7b97","text":"<p>\u4e00\u79cd\u8fd0\u7b97\u6709\u591a\u79cd\u8bed\u6cd5\u3002\u5728\u4e0b\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u7814\u7a76\u52a0\u6cd5\u8fd0\u7b97\u3002</p> <p>\u52a0\u6cd5\uff1a\u5f62\u5f0f\u4e00</p> <pre><code>y = torch.rand(5, 3)\nprint(x + y)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 2.5541,  0.0943,  0.9835],\n        [ 1.4911,  1.3117,  0.5220],\n        [-0.0078, -0.1161,  0.6687],\n        [ 0.8176,  1.1179,  1.9194],\n        [-0.3251, -0.2236,  0.7653]])\n</code></pre> <p>\u52a0\u6cd5\uff1a\u5f62\u5f0f\u4e8c</p> <pre><code>print(torch.add(x, y))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 2.5541,  0.0943,  0.9835],\n        [ 1.4911,  1.3117,  0.5220],\n        [-0.0078, -0.1161,  0.6687],\n        [ 0.8176,  1.1179,  1.9194],\n        [-0.3251, -0.2236,  0.7653]])\n</code></pre> <p>\u52a0\u6cd5\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\u4f5c\u4e3a\u53c2\u6570</p> <pre><code>result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 2.5541,  0.0943,  0.9835],\n        [ 1.4911,  1.3117,  0.5220],\n        [-0.0078, -0.1161,  0.6687],\n        [ 0.8176,  1.1179,  1.9194],\n        [-0.3251, -0.2236,  0.7653]])\n</code></pre> <p>\u52a0\u6cd5\uff1a\u539f\u4f4d/\u539f\u5730\u64cd\u4f5c(in-place\uff09</p> <pre><code># adds x to y\ny.add_(x)\nprint(y)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 2.5541,  0.0943,  0.9835],\n        [ 1.4911,  1.3117,  0.5220],\n        [-0.0078, -0.1161,  0.6687],\n        [ 0.8176,  1.1179,  1.9194],\n        [-0.3251, -0.2236,  0.7653]])\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p>\u4efb\u4f55\u4e00\u4e2ain-place\u6539\u53d8\u5f20\u91cf\u7684\u64cd\u4f5c\u540e\u9762\u90fd\u56fa\u5b9a\u4e00\u4e2a<code>_</code>\u3002\u4f8b\u5982<code>x.copy_(y)</code>\u3001<code>x.t_()</code>\u5c06\u66f4\u6539x</p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528\u50cf\u6807\u51c6\u7684NumPy\u4e00\u6837\u7684\u5404\u79cd\u7d22\u5f15\u64cd\u4f5c\uff1a</p> <pre><code>print(x[:, 1])\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([-0.6769,  0.7683, -0.5566,  0.3566, -0.6741])\n</code></pre> <p>\u6539\u53d8\u5f62\u72b6\uff1a\u5982\u679c\u60f3\u6539\u53d8\u5f62\u72b6\uff0c\u53ef\u4ee5\u4f7f\u7528<code>torch.view</code></p> <pre><code>x = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(x.size(), y.size(), z.size())\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n</code></pre> <p>\u5982\u679c\u662f\u4ec5\u5305\u542b\u4e00\u4e2a\u5143\u7d20\u7684tensor\uff0c\u53ef\u4ee5\u4f7f\u7528<code>.item()</code>\u6765\u5f97\u5230\u5bf9\u5e94\u7684python\u6570\u503c</p> <pre><code>x = torch.randn(1)\nprint(x)\nprint(x.item())\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([0.0445])\n0.0445479191839695\n</code></pre> <p>\u540e\u7eed\u9605\u8bfb\uff1a</p> <p>\u8d85\u8fc7100\u79cdtensor\u7684\u8fd0\u7b97\u64cd\u4f5c\uff0c\u5305\u62ec\u8f6c\u7f6e\uff0c\u7d22\u5f15\uff0c\u5207\u7247\uff0c\u6570\u5b66\u8fd0\u7b97\uff0c \u7ebf\u6027\u4ee3\u6570\uff0c\u968f\u673a\u6570\u7b49\uff0c\u5177\u4f53\u8bbf\u95ee\u8fd9\u91cc</p>"},{"location":"1.0/blitz_tensor_tutorial/#numpy","title":"\u6865\u63a5 NumPy","text":"<p>\u5c06\u4e00\u4e2aTorch\u5f20\u91cf\u8f6c\u6362\u4e3a\u4e00\u4e2aNumPy\u6570\u7ec4\u662f\u8f7b\u800c\u6613\u4e3e\u7684\u4e8b\u60c5\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002</p> <p>Torch\u5f20\u91cf\u548cNumPy\u6570\u7ec4\u5c06\u5171\u4eab\u5b83\u4eec\u7684\u5e95\u5c42\u5185\u5b58\u4f4d\u7f6e\uff0c\u56e0\u6b64\u5f53\u4e00\u4e2a\u6539\u53d8\u65f6,\u53e6\u5916\u4e5f\u4f1a\u6539\u53d8\u3002</p>"},{"location":"1.0/blitz_tensor_tutorial/#torchtensornumpy","title":"\u5c06torch\u7684Tensor\u8f6c\u5316\u4e3aNumPy\u6570\u7ec4","text":"<p>\u8f93\u5165\uff1a</p> <pre><code>a = torch.ones(5)\nprint(a)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([1., 1., 1., 1., 1.])\n</code></pre> <p>\u8f93\u5165\uff1a</p> <pre><code>b = a.numpy()\nprint(b)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>[1. 1. 1. 1. 1.]\n</code></pre> <p>\u770bNumPy\u6570\u7ec4\u662f\u5982\u4f55\u6539\u53d8\u91cc\u9762\u7684\u503c\u7684\uff1a</p> <pre><code>a.add_(1)\nprint(a)\nprint(b)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([2., 2., 2., 2., 2.])\n[2. 2. 2. 2. 2.]\n</code></pre>"},{"location":"1.0/blitz_tensor_tutorial/#numpytorch","title":"\u5c06NumPy\u6570\u7ec4\u8f6c\u5316\u4e3aTorch\u5f20\u91cf","text":"<p>\u770b\u6539\u53d8NumPy\u6570\u7ec4\u662f\u5982\u4f55\u81ea\u52a8\u6539\u53d8Torch\u5f20\u91cf\u7684\uff1a</p> <pre><code>import numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>[2. 2. 2. 2. 2.]\ntensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n</code></pre> <p>CPU\u4e0a\u7684\u6240\u6709\u5f20\u91cf(CharTensor\u9664\u5916)\u90fd\u652f\u6301\u4e0eNumpy\u7684\u76f8\u4e92\u8f6c\u6362\u3002</p>"},{"location":"1.0/blitz_tensor_tutorial/#cuda","title":"CUDA\u4e0a\u7684\u5f20\u91cf","text":"<p>\u5f20\u91cf\u53ef\u4ee5\u4f7f\u7528<code>.to</code>\u65b9\u6cd5\u79fb\u52a8\u5230\u4efb\u4f55\u8bbe\u5907(device\uff09\u4e0a\uff1a</p> <pre><code># \u5f53GPU\u53ef\u7528\u65f6,\u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\n# \u6211\u4eec\u5c06\u4f7f\u7528`torch.device`\u6765\u5c06tensor\u79fb\u5165\u548c\u79fb\u51faGPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    y = torch.ones_like(x, device=device)  # \u76f4\u63a5\u5728GPU\u4e0a\u521b\u5efatensor\n    x = x.to(device)                       # \u6216\u8005\u4f7f\u7528`.to(\"cuda\")`\u65b9\u6cd5\n    z = x + y\n    print(z)\n    print(z.to(\"cpu\", torch.double))       # `.to`\u4e5f\u80fd\u5728\u79fb\u52a8\u65f6\u6539\u53d8dtype\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([1.0445], device='cuda:0')\ntensor([1.0445], dtype=torch.float64)\n</code></pre>"},{"location":"1.0/bottleneck/","title":"torch.utils.bottleneck","text":"<p>\u8bd1\u8005:  belonHan</p> <p><code>torch.utils.bottleneck</code>\u662f \u8c03\u8bd5\u74f6\u9888<code>bottleneck</code>\u65f6\u9996\u5148\u7528\u5230\u7684\u5de5\u5177.\u5b83\u603b\u7ed3\u4e86python\u5206\u6790\u5de5\u5177\u4e0ePyTorch\u81ea\u52a8\u68af\u5ea6\u5206\u6790\u5de5\u5177\u5728\u811a\u672c\u8fd0\u884c\u4e2d\u60c5\u51b5.</p> <p>\u5728\u547d\u4ee4\u884c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4</p> <pre><code>python -m torch.utils.bottleneck /path/to/source/script.py [args]\n\n</code></pre> <p>\u5176\u4e2d <code>[args]</code> \u662f<code>script.py</code>\u811a\u672c\u7684\u53c2\u6570(\u4efb\u610f\u4e2a\u6570).\u8fd0\u884c<code>python -m torch.utils.bottleneck -h</code>\u547d\u4ee4\u83b7\u53d6\u66f4\u591a\u5e2e\u52a9\u8bf4\u660e.</p> <p>\u8b66\u544a</p> <p>\u8bf7\u786e\u4fdd\u811a\u672c\u5728\u5206\u6790\u65f6\u80fd\u591f\u5728\u6709\u9650\u65f6\u95f4\u5185\u9000\u51fa.</p> <p>\u8b66\u544a</p> <p>\u5f53\u8fd0\u884cCUDA\u4ee3\u7801\u65f6\uff0c\u7531\u4e8eCUDA\u5185\u6838\u7684\u5f02\u6b65\u7279\u6027, cProfile\u7684\u8f93\u51fa \u548ccpu\u6a21\u5f0f\u7684autograd\u5206\u6790\u5de5\u5177\u53ef\u80fd\u65e0\u6cd5\u663e\u793a\u6b63\u786e\u7684\u8ba1\u65f6: \u62a5\u544a\u7684CPU\u65f6\u95f4 \u662f\u7528\u4e8e\u542f\u52a8\u5185\u6838\u7684\u65f6\u95f4,\u4e0d\u5305\u62ec\u5728GPU\u4e0a\u6267\u884c\u7684\u65f6\u95f4\u3002 \u5728\u5e38\u89c4cpu\u6a21\u5f0f\u5206\u6790\u5668\u4e0b\uff0c\u540c\u6b65\u64cd\u4f5c\u662f\u975e\u5e38\u6602\u8d35\u7684\u3002\u5728\u8fd9\u79cd\u65e0\u6cd5\u51c6\u786e\u8ba1\u65f6\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f7f\u7528cuda\u6a21\u5f0f\u7684autograd\u5206\u6790\u5de5\u5177\u3002</p> <p>\u6ce8\u610f</p> <p>\u9009\u62e9\u67e5\u770b\u54ea\u4e2a\u5206\u6790\u5de5\u5177\u7684\u8f93\u51fa\u7ed3\u679c(CPU\u6a21\u5f0f\u8fd8\u662fCUDA\u6a21\u5f0f) ,\u9996\u5148\u5e94\u786e\u5b9a\u811a\u672c\u662f\u4e0d\u662fCPU\u5bc6\u96c6\u578b<code>CPU-bound</code>(\u201cCPU\u603b\u65f6\u95f4\u8fdc\u5927\u4e8eCUDA\u603b\u65f6\u95f4\u201d)\u3002\u5982\u679c\u662fcpu\u5bc6\u96c6\u578b\uff0c\u9009\u62e9\u67e5\u770bcpu\u6a21\u5f0f\u7684\u7ed3\u679c\u3002\u76f8\u53cd\uff0c\u5982\u679c\u5927\u90e8\u5206\u65f6\u95f4\u90fd\u8fd0\u884c\u5728GPU\u4e0a\uff0c\u518d\u67e5\u770bCUDA\u5206\u6790\u7ed3\u679c\u4e2d\u76f8\u5e94\u7684CUDA\u64cd\u4f5c\u3002</p> <p>\u5f53\u7136\uff0c\u5b9e\u9645\u60c5\u51b5\u53d6\u51b3\u4e8e\u60a8\u7684\u6a21\u578b\uff0c\u53ef\u80fd\u4f1a\u66f4\u590d\u6742\uff0c\u4e0d\u5c5e\u4e8e\u4e0a\u9762\u4e24\u79cd\u6781\u7aef\u60c5\u51b5\u3002\u9664\u4e86\u5206\u6790\u7ed3\u679c\u4e4b\u5916,\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528<code>nvprof</code>\u547d\u4ee4\u67e5\u770b<code>torch.autograd.profiler.emit_nvtx()</code>\u7684\u7ed3\u679c.\u7136\u800c\u9700\u8981\u6ce8\u610fNVTX\u7684\u5f00\u9500\u662f\u975e\u5e38\u9ad8\u7684,\u65f6\u95f4\u7ebf\u7ecf\u5e38\u4f1a\u6709\u4e25\u91cd\u7684\u504f\u5dee\u3002</p> <p>\u8b66\u544a</p> <p>\u5982\u679c\u60a8\u5728\u5206\u6790CUDA\u4ee3\u7801, <code>bottleneck</code>\u8fd0\u884c\u7684\u7b2c\u4e00\u4e2a\u5206\u6790\u5de5\u5177 (cProfile),\u5b83\u7684\u65f6\u95f4\u4e2d\u4f1a\u5305\u542bCUDA\u7684\u542f\u52a8(CUDA\u7f13\u5b58\u5206\u914d)\u65f6\u95f4\u3002\u5f53\u7136\uff0c\u5982\u679cCUDA\u542f\u52a8\u65f6\u95f4\u8fdc\u5c0f\u4e8e\u4ee3\u7801\u7684\u4e2d\u74f6\u9888,\u8fd9\u5c31\u88ab\u53ef\u4ee5\u5ffd\u7565\u3002</p> <p>\u66f4\u591a\u66f4\u590d\u6742\u5173\u4e8e\u5206\u6790\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5(\u6bd4\u5982\u591aGPU),\u8bf7\u70b9\u51fbhttps://docs.python.org/3/library/profile.html \u6216\u8005 <code>torch.autograd.profiler.profile()</code>.</p>"},{"location":"1.0/char_rnn_classification_tutorial/","title":"\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684RNN\u7f51\u7edc\u8fdb\u884c\u540d\u5b57\u5206\u7c7b","text":"<p>\u8bd1\u8005\uff1ahhxx2015</p> <p>\u6821\u5bf9\u8005\uff1ahijkzzz</p> <p>\u4f5c\u8005: Sean Robertson</p> <p>\u6211\u4eec\u5c06\u6784\u5efa\u548c\u8bad\u7ec3\u5b57\u7b26\u7ea7RNN\u6765\u5bf9\u5355\u8bcd\u8fdb\u884c\u5206\u7c7b\u3002 \u5b57\u7b26\u7ea7RNN\u5c06\u5355\u8bcd\u4f5c\u4e3a\u4e00\u7cfb\u5217\u5b57\u7b26\u8bfb\u53d6\uff0c\u5728\u6bcf\u4e00\u6b65\u8f93\u51fa\u9884\u6d4b\u548c\u201c\u9690\u85cf\u72b6\u6001\u201d\uff0c\u5c06\u5176\u5148\u524d\u7684\u9690\u85cf\u72b6\u6001\u8f93\u5165\u81f3\u4e0b\u4e00\u65f6\u523b\u3002 \u6211\u4eec\u5c06\u6700\u7ec8\u65f6\u523b\u8f93\u51fa\u4f5c\u4e3a\u9884\u6d4b\u7ed3\u679c\uff0c\u5373\u8868\u793a\u8be5\u8bcd\u5c5e\u4e8e\u54ea\u4e2a\u7c7b\u3002</p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u572818\u79cd\u8bed\u8a00\u6784\u6210\u7684\u51e0\u5343\u4e2a\u540d\u5b57\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u6839\u636e\u4e00\u4e2a\u540d\u5b57\u7684\u62fc\u5199\u9884\u6d4b\u5b83\u662f\u54ea\u79cd\u8bed\u8a00\u7684\u540d\u5b57\uff1a</p> <pre><code>$ python predict.py Hinton\n(-0.47) Scottish\n(-1.52) English\n(-3.57) Irish\n\n$ python predict.py Schmidhuber\n(-0.19) German\n(-2.48) Czech\n(-2.68) Dutch\n\n</code></pre> <p>\u63a8\u8350\u9605\u8bfb:</p> <p>\u6211\u9ed8\u8ba4\u4f60\u5df2\u7ecf\u5b89\u88c5\u597d\u4e86PyTorch\uff0c\u719f\u6089Python\u8bed\u8a00\uff0c\u7406\u89e3\u201c\u5f20\u91cf\u201d\u7684\u6982\u5ff5\uff1a</p> <ul> <li>https://pytorch.org/ PyTorch\u5b89\u88c5\u6307\u5357</li> <li>Deep Learning with PyTorch: A 60 Minute Blitz PyTorch\u5165\u95e8</li> <li>Learning PyTorch with Examples \u4e00\u4e9bPyTorch\u7684\u4f8b\u5b50</li> <li>PyTorch for Former Torch Users Lua Torch \u7528\u6237\u53c2\u8003</li> </ul> <p>\u4e8b\u5148\u5b66\u4e60\u5e76\u4e86\u89e3RNN\u7684\u5de5\u4f5c\u539f\u7406\u5bf9\u7406\u89e3\u8fd9\u4e2a\u4f8b\u5b50\u5341\u5206\u6709\u5e2e\u52a9:</p> <ul> <li>The Unreasonable Effectiveness of Recurrent Neural Networks \u5c55\u793a\u4e86\u4e00\u4e9b\u73b0\u5b9e\u751f\u6d3b\u4e2d\u7684\u4f8b\u5b50</li> <li>Understanding LSTM Networks \u662f\u5173\u4e8eLSTM\u7684\uff0c\u4f46\u4e5f\u63d0\u4f9b\u6709\u5173RNN\u7684\u4e00\u822c\u4fe1\u606f</li> </ul>"},{"location":"1.0/char_rnn_classification_tutorial/#_1","title":"\u51c6\u5907\u6570\u636e","text":"<p>\u70b9\u51fb\u8fd9\u91cc\u4e0b\u8f7d\u6570\u636e \u5e76\u5c06\u5176\u89e3\u538b\u5230\u5f53\u524d\u6587\u4ef6\u5939\u3002</p> <p>\u5728\"data/names\"\u6587\u4ef6\u5939\u4e0b\u662f\u540d\u79f0\u4e3a\"[language].txt\"\u768418\u4e2a\u6587\u672c\u6587\u4ef6\u3002\u6bcf\u4e2a\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u90fd\u6709\u4e00\u4e2a\u540d\u5b57\uff0c\u5b83\u4eec\u51e0\u4e4e\u90fd\u662f\u7f57\u9a6c\u5316\u7684\u6587\u672c(\u4f46\u662f\u6211\u4eec\u4ecd\u9700\u8981\u5c06\u5176\u4eceUnicode\u8f6c\u6362\u4e3aASCII\u7f16\u7801\uff09</p> <p>\u6211\u4eec\u6700\u7ec8\u4f1a\u5f97\u5230\u4e00\u4e2a\u8bed\u8a00\u5bf9\u5e94\u540d\u5b57\u5217\u8868\u7684\u5b57\u5178\uff0c<code>{language: [names ...]}</code></p> <p>\u901a\u7528\u53d8\u91cf\u201ccategory\u201d\u548c\u201cline\u201d(\u4f8b\u5b50\u4e2d\u7684\u8bed\u8a00\u548c\u540d\u5b57\u5355\u8bcd\uff09\u7528\u4e8e\u4ee5\u540e\u7684\u53ef\u6269\u5c55\u6027\u3002</p> <pre><code>from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\n\ndef findFiles(path): return glob.glob(path)\n\nprint(findFiles('data/names/*.txt'))\n\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\nprint(unicodeToAscii('\u015alus\u00e0rski'))\n\n# Build the category_lines dictionary, a list of names per language\ncategory_lines = {}\nall_categories = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>['data/names/Italian.txt', 'data/names/German.txt', 'data/names/Portuguese.txt', 'data/names/Chinese.txt', 'data/names/Greek.txt', 'data/names/Polish.txt', 'data/names/French.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Arabic.txt', 'data/names/Czech.txt', 'data/names/Russian.txt', 'data/names/Irish.txt', 'data/names/Dutch.txt', 'data/names/Scottish.txt', 'data/names/Vietnamese.txt', 'data/names/Korean.txt', 'data/names/Japanese.txt']\nSlusarski\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u6709\u4e86<code>category_lines</code>\uff0c\u4e00\u4e2a\u5b57\u5178\u53d8\u91cf\u5b58\u50a8\u6bcf\u4e00\u79cd\u8bed\u8a00\u53ca\u5176\u5bf9\u5e94\u7684\u6bcf\u4e00\u884c\u6587\u672c(\u540d\u5b57)\u5217\u8868\u7684\u6620\u5c04\u5173\u7cfb\u3002</p> <p>\u53d8\u91cf<code>all_categories</code>\u662f\u5168\u90e8\u8bed\u8a00\u79cd\u7c7b\u7684\u5217\u8868\uff0c</p> <p>\u53d8\u91cf<code>n_categories</code> \u662f\u8bed\u8a00\u79cd\u7c7b\u7684\u6570\u91cf\uff0c\u540e\u7eed\u4f1a\u4f7f\u7528</p> <pre><code>print(category_lines['Italian'][:5])\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n\n</code></pre>"},{"location":"1.0/char_rnn_classification_tutorial/#_2","title":"\u5355\u8bcd\u8f6c\u5316\u4e3a\u5f20\u91cf","text":"<p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u52a0\u8f7d\u4e86\u6240\u6709\u7684\u540d\u5b57\uff0c\u6211\u4eec\u9700\u8981\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u5f20\u91cf\u6765\u4f7f\u7528\u5b83\u4eec\u3002</p> <p>\u6211\u4eec\u4f7f\u7528\u5927\u5c0f\u4e3a<code>&lt;1 x n_letters&gt;</code>\u7684\u201cone-hot \u5411\u91cf\u201d\u8868\u793a\u4e00\u4e2a\u5b57\u6bcd\u3002</p> <p>\u4e00\u4e2aone-hot\u5411\u91cf\u6240\u6709\u4f4d\u7f6e\u90fd\u586b\u5145\u4e3a0\uff0c\u5e76\u5728\u5176\u8868\u793a\u7684\u5b57\u6bcd\u7684\u4f4d\u7f6e\u8868\u793a\u4e3a1\uff0c\u4f8b\u5982<code>\"b\" = &lt;0 1 0 0 0 ...&gt;</code>.(\u5b57\u6bcdb\u7684\u7f16\u53f7\u662f2\uff0c\u7b2c\u4e8c\u4e2a\u4f4d\u7f6e\u662f1\uff0c\u5176\u4ed6\u4f4d\u7f6e\u662f0\uff09</p> <p>\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a<code>&lt;line_length x 1 x n_letters&gt;</code>\u76842D\u77e9\u9635\u8868\u793a\u4e00\u4e2a\u5355\u8bcd</p> <p>\u989d\u5916\u76841\u7ef4\u662fbatch\u7684\u7ef4\u5ea6\uff0cPyTorch\u9ed8\u8ba4\u6240\u6709\u7684\u6570\u636e\u90fd\u662f\u6210batch\u5904\u7406\u7684\u3002\u6211\u4eec\u8fd9\u91cc\u53ea\u8bbe\u7f6e\u4e86batch\u7684\u5927\u5c0f\u4e3a1\u3002</p> <pre><code>import torch\n\n# \u4ece\u6240\u6709\u7684\u5b57\u6bcd\u4e2d\u5f97\u5230\u67d0\u4e2aletter\u7684\u7d22\u5f15\u7f16\u53f7, \u4f8b\u5982 \"a\" = 0\ndef letterToIndex(letter):\n    return all_letters.find(letter)\n\n# Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor\ndef letterToTensor(letter):\n    tensor = torch.zeros(1, n_letters)\n    tensor[0][letterToIndex(letter)] = 1\n    return tensor\n\n# Turn a line into a &lt;line_length x 1 x n_letters&gt;,\n# or an array of one-hot letter vectors\ndef lineToTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor\n\nprint(letterToTensor('J'))\n\nprint(lineToTensor('Jones').size())\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0.]])\ntorch.Size([5, 1, 57])\n\n</code></pre>"},{"location":"1.0/char_rnn_classification_tutorial/#_3","title":"\u6784\u9020\u795e\u7ecf\u7f51\u7edc","text":"<p>\u5728autograd\u4e4b\u524d\uff0c\u8981\u5728Torch\u4e2d\u6784\u5efa\u4e00\u4e2a\u53ef\u4ee5\u590d\u5236\u4e4b\u524d\u65f6\u523b\u5c42\u53c2\u6570\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>layer\u7684\u9690\u85cf\u72b6\u6001\u548c\u68af\u5ea6\u5c06\u4ea4\u7ed9\u8ba1\u7b97\u56fe\u81ea\u5df1\u5904\u7406\u3002</p> <p>\u8fd9\u610f\u5473\u7740\u4f60\u53ef\u4ee5\u50cf\u5b9e\u73b0\u7684\u5e38\u89c4\u7684 feed-forward \u5c42\u4e00\u6837\uff0c\u4ee5\u5f88\u7eaf\u7cb9\u7684\u65b9\u5f0f\u5b9e\u73b0RNN\u3002</p> <p>\u8fd9\u4e2aRNN\u7ec4\u4ef6 (\u51e0\u4e4e\u662f\u4ece\u8fd9\u91cc\u590d\u5236\u7684 the PyTorch for Torch users tutorial) \u4ec5\u4f7f\u7528\u4e24\u5c42 linear \u5c42\u5bf9\u8f93\u5165\u548c\u9690\u85cf\u5c42\u505a\u5904\u7406, </p> <p>\u5728\u6700\u540e\u6dfb\u52a0\u4e00\u5c42 LogSoftmax \u5c42\u9884\u6d4b\u6700\u7ec8\u8f93\u51fa\u3002</p> <p></p> <pre><code>import torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\nn_hidden = 128\nrnn = RNN(n_letters, n_hidden, n_categories)\n\n</code></pre> <p>\u8981\u8fd0\u884c\u6b64\u7f51\u7edc\u7684\u4e00\u4e2a\u6b65\u9aa4\uff0c\u6211\u4eec\u9700\u8981\u4f20\u9012\u4e00\u4e2a\u8f93\u5165(\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u662f\u5f53\u524d\u5b57\u6bcd\u7684Tensor\uff09\u548c\u4e00\u4e2a\u5148\u524d\u9690\u85cf\u7684\u72b6\u6001(\u6211\u4eec\u9996\u5148\u5c06\u5176\u521d\u59cb\u5316\u4e3a\u96f6\uff09\u3002</p> <p>\u6211\u4eec\u5c06\u8fd4\u56de\u8f93\u51fa(\u6bcf\u79cd\u8bed\u8a00\u7684\u6982\u7387\uff09\u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u72b6\u6001(\u4e3a\u6211\u4eec\u4e0b\u4e00\u6b65\u4fdd\u7559\u4f7f\u7528\uff09\u3002</p> <pre><code>input = letterToTensor('A')\nhidden =torch.zeros(1, n_hidden)\n\noutput, next_hidden = rnn(input, hidden)\n\n</code></pre> <p>\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u4e0d\u5e0c\u671b\u4e3a\u6bcf\u4e00\u6b65\u90fd\u521b\u5efa\u4e00\u4e2a\u65b0\u7684Tensor\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u4f7f\u7528<code>lineToTensor</code>\u51fd\u6570\u800c\u4e0d\u662f<code>letterToTensor</code>\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u5207\u7247\u65b9\u6cd5\u3002</p> <p>\u8fd9\u4e00\u6b65\u53ef\u4ee5\u901a\u8fc7\u9884\u5148\u8ba1\u7b97\u6279\u91cf\u7684\u5f20\u91cf\u8fdb\u4e00\u6b65\u4f18\u5316\u3002</p> <pre><code>input = lineToTensor('Albert')\nhidden = torch.zeros(1, n_hidden)\n\noutput, next_hidden = rnn(input[0], hidden)\nprint(output)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[-2.8857, -2.9005, -2.8386, -2.9397, -2.8594, -2.8785, -2.9361, -2.8270,\n         -2.9602, -2.8583, -2.9244, -2.9112, -2.8545, -2.8715, -2.8328, -2.8233,\n         -2.9685, -2.9780]], grad_fn=&lt;LogSoftmaxBackward&gt;)\n\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\u8f93\u51fa\u662f\u4e00\u4e2a<code>&lt;1 x n_categories&gt;</code>\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e00\u6761\u4ee3\u8868\u8fd9\u4e2a\u5355\u8bcd\u5c5e\u4e8e\u67d0\u4e00\u7c7b\u7684\u53ef\u80fd\u6027(\u8d8a\u9ad8\u53ef\u80fd\u6027\u8d8a\u5927\uff09</p>"},{"location":"1.0/char_rnn_classification_tutorial/#_4","title":"\u8bad\u7ec3","text":""},{"location":"1.0/char_rnn_classification_tutorial/#_5","title":"\u8bad\u7ec3\u524d\u7684\u51c6\u5907","text":"<p>\u8fdb\u884c\u8bad\u7ec3\u6b65\u9aa4\u4e4b\u524d\u6211\u4eec\u9700\u8981\u6784\u5efa\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\u3002</p> <p>\u7b2c\u4e00\u4e2a\u662f\u5f53\u6211\u4eec\u77e5\u9053\u8f93\u51fa\u7ed3\u679c\u5bf9\u5e94\u6bcf\u79cd\u7c7b\u522b\u7684\u53ef\u80fd\u6027\u65f6\uff0c\u89e3\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 <code>Tensor.topk</code>\u51fd\u6570\u5f97\u5230\u6700\u5927\u503c\u5728\u7ed3\u679c\u4e2d\u7684\u4f4d\u7f6e\u7d22\u5f15</p> <pre><code>def categoryFromOutput(output):\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput(output))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>('Vietnamese', 15)\n\n</code></pre> <p>\u6211\u4eec\u8fd8\u9700\u8981\u4e00\u79cd\u5feb\u901f\u83b7\u53d6\u8bad\u7ec3\u793a\u4f8b(\u5f97\u5230\u4e00\u4e2a\u540d\u5b57\u53ca\u5176\u6240\u5c5e\u7684\u8bed\u8a00\u7c7b\u522b\uff09\u7684\u65b9\u6cd5\uff1a</p> <pre><code>import random\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\ndef randomTrainingExample():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n    line_tensor = lineToTensor(line)\n    return category, line, category_tensor, line_tensor\n\nfor i in range(10):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    print('category =', category, '/ line =', line)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>category = Russian / line = Minkin\ncategory = French / line = Masson\ncategory = German / line = Hasek\ncategory = Dutch / line = Kloeten\ncategory = Scottish / line = Allan\ncategory = Italian / line = Agostini\ncategory = Japanese / line = Fumihiko\ncategory = Polish / line = Gajos\ncategory = Scottish / line = Duncan\ncategory = Arabic / line = Gerges\n\n</code></pre>"},{"location":"1.0/char_rnn_classification_tutorial/#_6","title":"\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc","text":"<p>\u73b0\u5728\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u53ea\u9700\u8981\u5411\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\u5927\u91cf\u7684\u6570\u636e\uff0c\u8ba9\u5b83\u505a\u51fa\u9884\u6d4b\uff0c\u5e76\u5c06\u5bf9\u9519\u53cd\u9988\u7ed9\u5b83\u3002</p> <p><code>nn.LogSoftmax</code>\u4f5c\u4e3a\u6700\u540e\u4e00\u5c42layer\u65f6\uff0c<code>nn.NLLLoss</code>\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u662f\u5408\u9002\u7684\u3002</p> <pre><code>criterion = nn.NLLLoss()\n\n</code></pre> <p>\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6bcf\u6b21\u5faa\u73af\u5c06\u4f1a\u53d1\u751f\uff1a</p> <ul> <li>\u6784\u5efa\u8f93\u5165\u548c\u76ee\u6807\u5f20\u91cf</li> <li>\u6784\u5efa0\u521d\u59cb\u5316\u7684\u9690\u85cf\u72b6\u6001</li> <li>\u8bfb\u5165\u6bcf\u4e00\u4e2a\u5b57\u6bcd<ul> <li>\u5c06\u5f53\u524d\u9690\u85cf\u72b6\u6001\u4f20\u9012\u7ed9\u4e0b\u4e00\u5b57\u6bcd</li> </ul> </li> <li>\u6bd4\u8f83\u6700\u7ec8\u7ed3\u679c\u548c\u76ee\u6807</li> <li>\u53cd\u5411\u4f20\u64ad</li> <li>\u8fd4\u56de\u7ed3\u679c\u548c\u635f\u5931</li> </ul> <pre><code>learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n\ndef train(category_tensor, line_tensor):\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()\n\n    for i in range(line_tensor.size()[0]):\n        output, hidden = rnn(line_tensor[i], hidden)\n\n    loss = criterion(output, category_tensor)\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    for p in rnn.parameters():\n        p.data.add_(-learning_rate, p.grad.data)\n\n    return output, loss.item()\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ea\u9700\u8981\u51c6\u5907\u4e00\u4e9b\u4f8b\u5b50\u6765\u8fd0\u884c\u7a0b\u5e8f\u3002</p> <p>\u7531\u4e8e<code>train</code>\u51fd\u6570\u540c\u65f6\u8fd4\u56de\u8f93\u51fa\u548c\u635f\u5931\uff0c\u6211\u4eec\u53ef\u4ee5\u6253\u5370\u5176\u8f93\u51fa\u7ed3\u679c\u5e76\u8ddf\u8e2a\u5176\u635f\u5931\u753b\u56fe\u3002</p> <p>\u7531\u4e8e\u67091000\u4e2a\u793a\u4f8b\uff0c\u6211\u4eec\u6bcf<code>print_every</code>\u6b21\u6253\u5370\u6837\u4f8b\uff0c\u5e76\u6c42\u5e73\u5747\u635f\u5931\u3002</p> <pre><code>import time\nimport math\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 1000\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output, loss = train(category_tensor, line_tensor)\n    current_loss += loss\n\n    # Print iter number, loss, name and guess\n    if iter % print_every == 0:\n        guess, guess_i = categoryFromOutput(output)\n        correct = '\u2713' if guess == category else '\u2717 (%s)' % category\n        print('%d  %d%% (%s) %.4f  %s / %s  %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n\n    # Add current loss avg to list of losses\n    if iter % plot_every == 0:\n        all_losses.append(current_loss / plot_every)\n        current_loss = 0\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>5000 5% (0m 11s) 2.0318 Jaeger / German \u2713\n10000 10% (0m 18s) 2.1296 Sokolofsky / Russian \u2717 (Polish)\n15000 15% (0m 26s) 1.2620 Jo / Korean \u2713\n20000 20% (0m 34s) 1.9295 Livson / Scottish \u2717 (Russian)\n25000 25% (0m 41s) 1.2325 Fortier / French \u2713\n30000 30% (0m 49s) 2.5714 Purdes / Dutch \u2717 (Czech)\n35000 35% (0m 56s) 2.3312 Bayer / Arabic \u2717 (German)\n40000 40% (1m 4s) 2.3792 Mitchell / Dutch \u2717 (Scottish)\n45000 45% (1m 12s) 1.3536 Maes / Dutch \u2713\n50000 50% (1m 20s) 2.6095 Sai / Chinese \u2717 (Vietnamese)\n55000 55% (1m 28s) 0.5883 Cheung / Chinese \u2713\n60000 60% (1m 35s) 1.5788 William / Irish \u2713\n65000 65% (1m 43s) 2.5809 Mulder / Scottish \u2717 (Dutch)\n70000 70% (1m 51s) 1.3440 Bruce / German \u2717 (Scottish)\n75000 75% (1m 58s) 1.1839 Romero / Italian \u2717 (Spanish)\n80000 80% (2m 6s) 2.6453 Reyes / Portuguese \u2717 (Spanish)\n85000 85% (2m 14s) 0.0290 Mcmillan / Scottish \u2713\n90000 90% (2m 22s) 0.7337 Riagan / Irish \u2713\n95000 95% (2m 30s) 2.6208 Maneates / Dutch \u2717 (Greek)\n100000 100% (2m 37s) 0.5170 Szwarc / Polish \u2713\n\n</code></pre>"},{"location":"1.0/char_rnn_classification_tutorial/#_7","title":"\u753b\u51fa\u7ed3\u679c","text":"<p>\u4ece<code>all_losses</code>\u5f97\u5230\u5386\u53f2\u635f\u5931\u8bb0\u5f55\uff0c\u53cd\u6620\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u60c5\u51b5\uff1a</p> <pre><code>import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n</code></pre> <p></p>"},{"location":"1.0/char_rnn_classification_tutorial/#_8","title":"\u8bc4\u4ef7\u7ed3\u679c","text":"<p>\u4e3a\u4e86\u4e86\u89e3\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u6df7\u6dc6\u77e9\u9635\uff0c\u663e\u793a\u6bcf\u79cd\u8bed\u8a00(\u884c\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u5c06\u5176\u9884\u6d4b\u4e3a\u54ea\u79cd\u8bed\u8a00(\u5217\uff09\u3002</p> <p>\u4e3a\u4e86\u8ba1\u7b97\u6df7\u6dc6\u77e9\u9635\uff0c\u4f7f\u7528<code>evaluate()</code>\u51fd\u6570\u5904\u7406\u4e86\u4e00\u6279\u6570\u636e\uff0c<code>evaluate()</code>\u51fd\u6570\u4e0e\u53bb\u6389\u53cd\u5411\u4f20\u64ad\u7684<code>train()</code>\u51fd\u6570\u5927\u4f53\u76f8\u540c\u3002</p> <pre><code># Keep track of correct guesses in a confusion matrix\nconfusion = torch.zeros(n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate(line_tensor):\n    hidden = rnn.initHidden()\n\n    for i in range(line_tensor.size()[0]):\n        output, hidden = rnn(line_tensor[i], hidden)\n\n    return output\n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    output = evaluate(line_tensor)\n    guess, guess_i = categoryFromOutput(output)\n    category_i = all_categories.index(category)\n    confusion[category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    confusion[i] = confusion[i] / confusion[i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(confusion.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n</code></pre> <p></p> <p>\u4f60\u53ef\u4ee5\u4ece\u4e3b\u8f74\u7ebf\u4ee5\u5916\u6311\u51fa\u4eae\u7684\u70b9\uff0c\u663e\u793a\u6a21\u578b\u9884\u6d4b\u9519\u4e86\u54ea\u4e9b\u8bed\u8a00\uff0c\u4f8b\u5982\u6c49\u8bed\u9884\u6d4b\u4e3a\u4e86\u97e9\u8bed\uff0c\u897f\u73ed\u7259\u9884\u6d4b\u4e3a\u4e86\u610f\u5927\u5229\u3002</p> <p>\u770b\u4e0a\u53bb\u5728\u5e0c\u814a\u8bed\u4e0a\u6548\u679c\u5f88\u597d\uff0c\u5728\u82f1\u8bed\u4e0a\u8868\u73b0\u6b20\u4f73\u3002(\u53ef\u80fd\u662f\u56e0\u4e3a\u82f1\u8bed\u4e0e\u5176\u4ed6\u8bed\u8a00\u7684\u91cd\u53e0\u8f83\u591a\uff09\u3002</p>"},{"location":"1.0/char_rnn_classification_tutorial/#_9","title":"\u5904\u7406\u7528\u6237\u8f93\u5165","text":"<pre><code>def predict(input_line, n_predictions=3):\n    print('\\n&gt; %s' % input_line)\n    with torch.no_grad():\n        output = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = output.topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_categories[category_index]))\n            predictions.append([value, all_categories[category_index]])\n\npredict('Dovesky')\npredict('Jackson')\npredict('Satoshi')\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>&gt; Dovesky\n(-0.74) Russian\n(-0.77) Czech\n(-3.31) English\n\n&gt; Jackson\n(-0.80) Scottish\n(-1.69) English\n(-1.84) Russian\n\n&gt; Satoshi\n(-1.16) Japanese\n(-1.89) Arabic\n(-1.90) Polish\n\n</code></pre> <p>\u6700\u7ec8\u7248\u7684\u811a\u672c in the Practical PyTorch repo \u5c06\u4e0a\u8ff0\u4ee3\u7801\u62c6\u5206\u4e3a\u51e0\u4e2a\u6587\u4ef6\uff1a</p> <ul> <li><code>data.py</code> (\u8bfb\u53d6\u6587\u4ef6)</li> <li><code>model.py</code> (\u6784\u9020RNN\u7f51\u7edc)</li> <li><code>train.py</code> (\u8fd0\u884c\u8bad\u7ec3\u8fc7\u7a0b)</li> <li><code>predict.py</code> (\u5728\u547d\u4ee4\u884c\u4e2d\u548c\u53c2\u6570\u4e00\u8d77\u8fd0\u884c<code>predict()</code>\u51fd\u6570)</li> <li><code>server.py</code> (\u4f7f\u7528bottle.py\u6784\u5efaJSON API\u7684\u9884\u6d4b\u670d\u52a1)</li> </ul> <p>\u8fd0\u884c <code>train.py</code> \u6765\u8bad\u7ec3\u548c\u4fdd\u5b58\u7f51\u7edc</p> <p>\u5c06<code>predict.py</code>\u548c\u4e00\u4e2a\u540d\u5b57\u7684\u5355\u8bcd\u4e00\u8d77\u8fd0\u884c\u67e5\u770b\u9884\u6d4b\u7ed3\u679c :</p> <pre><code>$ python predict.py Hazaki\n(-0.42) Japanese\n(-1.39) Polish\n(-3.51) Czech\n\n</code></pre> <p>\u8fd0\u884c <code>server.py</code> \u5e76\u8bbf\u95eehttp://localhost:5533/Yourname \u5f97\u5230JSON\u683c\u5f0f\u7684\u9884\u6d4b\u8f93\u51fa</p>"},{"location":"1.0/char_rnn_classification_tutorial/#_10","title":"\u7ec3\u4e60","text":"<ul> <li>\u5c1d\u8bd5\u5176\u5b83 (\u7c7b\u522b-&gt;\u884c\uff09 \u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982:<ul> <li>\u4efb\u4f55\u5355\u8bcd -&gt; \u8bed\u8a00</li> <li>\u59d3\u540d -&gt; \u6027\u522b</li> <li>\u89d2\u8272\u59d3\u540d -&gt; \u4f5c\u8005</li> <li>\u9875\u9762\u6807\u9898 -&gt; blog \u6216 subreddit</li> </ul> </li> <li>\u901a\u8fc7\u66f4\u5927\u548c\u66f4\u590d\u6742\u7684\u7f51\u7edc\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c<ul> <li>\u589e\u52a0\u66f4\u591alinear\u5c42</li> <li>\u5c1d\u8bd5 <code>nn.LSTM</code> \u548c <code>nn.GRU</code> \u5c42</li> <li>\u7ec4\u5408\u8fd9\u4e9b RNN\u6784\u9020\u66f4\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc</li> </ul> </li> </ul>"},{"location":"1.0/char_rnn_generation_tutorial/","title":"\u4f7f\u7528\u5b57\u7b26\u7ea7\u522b\u7279\u5f81\u7684RNN\u7f51\u7edc\u751f\u6210\u540d\u5b57","text":"<p>\u8bd1\u8005\uff1ahhxx2015</p> <p>\u6821\u5bf9\u8005\uff1ahijkzzz</p> <p>\u4f5c\u8005: Sean Robertson</p> <p>\u5728\u4e0a\u4e00\u4e2a \u4f8b\u5b50 \u4e2d\u6211\u4eec\u4f7f\u7528RNN\u7f51\u7edc\u5bf9\u540d\u5b57\u6240\u5c5e\u7684\u8bed\u8a00\u8fdb\u884c\u5206\u7c7b\u3002 \u8fd9\u4e00\u6b21\u6211\u4eec\u4f1a\u53cd\u8fc7\u6765\u6839\u636e\u8bed\u8a00\u751f\u6210\u540d\u5b57\u3002</p> <pre><code>&gt; python sample.py Russian RUS\nRovakov\nUantov\nShavakov\n\n&gt; python sample.py German GER\nGerren\nEreng\nRosher\n\n&gt; python sample.py Spanish SPA\nSalla\nParer\nAllan\n\n&gt; python sample.py Chinese CHI\nChan\nHang\nIun\n\n</code></pre> <p>\u6211\u4eec\u4ecd\u4f7f\u7528\u53ea\u6709\u51e0\u5c42\u7ebf\u6027\u5c42\u7684\u5c0f\u578bRNN\u3002 \u6700\u5927\u7684\u533a\u522b\u5728\u4e8e\uff0c\u8fd9\u91cc\u4e0d\u662f\u5728\u8bfb\u53d6\u4e00\u4e2a\u540d\u5b57\u7684\u6240\u6709\u5b57\u6bcd\u540e\u9884\u6d4b\u7c7b\u522b\uff0c\u800c\u662f\u8f93\u5165\u4e00\u4e2a\u7c7b\u522b\u4e4b\u540e\u5728\u6bcf\u4e00\u65f6\u523b\u8f93\u51fa\u4e00\u4e2a\u5b57\u6bcd\u3002 \u5faa\u73af\u9884\u6d4b\u5b57\u7b26\u4ee5\u5f62\u6210\u8bed\u8a00\u901a\u5e38\u4e5f\u88ab\u79f0\u4e3a\u201c\u8bed\u8a00\u6a21\u578b\u201d\u3002(\u4e5f\u53ef\u4ee5\u5c06\u5b57\u7b26\u6362\u6210\u5355\u8bcd\u6216\u66f4\u9ad8\u7ea7\u7684\u7ed3\u6784\u8fdb\u884c\u8fd9\u4e00\u8fc7\u7a0b\uff09</p> <p>\u9605\u8bfb\u5efa\u8bae:</p> <p>\u6211\u9ed8\u8ba4\u4f60\u5df2\u7ecf\u5b89\u88c5\u597d\u4e86PyTorch\uff0c\u719f\u6089Python\u8bed\u8a00\uff0c\u7406\u89e3\u201c\u5f20\u91cf\u201d\u7684\u6982\u5ff5\uff1a</p> <ul> <li>https://pytorch.org/ PyTorch\u5b89\u88c5\u6307\u5357</li> <li>Deep Learning with PyTorch: A 60 Minute Blitz PyTorch\u5165\u95e8</li> <li>Learning PyTorch with Examples \u4e00\u4e9bPyTorch\u7684\u4f8b\u5b50</li> <li>PyTorch for Former Torch Users Lua Torch \u7528\u6237\u53c2\u8003</li> </ul> <p>\u4e8b\u5148\u5b66\u4e60\u5e76\u4e86\u89e3RNN\u7684\u5de5\u4f5c\u539f\u7406\u5bf9\u7406\u89e3\u8fd9\u4e2a\u4f8b\u5b50\u5341\u5206\u6709\u5e2e\u52a9:</p> <ul> <li>The Unreasonable Effectiveness of Recurrent Neural Networks \u5c55\u793a\u4e86\u5f88\u591a\u5b9e\u9645\u7684\u4f8b\u5b50</li> <li>Understanding LSTM Networks \u662f\u5173\u4e8eLSTM\u7684\uff0c\u4f46\u4e5f\u63d0\u4f9b\u6709\u5173RNN\u7684\u8bf4\u660e</li> </ul>"},{"location":"1.0/char_rnn_generation_tutorial/#_1","title":"\u51c6\u5907\u6570\u636e","text":"<p>\u70b9\u51fb\u8fd9\u91cc\u4e0b\u8f7d\u6570\u636e \u5e76\u5c06\u5176\u89e3\u538b\u5230\u5f53\u524d\u6587\u4ef6\u5939\u3002</p> <p>\u6709\u5173\u6b64\u8fc7\u7a0b\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u4e0a\u4e00\u4e2a\u6559\u7a0b\u3002 \u7b80\u800c\u8a00\u4e4b\uff0c\u6709\u4e00\u4e9b\u7eaf\u6587\u672c\u6587\u4ef6<code>data/names/[Language].txt</code>\uff0c\u5b83\u4eec\u7684\u6bcf\u884c\u90fd\u6709\u4e00\u4e2a\u540d\u5b57\u3002 \u6211\u4eec\u6309\u884c\u5c06\u6587\u672c\u6309\u884c\u5207\u5206\u5f97\u5230\u4e00\u4e2a\u6570\u7ec4\uff0c\u5c06Unicode\u7f16\u7801\u8f6c\u5316\u4e3aASCII\u7f16\u7801\uff0c\u6700\u7ec8\u5f97\u5230<code>{language: [names ...]}</code>\u683c\u5f0f\u5b58\u50a8\u7684\u5b57\u5178\u53d8\u91cf\u3002</p> <pre><code>from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'N\u00e9\u00e0l\"))\n\n</code></pre> <p>Out:</p> <pre><code># categories: 18 ['Italian', 'German', 'Portuguese', 'Chinese', 'Greek', 'Polish', 'French', 'English', 'Spanish', 'Arabic', 'Czech', 'Russian', 'Irish', 'Dutch', 'Scottish', 'Vietnamese', 'Korean', 'Japanese']\nO'Neal\n\n</code></pre>"},{"location":"1.0/char_rnn_generation_tutorial/#_2","title":"\u6784\u9020\u795e\u7ecf\u7f51\u7edc","text":"<p>\u8fd9\u4e2a\u795e\u7ecf\u7f51\u7edc\u6bd4 \u4e0a\u4e00\u4e2aRNN\u6559\u7a0b\u4e2d\u7684\u7f51\u7edc\u589e\u52a0\u4e86\u989d\u5916\u7684\u7c7b\u522b\u5f20\u91cf\u53c2\u6570\uff0c\u8be5\u53c2\u6570\u4e0e\u5176\u4ed6\u8f93\u5165\u8fde\u63a5\u5728\u4e00\u8d77\u3002</p> <p>\u7c7b\u522b\u53ef\u4ee5\u50cf\u5b57\u6bcd\u4e00\u6837\u7ec4\u6210one-hot\u5411\u91cf\u6784\u6210\u5f20\u91cf\u8f93\u5165\u3002</p> <p>\u6211\u4eec\u5c06\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u662f\u4ec0\u4e48\u7684\u53ef\u80fd\u6027\u3002\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u5f53\u524d\u8f93\u51fa\u53ef\u80fd\u6027\u6700\u9ad8\u7684\u5b57\u6bcd\u4f5c\u4e3a\u4e0b\u4e00\u65f6\u523b\u8f93\u5165\u5b57\u6bcd\u3002</p> <p>\u5728\u7ec4\u5408\u9690\u85cf\u72b6\u6001\u548c\u8f93\u51fa\u4e4b\u540e\u6211\u589e\u52a0\u4e86\u7b2c\u4e8c\u4e2alinear\u5c42<code>o2o</code>\uff0c\u4f7f\u6a21\u578b\u7684\u6027\u80fd\u66f4\u597d\u3002\u5f53\u7136\u8fd8\u6709\u4e00\u4e2adropout\u5c42\uff0c\u53c2\u8003\u8fd9\u7bc7\u8bba\u6587\u968f\u673a\u5c06\u8f93\u5165\u90e8\u5206\u66ff\u6362\u4e3a0\u7ed9\u51fa\u7684\u53c2\u6570(dropout=0.1\uff09\u6765\u6a21\u7cca\u5904\u7406\u8f93\u5165\u9632\u6b62\u8fc7\u62df\u5408\u3002</p> <p>\u6211\u4eec\u5c06\u5b83\u6dfb\u52a0\u5230\u7f51\u7edc\u7684\u672b\u7aef\uff0c\u6545\u610f\u6dfb\u52a0\u4e00\u4e9b\u6df7\u4e71\u4f7f\u91c7\u6837\u7279\u5f81\u589e\u52a0\u3002</p> <p></p> <pre><code>import torch\nimport torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n        self.dropout = nn.Dropout(0.1)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, category, input, hidden):\n        input_combined = torch.cat((category, input, hidden), 1)\n        hidden = self.i2h(input_combined)\n        output = self.i2o(input_combined)\n        output_combined = torch.cat((hidden, output), 1)\n        output = self.o2o(output_combined)\n        output = self.dropout(output)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n</code></pre>"},{"location":"1.0/char_rnn_generation_tutorial/#_3","title":"\u8bad\u7ec3","text":""},{"location":"1.0/char_rnn_generation_tutorial/#_4","title":"\u8bad\u7ec3\u51c6\u5907","text":"<p>\u9996\u5148\uff0c\u6784\u9020\u4e00\u4e2a\u53ef\u4ee5\u968f\u673a\u83b7\u53d6\u6210\u5bf9\u8bad\u7ec3\u6570\u636e(category, line)\u7684\u51fd\u6570\u3002</p> <pre><code>import random\n\n# Random item from a list\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n# Get a random category and random line from that category\ndef randomTrainingPair():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    return category, line\n\n</code></pre> <p>\u5bf9\u4e8e\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f(\u5373\uff0c\u5bf9\u4e8e\u8981\u8bad\u7ec3\u5355\u8bcd\u4e2d\u7684\u6bcf\u4e2a\u5b57\u6bcd\uff09\uff0c\u7f51\u7edc\u7684\u8f93\u5165\u5c06\u662f\u201c(\u7c7b\u522b\uff0c\u5f53\u524d\u5b57\u6bcd\uff0c\u9690\u85cf\u72b6\u6001\uff09\u201d\uff0c\u8f93\u51fa\u5c06\u662f\u201c(\u4e0b\u4e00\u4e2a\u5b57\u6bcd\uff0c\u4e0b\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\uff09\u201d\u3002</p> <p>\u56e0\u6b64\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u8bad\u7ec3\u96c6\uff0c\u6211\u4eec\u5c06\u9700\u8981\u7c7b\u522b\u3001\u4e00\u7ec4\u8f93\u5165\u5b57\u6bcd\u548c\u4e00\u7ec4\u8f93\u51fa/\u76ee\u6807\u5b57\u6bcd\u3002</p> <p>\u5728\u6bcf\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\uff0c\u6211\u4eec\u4f7f\u7528\u5f53\u524d\u5b57\u6bcd\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5b57\u6bcd\uff0c\u6240\u4ee5\u8bad\u7ec3\u7528\u7684\u5b57\u6bcd\u5bf9\u6765\u81ea\u4e8e\u4e00\u4e2a\u5355\u8bcd\u3002\u4f8b\u5982 \u5bf9\u4e8e <code>\"ABCD&lt;EOS&gt;\"</code>\uff0c\u6211\u4eec\u5c06\u521b\u5efa(\u201cA\u201d\uff0c\u201cB\u201d\uff09\uff0c(\u201cB\u201d\uff0c\u201cC\u201d\uff09\uff0c(\u201cC\u201d\uff0c\u201cD\u201d\uff09\uff0c(\u201cD\u201d\uff0c\u201cEOS\u201d\uff09\uff09\u3002</p> <p></p> <p>\u7c7b\u522b\u5f20\u91cf\u662f\u4e00\u4e2a<code>&lt;1 x n_categories&gt;</code>\u5c3a\u5bf8\u7684one-hot \u5411\u91cf</p> <p>\u8bad\u7ec3\u65f6\uff0c\u6211\u4eec\u5728\u6bcf\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u90fd\u5c06\u5176\u63d0\u4f9b\u7ed9\u795e\u7ecf\u7f51\u7edc\u3002\u8fd9\u662f\u4e00\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u4e5f\u53ef\u9009\u62e9\u5c06\u5176\u4f5c\u4e3a\u521d\u59cb\u9690\u85cf\u72b6\u6001\u7684\u4e00\u90e8\u5206\uff0c\u6216\u8005\u5176\u4ed6\u4ec0\u4e48\u7ed3\u6784\u3002</p> <pre><code># One-hot vector for category9\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = torch.zeros(1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)\n\n</code></pre> <p>\u4e3a\u4e86\u65b9\u4fbf\u8bad\u7ec3\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a<code>randomTrainingExample</code>\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u968f\u673a\u83b7\u53d6(\u7c7b\u522b\uff0c\u884c\uff09\u7684\u5bf9\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u6240\u9700\u8981\u7684(\u7c7b\u522b\uff0c\u8f93\u5165\uff0c\u76ee\u6807\uff09\u683c\u5f0f\u5f20\u91cf\u3002</p> <pre><code># Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor\n\n</code></pre>"},{"location":"1.0/char_rnn_generation_tutorial/#_5","title":"\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc","text":"<p>\u548c\u53ea\u4f7f\u7528\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u8f93\u51fa\u7684\u5206\u7c7b\u4efb\u52a1\u76f8\u6bd4\uff0c\u8fd9\u6b21\u6211\u4eec\u6bcf\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u90fd\u4f1a\u8fdb\u884c\u4e00\u6b21\u9884\u6d4b\uff0c\u6240\u4ee5\u6bcf\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u6211\u4eec\u90fd\u4f1a\u8ba1\u7b97\u635f\u5931\u3002</p> <p>autograd\u7684\u795e\u5947\u4e4b\u5904\u5728\u4e8e\u60a8\u53ef\u4ee5\u5728\u6bcf\u4e00\u6b65\u4e2d\u7b80\u5355\u5730\u7d2f\u52a0\u8fd9\u4e9b\u635f\u5931\uff0c\u5e76\u5728\u6700\u540e\u53cd\u5411\u4f20\u64ad\u3002</p> <pre><code>criterion = nn.NLLLoss()\n\nlearning_rate = 0.0005\n\ndef train(category_tensor, input_line_tensor, target_line_tensor):\n    target_line_tensor.unsqueeze_(-1)\n    hidden = rnn.initHidden()\n\n    rnn.zero_grad()\n\n    loss = 0\n\n    for i in range(input_line_tensor.size(0)):\n        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n        l = criterion(output, target_line_tensor[i])\n        loss += l\n\n    loss.backward()\n\n    for p in rnn.parameters():\n        p.data.add_(-learning_rate, p.grad.data)\n\n    return output, loss.item() / input_line_tensor.size(0)\n\n</code></pre> <p>\u4e3a\u4e86\u8ddf\u8e2a\u8bad\u7ec3\u8017\u8d39\u7684\u65f6\u95f4\uff0c\u6211\u6dfb\u52a0\u4e00\u4e2a<code>timeSince(timestamp\uff09</code>\u51fd\u6570\uff0c\u5b83\u8fd4\u56de\u4e00\u4e2a\u4eba\u7c7b\u53ef\u8bfb\u7684\u5b57\u7b26\u4e32\uff1a</p> <pre><code>import time\nimport math\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n</code></pre> <p>\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5e73\u65f6\u4e00\u6837\u3002\u591a\u6b21\u8fd0\u884ctrain\uff0c\u7b49\u5f85\u51e0\u5206\u949f\uff0c\u6bcf<code>print_every</code>\u6b21\u6253\u5370\u5f53\u524d\u65f6\u95f4\u548c\u635f\u5931\u3002\u5728 <code>all_losses</code> \u4e2d\u4fdd\u7559\u6bcf<code>plot_every</code>\u6b21\u7684\u5e73\u5747\u635f\u5931\uff0c\u4ee5\u4fbf\u7a0d\u540e\u8fdb\u884c\u7ed8\u56fe\u3002</p> <pre><code>rnn = RNN(n_letters, 128, n_letters)\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 500\nall_losses = []\ntotal_loss = 0 # Reset every plot_every iters\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    output, loss = train(*randomTrainingExample())\n    total_loss += loss\n\n    if iter % print_every == 0:\n        print('%s (%d  %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n\n    if iter % plot_every == 0:\n        all_losses.append(total_loss / plot_every)\n        total_loss = 0\n\n</code></pre> <p>Out:</p> <pre><code>0m 21s (5000 5%) 2.5152\n0m 43s (10000 10%) 2.7758\n1m 4s (15000 15%) 2.2884\n1m 25s (20000 20%) 3.2404\n1m 47s (25000 25%) 2.7298\n2m 8s (30000 30%) 3.4301\n2m 29s (35000 35%) 2.2306\n2m 51s (40000 40%) 2.5628\n3m 12s (45000 45%) 1.7700\n3m 34s (50000 50%) 2.4657\n3m 55s (55000 55%) 2.1909\n4m 16s (60000 60%) 2.1004\n4m 38s (65000 65%) 2.3524\n4m 59s (70000 70%) 2.3339\n5m 21s (75000 75%) 2.3936\n5m 42s (80000 80%) 2.1886\n6m 3s (85000 85%) 2.0739\n6m 25s (90000 90%) 2.5451\n6m 46s (95000 95%) 1.5104\n7m 7s (100000 100%) 2.4600\n\n</code></pre>"},{"location":"1.0/char_rnn_generation_tutorial/#_6","title":"\u635f\u5931\u6570\u636e\u4f5c\u56fe","text":"<p>\u4ece<code>all_losses</code>\u5f97\u5230\u5386\u53f2\u635f\u5931\u8bb0\u5f55\uff0c\u53cd\u6620\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u60c5\u51b5\uff1a</p> <pre><code>import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n</code></pre> <p></p>"},{"location":"1.0/char_rnn_generation_tutorial/#_7","title":"\u7f51\u7edc\u91c7\u6837","text":"<p>\u6211\u4eec\u6bcf\u6b21\u7ed9\u7f51\u7edc\u63d0\u4f9b\u4e00\u4e2a\u5b57\u6bcd\u5e76\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u662f\u4ec0\u4e48\uff0c\u5c06\u9884\u6d4b\u5230\u7684\u5b57\u6bcd\u7ee7\u7eed\u8f93\u5165\uff0c\u76f4\u5230\u5f97\u5230EOS\u5b57\u7b26\u7ed3\u675f\u5faa\u73af\u3002</p> <ul> <li>\u7528\u8f93\u5165\u7c7b\u522b\u3001\u8d77\u59cb\u5b57\u6bcd\u548c\u7a7a\u9690\u85cf\u72b6\u6001\u521b\u5efa\u8f93\u5165\u5f20\u91cf\u3002</li> <li>\u7528\u8d77\u59cb\u5b57\u6bcd\u6784\u5efa\u4e00\u4e2a\u5b57\u7b26\u4e32\u53d8\u91cf <code>output_name</code></li> <li> <p>\u5f97\u5230\u6700\u5927\u8f93\u51fa\u957f\u5ea6\uff0c</p> <ul> <li>\u5c06\u5f53\u524d\u5b57\u6bcd\u4f20\u5165\u795e\u7ecf\u7f51\u7edc</li> <li>\u4ece\u524d\u4e00\u5c42\u5f97\u5230\u4e0b\u4e00\u4e2a\u5b57\u6bcd\u548c\u4e0b\u4e00\u4e2a\u9690\u85cf\u72b6\u6001</li> <li>\u5982\u679c\u5b57\u6bcd\u662fEOS\uff0c\u5728\u8fd9\u91cc\u505c\u6b62</li> <li>\u5982\u679c\u662f\u4e00\u4e2a\u666e\u901a\u7684\u5b57\u6bcd\uff0c\u6dfb\u52a0\u5230<code>output_name</code>\u53d8\u91cf\u5e76\u7ee7\u7eed\u5faa\u73af</li> </ul> </li> <li> <p>\u8fd4\u56de\u6700\u7ec8\u5f97\u5230\u7684\u540d\u5b57\u5355\u8bcd</p> </li> </ul> <p>\u53e6\u4e00\u79cd\u7b56\u7565\u662f\uff0c\u4e0d\u5fc5\u7ed9\u7f51\u7edc\u4e00\u4e2a\u8d77\u59cb\u5b57\u6bcd\uff0c\u800c\u662f\u5728\u8bad\u7ec3\u4e2d\u63d0\u4f9b\u4e00\u4e2a\u201c\u5b57\u7b26\u4e32\u5f00\u59cb\u201d\u7684\u6807\u8bb0\uff0c\u5e76\u8ba9\u7f51\u7edc\u81ea\u5df1\u9009\u62e9\u8d77\u59cb\u7684\u5b57\u6bcd\u3002</p> <pre><code>max_length = 20\n\n# Sample from a category and starting letter\ndef sample(category, start_letter='A'):\n    with torch.no_grad():  # no need to track history in sampling\n        category_tensor = categoryTensor(category)\n        input = inputTensor(start_letter)\n        hidden = rnn.initHidden()\n\n        output_name = start_letter\n\n        for i in range(max_length):\n            output, hidden = rnn(category_tensor, input[0], hidden)\n            topv, topi = output.topk(1)\n            topi = topi[0][0]\n            if topi == n_letters - 1:\n                break\n            else:\n                letter = all_letters[topi]\n                output_name += letter\n            input = inputTensor(letter)\n\n        return output_name\n\n# Get multiple samples from one category and multiple starting letters\ndef samples(category, start_letters='ABC'):\n    for start_letter in start_letters:\n        print(sample(category, start_letter))\n\nsamples('Russian', 'RUS')\n\nsamples('German', 'GER')\n\nsamples('Spanish', 'SPA')\n\nsamples('Chinese', 'CHI')\n\n</code></pre> <p>Out:</p> <pre><code>Rovanik\nUakilovev\nShaveri\nGarter\nEren\nRomer\nSanta\nParera\nArtera\nChan\nHa\nIua\n\n</code></pre>"},{"location":"1.0/char_rnn_generation_tutorial/#_8","title":"\u7ec3\u4e60","text":"<ul> <li>\u5c1d\u8bd5\u5176\u5b83 (\u7c7b\u522b-&gt;\u884c\uff09 \u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982:<ul> <li>\u7cfb\u5217\u5c0f\u8bf4  -&gt; \u89d2\u8272\u540d\u79f0</li> <li>\u8bcd\u6027 -&gt; \u5355\u8bcd</li> <li>\u56fd\u5bb6 -&gt; \u57ce\u5e02</li> </ul> </li> <li>\u5c1d\u8bd5\u201cstart of sentence\u201d \u6807\u8bb0\uff0c\u4f7f\u91c7\u6837\u7684\u5f00\u59cb\u8fc7\u7a0b\u4e0d\u9700\u8981\u6307\u5b9a\u8d77\u59cb\u5b57\u6bcd</li> <li>\u901a\u8fc7\u66f4\u5927\u548c\u66f4\u590d\u6742\u7684\u7f51\u7edc\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c<ul> <li>\u5c1d\u8bd5 <code>nn.LSTM</code> \u548c <code>nn.GRU</code> \u5c42</li> <li>\u7ec4\u5408\u8fd9\u4e9b RNN\u6784\u9020\u66f4\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc</li> </ul> </li> </ul>"},{"location":"1.0/chatbot_tutorial/","title":"\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b","text":"<p>\u4f5c\u8005: Matthew Inkawhich</p> <p>\u8bd1\u8005: \u6bdb\u6bdb\u866b</p> <p>\u6821\u9a8c: \u7247\u523b</p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u597d\u73a9\u548c\u6709\u8da3\u7684\u5faa\u73af\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6a21\u578b\u7528\u4f8b\u3002\u6211\u4eec\u5c06\u7528 Cornell Movie-Dialogs Corpus \u5904\u7684\u7535\u5f71\u5267\u672c\u6765\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u804a\u5929\u673a\u5668\u4eba\u3002</p> <p>\u5728\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9886\u57df\u4e2d\u5bf9\u8bdd\u6a21\u578b\u662f\u4e00\u4e2a\u975e\u5e38\u70ed\u95e8\u7684\u8bdd\u9898\u3002\u804a\u5929\u673a\u5668\u4eba\u53ef\u4ee5\u5728\u5404\u79cd\u8bbe\u7f6e\u4e2d\u627e\u5230\uff0c\u5305\u62ec\u5ba2\u6237\u670d\u52a1\u5e94\u7528\u548c\u5728\u7ebf\u5e2e\u52a9\u3002\u8fd9\u4e9b\u673a\u5668\u4eba\u901a\u5e38\u7531\u57fa\u4e8e\u68c0\u7d22\u7684\u6a21\u578b\u63d0\u4f9b\u652f\u6301\uff0c\u8fd9\u4e9b\u8f93\u51fa\u662f\u67d0\u4e9b\u5f62\u5f0f\u95ee\u9898\u9884\u5148\u5b9a\u4e49\u7684\u54cd\u5e94\u3002\u5728\u50cf\u516c\u53f8IT\u670d\u52a1\u53f0\u8fd9\u6837\u9ad8\u5ea6\u53d7\u9650\u5236\u7684\u9886\u57df\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u8db3\u591f\u4e86\uff0c\u4f46\u662f\uff0c\u5bf9\u4e8e\u66f4\u4e00\u822c\u7684\u7528\u4f8b\u5b83\u4eec\u4e0d\u591f\u5065\u58ee\u3002\u6559\u4e00\u53f0\u673a\u5668\u4e0e\u591a\u9886\u57df\u7684\u4eba\u8fdb\u884c\u6709\u610f\u4e49\u7684\u5bf9\u8bdd\u662f\u4e00\u4e2a\u8fdc\u672a\u89e3\u51b3\u7684\u7814\u7a76\u95ee\u9898\u3002\u6700\u8fd1\uff0c\u6df1\u5ea6\u5b66\u4e60\u70ed\u6f6e\u5df2\u7ecf\u5141\u8bb8\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\uff0c\u5982\u8c37\u6b4c\u7684\u795e\u7ecf\u5bf9\u8bdd\u6a21\u578b Neural Conversational Model\uff0c\u8fd9\u6807\u5fd7\u7740\u5411\u591a\u9886\u57df\u751f\u6210\u5bf9\u8bdd\u6a21\u578b\u8fc8\u51fa\u4e86\u4e00\u5927\u6b65\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u5728PyTorch\u4e2d\u5b9e\u73b0\u8fd9\u79cd\u6a21\u578b\u3002</p> <p></p> <pre><code>&gt; hello?\nBot: hello .\n&gt; where am I?\nBot: you re in a hospital .\n&gt; who are you?\nBot: i m a lawyer .\n&gt; how are you doing?\nBot: i m fine .\n&gt; are you my friend?\nBot: no .\n&gt; you're under arrest\nBot: i m trying to help you !\n&gt; i'm just kidding\nBot: i m sorry .\n&gt; where are you from?\nBot: san francisco .\n&gt; it's time for me to leave\nBot: i know .\n&gt; goodbye\nBot: goodbye .\n\n</code></pre> <p>\u6559\u7a0b\u8981\u70b9</p> <ul> <li>\u5bf9 Cornell Movie-Dialogs Corpus \u6570\u636e\u96c6\u7684\u52a0\u8f7d\u548c\u9884\u5904\u7406 </li> <li>\u7528 Luong attention mechanism(s) \u5b9e\u73b0\u4e00\u4e2asequence-to-sequence\u6a21\u578b</li> <li>\u4f7f\u7528\u5c0f\u6279\u91cf\u6570\u636e\u8054\u5408\u8bad\u7ec3\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668\u6a21\u578b</li> <li>\u5b9e\u73b0\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u6a21\u5757</li> <li>\u4e0e\u8bad\u7ec3\u597d\u7684\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8</li> </ul> <p>\u9e23\u8c22</p> <p>\u672c\u6559\u7a0b\u501f\u9274\u4ee5\u4e0b\u6e90\u7801\uff1a</p> <ol> <li>Yuan-Kuei Wu's pytorch-chatbot implementation: https://github.com/ywk991112/pytorch-chatbot</li> <li>Sean Robertson's practical-pytorch seq2seq-translation example: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</li> <li>FloydHub's Cornell Movie Corpus preprocessing code: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</li> </ol>"},{"location":"1.0/chatbot_tutorial/#_2","title":"\u51c6\u5907\u5de5\u4f5c","text":"<p>\u9996\u5148\uff0c\u4e0b\u8f7d\u6570\u636e\u6587\u4ef6 here \u5e76\u5c06\u5176\u653e\u5165\u5f53\u524d\u76ee\u5f55\u4e0b\u7684<code>data/</code>\u6587\u4ef6\u5939\u4e0b</p> <p>\u4e4b\u540e\uff0c\u8ba9\u6211\u4eec\u5f15\u5165\u4e00\u4e9b\u5fc5\u987b\u7684\u5305\u3002</p> <pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_3","title":"\u52a0\u8f7d\u548c\u9884\u5904\u7406\u6570\u636e","text":"<p>\u4e0b\u4e00\u6b65\u5c31\u662f\u683c\u5f0f\u5316\u5904\u7406\u6211\u4eec\u7684\u6570\u636e\u6587\u4ef6\u5e76\u52a0\u8f7d\u5230\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u7684\u7ed3\u6784\u4e2d</p> <p>Cornell Movie-Dialogs Corpus \u662f\u4e00\u4e2a\u4e30\u5bcc\u7684\u7535\u5f71\u89d2\u8272\u5bf9\u8bdd\u6570\u636e\u96c6\uff1a</p> <ul> <li>10,292 \u5bf9\u7535\u5f71\u89d2\u8272\u7684220,579 \u6b21\u5bf9\u8bdd</li> <li>617\u90e8\u7535\u5f71\u4e2d\u76849,035\u7535\u5f71\u89d2\u8272</li> <li>\u603b\u5171304,713\u4e2d\u8bed\u8c03</li> </ul> <p>\u8fd9\u4e2a\u6570\u636e\u96c6\u5e9e\u5927\u800c\u591a\u6837\uff0c\u5728\u8bed\u8a00\u5f62\u5f0f\u3001\u65f6\u95f4\u6bb5\u3001\u60c5\u611f\u4e0a\u7b49\u90fd\u6709\u5f88\u5927\u7684\u53d8\u5316\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u79cd\u591a\u6837\u6027\u4f7f\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u9002\u5e94\u591a\u79cd\u5f62\u5f0f\u7684\u8f93\u5165\u548c\u67e5\u8be2\u3002</p> <p>\u9996\u5148\uff0c\u6211\u4eec\u901a\u8fc7\u6570\u636e\u6587\u4ef6\u7684\u67d0\u4e9b\u884c\u6765\u67e5\u770b\u539f\u59cb\u6570\u636e\u7684\u683c\u5f0f</p> <pre><code>corpus_name = \"cornell movie-dialogs corpus\"\ncorpus = os.path.join(\"data\", corpus_name)\n\ndef printLines(file, n=10):\n    with open(file, 'rb') as datafile:\n        lines = datafile.readlines()\n    for line in lines[:n]:\n        print(line)\n\nprintLines(os.path.join(corpus, \"movie_lines.txt\"))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\nb'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\nb'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\nb'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\nb\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\nb'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\nb\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\nb'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\nb'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\nb'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_4","title":"\u521b\u5efa\u683c\u5f0f\u5316\u6570\u636e\u6587\u4ef6","text":"<p>\u4e3a\u4e86\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u683c\u5f0f\u826f\u597d\u7684\u6570\u636e\u6587\u4ef6\uff0c\u5176\u4e2d\u6bcf\u4e00\u884c\u5305\u542b\u4e00\u4e2a\u7531 <code>tab</code> \u5236\u8868\u7b26\u5206\u9694\u7684\u67e5\u8be2\u8bed\u53e5\u548c\u54cd\u5e94\u8bed\u53e5\u5bf9\u3002</p> <p>\u4ee5\u4e0b\u51fd\u6570\u4fbf\u4e8e\u89e3\u6790\u539f\u59cb movie_lines.txt \u6570\u636e\u6587\u4ef6\u3002</p> <ul> <li><code>loadLines</code> \u5c06\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u62c6\u5206\u4e3a\u5b57\u6bb5(lineID, characterID, movieID, character, text)\u7ec4\u5408\u7684\u5b57\u5178 </li> <li><code>loadConversations</code> \u6839\u636e movie_conversations.txt \u5c06 <code>loadLines</code> \u4e2d\u7684\u6bcf\u4e00\u884c\u6570\u636e\u8fdb\u884c\u5f52\u7c7b</li> <li><code>extractSentencePairs</code> \u4ece\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u4e00\u5bf9\u53e5\u5b50</li> </ul> <pre><code># \u5c06\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u62c6\u5206\u4e3a\u5b57\u6bb5\u5b57\u5178\n# line = {\n#     'L183198': {\n#         'lineID': 'L183198', \n#         'characterID': 'u5022', \n#         'movieID': 'm333', \n#         'character': 'FRANKIE', \n#         'text': \"Well we'd sure like to help you.\\n\"\n#     }, {...}\n# }\ndef loadLines(fileName, fields):\n    lines = {}\n    with open(fileName, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            values = line.split(\" +++$+++ \")\n            # Extract fields\n            lineObj = {}\n            for i, field in enumerate(fields):\n                lineObj[field] = values[i]\n            lines[lineObj['lineID']] = lineObj\n    return lines\n\n# \u5c06 `loadLines` \u4e2d\u7684\u884c\u5b57\u6bb5\u5206\u7ec4\u4e3a\u57fa\u4e8e *movie_conversations.txt* \u7684\u5bf9\u8bdd\n# [{\n#   'character1ID': 'u0',\n#   'character2ID': 'u2',\n#   'movieID': 'm0',\n#   'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n#   'lines': [{\n#       'lineID': 'L194',\n#       'characterID': 'u0',\n#       'movieID': 'm0',\n#       'character': 'BIANCA',\n#       'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'\n#   }, {\n#       'lineID': 'L195',\n#       'characterID': 'u2',\n#       'movieID': 'm0',\n#       'character': 'CAMERON',\n#       'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n#   }, {\n#       'lineID': 'L196',\n#       'characterID': 'u0',\n#       'movieID': 'm0',\n#       'character': 'BIANCA',\n#       'text': 'Not the hacking and gagging and spitting part.  Please.\\n'\n#   }, {\n#       'lineID': 'L197',\n#       'characterID': 'u2',\n#       'movieID': 'm0',\n#       'character': 'CAMERON',\n#       'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n#   }]\n# }, {...}]\ndef loadConversations(fileName, lines, fields):\n    conversations = []\n    with open(fileName, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            values = line.split(\" +++$+++ \")\n            # Extract fields\n            convObj = {}\n            for i, field in enumerate(fields):\n                convObj[field] = values[i]\n            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n            lineIds = eval(convObj[\"utteranceIDs\"])\n            # Reassemble lines\n            convObj[\"lines\"] = []\n            for lineId in lineIds:\n                convObj[\"lines\"].append(lines[lineId])\n            conversations.append(convObj)\n    return conversations\n\n# \u4ece\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u4e00\u5bf9\u53e5\u5b50\ndef extractSentencePairs(conversations):\n    qa_pairs = []\n    for conversation in conversations:\n        # Iterate over all the lines of the conversation\n        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n            # Filter wrong samples (if one of the lists is empty)\n            if inputLine and targetLine:\n                qa_pairs.append([inputLine, targetLine])\n    return qa_pairs\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u5c06\u8c03\u7528\u8fd9\u4e9b\u51fd\u6570\u6765\u521b\u5efa\u6587\u4ef6\uff0c\u6211\u4eec\u547d\u540d\u4e3a formatted_movie_lines.txt.</p> <pre><code># Define path to new file\ndatafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n\ndelimiter = '\\t'\n# Unescape the delimiter\ndelimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n\n# Initialize lines dict, conversations list, and field ids\nlines = {}\nconversations = []\nMOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\nMOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n\n# Load lines and process conversations\nprint(\"\\nProcessing corpus...\")\nlines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\nprint(\"\\nLoading conversations...\")\nconversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n                                  lines, MOVIE_CONVERSATIONS_FIELDS)\n\n# Write new csv file\nprint(\"\\nWriting newly formatted file...\")\nwith open(datafile, 'w', encoding='utf-8') as outputfile:\n    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n    for pair in extractSentencePairs(conversations):\n        writer.writerow(pair)\n\n# Print a sample of lines\nprint(\"\\nSample lines from file:\")\nprintLines(datafile)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Processing corpus...\n\nLoading conversations...\n\nWriting newly formatted file...\n\nSample lines from file:\nb\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\nb\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\nb\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\nb\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\"\nb\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\"\nb\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\"\nb\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\"\nb'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'\nb\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\"\nb'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n'\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_5","title":"\u52a0\u8f7d\u548c\u6e05\u6d17\u6570\u636e","text":"<p>\u6211\u4eec\u4e0b\u4e00\u4e2a\u4efb\u52a1\u662f\u521b\u5efa\u8bcd\u6c47\u8868\u5e76\u5c06\u67e5\u8be2/\u54cd\u5e94\u53e5\u5b50\u5bf9(\u5bf9\u8bdd\uff09\u52a0\u8f7d\u5230\u5185\u5b58\u3002</p> <p>\u6ce8\u610f\u6211\u4eec\u6b63\u5728\u5904\u7406\u8bcd\u5e8f\uff0c\u8fd9\u4e9b\u8bcd\u5e8f\u6ca1\u6709\u6620\u5c04\u5230\u79bb\u6563\u6570\u503c\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u901a\u8fc7\u6570\u636e\u96c6\u4e2d\u7684\u5355\u8bcd\u6765\u521b\u5efa\u4e00\u4e2a\u7d22\u5f15\u3002</p> <p>\u4e3a\u6b64\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a<code>Voc</code>\u7c7b,\u5b83\u4f1a\u5b58\u50a8\u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u3001\u7d22\u5f15\u5230\u5355\u8bcd\u7684\u53cd\u5411\u6620\u5c04\u3001\u6bcf\u4e2a\u5355\u8bcd\u7684\u8ba1\u6570\u548c\u603b\u5355\u8bcd\u91cf\u3002\u8fd9\u4e2a\u7c7b\u63d0\u4f9b\u5411\u8bcd\u6c47\u8868\u4e2d\u6dfb\u52a0\u5355\u8bcd\u7684\u65b9\u6cd5(<code>addWord</code>)\u3001\u6dfb\u52a0\u6240\u6709\u5355\u8bcd\u5230\u53e5\u5b50\u4e2d\u7684\u65b9\u6cd5 (<code>addSentence</code>) \u548c\u6e05\u6d17\u4e0d\u5e38\u89c1\u7684\u5355\u8bcd\u65b9\u6cd5(<code>trim</code>)\u3002\u66f4\u591a\u7684\u6570\u636e\u6e05\u6d17\u5728\u540e\u9762\u8fdb\u884c\u3002</p> <pre><code># Default word tokens\nPAD_token = 0  # Used for padding short sentences\nSOS_token = 1  # Start-of-sentence token\nEOS_token = 2  # End-of-sentence token\n\nclass Voc:\n    def __init__(self, name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3  # Count SOS, EOS, PAD\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            self.word2count[word] += 1\n\n    # \u5220\u9664\u4f4e\u4e8e\u7279\u5b9a\u8ba1\u6570\u9608\u503c\u7684\u5355\u8bcd\n    def trim(self, min_count):\n        if self.trimmed:\n            return\n        self.trimmed = True\n\n        keep_words = []\n\n        for k, v in self.word2count.items():\n            if v &gt;= min_count:\n                keep_words.append(k)\n\n        print('keep_words {} / {} = {:.4f}'.format(\n            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n        ))\n\n        # Reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count default tokens\n\n        for word in keep_words:\n            self.addWord(word)\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u7ec4\u88c5\u8bcd\u6c47\u8868\u548c\u67e5\u8be2/\u54cd\u5e94\u8bed\u53e5\u5bf9\u3002\u5728\u4f7f\u7528\u6570\u636e\u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u505a\u4e00\u4e9b\u9884\u5904\u7406\u3002</p> <p>\u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u4f7f\u7528<code>unicodeToAscii</code>\u5c06unicode\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3aASCII\u3002\u7136\u540e\uff0c\u6211\u4eec\u5e94\u8be5\u5c06\u6240\u6709\u5b57\u6bcd\u8f6c\u6362\u4e3a\u5c0f\u5199\u5b57\u6bcd\u5e76\u6e05\u6d17\u6389\u9664\u57fa\u672c\u6807\u70b9\u4e4b\u5916\u7684\u6240\u6709\u975e\u5b57\u6bcd\u5b57\u7b26 (<code>normalizeString</code>)\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u5e2e\u52a9\u8bad\u7ec3\u6536\u655b\uff0c\u6211\u4eec\u5c06\u8fc7\u6ee4\u6389\u957f\u5ea6\u5927\u4e8e<code>MAX_LENGTH</code> \u7684\u53e5\u5b50 (<code>filterPairs</code>)\u3002</p> <pre><code>MAX_LENGTH = 10  # Maximum sentence length to consider\n\n# Turn a Unicode string to plain ASCII, thanks to\n# https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# \u521d\u59cb\u5316Voc\u5bf9\u8c61 \u548c \u683c\u5f0f\u5316pairs\u5bf9\u8bdd\u5b58\u653e\u5230list\u4e2d\ndef readVocs(datafile, corpus_name):\n    print(\"Reading lines...\")\n    # Read the file and split into lines\n    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n    voc = Voc(corpus_name)\n    return voc, pairs\n\n# \u5982\u679c\u5bf9 'p' \u4e2d\u7684\u4e24\u4e2a\u53e5\u5b50\u90fd\u4f4e\u4e8e MAX_LENGTH \u9608\u503c\uff0c\u5219\u8fd4\u56deTrue\ndef filterPair(p):\n    # Input sequences need to preserve the last word for EOS token\n    return len(p[0].split(' ')) &lt; MAX_LENGTH and len(p[1].split(' ')) &lt; MAX_LENGTH\n\n# \u8fc7\u6ee4\u6ee1\u8db3\u6761\u4ef6\u7684 pairs \u5bf9\u8bdd\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\n# \u4f7f\u7528\u4e0a\u9762\u5b9a\u4e49\u7684\u51fd\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a\u586b\u5145\u7684voc\u5bf9\u8c61\u548c\u5bf9\u5217\u8868\ndef loadPrepareData(corpus, corpus_name, datafile, save_dir):\n    print(\"Start preparing training data ...\")\n    voc, pairs = readVocs(datafile, corpus_name)\n    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n    print(\"Counting words...\")\n    for pair in pairs:\n        voc.addSentence(pair[0])\n        voc.addSentence(pair[1])\n    print(\"Counted words:\", voc.num_words)\n    return voc, pairs\n\n# Load/Assemble voc and pairs\nsave_dir = os.path.join(\"data\", \"save\")\nvoc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n# Print some pairs to validate\nprint(\"\\npairs:\")\nfor pair in pairs[:10]:\n    print(pair)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Start preparing training data ...\nReading lines...\nRead 221282 sentence pairs\nTrimmed to 64271 sentence pairs\nCounting words...\nCounted words: 18008\n\npairs:\n['there .', 'where ?']\n['you have my word . as a gentleman', 'you re sweet .']\n['hi .', 'looks like things worked out tonight huh ?']\n['you know chastity ?', 'i believe we share an art instructor']\n['have fun tonight ?', 'tons']\n['well no . . .', 'then that s all you had to say .']\n['then that s all you had to say .', 'but']\n['but', 'you always been this selfish ?']\n['do you listen to this crap ?', 'what crap ?']\n['what good stuff ?', 'the real you .']\n\n</code></pre> <p>\u53e6\u4e00\u79cd\u6709\u5229\u4e8e\u8ba9\u8bad\u7ec3\u66f4\u5feb\u6536\u655b\u7684\u7b56\u7565\u662f\u53bb\u9664\u8bcd\u6c47\u8868\u4e2d\u5f88\u5c11\u4f7f\u7528\u7684\u5355\u8bcd\u3002\u51cf\u5c11\u7279\u5f81\u7a7a\u95f4\u4e5f\u4f1a\u964d\u4f4e\u6a21\u578b\u5b66\u4e60\u76ee\u6807\u51fd\u6570\u7684\u96be\u5ea6\u3002\u6211\u4eec\u901a\u8fc7\u4ee5\u4e0b\u4e24\u4e2a\u6b65\u9aa4\u5b8c\u6210\u8fd9\u4e2a\u64cd\u4f5c:</p> <ol> <li>\u4f7f\u7528 <code>voc.trim</code> \u51fd\u6570\u53bb\u9664 <code>MIN_COUNT</code> \u9608\u503c\u4ee5\u4e0b\u5355\u8bcd \u3002</li> <li>\u5982\u679c\u53e5\u5b50\u4e2d\u5305\u542b\u8bcd\u9891\u8fc7\u5c0f\u7684\u5355\u8bcd\uff0c\u90a3\u4e48\u6574\u4e2a\u53e5\u5b50\u4e5f\u88ab\u8fc7\u6ee4\u6389\u3002</li> </ol> <pre><code>MIN_COUNT = 3    # Minimum word count threshold for trimming\n\ndef trimRareWords(voc, pairs, MIN_COUNT):\n    # Trim words used under the MIN_COUNT from the voc\n    voc.trim(MIN_COUNT)\n    # Filter out pairs with trimmed words\n    keep_pairs = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        output_sentence = pair[1]\n        keep_input = True\n        keep_output = True\n        # Check input sentence\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_input = False\n                break\n        # Check output sentence\n        for word in output_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_output = False\n                break\n\n        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n        if keep_input and keep_output:\n            keep_pairs.append(pair)\n\n    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n    return keep_pairs\n\n# Trim voc and pairs\npairs = trimRareWords(voc, pairs, MIN_COUNT)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>keep_words 7823 / 18005 = 0.4345\nTrimmed from 64271 pairs to 53165, 0.8272 of total\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_6","title":"\u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e","text":"<p>\u5c3d\u7ba1\u6211\u4eec\u5df2\u7ecf\u6295\u5165\u4e86\u5927\u91cf\u7cbe\u529b\u6765\u51c6\u5907\u548c\u6e05\u6d17\u6211\u4eec\u7684\u6570\u636e\u53d8\u6210\u4e00\u4e2a\u5f88\u597d\u7684\u8bcd\u6c47\u5bf9\u8c61\u548c\u4e00\u7cfb\u5217\u7684\u53e5\u5b50\u5bf9\uff0c\u4f46\u6211\u4eec\u7684\u6a21\u578b\u6700\u7ec8\u5e0c\u671b\u4ee5numerical torch \u5f20\u91cf\u4f5c\u4e3a\u8f93\u5165\u3002 \u53ef\u4ee5\u5728 seq2seq translation tutorial \u4e2d\u627e\u5230\u4e3a\u6a21\u578b\u51c6\u5907\u5904\u7406\u6570\u636e\u7684\u4e00\u79cd\u65b9\u6cd5\u3002 \u5728\u8be5\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528batch size \u5927\u5c0f\u4e3a1\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u6240\u8981\u505a\u7684\u5c31\u662f\u5c06\u53e5\u5b50\u5bf9\u4e2d\u7684\u5355\u8bcd\u8f6c\u6362\u4e3a\u8bcd\u6c47\u8868\u4e2d\u7684\u76f8\u5e94\u7d22\u5f15\uff0c\u5e76\u5c06\u5176\u63d0\u4f9b\u7ed9\u6a21\u578b\u3002</p> <p>\u4f46\u662f\uff0c\u5982\u679c\u4f60\u60f3\u8981\u52a0\u901f\u8bad\u7ec3\u6216\u8005\u60f3\u8981\u5229\u7528GPU\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\uff0c\u5219\u9700\u8981\u4f7f\u7528\u5c0f\u6279\u91cf <code>mini-batches</code> \u6765\u8bad\u7ec3\u3002</p> <p>\u4f7f\u7528\u5c0f\u6279\u91cf <code>mini-batches</code> \u4e5f\u610f\u5473\u7740\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u6279\u91cf\u5904\u7406\u4e2d\u53e5\u5b50\u957f\u5ea6\u7684\u53d8\u5316\u3002 \u4e3a\u4e86\u5bb9\u7eb3\u540c\u4e00\u6279\u6b21\u4e2d\u4e0d\u540c\u5927\u5c0f\u7684\u53e5\u5b50\uff0c\u6211\u4eec\u5c06\u4f7f\u6211\u4eec\u7684\u6279\u91cf\u8f93\u5165\u5f20\u91cf\u5927\u5c0f (max_length\uff0cbatch_size)\uff0c\u5176\u4e2d\u77ed\u4e8e max_length \u7684\u53e5\u5b50\u5728 EOS_token \u4e4b\u540e\u8fdb\u884c\u96f6\u586b\u5145(zero padded\uff09\u3002</p> <p>\u5982\u679c\u6211\u4eec\u7b80\u5355\u5730\u901a\u8fc7\u5c06\u5355\u8bcd\u8f6c\u6362\u4e3a\u7d22\u5f15 <code>indicesFromSentence</code> \u548c\u96f6\u586b\u5145 <code>zero-pad</code> \u5c06\u6211\u4eec\u7684\u82f1\u6587\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\uff0c\u6211\u4eec\u7684\u5f20\u91cf\u5c06\u5177\u6709\u5927\u5c0f <code>(batch_size\uff0cmax_length)</code>\uff0c\u5e76\u4e14\u7d22\u5f15\u7b2c\u4e00\u7ef4\u5c06\u5728\u6240\u6709\u65f6\u95f4\u6b65\u9aa4\u4e2d\u8fd4\u56de\u5b8c\u6574\u5e8f\u5217\u3002 \u4f46\u662f\uff0c\u6211\u4eec\u9700\u8981\u6cbf\u7740\u65f6\u95f4\u5bf9\u6211\u4eec\u6279\u91cf\u6570\u636e\u8fdb\u884c\u7d22\u5f15\u5e76\u4e14\u5305\u62ec\u6279\u91cf\u6570\u636e\u4e2d\u6240\u6709\u5e8f\u5217\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u8f93\u5165\u6279\u5904\u7406\u5927\u5c0f\u8f6c\u6362\u4e3a <code>(max_length\uff0cbatch_size)</code>\uff0c\u4ee5\u4fbf\u8de8\u7b2c\u4e00\u7ef4\u7684\u7d22\u5f15\u8fd4\u56de\u6279\u5904\u7406\u4e2d\u6240\u6709\u53e5\u5b50\u7684\u65f6\u95f4\u6b65\u957f\u3002 \u6211\u4eec\u5728 <code>zeroPadding</code> \u51fd\u6570\u4e2d\u9690\u5f0f\u5904\u7406\u8fd9\u4e2a\u8f6c\u7f6e\u3002</p> <p></p> <p><code>inputvar</code> \u51fd\u6570\u5904\u7406\u5c06\u53e5\u5b50\u8f6c\u6362\u4e3a\u5f20\u91cf\u7684\u8fc7\u7a0b\uff0c\u6700\u7ec8\u521b\u5efa\u6b63\u786e\u5927\u5c0f\u7684\u96f6\u586b\u5145\u5f20\u91cf\u3002\u5b83\u8fd8\u8fd4\u56de\u6279\u5904\u7406\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u7684\u957f\u5ea6\u5f20\u91cf <code>(tensor of lengths)</code>\uff0c\u957f\u5ea6\u5f20\u91cf\u7a0d\u540e\u5c06\u4f20\u9012\u7ed9\u6211\u4eec\u7684\u89e3\u7801\u5668\u3002</p> <p><code>outputvar</code> \u51fd\u6570\u6267\u884c\u4e0e <code>inputvar</code> \u7c7b\u4f3c\u7684\u51fd\u6570\uff0c\u4f46\u4ed6\u4e0d\u8fd4\u56de\u957f\u5ea6\u5f20\u91cf\uff0c\u800c\u662f\u8fd4\u56de\u4e8c\u8fdb\u5236 mask tensor \u548c\u6700\u5927\u76ee\u6807\u53e5\u5b50\u957f\u5ea6\u3002\u4e8c\u8fdb\u5236 mask tensor \u7684\u5927\u5c0f\u4e0e\u8f93\u51fa\u76ee\u6807\u5f20\u91cf\u7684\u5927\u5c0f\u76f8\u540c\uff0c\u4f46\u4f5c\u4e3a PAD_token \u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f0\u800c\u5176\u4ed6\u5143\u7d20\u90fd\u662f1\u3002</p> <p><code>batch2traindata</code> \u53ea\u9700\u8981\u53d6\u4e00\u6279\u53e5\u5b50\u5bf9\uff0c\u5e76\u4f7f\u7528\u4e0a\u8ff0\u51fd\u6570\u8fd4\u56de\u8f93\u5165\u5f20\u91cf\u548c\u76ee\u6807\u5f20\u91cf\u3002</p> <pre><code>def indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n\n# zip \u5bf9\u6570\u636e\u8fdb\u884c\u5408\u5e76\u4e86\uff0c\u76f8\u5f53\u4e8e\u884c\u5217\u8f6c\u7f6e\u4e86\ndef zeroPadding(l, fillvalue=PAD_token):\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n\n# \u8bb0\u5f55 PAD_token\u7684\u4f4d\u7f6e\u4e3a0\uff0c \u5176\u4ed6\u7684\u4e3a1\ndef binaryMatrix(l, value=PAD_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\n# \u8fd4\u56de\u586b\u5145\u524d(\u52a0\u5165\u7ed3\u675findex EOS_token\u505a\u6807\u8bb0\uff09\u7684\u957f\u5ea6 \u548c \u586b\u5145\u540e\u7684\u8f93\u5165\u5e8f\u5217\u5f20\u91cf\ndef inputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n\n# \u8fd4\u56de\u586b\u5145\u524d(\u52a0\u5165\u7ed3\u675findex EOS_token\u505a\u6807\u8bb0\uff09\u6700\u957f\u7684\u4e00\u4e2a\u957f\u5ea6 \u548c \u586b\u5145\u540e\u7684\u8f93\u5165\u5e8f\u5217\u5f20\u91cf, \u548c \u586b\u5145\u540e\u7684\u6807\u8bb0 mask\ndef outputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    max_target_len = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.ByteTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar, mask, max_target_len\n\n# Returns all items for a given batch of pairs\ndef batch2TrainData(voc, pair_batch):\n    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n    input_batch, output_batch = [], []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp, lengths = inputVar(input_batch, voc)\n    output, mask, max_target_len = outputVar(output_batch, voc)\n    return inp, lengths, output, mask, max_target_len\n\n# Example for validation\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable)\nprint(\"lengths:\", lengths)\nprint(\"target_variable:\", target_variable)\nprint(\"mask:\", mask)\nprint(\"max_target_len:\", max_target_len)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>input_variable: tensor([[ 614,  281,   77,  387,  965],\n        [  83,   25,   53,   25, 6430],\n        [  11,  697, 5046,  920,    4],\n        [1054,   50,   14,  174,    2],\n        [  11,    7,    7,    6,    0],\n        [   7, 1825,    6,    2,    0],\n        [  14,  234,    2,    0,    0],\n        [5401,   36,    0,    0,    0],\n        [   4,    4,    0,    0,    0],\n        [   2,    2,    0,    0,    0]])\nlengths: tensor([10, 10,  7,  6,  4])\ntarget_variable: tensor([[  25,    7,    7,  601,   45],\n        [ 356,  697,   53,    4,  410],\n        [   7, 2182, 1231,    2,  218],\n        [   4,    4, 5240,    0,  492],\n        [   2,    2,    6,    0,  227],\n        [   0,    0,    2,    0,    4],\n        [   0,    0,    0,    0,    2]])\nmask: tensor([[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 0, 1],\n        [1, 1, 1, 0, 1],\n        [0, 0, 1, 0, 1],\n        [0, 0, 0, 0, 1]], dtype=torch.uint8)\nmax_target_len: 7\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_7","title":"\u5b9a\u4e49\u6a21\u578b","text":""},{"location":"1.0/chatbot_tutorial/#seq2seq","title":"Seq2Seq\u6a21\u578b","text":"<p>\u6211\u4eec\u804a\u5929\u673a\u5668\u4eba\u7684\u5927\u8111\u662f\u5e8f\u5217\u5230\u5e8f\u5217(seq2seq\uff09\u6a21\u578b\u3002 seq2seq\u6a21\u578b\u7684\u76ee\u6807\u662f\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u6a21\u578b\u5c06\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u4f5c\u4e3a\u8f93\u51fa\u8fd4\u56de\u3002</p> <p>Sutskever et al. \u53d1\u73b0\u901a\u8fc7\u4e00\u8d77\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u7684RNN\uff0c\u6211\u4eec\u53ef\u4ee5\u5b8c\u6210\u8fd9\u9879\u4efb\u52a1\u3002 \u7b2c\u4e00\u4e2aRNN\u5145\u5f53\u7f16\u7801\u5668\uff0c\u5176\u5c06\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u56fa\u5b9a\u957f\u5ea6\u4e0a\u4e0b\u6587\u5411\u91cf\u3002 \u7406\u8bba\u4e0a\uff0c\u8be5\u4e0a\u4e0b\u6587\u5411\u91cf(RNN\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff09\u5c06\u5305\u542b\u5173\u4e8e\u8f93\u5165\u5230\u673a\u5668\u4eba\u7684\u67e5\u8be2\u8bed\u53e5\u7684\u8bed\u4e49\u4fe1\u606f\u3002 \u7b2c\u4e8c\u4e2aRNN\u662f\u4e00\u4e2a\u89e3\u7801\u5668\uff0c\u5b83\u63a5\u6536\u8f93\u5165\u6587\u5b57\u548c\u4e0a\u4e0b\u6587\u77e2\u91cf\uff0c\u5e76\u8fd4\u56de\u5e8f\u5217\u4e2d\u4e0b\u4e00\u53e5\u6587\u5b57\u7684\u6982\u7387\u548c\u5728\u4e0b\u4e00\u6b21\u8fed\u4ee3\u4e2d\u4f7f\u7528\u7684\u9690\u85cf\u72b6\u6001\u3002</p> <p></p> <p>\u56fe\u7247\u6765\u6e90: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</p>"},{"location":"1.0/chatbot_tutorial/#_8","title":"\u7f16\u7801\u5668","text":"<p>\u7f16\u7801\u5668RNN\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u8f93\u5165\u4e00\u4e2a\u8bed\u53e5\u8f93\u51fa\u4e00\u4e2atoken(\u4f8b\u5982\uff0c\u4e00\u4e2a\u5355\u8bcd\uff09\uff0c\u540c\u65f6\u5728\u8fd9\u65f6\u95f4\u5185\u8f93\u51fa\u201c\u8f93\u51fa\u201d\u5411\u91cf\u548c\u201c\u9690\u85cf\u72b6\u6001\u201d\u5411\u91cf\u3002 \u7136\u540e\u5c06\u9690\u85cf\u72b6\u6001\u5411\u91cf\u4f20\u9012\u5230\u4e0b\u4e00\u6b65\uff0c\u5e76\u8bb0\u5f55\u8f93\u51fa\u5411\u91cf\u3002 \u7f16\u7801\u5668\u5c06\u5176\u5728\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e00\u70b9\u5904\u770b\u5230\u7684\u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u7cfb\u5217\u70b9\uff0c\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e9b\u70b9\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u751f\u6210\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002</p> <p>\u6211\u4eec\u7684\u7f16\u7801\u5668\u7684\u6838\u5fc3\u662f\u7531  Cho et al. \u7b49\u4eba\u53d1\u660e\u7684\u591a\u5c42\u95e8\u5faa\u73af\u5355\u5143\u3002 \u57282014\u5e74\uff0c\u6211\u4eec\u5c06\u4f7f\u7528GRU\u7684\u53cc\u5411\u53d8\u4f53\uff0c\u8fd9\u610f\u5473\u7740\u57fa\u672c\u4e0a\u6709\u4e24\u4e2a\u72ec\u7acb\u7684RNN\uff1a\u4e00\u4e2a\u4ee5\u6b63\u5e38\u7684\u987a\u5e8f\u8f93\u5165\u8f93\u5165\u5e8f\u5217\uff0c\u53e6\u4e00\u4e2a\u4ee5\u76f8\u53cd\u7684\u987a\u5e8f\u8f93\u5165\u8f93\u5165\u5e8f\u5217\u3002 \u6bcf\u4e2a\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u6c42\u548c\u3002 \u4f7f\u7528\u53cc\u5411GRU\u5c06\u4e3a\u6211\u4eec\u63d0\u4f9b\u7f16\u7801\u8fc7\u53bb\u548c\u672a\u6765\u4e0a\u4e0b\u6587\u7684\u4f18\u52bf\u3002</p> <p>\u53cc\u5411RNN\uff1a</p> <p></p> <p>\u56fe\u7247\u6765\u6e90: https://colah.github.io/posts/2015-09-NN-Types-FP/</p> <p>\u6ce8\u610f:<code>embedding</code>\u5c42\u7528\u4e8e\u5728\u4efb\u610f\u5927\u5c0f\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9\u6211\u4eec\u7684\u5355\u8bcd\u7d22\u5f15\u8fdb\u884c\u7f16\u7801\u3002 \u5bf9\u4e8e\u6211\u4eec\u7684\u6a21\u578b\uff0c\u6b64\u56fe\u5c42\u4f1a\u5c06\u6bcf\u4e2a\u5355\u8bcd\u6620\u5c04\u5230\u5927\u5c0f\u4e3ahidden_size\u7684\u7279\u5f81\u7a7a\u95f4\u3002 \u8bad\u7ec3\u540e\uff0c\u8fd9\u4e9b\u503c\u4f1a\u88ab\u7f16\u7801\u6210\u548c\u4ed6\u4eec\u76f8\u4f3c\u7684\u6709\u610f\u4e49\u8bcd\u8bed\u3002</p> <p>\u6700\u540e\uff0c\u5982\u679c\u5c06\u586b\u5145\u7684\u4e00\u6279\u5e8f\u5217\u4f20\u9012\u7ed9RNN\u6a21\u5757\uff0c\u6211\u4eec\u5fc5\u987b\u5206\u522b\u4f7f\u7528<code>torch.nn.utils.rnn.pack_padded_sequence</code>\u548c<code>torch.nn.utils.rnn.pad_packed_sequence</code>\u5728RNN\u4f20\u9012\u65f6\u5206\u522b\u8fdb\u884c\u586b\u5145\u548c\u53cd\u586b\u5145\u3002</p> <p>\u8ba1\u7b97\u56fe:</p> <ol> <li>\u5c06\u5355\u8bcd\u7d22\u5f15\u8f6c\u6362\u4e3a\u8bcd\u5d4c\u5165 embeddings\u3002</li> <li>\u4e3aRNN\u6a21\u5757\u6253\u5305\u586b\u5145\u6279\u6b21\u5e8f\u5217\u3002</li> <li>\u901a\u8fc7GRU\u8fdb\u884c\u524d\u5411\u4f20\u64ad\u3002</li> <li>\u53cd\u586b\u5145\u3002</li> <li>\u5bf9\u53cc\u5411GRU\u8f93\u51fa\u6c42\u548c\u3002</li> <li>\u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u3002</li> </ol> <p>\u8f93\u5165:</p> <ul> <li><code>input_seq</code>\uff1a\u4e00\u6279\u8f93\u5165\u53e5\u5b50; shape =(max_length\uff0cbatch_size\uff09</li> <li><code>input_lengths</code>\uff1a\u4e00\u6279\u6b21\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u5bf9\u5e94\u7684\u53e5\u5b50\u957f\u5ea6\u5217\u8868;shape=(batch_size)</li> <li><code>hidden</code>:\u9690\u85cf\u72b6\u6001; shape =(n_layers x num_directions\uff0cbatch_size\uff0chidden_size)</li> </ul> <p>\u8f93\u51fa:</p> <ul> <li><code>outputs</code>\uff1aGRU\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u5c42\u7684\u8f93\u51fa\u7279\u5f81(\u53cc\u5411\u8f93\u51fa\u4e4b\u548c\uff09; shape =(max_length\uff0cbatch_size\uff0chidden_size\uff09</li> <li><code>hidden</code>\uff1a\u4eceGRU\u66f4\u65b0\u9690\u85cf\u72b6\u6001; shape =(n_layers x num_directions\uff0cbatch_size\uff0chidden_size\uff09</li> </ul> <pre><code>class EncoderRNN(nn.Module):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n        #   because our input size is a word embedding with number of features == hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Convert word indexes to embeddings\n        embedded = self.embedding(input_seq)\n        # Pack padded batch of sequences for RNN module\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        # Forward pass through GRU\n        outputs, hidden = self.gru(packed, hidden)\n        # Unpack padding\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        # Sum bidirectional GRU outputs\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        # Return output and final hidden state\n        return outputs, hidden\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_9","title":"\u89e3\u7801\u5668","text":"<p>\u89e3\u7801\u5668RNN\u4ee5token-by-token\u7684\u65b9\u5f0f\u751f\u6210\u54cd\u5e94\u8bed\u53e5\u3002 \u5b83\u4f7f\u7528\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u548c\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6765\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002 \u5b83\u6301\u7eed\u751f\u6210\u5355\u8bcd\uff0c\u76f4\u5230\u8f93\u51fa\u662fEOS_token\uff0c\u8fd9\u4e2a\u8868\u793a\u53e5\u5b50\u7684\u7ed3\u5c3e\u3002 \u4e00\u4e2avanilla seq2seq\u89e3\u7801\u5668\u7684\u5e38\u89c1\u95ee\u9898\u662f\uff0c\u5982\u679c\u6211\u4eec\u53ea\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u5411\u91cf\u6765\u7f16\u7801\u6574\u4e2a\u8f93\u5165\u5e8f\u5217\u7684\u542b\u4e49\uff0c\u90a3\u4e48\u6211\u4eec\u5f88\u53ef\u80fd\u4f1a\u4e22\u5931\u4fe1\u606f\u3002\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u8f93\u5165\u5e8f\u5217\u65f6\uff0c\u8fd9\u6781\u5927\u5730\u9650\u5236\u4e86\u6211\u4eec\u7684\u89e3\u7801\u5668\u7684\u80fd\u529b\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c,Bahdanau et al. \u7b49\u4eba\u521b\u5efa\u4e86\u4e00\u79cd\u201cattention mechanism\u201d\uff0c\u5141\u8bb8\u89e3\u7801\u5668\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u7684\u67d0\u4e9b\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u5728\u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u5b8c\u5168\u56fa\u5b9a\u7684\u4e0a\u4e0b\u6587\u3002</p> <p>\u5728\u4e00\u4e2a\u9ad8\u7684\u5c42\u7ea7\u4e2d\uff0c\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u9690\u85cf\u72b6\u6001\u548c\u7f16\u7801\u5668\u8f93\u51fa\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u3002 \u8f93\u51fa\u6ce8\u610f\u529b\u7684\u6743\u91cd\u4e0e\u8f93\u5165\u5e8f\u5217\u5177\u6709\u76f8\u540c\u7684\u5927\u5c0f\uff0c\u5141\u8bb8\u6211\u4eec\u5c06\u5b83\u4eec\u4e58\u4ee5\u7f16\u7801\u5668\u8f93\u51fa\uff0c\u7ed9\u51fa\u4e00\u4e2a\u52a0\u6743\u548c\uff0c\u8868\u793a\u8981\u6ce8\u610f\u7684\u7f16\u7801\u5668\u8f93\u51fa\u90e8\u5206\u3002 Sean Robertson \u7684\u56fe\u7247\u5f88\u597d\u5730\u63cf\u8ff0\u4e86\u8fd9\u4e00\u70b9\uff1a</p> <p></p> <p>Luong et al. \u901a\u8fc7\u521b\u9020\u201cGlobal attention\u201d\uff0c\u6539\u5584\u4e86Bahdanau et al. \u7684\u57fa\u7840\u5de5\u4f5c\u3002 \u5173\u952e\u7684\u533a\u522b\u5728\u4e8e\uff0c\u5bf9\u4e8e\u201cGlobal attention\u201d\uff0c\u6211\u4eec\u8003\u8651\u6240\u6709\u7f16\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\uff0c\u800c\u4e0d\u662fBahdanau\u7b49\u4eba\u7684\u201cLocal attention\u201d\uff0c\u5b83\u53ea\u8003\u8651\u5f53\u524d\u6b65\u4e2d\u7f16\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u3002 \u53e6\u4e00\u4e2a\u533a\u522b\u5728\u4e8e\uff0c\u901a\u8fc7\u201cGlobal attention\u201d\uff0c\u6211\u4eec\u4ec5\u4f7f\u7528\u5f53\u524d\u6b65\u7684\u89e3\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd(\u6216\u8005\u80fd\u91cf\uff09\u3002 Bahdanau\u7b49\u4eba\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u9700\u8981\u77e5\u9053\u524d\u4e00\u6b65\u4e2d\u89e3\u7801\u5668\u7684\u72b6\u6001\u3002 \u6b64\u5916\uff0cLuong\u7b49\u4eba\u63d0\u4f9b\u5404\u79cd\u65b9\u6cd5\u6765\u8ba1\u7b97\u7f16\u7801\u5668\u8f93\u51fa\u548c\u89e3\u7801\u5668\u8f93\u51fa\u4e4b\u95f4\u7684\u6ce8\u610f\u6743\u91cd(\u80fd\u91cf\uff09\uff0c\u79f0\u4e4b\u4e3a\u201cscore functions\u201d\uff1a</p> <p></p> <p>\u5176\u4e2d \\(\\(h_t\\)\\) = \u5f53\u524d\u76ee\u6807\u89e3\u7801\u5668\u72b6\u6001\uff0c\\(\\(\\bar{h}_s\\)\\) = \u6240\u6709\u7f16\u7801\u5668\u72b6\u6001\u3002</p> <p>\u603b\u4f53\u800c\u8a00\uff0cGlobal attention\u673a\u5236\u53ef\u4ee5\u901a\u8fc7\u4e0b\u56fe\u8fdb\u884c\u603b\u7ed3\u3002 \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5c06\u201cAttention Layer\u201d\u7528\u4e00\u4e2a\u540d\u4e3a <code>Attn</code> \u7684 <code>nn.Module</code> \u6765\u5355\u72ec\u5b9e\u73b0\u3002 \u8be5\u6a21\u5757\u7684\u8f93\u51fa\u662f\u7ecf\u8fc7softmax\u6807\u51c6\u5316\u540e\u6743\u91cd\u5f20\u91cf\u7684\u5927\u5c0f(batch_size\uff0c1\uff0cmax_length\uff09\u3002</p> <p></p> <pre><code># Luong attention layer\nclass Attn(torch.nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n        self.method = method\n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, \"is not an appropriate attention method.\")\n        self.hidden_size = hidden_size\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n\n    def dot_score(self, hidden, encoder_output):\n        return torch.sum(hidden * encoder_output, dim=2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n\n    def forward(self, hidden, encoder_outputs):\n        # Calculate the attention weights (energies) based on the given method\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden, encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden, encoder_outputs)\n\n        # Transpose max_length and batch_size dimensions\n        attn_energies = attn_energies.t()\n\n        # Return the softmax normalized probability scores (with added dimension)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u6ce8\u610f\u529b\u5b50\u6a21\u5757\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u73b0\u771f\u5b9e\u7684\u89e3\u7801\u5668\u6a21\u578b\u3002 \u5bf9\u4e8e\u89e3\u7801\u5668\uff0c\u6211\u4eec\u5c06\u6bcf\u6b21\u624b\u52a8\u8fdb\u884c\u4e00\u6279\u6b21\u7684\u8f93\u5165\u3002 \u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u8bcd\u5d4c\u5165\u5f20\u91cf\u548cGRU\u8f93\u51fa\u90fd\u5c06\u5177\u6709\u76f8\u540c\u5927\u5c0f(1\uff0cbatch_size\uff0chidden_size\uff09\u3002</p> <p>\u8ba1\u7b97\u56fe:</p> <ol> <li>\u83b7\u53d6\u5f53\u524d\u8f93\u5165\u7684\u8bcd\u5d4c\u5165</li> <li>\u901a\u8fc7\u5355\u5411GRU\u8fdb\u884c\u524d\u5411\u4f20\u64ad</li> <li>\u901a\u8fc72\u8f93\u51fa\u7684\u5f53\u524dGRU\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd</li> <li>\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u4e58\u4ee5\u7f16\u7801\u5668\u8f93\u51fa\u4ee5\u83b7\u5f97\u65b0\u7684\u201cweighted sum\u201d\u4e0a\u4e0b\u6587\u5411\u91cf</li> <li>\u4f7f\u7528Luong eq.5\u8fde\u63a5\u52a0\u6743\u4e0a\u4e0b\u6587\u5411\u91cf\u548cGRU\u8f93\u51fa</li> <li>\u4f7f\u7528Luong eq.6\u9884\u6d4b\u4e0b\u4e00\u4e2a\u5355\u8bcd(\u6ca1\u6709softmax\uff09</li> <li>\u8fd4\u56de\u8f93\u51fa\u548c\u6700\u7ec8\u9690\u85cf\u72b6\u6001</li> </ol> <p>\u8f93\u5165:</p> <ul> <li><code>input_step</code>\uff1a\u6bcf\u4e00\u6b65\u8f93\u5165\u5e8f\u5217\u6279\u6b21(\u4e00\u4e2a\u5355\u8bcd\uff09; shape =(1\uff0cbatch_size\uff09</li> <li><code>last_hidden</code>\uff1aGRU\u7684\u6700\u7ec8\u9690\u85cf\u5c42; shape =(n_layers x num_directions\uff0cbatch_size\uff0chidden_size\uff09</li> <li><code>encoder_outputs</code>\uff1a\u7f16\u7801\u5668\u6a21\u578b\u7684\u8f93\u51fa; shape =(max_length\uff0cbatch_size\uff0chidden_size\uff09</li> </ul> <p>\u8f93\u51fa:</p> <ul> <li><code>output</code>: \u4e00\u4e2asoftmax\u6807\u51c6\u5316\u540e\u7684\u5f20\u91cf\uff0c \u4ee3\u8868\u4e86\u6bcf\u4e2a\u5355\u8bcd\u5728\u89e3\u7801\u5e8f\u5217\u4e2d\u662f\u4e0b\u4e00\u4e2a\u8f93\u51fa\u5355\u8bcd\u7684\u6982\u7387; shape =(batch_size\uff0cvoc.num_words\uff09</li> <li><code>hidden</code>: GRU\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001; shape =(n_layers x num_directions\uff0cbatch_size\uff0chidden_size\uff09</li> </ul> <pre><code>class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        # Note: we run this one step (word) at a time\n        # Get embedding of current input word\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # Forward through unidirectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        # Calculate attention weights from the current GRU output\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n        # Concatenate weighted context vector and GRU output using Luong eq. 5\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # Predict next word using Luong eq. 6\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        # Return output and final hidden state\n        return output, hidden\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_10","title":"\u5b9a\u4e49\u8bad\u7ec3\u6b65\u9aa4","text":""},{"location":"1.0/chatbot_tutorial/#masked","title":"Masked \u635f\u5931","text":"<p>\u7531\u4e8e\u6211\u4eec\u5904\u7406\u7684\u662f\u6279\u91cf\u586b\u5145\u5e8f\u5217\uff0c\u56e0\u6b64\u5728\u8ba1\u7b97\u635f\u5931\u65f6\u6211\u4eec\u4e0d\u80fd\u7b80\u5355\u5730\u8003\u8651\u5f20\u91cf\u7684\u6240\u6709\u5143\u7d20\u3002 \u6211\u4eec\u5b9a\u4e49<code>maskNLLLoss</code>\u53ef\u4ee5\u6839\u636e\u89e3\u7801\u5668\u7684\u8f93\u51fa\u5f20\u91cf\u3001\u63cf\u8ff0\u76ee\u6807\u5f20\u91cf\u586b\u5145\u7684binary mask\u5f20\u91cf\u6765\u8ba1\u7b97\u635f\u5931\u3002 \u8be5\u635f\u5931\u51fd\u6570\u8ba1\u7b97\u4e0emask tensor\u4e2d\u76841\u5bf9\u5e94\u7684\u5143\u7d20\u7684\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136\u3002</p> <pre><code>def maskNLLLoss(inp, target, mask):\n    nTotal = mask.sum()\n    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n    loss = crossEntropy.masked_select(mask).mean()\n    loss = loss.to(device)\n    return loss, nTotal.item()\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_11","title":"\u5355\u6b21\u8bad\u7ec3\u8fed\u4ee3","text":"<p><code>train</code> \u51fd\u6570\u5305\u542b\u5355\u6b21\u8bad\u7ec3\u8fed\u4ee3\u7684\u7b97\u6cd5(\u5355\u6279\u8f93\u5165\uff09\u3002</p> <p>\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e9b\u5de7\u5999\u7684\u6280\u5de7\u6765\u5e2e\u52a9\u878d\u5408\uff1a * \u7b2c\u4e00\u4e2a\u6280\u5de7\u662f\u4f7f\u7528 teacher forcing\u3002 \u8fd9\u610f\u5473\u7740\u5728\u4e00\u4e9b\u6982\u7387\u662f\u7531<code>teacher_forcing_ratio</code>\u8bbe\u7f6e\uff0c\u6211\u4eec\u4f7f\u7528\u5f53\u524d\u76ee\u6807\u5355\u8bcd\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u4e0b\u4e00\u4e2a\u8f93\u5165\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u89e3\u7801\u5668\u7684\u5f53\u524d\u63a8\u6d4b\u3002 \u8be5\u6280\u5de7\u5145\u5f53\u89e3\u7801\u5668\u7684 training wheels\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u3002 \u7136\u800c\uff0cteacher forcing \u53ef\u80fd\u5bfc\u81f4\u63a8\u5bfc\u4e2d\u7684\u6a21\u578b\u4e0d\u7a33\u5b9a\uff0c\u56e0\u4e3a\u89e3\u7801\u5668\u53ef\u80fd\u6ca1\u6709\u8db3\u591f\u7684\u673a\u4f1a\u5728\u8bad\u7ec3\u671f\u95f4\u771f\u6b63\u5730\u5236\u4f5c\u81ea\u5df1\u7684\u8f93\u51fa\u5e8f\u5217\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u6ce8\u610f\u6211\u4eec\u5982\u4f55\u8bbe\u7f6e<code>teacher_forcing_ratio</code>\uff0c\u540c\u65f6\u4e0d\u8981\u88ab\u5feb\u901f\u7684\u6536\u655b\u6240\u8ff7\u60d1\u3002 * \u6211\u4eec\u5b9e\u73b0\u7684\u7b2c\u4e8c\u4e2a\u6280\u5de7\u662f\u68af\u5ea6\u88c1\u526a(gradient clipping)\u3002 \u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u5bf9\u6297\u201c\u7206\u70b8\u68af\u5ea6(exploding gradient\uff09\u201d\u95ee\u9898\u7684\u5e38\u7528\u6280\u672f\u3002 \u672c\u8d28\u4e0a\uff0c\u901a\u8fc7\u5c06\u68af\u5ea6\u526a\u5207\u6216\u9608\u503c\u5316\u5230\u6700\u5927\u503c\uff0c\u6211\u4eec\u53ef\u4ee5\u9632\u6b62\u5728\u635f\u5931\u51fd\u6570\u4e2d\u68af\u5ea6\u4ee5\u6307\u6570\u65b9\u5f0f\u589e\u957f\u5e76\u53d1\u751f\u6ea2\u51fa(NaN\uff09\u6216\u8005\u8d8a\u8fc7\u68af\u5ea6\u9661\u5ced\u7684\u60ac\u5d16\u3002</p> <p></p> <p>\u56fe\u7247\u6765\u6e90: Goodfellow et al. Deep Learning. 2016. https://www.deeplearningbook.org/</p> <p>Sequence of Operations:</p> <p>\u64cd\u4f5c\u987a\u5e8f:</p> <ol> <li>\u901a\u8fc7\u7f16\u7801\u5668\u524d\u5411\u8ba1\u7b97\u6574\u4e2a\u6279\u6b21\u8f93\u5165\u3002</li> <li>\u5c06\u89e3\u7801\u5668\u8f93\u5165\u521d\u59cb\u5316\u4e3aSOS_token\uff0c\u5c06\u9690\u85cf\u72b6\u6001\u521d\u59cb\u5316\u4e3a\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u3002</li> <li>\u901a\u8fc7\u89e3\u7801\u5668\u4e00\u6b21\u4e00\u6b65\u5730\u524d\u5411\u8ba1\u7b97\u8f93\u5165\u4e00\u6279\u5e8f\u5217\u3002</li> <li>\u5982\u679cteacher forcing\u7b97\u6cd5\uff1a\u5c06\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u8bbe\u7f6e\u4e3a\u5f53\u524d\u76ee\u6807; \u5426\u5219\uff1a\u5c06\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u8f93\u5165\u8bbe\u7f6e\u4e3a\u5f53\u524d\u89e3\u7801\u5668\u8f93\u51fa\u3002</li> <li>\u8ba1\u7b97\u5e76\u7d2f\u79ef\u635f\u5931\u3002</li> <li>\u6267\u884c\u53cd\u5411\u4f20\u64ad\u3002</li> <li>\u88c1\u526a\u68af\u5ea6\u3002</li> <li>\u66f4\u65b0\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u53c2\u6570\u3002</li> </ol> <p>\u6ce8\u610f:</p> <p>PyTorch\u7684RNN\u6a21\u5757(<code>RNN</code>\uff0c<code>LSTM</code>\uff0c<code>GRU</code>\uff09\u53ef\u4ee5\u50cf\u4efb\u4f55\u5176\u4ed6\u975e\u91cd\u590d\u5c42\u4e00\u6837\u4f7f\u7528\uff0c\u53ea\u9700\u5c06\u6574\u4e2a\u8f93\u5165\u5e8f\u5217(\u6216\u4e00\u6279\u5e8f\u5217\uff09\u4f20\u9012\u7ed9\u5b83\u4eec\u3002 \u6211\u4eec\u5728<code>\u7f16\u7801\u5668</code>\u4e2d\u4f7f\u7528<code>GRU</code>\u5c42\u5c31\u662f\u8fd9\u6837\u7684\u3002 \u5b9e\u9645\u60c5\u51b5\u662f\uff0c\u5728\u8ba1\u7b97\u4e2d\u6709\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\u5faa\u73af\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u7684\u6bcf\u4e00\u6b65\u3002 \u6216\u8005\uff0c\u4f60\u6bcf\u6b21\u53ea\u8fd0\u884c\u4e00\u4e2a\u6a21\u5757\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u624b\u52a8\u5faa\u73af\u904d\u5386\u5e8f\u5217\u5c31\u50cf\u6211\u4eec\u5fc5\u987b\u4e3a<code>\u89e3\u7801\u5668</code>\u6a21\u578b\u505a\u7684\u90a3\u6837\u3002 \u53ea\u8981\u4f60\u6b63\u786e\u7684\u7ef4\u62a4\u8fd9\u4e9b\u6a21\u578b\u7684\u6a21\u5757\uff0c\u5c31\u53ef\u4ee5\u975e\u5e38\u7b80\u5355\u7684\u5b9e\u73b0\u987a\u5e8f\u6a21\u578b\u3002</p> <pre><code>def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n\n    # Zero gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Set device options\n    input_variable = input_variable.to(device)\n    lengths = lengths.to(device)\n    target_variable = target_variable.to(device)\n    mask = mask.to(device)\n\n    # Initialize variables\n    loss = 0\n    print_losses = []\n    n_totals = 0\n\n    # Forward pass through encoder\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n\n    # Create initial decoder input (start with SOS tokens for each sentence)\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n\n    # Set initial decoder hidden state to the encoder's final hidden state\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    # Determine if we are using teacher forcing this iteration\n    use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False\n\n    # Forward batch of sequences through decoder one time step at a time\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # Teacher forcing: next input is current target\n            decoder_input = target_variable[t].view(1, -1)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    else:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # No teacher forcing: next input is decoder's own current output\n            _, topi = decoder_output.topk(1)\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n\n    # Perform backpropatation\n    loss.backward()\n\n    # Clip gradients: gradients are modified in place\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n    # Adjust model weights\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return sum(print_losses) / n_totals\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_12","title":"\u8bad\u7ec3\u8fed\u4ee3","text":"<p>\u73b0\u5728\u7ec8\u4e8e\u5c06\u5b8c\u6574\u7684\u8bad\u7ec3\u6b65\u9aa4\u4e0e\u6570\u636e\u7ed3\u5408\u5728\u4e00\u8d77\u4e86\u3002 \u7ed9\u5b9a\u4f20\u9012\u7684\u6a21\u578b\uff0c\u4f18\u5316\u5668\uff0c\u6570\u636e\u7b49\uff0c<code>trainIters</code>\u51fd\u6570\u8d1f\u8d23\u8fd0\u884c<code>n_iterations</code>\u7684\u8bad\u7ec3\u3002\u8fd9\u4e2a\u529f\u80fd\u4e0d\u8a00\u81ea\u660e\uff0c\u56e0\u4e3a\u6211\u4eec\u901a\u8fc7<code>train</code>\u51fd\u6570\u7684\u5b8c\u6210\u4e86\u7e41\u91cd\u5de5\u4f5c\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff0c\u5f53\u6211\u4eec\u4fdd\u5b58\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u4f1a\u4fdd\u5b58\u4e00\u4e2a\u5305\u542b\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668<code>state_dicts</code>(\u53c2\u6570\uff09\u3001\u4f18\u5316\u5668\u7684state_dicts\u3001\u635f\u5931\u3001\u8fed\u4ee3\u7b49\u7684\u538b\u7f29\u5305\u3002\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u4fdd\u5b58\u6a21\u578b\u5c06\u4e3a\u6211\u4eeccheckpoint,\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\u3002 \u52a0\u8f7dcheckpoint\u540e\uff0c\u6211\u4eec\u5c06\u80fd\u591f\u4f7f\u7528\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u63a8\u7406\uff0c\u6216\u8005\u6211\u4eec\u53ef\u4ee5\u5728\u6211\u4eec\u4e2d\u65ad\u7684\u5730\u65b9\u7ee7\u7eed\u8bad\u7ec3\u3002</p> <pre><code>def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n\n    # Load batches for each iteration\n    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n                      for _ in range(n_iteration)]\n\n    # Initializations\n    print('Initializing ...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n\n    # Training loop\n    print(\"Training...\")\n    for iteration in range(start_iteration, n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        # Extract fields from batch\n        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n\n        # Run a training iteration with batch\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n        print_loss += loss\n\n        # Print progress\n        if iteration % print_every == 0:\n            print_loss_avg = print_loss / print_every\n            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n            print_loss = 0\n\n        # Save checkpoint\n        if (iteration % save_every == 0):\n            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                'iteration': iteration,\n                'en': encoder.state_dict(),\n                'de': decoder.state_dict(),\n                'en_opt': encoder_optimizer.state_dict(),\n                'de_opt': decoder_optimizer.state_dict(),\n                'loss': loss,\n                'voc_dict': voc.__dict__,\n                'embedding': embedding.state_dict()\n            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_13","title":"\u8bc4\u4f30\u5b9a\u4e49","text":"<p>\u5728\u8bad\u7ec3\u6a21\u578b\u540e\uff0c\u6211\u4eec\u5e0c\u671b\u80fd\u591f\u81ea\u5df1\u4e0e\u673a\u5668\u4eba\u4ea4\u8c08\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u5982\u4f55\u89e3\u7801\u7f16\u7801\u8f93\u5165\u3002</p>"},{"location":"1.0/chatbot_tutorial/#_14","title":"\u8d2a\u5a6a\u89e3\u7801","text":"<p>\u8d2a\u5a6a\u89e3\u7801\u662f\u6211\u4eec\u5728\u4e0d\u4f7f\u7528 teacher forcing\u65f6\u5728\u8bad\u7ec3\u671f\u95f4\u4f7f\u7528\u7684\u89e3\u7801\u65b9\u6cd5\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u6b65\uff0c\u6211\u4eec\u53ea\u9700\u4ece\u5177\u6709\u6700\u9ad8softmax\u503c\u7684<code>decoder_output</code>\u4e2d\u9009\u62e9\u5355\u8bcd\u3002 \u8be5\u89e3\u7801\u65b9\u6cd5\u5728\u5355\u6b65\u957f\u7ea7\u522b\u4e0a\u662f\u6700\u4f73\u7684\u3002</p> <p>\u4e3a\u4e86\u4fbf\u4e8e\u8d2a\u5a6a\u89e3\u7801\u64cd\u4f5c\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a<code>GreedySearchDecoder</code>\u7c7b\u3002 \u5f53\u8fd0\u884c\u65f6\uff0c\u7c7b\u7684\u5b9e\u4f8b\u5316\u5bf9\u8c61\u8f93\u5165\u5e8f\u5217(<code>input_seq</code>\uff09\u7684\u5927\u5c0f\u662f(input_seq length\uff0c1\uff09\uff0c\u6807\u91cf\u8f93\u5165(<code>input_length</code>\uff09\u957f\u5ea6\u7684\u5f20\u91cf\u548c<code>max_length</code>\u6765\u7ea6\u675f\u54cd\u5e94\u53e5\u5b50\u957f\u5ea6\u3002 \u4f7f\u7528\u4ee5\u4e0b\u8ba1\u7b97\u56fe\u6765\u8bc4\u4f30\u8f93\u5165\u53e5\u5b50\uff1a</p> <p>\u8ba1\u7b97\u56fe:</p> <ol> <li>\u901a\u8fc7\u7f16\u7801\u5668\u6a21\u578b\u524d\u5411\u8ba1\u7b97\u3002</li> <li>\u51c6\u5907\u7f16\u7801\u5668\u7684\u6700\u7ec8\u9690\u85cf\u5c42\uff0c\u4f5c\u4e3a\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u9690\u85cf\u8f93\u5165\u3002</li> <li>\u5c06\u89e3\u7801\u5668\u7684\u7b2c\u4e00\u4e2a\u8f93\u5165\u521d\u59cb\u5316\u4e3aSOS_token\u3002</li> <li>\u5c06\u521d\u59cb\u5316\u5f20\u91cf\u8ffd\u52a0\u5230\u89e3\u7801\u540e\u7684\u5355\u8bcd\u4e2d\u3002</li> <li>\u4e00\u6b21\u8fed\u4ee3\u89e3\u7801\u4e00\u4e2a\u5355\u8bcdtoken\uff1a  <ol> <li>\u901a\u8fc7\u89e3\u7801\u5668\u8fdb\u884c\u524d\u5411\u8ba1\u7b97\u3002</li> <li>\u83b7\u5f97\u6700\u53ef\u80fd\u7684\u5355\u8bcdtoken\u53ca\u5176softmax\u5206\u6570\u3002</li> <li>\u8bb0\u5f55token\u548c\u5206\u6570\u3002</li> <li>\u51c6\u5907\u5f53\u524dtoken\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u89e3\u7801\u5668\u7684\u8f93\u5165\u3002</li> </ol> </li> <li>\u8fd4\u56de\u6536\u96c6\u5230\u7684\u5355\u8bcd tokens \u548c \u5206\u6570\u3002</li> </ol> <pre><code>class GreedySearchDecoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(GreedySearchDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input_seq, input_length, max_length):\n        # Forward input through encoder model\n        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n        decoder_hidden = encoder_hidden[:decoder.n_layers]\n        # Initialize decoder input with SOS_token\n        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n        # Initialize tensors to append decoded words to\n        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n        all_scores = torch.zeros([0], device=device)\n        # Iteratively decode one word token at a time\n        for _ in range(max_length):\n            # Forward pass through decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            # Obtain most likely word token and its softmax score\n            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n            # Record token and score\n            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n            # Prepare current token to be next decoder input (add a dimension)\n            decoder_input = torch.unsqueeze(decoder_input, 0)\n        # Return collections of word tokens and scores\n        return all_tokens, all_scores\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_15","title":"\u8bc4\u4f30\u6211\u4eec\u7684\u6587\u672c","text":"<p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u89e3\u7801\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u7f16\u5199\u7528\u4e8e\u8bc4\u4f30\u5b57\u7b26\u4e32\u8f93\u5165\u53e5\u5b50\u7684\u51fd\u6570\u3002 <code>evaluate</code>\u51fd\u6570\u7ba1\u7406\u8f93\u5165\u53e5\u5b50\u7684\u4f4e\u5c42\u7ea7\u5904\u7406\u8fc7\u7a0b\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528batch_size == 1\u5c06\u53e5\u5b50\u683c\u5f0f\u5316\u4e3a\u8f93\u5165\u6279\u91cf\u7684\u5355\u8bcd\u7d22\u5f15\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u53e5\u5b50\u7684\u5355\u8bcd\u8f6c\u6362\u4e3a\u76f8\u5e94\u7684\u7d22\u5f15\uff0c\u5e76\u901a\u8fc7\u8f6c\u6362\u7ef4\u5ea6\u6765\u4e3a\u6211\u4eec\u7684\u6a21\u578b\u51c6\u5907\u5f20\u91cf\u3002\u6211\u4eec\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a <code>lengths</code> \u5f20\u91cf\uff0c\u5176\u4e2d\u5305\u542b\u8f93\u5165\u53e5\u5b50\u7684\u957f\u5ea6\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>lengths</code> \u662f\u6807\u91cf\u56e0\u4e3a\u6211\u4eec\u4e00\u6b21\u53ea\u8bc4\u4f30\u4e00\u4e2a\u53e5\u5b50(batch_size == 1\uff09\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f7f\u7528\u6211\u4eec\u7684<code>GreedySearchDecoder</code>\u5b9e\u4f8b\u5316\u540e\u7684\u5bf9\u8c61(<code>searcher</code>\uff09\u83b7\u5f97\u89e3\u7801\u54cd\u5e94\u53e5\u5b50\u7684\u5f20\u91cf\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u54cd\u5e94\u7684\u7d22\u5f15\u8f6c\u6362\u4e3a\u5355\u8bcd\u5e76\u8fd4\u56de\u5df2\u89e3\u7801\u5355\u8bcd\u7684\u5217\u8868\u3002</p> <p><code>evaluateInput</code>\u5145\u5f53\u804a\u5929\u673a\u5668\u4eba\u7684\u7528\u6237\u63a5\u53e3\u3002\u8c03\u7528\u65f6\uff0c\u5c06\u751f\u6210\u4e00\u4e2a\u8f93\u5165\u6587\u672c\u5b57\u6bb5\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u5176\u4e2d\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u3002\u5728\u8f93\u5165\u6211\u4eec\u7684\u8f93\u5165\u53e5\u5b50\u5e76\u6309Enter\u540e\uff0c\u6211\u4eec\u7684\u6587\u672c\u4ee5\u4e0e\u8bad\u7ec3\u6570\u636e\u76f8\u540c\u7684\u65b9\u5f0f\u6807\u51c6\u5316\uff0c\u5e76\u6700\u7ec8\u88ab\u8f93\u5165\u5230\u8bc4\u4f30\u51fd\u6570\u4ee5\u83b7\u5f97\u89e3\u7801\u7684\u8f93\u51fa\u53e5\u5b50\u3002\u6211\u4eec\u5faa\u73af\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u7ee7\u7eed\u4e0e\u6211\u4eec\u7684\u673a\u5668\u4eba\u804a\u5929\u76f4\u5230\u6211\u4eec\u8f93\u5165\u201cq\u201d\u6216\u201cquit\u201d\u3002</p> <p>\u6700\u540e\uff0c\u5982\u679c\u8f93\u5165\u7684\u53e5\u5b50\u5305\u542b\u4e00\u4e2a\u4e0d\u5728\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\uff0c\u6211\u4eec\u4f1a\u901a\u8fc7\u6253\u5370\u9519\u8bef\u6d88\u606f\u5e76\u63d0\u793a\u7528\u6237\u8f93\u5165\u53e6\u4e00\u4e2a\u53e5\u5b50\u6765\u4f18\u96c5\u5730\u5904\u7406\u3002</p> <pre><code>def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n    ### Format input sentence as a batch\n    # words -&gt; indexes\n    indexes_batch = [indexesFromSentence(voc, sentence)]\n    # Create lengths tensor\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    # Transpose dimensions of batch to match models' expectations\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n    # Use appropriate device\n    input_batch = input_batch.to(device)\n    lengths = lengths.to(device)\n    # Decode sentence with searcher\n    tokens, scores = searcher(input_batch, lengths, max_length)\n    # indexes -&gt; words\n    decoded_words = [voc.index2word[token.item()] for token in tokens]\n    return decoded_words\n\ndef evaluateInput(encoder, decoder, searcher, voc):\n    input_sentence = ''\n    while(1):\n        try:\n            # Get input sentence\n            input_sentence = input('&gt; ')\n            # Check if it is quit case\n            if input_sentence == 'q' or input_sentence == 'quit': break\n            # Normalize sentence\n            input_sentence = normalizeString(input_sentence)\n            # Evaluate sentence\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n            # Format and print response sentence\n            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n            print('Bot:', ' '.join(output_words))\n\n        except KeyError:\n            print(\"Error: Encountered unknown word.\")\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_16","title":"\u8fd0\u884c\u6a21\u578b","text":"<p>\u6700\u540e\uff0c\u662f\u65f6\u5019\u8fd0\u884c\u6211\u4eec\u7684\u6a21\u578b\u4e86\uff01</p> <p>\u65e0\u8bba\u6211\u4eec\u662f\u5426\u60f3\u8981\u8bad\u7ec3\u6216\u6d4b\u8bd5\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\uff0c\u6211\u4eec\u90fd\u5fc5\u987b\u521d\u59cb\u5316\u5404\u4e2a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u3002 \u5728\u63a5\u4e0b\u6765\u7684\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u8bbe\u7f6e\u6240\u9700\u8981\u7684\u914d\u7f6e\uff0c\u9009\u62e9\u4ece\u5934\u5f00\u59cb\u6216\u8bbe\u7f6e\u68c0\u67e5\u70b9\u4ee5\u4ece\u4e2d\u52a0\u8f7d\uff0c\u5e76\u6784\u5efa\u548c\u521d\u59cb\u5316\u6a21\u578b\u3002 \u60a8\u53ef\u4ee5\u968f\u610f\u4f7f\u7528\u4e0d\u540c\u7684\u914d\u7f6e\u6765\u4f18\u5316\u6027\u80fd\u3002</p> <pre><code># Configure models\nmodel_name = 'cb_model'\nattn_model = 'dot'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nbatch_size = 64\n\n# Set checkpoint to load from; set to None if starting from scratch\nloadFilename = None\ncheckpoint_iter = 4000\n#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n\n# Load model if a loadFilename is provided\nif loadFilename:\n    # If loading on same machine the model was trained on\n    checkpoint = torch.load(loadFilename)\n    # If loading a model trained on GPU to CPU\n    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n    encoder_sd = checkpoint['en']\n    decoder_sd = checkpoint['de']\n    encoder_optimizer_sd = checkpoint['en_opt']\n    decoder_optimizer_sd = checkpoint['de_opt']\n    embedding_sd = checkpoint['embedding']\n    voc.__dict__ = checkpoint['voc_dict']\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.num_words, hidden_size)\nif loadFilename:\n    embedding.load_state_dict(embedding_sd)\n# Initialize encoder &amp; decoder models\nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\nif loadFilename:\n    encoder.load_state_dict(encoder_sd)\n    decoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Building encoder and decoder ...\nModels built and ready to go!\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_17","title":"\u6267\u884c\u8bad\u7ec3","text":"<p>\u5982\u679c\u8981\u8bad\u7ec3\u6a21\u578b\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u90e8\u5206\u3002</p> <p>\u9996\u5148\u6211\u4eec\u8bbe\u7f6e\u8bad\u7ec3\u53c2\u6570\uff0c\u7136\u540e\u521d\u59cb\u5316\u6211\u4eec\u7684\u4f18\u5316\u5668\uff0c\u6700\u540e\u6211\u4eec\u8c03\u7528<code>trainIters</code>\u51fd\u6570\u6765\u8fd0\u884c\u6211\u4eec\u7684\u8bad\u7ec3\u8fed\u4ee3\u3002</p> <pre><code># Configure training/optimization\nclip = 50.0\nteacher_forcing_ratio = 1.0\nlearning_rate = 0.0001\ndecoder_learning_ratio = 5.0\nn_iteration = 4000\nprint_every = 1\nsave_every = 500\n\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\nif loadFilename:\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n\n# Run training iterations\nprint(\"Starting Training!\")\ntrainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n           print_every, save_every, clip, corpus_name, loadFilename)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Building optimizers ...\nStarting Training!\nInitializing ...\nTraining...\nIteration: 1; Percent complete: 0.0%; Average loss: 8.9717\nIteration: 2; Percent complete: 0.1%; Average loss: 8.8521\nIteration: 3; Percent complete: 0.1%; Average loss: 8.6360\nIteration: 4; Percent complete: 0.1%; Average loss: 8.4234\nIteration: 5; Percent complete: 0.1%; Average loss: 7.9403\nIteration: 6; Percent complete: 0.1%; Average loss: 7.3892\nIteration: 7; Percent complete: 0.2%; Average loss: 7.0589\nIteration: 8; Percent complete: 0.2%; Average loss: 7.0130\nIteration: 9; Percent complete: 0.2%; Average loss: 6.7383\nIteration: 10; Percent complete: 0.2%; Average loss: 6.5343\n...\nIteration: 3991; Percent complete: 99.8%; Average loss: 2.6607\nIteration: 3992; Percent complete: 99.8%; Average loss: 2.6188\nIteration: 3993; Percent complete: 99.8%; Average loss: 2.8319\nIteration: 3994; Percent complete: 99.9%; Average loss: 2.5817\nIteration: 3995; Percent complete: 99.9%; Average loss: 2.4979\nIteration: 3996; Percent complete: 99.9%; Average loss: 2.7317\nIteration: 3997; Percent complete: 99.9%; Average loss: 2.5969\nIteration: 3998; Percent complete: 100.0%; Average loss: 2.2275\nIteration: 3999; Percent complete: 100.0%; Average loss: 2.7124\nIteration: 4000; Percent complete: 100.0%; Average loss: 2.5975\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_18","title":"\u8fd0\u884c\u8bc4\u4f30","text":"<p>To chat with your model, run the following block. \u8fd0\u884c\u4ee5\u4e0b\u90e8\u5206\u6765\u4e0e\u4f60\u7684\u6a21\u578b\u804a\u5929</p> <pre><code># Set dropout layers to eval mode\nencoder.eval()\ndecoder.eval()\n\n# Initialize search module\nsearcher = GreedySearchDecoder(encoder, decoder)\n\n# Begin chatting (uncomment and run the following line to begin)\n# evaluateInput(encoder, decoder, searcher, voc)\n\n</code></pre>"},{"location":"1.0/chatbot_tutorial/#_19","title":"\u7ed3\u8bba","text":"<p>\u4f19\u8ba1\u4eec\uff0c\u8fd9\u5c31\u662f\u8fd9\u4e00\u5207\u3002 \u606d\u559c\uff0c\u60a8\u73b0\u5728\u77e5\u9053\u6784\u5efa\u751f\u6210\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\uff01 \u5982\u679c\u60a8\u6709\u5174\u8da3\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u53c2\u6570\u4ee5\u53ca\u81ea\u5b9a\u4e49\u8bad\u7ec3\u6a21\u578b\u7684\u6570\u636e\u6765\u5b9a\u5236\u804a\u5929\u673a\u5668\u4eba\u7684\u884c\u4e3a\u3002</p> <p>\u67e5\u770b\u5176\u4ed6\u6559\u7a0b\uff0c\u4e86\u89e3PyTorch\u4e2d\u66f4\u9177\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7a0b\u5e8f\uff01</p>"},{"location":"1.0/checkpoint/","title":"torch.utils.checkpoint","text":"<p>\u8bd1\u8005:  belonHan</p> <p>\u6ce8\u610f</p> <p>checkpointing\u7684\u5b9e\u73b0\u65b9\u6cd5\u662f\u5728\u5411\u540e\u4f20\u64ad\u671f\u95f4\u91cd\u65b0\u8fd0\u884c\u5df2\u88abcheckpint\u7684\u524d\u5411\u4f20\u64ad\u6bb5\u3002 \u6240\u4ee5\u4f1a\u5bfc\u81f4\u50cfRNG\u8fd9\u7c7b(\u6a21\u578b)\u7684\u6301\u4e45\u5316\u7684\u72b6\u6001\u6bd4\u5b9e\u9645\u66f4\u8d85\u524d\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ccheckpoint\u5305\u542b\u4e86\u4f7f\u7528RNG\u72b6\u6001\u7684\u903b\u8f91(\u4f8b\u5982\u901a\u8fc7dropout)\uff0c\u4e0enon-checkpointed\u4f20\u9012\u76f8\u6bd4,checkpointed\u5177\u6709\u66f4\u786e\u5b9a\u7684\u8f93\u51fa\u3002RNG\u72b6\u6001\u7684\u5b58\u50a8\u903b\u8f91\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u5931\u3002\u5982\u679c\u4e0d\u9700\u8981\u786e\u5b9a\u7684\u8f93\u51fa\uff0c\u8bbe\u7f6e\u5168\u5c40\u6807\u5fd7(global flag) <code>torch.utils.checkpoint.preserve_rng_state=False</code> \u5ffd\u7565RNG\u72b6\u6001\u5728checkpoint\u65f6\u7684\u5b58\u53d6\u3002</p> <pre><code>torch.utils.checkpoint.checkpoint(function, *args)\n</code></pre> <p>checkpoint\u6a21\u578b\u6216\u6a21\u578b\u7684\u4e00\u90e8\u5206</p> <p>checkpoint\u901a\u8fc7\u8ba1\u7b97\u6362\u5185\u5b58\u7a7a\u95f4\u6765\u5de5\u4f5c\u3002\u4e0e\u5411\u540e\u4f20\u64ad\u4e2d\u5b58\u50a8\u6574\u4e2a\u8ba1\u7b97\u56fe\u7684\u6240\u6709\u4e2d\u95f4\u6fc0\u6d3b\u4e0d\u540c\u7684\u662f\uff0ccheckpoint\u4e0d\u4f1a\u4fdd\u5b58\u4e2d\u95f4\u6fc0\u6d3b\u90e8\u5206\uff0c\u800c\u662f\u5728\u53cd\u5411\u4f20\u9012\u4e2d\u91cd\u65b0\u8ba1\u7b97\u5b83\u4eec\u3002\u5b83\u88ab\u5e94\u7528\u4e8e\u6a21\u578b\u7684\u4efb\u4f55\u90e8\u5206\u3002</p> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u6b63\u5411\u4f20\u64ad\u4e2d\uff0c<code>function</code>\u5c06\u4ee5<code>torch.no_grad()</code>\u65b9\u5f0f\u8fd0\u884c \uff0c\u5373\u4e0d\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b,\u4f46\u4fdd\u5b58\u8f93\u5165\u5143\u7ec4\u548c <code>function</code>\u7684\u53c2\u6570\u3002\u5728\u5411\u540e\u4f20\u64ad\u4e2d\uff0c\u4fdd\u5b58\u7684\u8f93\u5165\u53d8\u91cf\u4ee5\u53ca <code>function</code>\u4f1a\u88ab\u53d6\u56de\uff0c\u5e76\u4e14<code>function</code>\u5728\u6b63\u5411\u4f20\u64ad\u4e2d\u88ab\u91cd\u65b0\u8ba1\u7b97.\u73b0\u5728\u8ddf\u8e2a\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u6fc0\u6d3b\u503c\u6765\u8ba1\u7b97\u68af\u5ea6\u3002</p> <p>Warning \u8b66\u544a</p> <p>Checkpointing \u5728 <code>torch.autograd.grad()</code>\u4e2d\u4e0d\u8d77\u4f5c\u7528, \u4ec5\u4f5c\u7528\u4e8e <code>torch.autograd.backward()</code>.</p> <p>\u8b66\u544a</p> <p>\u5982\u679cfunction\u5728\u5411\u540e\u6267\u884c\u548c\u524d\u5411\u6267\u884c\u4e0d\u540c\uff0c\u4f8b\u5982,\u7531\u4e8e\u67d0\u4e2a\u5168\u5c40\u53d8\u91cf\uff0ccheckpoint\u7248\u672c\u5c06\u4f1a\u4e0d\u540c\uff0c\u5e76\u4e14\u65e0\u6cd5\u88ab\u68c0\u6d4b\u5230\u3002</p> <p>\u53c2\u6570:</p> <ul> <li>function - \u63cf\u8ff0\u5728\u6a21\u578b\u7684\u6b63\u5411\u4f20\u9012\u6216\u6a21\u578b\u7684\u4e00\u90e8\u5206\u4e2d\u8fd0\u884c\u7684\u5185\u5bb9\u3002\u5b83\u4e5f\u5e94\u8be5\u77e5\u9053\u5982\u4f55\u5904\u7406\u4f5c\u4e3a\u5143\u7ec4\u4f20\u9012\u7684\u8f93\u5165\u3002\u4f8b\u5982\uff0c\u5728LSTM\u4e2d\uff0c\u5982\u679c\u7528\u6237\u901a\u8fc7 \uff0c\u5e94\u6b63\u786e\u4f7f\u7528\u7b2c\u4e00\u4e2a\u8f93\u5165\u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u8f93\u5165(activation, hidden)functionactivationhidden</li> <li>args \u2013 \u5305\u542b\u8f93\u5165\u7684\u5143\u7ec4function</li> </ul> Returns: \u8f93\u51fa <pre><code>torch.utils.checkpoint.checkpoint_sequential(functions, segments, *inputs)\n</code></pre> <p>\u7528\u4e8echeckpoint sequential\u6a21\u578b\u7684\u8f85\u52a9\u51fd\u6570</p> <p>Sequential\u6a21\u578b\u6309\u987a\u5e8f\u6267\u884c\u6a21\u5757/\u51fd\u6570\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u6837\u7684\u6a21\u578b\u5212\u5206\u4e3a\u4e0d\u540c\u7684\u6bb5(segment)\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u6bb5\u8fdb\u884ccheckpoint\u3002\u9664\u6700\u540e\u4e00\u6bb5\u5916\u7684\u6240\u6709\u6bb5\u90fd\u5c06\u4ee5<code>torch.no_grad()</code>\u65b9\u5f0f\u8fd0\u884c\uff0c\u5373\uff0c\u4e0d\u5b58\u50a8\u4e2d\u95f4\u6d3b\u52a8\u3002\u5c06\u4fdd\u5b58\u6bcf\u4e2acheckpoint\u6bb5\u7684\u8f93\u5165\uff0c\u4ee5\u4fbf\u5728\u5411\u540e\u4f20\u9012\u4e2d\u91cd\u65b0\u8fd0\u884c\u8be5\u6bb5\u3002</p> <p>checkpointing\u5de5\u4f5c\u65b9\u5f0f: <code>checkpoint()</code>.</p> <p>\u8b66\u544a</p> <p>Checkpointing\u65e0\u6cd5\u4f5c\u7528\u4e8e<code>torch.autograd.grad()</code>, \u53ea\u4f5c\u7528\u4e8e<code>torch.autograd.backward()</code>.</p> <p>\u53c2\u6570:</p> <ul> <li>functions \u2013 \u6309\u987a\u5e8f\u6267\u884c\u7684\u6a21\u578b\uff0c \u4e00\u4e2a <code>torch.nn.Sequential</code>\u5bf9\u8c61,\u6216\u8005\u4e00\u4e2a\u7531modules\u6216functions\u7ec4\u6210\u7684list\u3002</li> <li>segments \u2013 \u6bb5\u7684\u6570\u91cf</li> <li>inputs \u2013 \u8f93\u5165,Tensor\u7ec4\u6210\u7684\u5143\u7ec4</li> </ul> Returns: \u6309\u987a\u5e8f\u8fd4\u56de\u6bcf\u4e2a<code>*inputs</code>\u7684\u7ed3\u679c <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; model = nn.Sequential(...)\n&gt;&gt;&gt; input_var = checkpoint_sequential(model, chunks, input_var)\n\n</code></pre>"},{"location":"1.0/cpp_export/","title":"\u5728C++\u4e2d\u52a0\u8f7dPYTORCH\u6a21\u578b","text":"<p>\u8bd1\u8005\uff1atalengu</p> <p>PyTorch\u7684\u4e3b\u8981\u63a5\u53e3\u4e3aPython\u3002\u867d\u7136Python\u6709\u52a8\u6001\u7f16\u7a0b\u548c\u6613\u4e8e\u8fed\u4ee3\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\uff0c\u6b63\u662fPython\u7684\u8fd9\u4e9b\u5c5e\u6027\u4f1a\u5e26\u6765\u4e0d\u5229\u3002\u6211\u4eec\u7ecf\u5e38\u9047\u5230\u7684\u751f\u4ea7\u73af\u5883\uff0c\u8981\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u4e25\u683c\u90e8\u7f72\u8981\u6c42\u3002\u5bf9\u4e8e\u751f\u4ea7\u573a\u666f\u800c\u8a00\uff0cC++\u901a\u5e38\u662f\u9996\u9009\u8bed\u8a00\uff0c\u4e5f\u80fd\u5f88\u65b9\u4fbf\u7684\u5c06\u5176\u7ed1\u5b9a\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\uff0c\u5982Java\uff0cRust\u6216Go\u3002\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u4ece\u5c06PyTorch\u8bad\u7ec3\u7684\u6a21\u578b\u5e8f\u5217\u5316\u8868\u793a\uff0c\u5230C++\u8bed\u8a00_\u52a0\u8f7d_\u548c_\u6267\u884c_\u7684\u8fc7\u7a0b\u3002</p>"},{"location":"1.0/cpp_export/#pytorchtorch-script","title":"\u7b2c\u4e00\u6b65\uff1a\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3aTorch Script","text":"<p>PyTorch\u6a21\u578b\u4ecePython\u5230C++\u7684\u8f6c\u6362\u7531Torch Script\u5b9e\u73b0\u3002Torch Script\u662fPyTorch\u6a21\u578b\u7684\u4e00\u79cd\u8868\u793a\uff0c\u53ef\u7531Torch Script\u7f16\u8bd1\u5668\u7406\u89e3\uff0c\u7f16\u8bd1\u548c\u5e8f\u5217\u5316\u3002\u5982\u679c\u4f7f\u7528\u57fa\u7840\u7684\u201ceager\u201dAPI\u7f16\u5199\u7684PyTorch\u6a21\u578b\uff0c\u5219\u5fc5\u987b\u5148\u5c06\u6a21\u578b\u8f6c\u6362\u4e3aTorch Script\uff0c\u5f53\u7136\u8fd9\u4e5f\u662f\u6bd4\u8f83\u5bb9\u6613\u7684\u3002\u5982\u679c\u5df2\u6709\u6a21\u578b\u7684Torch Script\uff0c\u5219\u53ef\u4ee5\u8df3\u5230\u672c\u6559\u7a0b\u7684\u4e0b\u4e00\u90e8\u5206\u3002</p> <p>\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3aTorch Script\u6709\u4e24\u79cd\u65b9\u6cd5\u3002 \u7b2c\u4e00\u79cd\u65b9\u6cd5\u662fTracing\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6837\u672c\u8f93\u5165\u5230\u6a21\u578b\u4e2d\u4e00\u6b21\u6765\u5bf9\u8be5\u8fc7\u7a0b\u8fdb\u884c\u8bc4\u4f30\u4ece\u800c\u6355\u83b7\u6a21\u578b\u7ed3\u6784.\u5e76\u8bb0\u5f55\u8be5\u6837\u672c\u5728\u6a21\u578b\u4e2d\u7684flow\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u6a21\u578b\u4e2d\u5f88\u5c11\u4f7f\u7528\u63a7\u5236flow\u7684\u6a21\u578b\u3002 \u7b2c\u4e8c\u4e2a\u65b9\u6cd5\u5c31\u662f\u5411\u6a21\u578b\u6dfb\u52a0\u663e\u5f0f\u6ce8\u91ca(Annotation)\uff0c\u901a\u77e5Torch Script\u7f16\u8bd1\u5668\u5b83\u53ef\u4ee5\u76f4\u63a5\u89e3\u6790\u548c\u7f16\u8bd1\u6a21\u578b\u4ee3\u7801\uff0c\u53d7Torch Script\u8bed\u8a00\u5f3a\u52a0\u7684\u7ea6\u675f\u3002</p> <p>\u5c0f\u8d34\u58eb \u53ef\u4ee5\u5728\u5b98\u65b9\u7684Torch Script \u53c2\u8003\u4e2d\u627e\u5230\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u5b8c\u6574\u6587\u6863\uff0c\u4ee5\u53ca\u6709\u5173\u4f7f\u7528\u54ea\u4e2a\u65b9\u6cd5\u7684\u7ec6\u8282\u6307\u5bfc\u3002</p>"},{"location":"1.0/cpp_export/#tracingtorch-script","title":"\u5229\u7528Tracing\u5c06\u6a21\u578b\u8f6c\u6362\u4e3aTorch Script","text":"<p>\u8981\u901a\u8fc7tracing\u6765\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3aTorch\u811a\u672c,\u5fc5\u987b\u5c06\u6a21\u578b\u7684\u5b9e\u4f8b\u4ee5\u53ca\u6837\u672c\u8f93\u5165\u4f20\u9012\u7ed9<code>torch.jit.trace</code>\u51fd\u6570\u3002\u8fd9\u5c06\u751f\u6210\u4e00\u4e2a <code>torch.jit.ScriptModule</code>\u5bf9\u8c61\uff0c\u5e76\u5728\u6a21\u5757\u7684<code>forward</code>\u65b9\u6cd5\u4e2d\u5d4c\u5165\u6a21\u578b\u8bc4\u4f30\u7684\u8ddf\u8e2a\uff1a</p> <pre><code>import torch\nimport torchvision\n\n# \u83b7\u53d6\u6a21\u578b\u5b9e\u4f8b\nmodel = torchvision.models.resnet18()\n\n# \u751f\u6210\u4e00\u4e2a\u6837\u672c\u4f9b\u7f51\u7edc\u524d\u5411\u4f20\u64ad forward()\nexample = torch.rand(1, 3, 224, 224)\n\n# \u4f7f\u7528 torch.jit.trace \u751f\u6210 torch.jit.ScriptModule \u6765\u8ddf\u8e2a\ntraced_script_module = torch.jit.trace(model, example)\n\n</code></pre> <p>\u73b0\u5728\uff0c\u8ddf\u8e2a\u7684<code>ScriptModule</code>\u53ef\u4ee5\u4e0e\u5e38\u89c4PyTorch\u6a21\u5757\u8fdb\u884c\u76f8\u540c\u7684\u8ba1\u7b97\uff1a</p> <pre><code>In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))\nIn[2]: output[0, :5]\nOut[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)\n\n</code></pre>"},{"location":"1.0/cpp_export/#annotationmodeltorch-script","title":"\u901a\u8fc7Annotation\u5c06Model\u8f6c\u6362\u4e3aTorch Script","text":"<p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\uff0c\u5982\u679c\u6a21\u578b\u4f7f\u7528\u7279\u5b9a\u5f62\u5f0f\u7684\u63a7\u5236\u6d41\uff0c\u5982\u679c\u60f3\u8981\u76f4\u63a5\u5728Torch Script\u4e2d\u7f16\u5199\u6a21\u578b\u5e76\u76f8\u5e94\u5730\u6807\u6ce8(annotate)\u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5047\u8bbe\u6709\u4ee5\u4e0b\u666e\u901a\u7684 Pytorch\u6a21\u578b\uff1a</p> <pre><code>import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\n</code></pre> <p>\u7531\u4e8e\u6b64\u6a21\u5757\u7684<code>forward</code>\u65b9\u6cd5\u4f7f\u7528\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u63a7\u5236\u6d41\uff0c\u56e0\u6b64\u5b83\u4e0d\u9002\u5408\u5229\u7528Tracing\u7684\u65b9\u6cd5\u751f\u6210Torch Script\u3002\u4e3a\u6b64,\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f<code>torch.jit.ScriptModule</code>\u5e76\u5c06<code>@ torch.jit.script_method</code>\u6807\u6ce8\u6dfb\u52a0\u5230\u6a21\u578b\u7684<code>forward</code>\u4e2d\u7684\u65b9\u6cd5\uff0c\u6765\u5c06model\u8f6c\u6362\u4e3a<code>ScriptModule</code>\uff1a</p> <pre><code>import torch\n\nclass MyModule(torch.jit.ScriptModule):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    @torch.jit.script_method\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\nmy_script_module = MyModule()\n\n</code></pre> <p>\u73b0\u5728\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684<code>MyModule</code>\u5bf9\u8c61\u4f1a\u76f4\u63a5\u751f\u6210\u4e00\u4e2a\u53ef\u5e8f\u5217\u5316\u7684<code>ScriptModule</code>\u5b9e\u4f8b\u4e86\u3002</p>"},{"location":"1.0/cpp_export/#script-module","title":"\u7b2c\u4e8c\u6b65\uff1a\u5c06Script Module\u5e8f\u5217\u5316\u4e3a\u4e00\u4e2a\u6587\u4ef6","text":"<p>\u4e0d\u8bba\u662f\u4ece\u4e0a\u9762\u4e24\u79cd\u65b9\u6cd5\u7684\u54ea\u4e00\u79cd\u65b9\u6cd5\u83b7\u5f97\u4e86<code>ScriptModule</code>,\u90fd\u53ef\u4ee5\u5c06\u5f97\u5230\u7684<code>ScriptModule</code>\u5e8f\u5217\u5316\u4e3a\u4e00\u4e2a\u6587\u4ef6,\u7136\u540eC++\u5c31\u53ef\u4ee5\u4e0d\u4f9d\u8d56\u4efb\u4f55Python\u4ee3\u7801\u6765\u6267\u884c\u8be5Script\u6240\u5bf9\u5e94\u7684Pytorch\u6a21\u578b\u3002 \u5047\u8bbe\u6211\u4eec\u60f3\u8981\u5e8f\u5217\u5316\u524d\u9762trace\u793a\u4f8b\u4e2d\u663e\u793a\u7684<code>ResNet18</code>\u6a21\u578b\u3002\u8981\u6267\u884c\u6b64\u5e8f\u5217\u5316\uff0c\u53ea\u9700\u5728\u6a21\u5757\u4e0a\u8c03\u7528 save\u5e76\u7ed9\u4e2a\u6587\u4ef6\u540d\uff1a</p> <pre><code>traced_script_module.save(\"model.pt\")\n\n</code></pre> <p>\u8fd9\u5c06\u5728\u5de5\u4f5c\u76ee\u5f55\u4e2d\u751f\u6210\u4e00\u4e2a<code>model.pt</code>\u6587\u4ef6\u3002\u73b0\u5728\u53ef\u4ee5\u79bb\u5f00Python\uff0c\u5e76\u51c6\u5907\u8de8\u8d8a\u5230C ++\u8bed\u8a00\u8c03\u7528\u3002</p>"},{"location":"1.0/cpp_export/#cscript-module","title":"\u7b2c\u4e09\u6b65:\u5728C++\u4e2d\u52a0\u8f7d\u4f60\u7684Script Module","text":"<p>\u8981\u5728C ++\u4e2d\u52a0\u8f7d\u5e8f\u5217\u5316\u7684PyTorch\u6a21\u578b\uff0c\u5e94\u7528\u7a0b\u5e8f\u5fc5\u987b\u4f9d\u8d56\u4e8e<code>PyTorch C ++ API</code> - \u4e5f\u79f0\u4e3a_LibTorch_\u3002_LibTorch\u53d1\u884c\u7248_\u5305\u542b\u4e00\u7ec4\u5171\u4eab\u5e93\uff0c\u5934\u6587\u4ef6\u548c<code>CMake</code>\u6784\u5efa\u914d\u7f6e\u6587\u4ef6\u3002\u867d\u7136CMake\u4e0d\u662f\u4f9d\u8d56LibTorch\u7684\u8981\u6c42\uff0c\u4f46\u5b83\u662f\u63a8\u8350\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5c06\u6765\u4f1a\u5f97\u5230\u5f88\u597d\u7684\u652f\u6301\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528CMake\u548cLibTorch\u6784\u5efa\u4e00\u4e2a\u6700\u5c0f\u7684C++\u5e94\u7528\u7a0b\u5e8f\uff0c\u52a0\u8f7d\u5e76\u6267\u884c\u5e8f\u5217\u5316\u7684PyTorch\u6a21\u578b\u3002</p>"},{"location":"1.0/cpp_export/#c","title":"\u6700\u5c0f\u7684C++\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u4ee5\u4e0b\u5185\u5bb9\u53ef\u4ee5\u505a\u5230\u52a0\u8f7d\u6a21\u5757\uff1a</p> <pre><code>#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n\n  // Deserialize the ScriptModule from a file using torch::jit::load().\n  std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]);\n\n  assert(module != nullptr);\n  std::cout &lt;&lt; \"ok\\n\";\n}\n\n</code></pre> <p><code>&lt;torch/script.h&gt;</code>\u5934\u6587\u4ef6\u5305\u542b\u8fd0\u884c\u8be5\u793a\u4f8b\u6240\u9700\u7684LibTorch\u5e93\u4e2d\u7684\u6240\u6709\u76f8\u5173<code>include</code>\u3002main\u51fd\u6570\u63a5\u53d7\u5e8f\u5217\u5316<code>ScriptModule</code>\u7684\u6587\u4ef6\u8def\u5f84\u4f5c\u4e3a\u5176\u552f\u4e00\u7684\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u7136\u540e\u4f7f\u7528<code>torch::jit::load()</code>\u51fd\u6570\u53cd\u5e8f\u5217\u5316\u6a21\u5757\uff0c\u5f97\u5230\u4e00\u4e2a\u6307\u5411<code>torch::jit::script::Module</code>\u7684\u5171\u4eab\u6307\u9488\uff0c\u76f8\u5f53\u4e8eC ++\u4e2d\u7684<code>torch.jit.ScriptModule</code>\u5bf9\u8c61\u3002\u6700\u540e\uff0c\u6211\u4eec\u53ea\u9a8c\u8bc1\u6b64\u6307\u9488\u4e0d\u4e3anull\u3002\u6211\u4eec\u5c55\u793a\u5982\u4f55\u5728\u63a5\u4e0b\u6765\u6267\u884c\u5b83\u3002</p>"},{"location":"1.0/cpp_export/#libtorch","title":"\u4f9d\u8d56\u5e93LibTorch\u548c\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u6211\u4eec\u5c06\u4e0a\u9762\u7684\u4ee3\u7801\u4fdd\u5b58\u5230\u540d\u4e3a<code>example-app.cpp</code>\u7684\u6587\u4ef6\u4e2d\u3002\u5bf9\u5e94\u7684\u6784\u5efa\u5b83\u7684\u7b80\u5355<code>CMakeLists.txt</code>\u4e3a\uff1a</p> <pre><code>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(custom_ops)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\nset_property(TARGET example-app PROPERTY CXX_STANDARD 11)\n\n</code></pre> <p>\u6211\u4eec\u6784\u5efa\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\u7684\u6700\u540e\u4e00\u4ef6\u4e8b\u662f\u4e0b\u8f7dLibTorch\u53d1\u884c\u7248\u3002\u4ecePyTorch\u7f51\u7ad9\u7684\u4e0b\u8f7d\u9875\u9762\u83b7\u53d6\u6700\u65b0\u7684\u7a33\u5b9a\u7248\u672c download page\u3002\u5982\u679c\u4e0b\u8f7d\u5e76\u89e3\u538b\u7f29\u6700\u65b0\u5b58\u6863\uff0c\u5219\u6709\u4ee5\u4e0b\u76ee\u5f55\u7ed3\u6784\uff1a</p> <pre><code>libtorch/\n  bin/\n  include/\n  lib/\n  share/\n\n</code></pre> <ul> <li><code>lib/</code> \u5305\u542b\u542b\u94fe\u63a5\u7684\u5171\u4eab\u5e93,</li> <li><code>include/</code> \u5305\u542b\u7a0b\u5e8f\u9700\u8981<code>include</code>\u7684\u5934\u6587\u4ef6,</li> <li><code>share/</code>\u5305\u542b\u5fc5\u8981\u7684CMake\u914d\u7f6e\u6587\u4ef6\u4f7f\u5f97 <code>find_package(Torch)</code> \u3002</li> </ul> <p>\u5c0f\u8d34\u58eb \u5728Windows\u5e73\u53f0\u4e0a, debug and release builds are not ABI-compatible. \u5982\u679c\u8981\u4f7f\u7528debug, \u8981\u4f7f\u7528 \u6e90\u7801\u7f16\u8bd1 PyTorch\u65b9\u6cd5\u3002</p> <p>\u6700\u540e\u4e00\u6b65\u662f\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u3002\u4e3a\u6b64\uff0c\u5047\u8bbe\u6211\u4eec\u7684\u793a\u4f8b\u76ee\u5f55\u5e03\u5c40\u5982\u4e0b\uff1a</p> <pre><code>example-app/\n  CMakeLists.txt\n  example-app.cpp\n\n</code></pre> <p>\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ece<code>example-app/</code>\u6587\u4ef6\u5939\u4e2d\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\uff1a</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\nmake\n\n</code></pre> <p>\u5176\u4e2d <code>/path/to/libtorch</code> \u5e94\u8be5\u662f\u89e3\u538b\u7f29\u7684LibTorch\u53d1\u884c\u7248\u7684\u5b8c\u6574\u8def\u5f84\u3002\u5982\u679c\u4e00\u5207\u987a\u5229\uff0c\u5b83\u5c06\u770b\u8d77\u6765\u50cf\u8fd9\u6837\uff1a</p> <pre><code>root@4b5a67132e81:/example-app# mkdir build\nroot@4b5a67132e81:/example-app# cd build\nroot@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example-app/build\nroot@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app\n\n</code></pre> <p>\u5982\u679c\u6211\u4eec\u63d0\u4f9b\u524d\u9762\u7684\u5e8f\u5217\u5316<code>ResNet18</code>\u6a21\u578b\u7684\u8def\u5f84\u7ed9<code>example-app</code>\uff0cC++\u8f93\u51fa\u7684\u7ed3\u679c\u5e94\u8be5\u662f OK:</p> <pre><code>root@4b5a67132e81:/example-app/build# ./example-app model.pt\nok\n\n</code></pre>"},{"location":"1.0/cpp_export/#cscript-module_1","title":"\u5728C++\u4ee3\u7801\u4e2d\u8fd0\u884cScript Module","text":"<p>\u5728C ++\u4e2d\u6210\u529f\u52a0\u8f7d\u4e86\u6211\u4eec\u7684\u5e8f\u5217\u5316<code>ResNet18</code>\u540e\uff0c\u6211\u4eec\u518d\u52a0\u51e0\u884c\u6267\u884c\u4ee3\u7801\uff0c\u6dfb\u52a0\u5230C++\u5e94\u7528\u7a0b\u5e8f\u7684<code>main()</code>\u51fd\u6570\u4e2d\uff1a</p> <pre><code>// Create a vector of inputs.\nstd::vector&lt;torch::jit::IValue&gt; inputs;\ninputs.push_back(torch::ones({1, 3, 224, 224}));\n\n// Execute the model and turn its output into a tensor.\nat::Tensor output = module-&gt;forward(inputs).toTensor();\n\nstd::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) &lt;&lt; '\\n';\n\n</code></pre> <p>\u524d\u4e24\u884c\u8bbe\u7f6e\u6211\u4eec\u6a21\u578b\u7684\u8f93\u5165\u3002 \u521b\u5efa\u4e86\u4e00\u4e2a <code>torch::jit::IValue</code> (<code>script::Module</code> \u5bf9\u8c61\u53ef\u63a5\u53d7\u548c\u8fd4\u56de\u7684\u4e00\u79cd\u6570\u636e\u7c7b\u578b) \u7684\u5411\u91cf\u548c\u6dfb\u52a0\u4e00\u4e2a\u8f93\u5165\u3002\u8981\u521b\u5efa\u8f93\u5165\u5f20\u91cf\uff0c\u6211\u4eec\u4f7f\u7528<code>torch::ones()</code>(C++ API\uff09\u548cpython\u4e2d\u7684<code>torch.ones</code> \u4e00\u6837\u3002 \u7136\u540e\u6211\u4eec\u8fd0\u884c<code>script::Module</code>\u7684<code>forward</code>\u65b9\u6cd5\uff0c\u4f20\u5165\u6211\u4eec\u521b\u5efa\u7684\u8f93\u5165\u5411\u91cf\uff0c\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684<code>IValue</code>\uff0c\u901a\u8fc7\u8c03\u7528<code>toTensor()</code>\u53ef\u5c06\u5176\u8f6c\u6362\u4e3a\u5f20\u91cf\u3002</p> <p>\u5c0f\u8d34\u58eb \u66f4\u591a\u5173\u4e8e<code>torch::ones</code> \u548c PyTorch\u7684\u5bf9\u5e94 C++ API\u7684\u5185\u5bb9 https://pytorch.org/cppdocs\u3002PyTorch C++ API \u548cPython API\u5dee\u4e0d\u591a\uff0c\u53ef\u4ee5\u4f7f\u4f60\u50cfpython \u4e2d\u4e00\u6837\u64cd\u4f5c\u5904\u7406tensors\u3002</p> <p>\u5728\u6700\u540e\u4e00\u884c\u4e2d\uff0c\u6211\u4eec\u6253\u5370\u8f93\u51fa\u7684\u524d\u4e94\u4e2a\u6761\u76ee\u3002\u7531\u4e8e\u6211\u4eec\u5728\u672c\u6559\u7a0b\u524d\u9762\u7684Python\u4e2d\u4e3a\u6211\u4eec\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u76f8\u540c\u7684\u8f93\u5165\uff0c\u56e0\u6b64\u7406\u60f3\u60c5\u51b5\u4e0b\u6211\u4eec\u5e94\u8be5\u770b\u5230\u76f8\u540c\u7684\u8f93\u51fa\u3002\u8ba9\u6211\u4eec\u901a\u8fc7\u91cd\u65b0\u7f16\u8bd1\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u5e76\u4f7f\u7528\u76f8\u540c\u7684\u5e8f\u5217\u5316\u6a21\u578b\u8fd0\u884c\u5b83\u6765\u5c1d\u8bd5\uff1a</p> <pre><code>root@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app\nroot@4b5a67132e81:/example-app/build# ./example-app model.pt\n-0.2698 -0.0381  0.4023 -0.3010 -0.0448\n[ Variable[CPUFloatType]{1,5} ]\n\n</code></pre> <p>\u4f5c\u4e3a\u53c2\u8003\uff0c\u4e4b\u524dPython\u4ee3\u7801\u7684\u8f93\u51fa\u662f\uff1a</p> <pre><code>tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)\n\n</code></pre> <p>\u7531\u6b64\u53ef\u89c1,C++\u7684\u8f93\u51fa\u4e0ePython\u7684\u8f93\u51fa\u662f\u4e00\u6837\u7684,\u6210\u529f\u5566!</p> <p>\u5c0f\u8d34\u58eb \u5c06\u4f60\u7684\u6a21\u578b\u653e\u5230GPU\u4e0a\uff0c\u53ef\u4ee5\u5199\u6210<code>model-&gt;to(at::kCUDA);</code>\u3002\u786e\u4fdd\u4f60\u7684\u8f93\u5165\u4e5f\u5728CUDA\u7684\u5b58\u50a8\u7a7a\u95f4\u91cc\u9762\uff0c\u53ef\u4ee5\u4f7f\u7528<code>tensor.to(at::kCUDA)</code>\u68c0\u67e5\uff0c\u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5728CUDA\u91cc\u9762\u7684tensor\u3002</p>"},{"location":"1.0/cpp_export/#api","title":"\u7b2c\u4e94\u6b65:\u8fdb\u9636\u6559\u7a0b\u548c\u8be6\u7ec6API","text":"<p>\u672c\u6559\u7a0b\u5e0c\u671b\u80fd\u4f7f\u4f60\u7406\u89e3PyTorch\u6a21\u578b\u4ecepython\u5230c++\u7684\u8c03\u7528\u8fc7\u7a0b\u3002\u901a\u8fc7\u4e0a\u8ff0\u6559\u7a0b\uff0c\u4f60\u80fd\u591f\u901a\u8fc7\u201ceager\u201d PyTorch\u505a\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u8f6c\u6210<code>ScriptModule</code>\uff0c\u5e76\u5e8f\u5217\u5316\u4fdd\u5b58\u3002\u7136\u540e\u5728C++\u91cc\u9762\u901a\u8fc7 <code>script::Module</code>\u52a0\u8f7d\u8fd0\u884c\u6a21\u578b\u3002</p> <p>\u5f53\u7136\uff0c\u8fd8\u6709\u597d\u591a\u5185\u5bb9\u6211\u4eec\u6ca1\u6709\u6d89\u53ca\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5e0c\u671b\u5728C++\u6216\u8005CUDA\u4e2d\u5b9e\u73b0<code>ScriptModule</code>\u4e2d\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u5728C++\u8c03\u7528\u8fd0\u884c<code>ScriptModule</code>\u6a21\u578b\u3002\u8fd9\u79cd\u662f\u53ef\u4ee5\u505a\u5230\u7684\uff0c\u53ef\u4ee5\u53c2\u8003this\u3002\u4e0b\u9762\u8fd8\u6709\u4e00\u4e9b\u6587\u6863\u53ef\u4ee5\u53c2\u8003\uff0c\u6bd4\u8f83\u6709\u5e2e\u52a9\uff1a</p> <ul> <li>Torch Script \u53c2\u8003: https://pytorch.org/docs/master/jit.html</li> <li>PyTorch C++ API \u6587\u6863: https://pytorch.org/cppdocs/</li> <li>PyTorch Python API \u6587\u6863: https://pytorch.org/docs/</li> </ul> <p>\u5982\u679c\u6709\u4efb\u4f55bug\u6216\u8005\u95ee\u9898\uff0c\u53ef\u4ee5\u5411\u793e\u533a Pytorch forum \u6216\u8005 Pytorch GitHub issues \u5bfb\u6c42\u5e2e\u52a9\u3002</p>"},{"location":"1.0/cpp_extension/","title":"\u81ea\u5b9a\u4e49 C++ \u4e0e CUDA \u62d3\u5c55","text":"<p>\u8bd1\u8005\uff1aP3n9W31</p> <p>Author: Peter Goldsborough</p> <p>PyTorch \u63d0\u4f9b\u4e86\u5927\u91cf\u4e0e\u795e\u7ecf\u7f51\u7edc\uff0c\u4efb\u610f\u5f20\u91cf\u4ee3\u6570(arbitrary tensor algebra\uff09\uff0c\u6570\u636e\u5904\u7406(data wrangling\uff09\u548c\u5176\u4ed6\u76ee\u7684\u76f8\u5173\u7684\u64cd\u4f5c\u3002\u7136\u800c\uff0c\u4f60\u53ef\u80fd\u53d1\u73b0\u4f60\u8fd8\u662f\u4f1a\u9700\u8981\u4e00\u4e9b\u66f4\u52a0\u81ea\u5b9a\u4e49\u7684\u64cd\u4f5c\u3002\u4f8b\u5982\uff0c\u4f60\u6709\u65f6\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u4e00\u4e2a\u4f60\u5728\u67d0\u7bc7\u8bba\u6587\u4e2d\u627e\u5230\u7684\u4e00\u4e2a\u65b0\u578b\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u6216\u8005\u662f\u5b9e\u73b0\u4e00\u4e2a\u4e3a\u4e86\u4f60\u7684\u7814\u7a76\u6240\u5f00\u53d1\u7684\u65b0\u64cd\u4f5c\u3002</p> <p>\u5728 PyTorch \u4e2d\u96c6\u6210\u8fd9\u79cd\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u901a\u8fc7 Python \u8bed\u8a00\u5bf9<code>Function</code>\u548c<code>Module</code>\u8fdb\u884c\u6269\u5199\uff0c\u6b63\u5982\u5728 \u8fd9\u91cc\u6240\u63cf\u8ff0\u7684\u90a3\u6837\u3002\u8fd9\u79cd\u65b9\u5f0f\u80fd\u8ba9\u4f60\u5145\u5206\u7684\u53d1\u6325\u81ea\u52a8\u5fae\u5206(automatic differentiation\uff09(\u8ba9\u4f60\u4e0d\u7528\u53bb\u7f16\u5199\u4e00\u4e9b\u884d\u751f\u7684\u51fd\u6570\uff09\u4e0e Python \u8bed\u8a00\u7684\u5e38\u89c4\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u529b(usual expressiveness\uff09\u7684\u80fd\u529b\u3002\u4f46\u662f\u6709\u65f6\u5019\uff0c\u53ef\u80fd\u5728 C++ \u8bed\u8a00\u4e2d\u80fd\u591f\u66f4\u597d\u5730\u5b9e\u73b0\u4f60\u7684\u4e00\u4e9b\u64cd\u4f5c\u3002\u4f8b\u5982\uff0c\u4f60\u7684\u4ee3\u7801\u53ef\u80fd\u56e0\u4e3a\u88ab\u975e\u5e38\u9891\u7e41\u7684\u4f7f\u7528\u800c\u9700\u8981 \u5341\u5206 \u5feb\u901f\uff0c\u6216\u8005\u662f\u5373\u4f7f\u8c03\u7528\u7684\u6b21\u6570\u5f88\u5c11\u4e5f\u4f1a\u5e26\u6765\u4e0d\u5c0f\u7684\u6027\u80fd\u8d1f\u62c5\u3002\u53e6\u4e00\u4e2a\u539f\u56e0\u662f\u4f60\u7684\u4ee3\u7801\u53ef\u80fd\u662f\u5efa\u7acb\u5728 C \u6216 C++ \u8bed\u8a00\u4e4b\u4e0a\u7684\uff0c\u6216\u8005\u4f60\u7684\u4ee3\u7801\u9700\u8981\u4e0e C \u6216 C++ \u8bed\u8a00\u8fdb\u884c\u4ea4\u4e92\u4e0e\u5bf9\u63a5\u3002\u4e3a\u4e86\u89e3\u51b3\u4e0a\u8ff0\u7684\u8fd9\u4e9b\u60c5\u51b5\uff0cPyTorch \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7528\u4e8e\u7f16\u5199\u81ea\u5b9a\u4e49  C++ \u6269\u5c55 \u7684\u65b9\u6cd5\u3002</p> <p>C++ \u62d3\u5c55\u662f\u6211\u4eec\u5f00\u53d1\u7684\u4e00\u79cd\u80fd\u591f\u8ba9\u7528\u6237(\u4f60\uff09\u81ea\u884c\u521b\u5efa\u4e00\u4e9b \u6240\u542b\u8d44\u6e90\u4e4b\u5916 \u7684\u64cd\u4f5c\u7684\u673a\u5236\uff0c\u4f8b\u5982\uff0c\u4e0e PyTorch \u7684\u540e\u7aef\u5206\u79bb\u5f00\u6765\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0e PyTorch \u539f\u751f\u64cd\u4f5c\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f \u4e0d\u540c\u7684 \u3002C++ \u6269\u5c55\u65e8\u5728\u4e3a\u4f60\u63d0\u4f9b\u4e0e PyTorch \u540e\u7aef\u96c6\u6210\u64cd\u4f5c\u76f8\u5173\u7684\u5927\u90e8\u5206\u6837\u677f(boilerplate\uff09\uff0c\u540c\u65f6\u4e3a\u57fa\u4e8e PyTorch \u7684\u9879\u76ee\u63d0\u4f9b\u9ad8\u5ea6\u7075\u6d3b\u6027\u3002\u7136\u800c\uff0c\u4e00\u65e6\u4f60\u5c06\u4f60\u7684\u64cd\u4f5c\u5b9a\u4e49\u4e3a\u4e86 C++ \u62d3\u5c55\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u539f\u751f PyTorch \u51fd\u6570\u5c31\u4e3b\u8981\u662f\u4ee3\u7801\u7ec4\u7ec7\u7684\u95ee\u9898\u4e86\uff0c\u5982\u679c\u4f60\u51b3\u5b9a\u5728\u4e0a\u6e38\u63d0\u4f9b\u64cd\u4f5c\uff0c\u5219\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p>"},{"location":"1.0/cpp_extension/#_1","title":"\u52a8\u673a\u4e0e\u4f8b\u5b50","text":"<p>\u672c\u7bc7\u6587\u7ae0\u7684\u5269\u4f59\u90e8\u5206\u5c06\u4ecb\u7ecd\u4e00\u4e2a\u7f16\u5199\u548c\u4f7f\u7528 C++(\u4ee5\u53ca CUDA\uff09\u62d3\u5c55\u7684\u5b9e\u4f8b\u3002\u5982\u679c\u4f60\u73b0\u5728\u6b63\u5728\u88ab\u4e00\u76f4\u50ac\u7740\u6216\u662f\u5728\u4eca\u5929\u4e4b\u524d\u6ca1\u6709\u628a\u8be5\u64cd\u4f5c\u5b8c\u6210\u4f60\u5c31\u4f1a\u88ab\u89e3\u96c7\u7684\u8bdd\uff0c\u4f60\u53ef\u4ee5\u8df3\u8fc7\u8fd9\u4e00\u7ae0\u8282\uff0c\u76f4\u63a5\u53bb\u4e0b\u4e00\u8282\u7684\u5b9e\u65bd\u7ec6\u8282\u90e8\u5206\u67e5\u770b\u3002</p> <p>\u5047\u8bbe\u4f60\u5df2\u7ecf\u627e\u5230\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5faa\u73af(recurrent\uff09\u7684\u5355\u5143\uff0c\u5b83\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u8be5\u5faa\u73af\u5355\u5143\u7c7b\u4f3c\u4e8e LSTM\uff0c\u4f46\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u5b83\u7f3a\u5c11\u4e86 \u9057\u5fd8\u95e8 \u5e76\u4f7f\u7528 \u6307\u6570\u7ebf\u6027\u5355\u5143 (ELU\uff09\u4f5c\u4e3a\u5176\u5185\u90e8\u6fc0\u6d3b\u529f\u80fd\u3002\u56e0\u4e3a\u8fd9\u4e2a\u5355\u5143\u6c38\u8fdc\u90fd\u4e0d\u4f1a\u5fd8\u8bb0\uff0c\u6240\u4ee5\u6211\u4eec\u53eb\u5b83 LLTM\uff0c\u6216\u662f \u957f\u957f\u671f\u8bb0\u5fc6 (Long-Long-Term-Memory\uff09\u5355\u5143\u3002</p> <p>\u5728 LLTMs \u4e2d\u7684\u8fd9\u4e24\u4e2a\u4e0e\u666e\u901a\u7684 LSTMs \u7684\u4e0d\u540c\u70b9\u662f\u5341\u5206\u91cd\u8981\u7684\uff0c\u4ee5\u81f3\u4e8e\u6211\u4eec\u4e0d\u80fd\u901a\u8fc7\u914d\u7f6e PyTorch \u4e2d\u7684 <code>LSTMCell</code> \u6765\u8fbe\u5230\u6211\u4eec\u7684\u76ee\u6807\u3002\u6240\u4ee5\u6211\u4eec\u5c06\u53ea\u80fd\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u6a21\u5757\u3002\u7b2c\u4e00\u4e2a\u4e5f\u662f\u6700\u7b80\u5355\u7684\u65b9\u6cd5 - \u53ef\u80fd\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u662f\u826f\u597d\u7684\u7b2c\u4e00\u6b65\u2014\u2014\u662f\u4f7f\u7528 Python \u5728\u7eaf PyTorch \u4e2d\u5b9e\u73b0\u6211\u4eec\u6240\u9700\u7684\u529f\u80fd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9700\u8981\u7ee7\u627f <code>torch.nn.Module</code> \u5e76\u5b9e\u73b0 LLTM \u7684\u6b63\u5411\u4f20\u9012\u3002 \u8fd9\u770b\u8d77\u6765\u5c31\u50cf\u8fd9\u6837\uff1a</p> <pre><code>class LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = F.sigmoid(gates[0])\n        output_gate = F.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = F.tanh(new_cell) * output_gate\n\n        return new_h, new_cell\n\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u6309\u9884\u671f\u4f7f\u7528\u5b83\uff1a</p> <pre><code>import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))\n\n</code></pre> <p>\u5f53\u7136\uff0c\u5982\u679c\u53ef\u80fd\u7684\u8bdd\uff0c\u4f60\u5e94\u8be5\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u6765\u6269\u5c55 PyTorch\u3002\u7531\u4e8e PyTorch \u5bf9 CPU \u4e0e GPU \u7684\u64cd\u4f5c\u5b9e\u65bd\u4e86\u9ad8\u5ea6\u4f18\u5316\uff0c\u7531 NVIDIA cuDNN\uff0cIntel MKL \u6216\u662f NNPACK \u7b49\u5e93\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u50cf\u4e0a\u9762\u90a3\u6837\u7684 PyTorch \u4ee3\u7801\u4e00\u822c\u60c5\u51b5\u4e0b\u90fd\u662f\u8db3\u591f\u5feb\u901f\u7684\u3002\u4f46\u662f\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u770b\u5230\u4e3a\u4ec0\u4e48\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fd8\u6709\u8fdb\u4e00\u6b65\u6539\u8fdb\u6027\u80fd\u7684\u7a7a\u95f4\u3002\u6700\u660e\u663e\u7684\u539f\u56e0\u662fPyTorch\u4e0d\u4e86\u89e3\u4f60\u6b63\u5728\u5b9e\u65bd\u7684 \u7b97\u6cd5 \u3002\u5b83\u53ea\u77e5\u9053\u4f60\u7528\u4e8e\u7f16\u5199\u7b97\u6cd5\u7684\u5404\u4e2a\u72ec\u7acb\u64cd\u4f5c\u3002\u56e0\u6b64\uff0cPyTorch \u5fc5\u987b\u9010\u4e2a\u6267\u884c\u4f60\u7684\u64cd\u4f5c\u3002\u7531\u4e8e\u5bf9\u64cd\u4f5c\u7684\u5b9e\u73b0(\u6216 \u6838 )\u7684\u6bcf\u6b21\u5355\u72ec\u7684\u8c03\u7528\u90fd\u53ef\u80fd(\u53ef\u80fd\u6d89\u53ca\u542f\u52a8 CUDA \u5185\u6838\uff09\u5177\u6709\u4e00\u5b9a\u91cf\u7684\u5f00\u9500\uff0c\u56e0\u6b64\u8fd9\u79cd\u5f00\u9500\u53ef\u80fd\u5728\u8bb8\u591a\u51fd\u6570\u7684\u8c03\u7528\u4e2d\u53d8\u5f97\u663e\u7740\u3002\u6b64\u5916\uff0c\u8fd0\u884c\u6211\u4eec\u7684\u4ee3\u7801\u7684 Python \u89e3\u91ca\u5668\u672c\u8eab\u5c31\u53ef\u4ee5\u51cf\u6162\u6211\u4eec\u7684\u7a0b\u5e8f\u3002</p> <p>\u56e0\u6b64\uff0c\u4e00\u4e2a\u660e\u663e\u53ef\u4ee5\u52a0\u5feb\u901f\u5ea6\u7684\u65b9\u6cd5\u662f\u7528 C++(\u6216 CUDA\uff09\u5b8c\u6210\u90e8\u5206\u4ee3\u7801\u7684\u91cd\u5199\u90e8\u5206\u5e76_\u878d\u5408_\u7279\u5b9a\u7684\u64cd\u4f5c\u7ec4\u3002\u878d\u5408\u610f\u5473\u7740\u5c06\u8bb8\u591a\u51fd\u6570\u7684\u5b9e\u73b0\u7ec4\u5408\u5230\u5355\u4e2a\u51fd\u6570\u4e2d\uff0c\u8fd9\u4e9b\u51fd\u6570\u4f1a\u4ece\u66f4\u5c11\u7684\u5185\u6838\u542f\u52a8\u4e2d\u53d7\u76ca\uff0c\u6b64\u5916\uff0c\u8fd9\u4e9b\u51fd\u6570\u8fd8\u4f1a\u4ece\u6211\u4eec\u901a\u8fc7\u63d0\u9ad8\u5168\u5c40\u6570\u636e\u6d41\u7684\u53ef\u89c1\u6027\u6765\u6267\u884c\u7684\u5176\u4ed6\u4f18\u5316\u4e2d\u83b7\u76ca\u3002</p> <p>\u8ba9\u6211\u4eec\u6765\u770b\u770b\u6211\u4eec\u53ef\u4ee5\u600e\u6837\u4f7f\u7528 C++ \u62d3\u5c55\u6765\u5b9e\u73b0\u4e00\u4e2a_\u878d\u5408_\u7248\u672c\u7684 LLTM\u3002\u6211\u4eec\u9996\u5148\u4f7f\u7528\u7eaf C++ \u5b8c\u6210\u4ee3\u7801\u7f16\u5199\uff0c\u4f7f\u7528\u9a71\u52a8\u4e86\u5927\u90e8\u5206 PyTorch \u540e\u7aef\u7684 ATen \u5e93\uff0c\u5e76\u770b\u770b\u5b83\u80fd\u8ba9\u6211\u4eec\u591a\u7b80\u5355\u5c31\u5b8c\u6210 Python \u4ee3\u7801\u7684\u8f6c\u6362\u3002\u7136\u540e\u6211\u4eec\u5c06\u901a\u8fc7\u5c06\u4e00\u90e8\u5206\u7684\u6a21\u578b\u79fb\u52a8\u5230 CUDA \u5185\u6838\u4ee5\u4ece GPU \u63d0\u4f9b\u7684\u5927\u89c4\u6a21\u5e76\u884c\u6027\u4e2d\u53d7\u76ca\uff0c\u6765\u8fdb\u4e00\u6b65\u52a0\u5feb\u901f\u5ea6\u3002</p>"},{"location":"1.0/cpp_extension/#c","title":"\u7f16\u5199\u4e00\u4e2a C++ \u62d3\u5c55","text":"<p>C++ \u6269\u5c55\u6709\u4e24\u79cd\u5f62\u5f0f\uff1a\u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528<code>setuptools</code>\u6765\u8fdb\u884c\u201c\u63d0\u524d\u201d\u6784\u5efa\uff0c\u6216\u8005\u901a\u8fc7<code>torch.utils.cpp_extension.load()</code>\u6765\u5b9e\u73b0\u201c\u5b9e\u65f6\u201d\u6784\u5efa\u3002\u6211\u4eec\u5c06\u4ece\u7b2c\u4e00\u79cd\u65b9\u6cd5\u5f00\u59cb\uff0c\u7a0d\u540e\u518d\u8ba8\u8bba\u540e\u8005\u3002</p>"},{"location":"1.0/cpp_extension/#setuptools","title":"\u4f7f\u7528<code>setuptools</code>\u8fdb\u884c\u6784\u5efa","text":"<p>\u5bf9\u4e8e\"\u63d0\u524d\"\u8fd9\u79cd\u5f62\u5f0f\uff0c\u6211\u4eec\u901a\u8fc7\u7f16\u5199\u4e00\u4e2a<code>setup.py</code>\u811a\u672c\u6765\u6784\u5efa\u6211\u4eec\u7684 C++ \u6269\u5c55\uff0c\u8be5\u811a\u672c\u4f7f\u7528 setuptools \u6765\u7f16\u8bd1\u6211\u4eec\u7684 C++ \u4ee3\u7801\u3002 \u5bf9\u4e8e LLTM \u800c\u8a00\uff0c\u5b83\u770b\u8d77\u6765\u5c31\u50cf\u4e0b\u9762\u8fd9\u6837\u7b80\u5355\uff1a</p> <pre><code>from setuptools import setup\nfrom torch.utils.cpp_extension import CppExtension, BuildExtension\n\nsetup(name='lltm',\n      ext_modules=[CppExtension('lltm', ['lltm.cpp'])],\n      cmdclass={'build_ext': BuildExtension})\n\n</code></pre> <p>\u5728\u8fd9\u6bb5\u4ee3\u7801\u4e2d\uff0c<code>CppExtension</code>\u662f<code>setuptools.Extension</code>\u7684\u4e00\u4e2a\u4fbf\u5229\u7684\u5305\u88c5\u5668(wrapper\uff09\uff0c\u5b83\u4f20\u9012\u6b63\u786e\u7684\u5305\u542b\u8def\u5f84\u5e76\u5c06\u6269\u5c55\u8bed\u8a00\u8bbe\u7f6e\u4e3a C++\u3002 \u7b49\u6548\u7684\u666e\u901a<code>setuptools</code>\u4ee3\u7801\u50cf\u4e0b\u9762\u8fd9\u6837\u7b80\u5355\uff1a</p> <pre><code>setuptools.Extension(\n   name='lltm',\n   sources=['lltm.cpp'],\n   include_dirs=torch.utils.cpp_extension.include_paths(),\n   language='c++')\n\n</code></pre> <p><code>BuildExtension</code>\u6267\u884c\u8bb8\u591a\u5fc5\u9700\u7684\u914d\u7f6e\u6b65\u9aa4\u548c\u68c0\u67e5\uff0c\u5e76\u5728\u6df7\u5408 C++/CUDA \u6269\u5c55\u7684\u60c5\u51b5\u4e0b\u7ba1\u7406\u6df7\u5408\u7f16\u8bd1\u3002 \u8fd9\u5c31\u662f\u6211\u4eec\u73b0\u5728\u771f\u6b63\u9700\u8981\u4e86\u89e3\u7684\u5173\u4e8e\u6784\u5efa C++ \u6269\u5c55\u7684\u6240\u6709\u5185\u5bb9\uff01\u73b0\u5728\u8ba9\u6211\u4eec\u6765\u770b\u770b\u6211\u4eec\u7684 C++ \u6269\u5c55\u7684\u5b9e\u73b0\uff0c\u5b83\u6269\u5c55\u5230\u4e86<code>lltm.cpp</code>\u4e2d\u3002</p>"},{"location":"1.0/cpp_extension/#c_1","title":"\u7f16\u5199 C++ \u64cd\u4f5c","text":"<p>\u8ba9\u6211\u4eec\u5f00\u59cb\u7528 C++ \u5b9e\u73b0 LLTM\uff01\u6211\u4eec\u5411\u540e\u4f20\u9012\u6240\u9700\u7684\u4e00\u4e2a\u51fd\u6570\u662f sigmoid \u7684\u5bfc\u6570\u3002\u8fd9\u662f\u4e00\u6bb5\u8db3\u591f\u5c0f\u7684\u4ee3\u7801\uff0c\u7528\u4e8e\u8ba8\u8bba\u7f16\u5199 C++ \u6269\u5c55\u65f6\u53ef\u7528\u7684\u6574\u4f53\u73af\u5883\uff1a</p> <pre><code>#include &lt;torch/torch.h&gt;\n\n#include &lt;iostream&gt;\n\nat::Tensor d_sigmoid(at::Tensor z) {\n  auto s = at::sigmoid(z);\n  return (1 - s) * s;\n}\n\n</code></pre> <p><code>torch / torch.h</code>\u662f\u4e00\u7ad9\u5f0f(one-stop\uff09\u5934\u6587\u4ef6\uff0c\u5305\u542b\u7f16\u5199 C++ \u6269\u5c55\u6240\u9700\u7684\u6240\u6709 PyTorch \u4f4d\u3002 \u8fd9\u5305\u62ec\uff1a</p> <ul> <li>ATen \u5e93\uff0c\u6211\u4eec\u4e3b\u8981\u7684\u5f20\u91cf\u8ba1\u7b97\u63a5\u53e3</li> <li>pybind11\uff0c\u6211\u4eec\u4e3a C++ \u4ee3\u7801\u521b\u5efa Python \u7ed1\u5b9a\u7684\u65b9\u6cd5</li> <li>\u7ba1\u7406 ATen \u548c pybind11 \u4e4b\u95f4\u4ea4\u4e92\u7ec6\u8282\u7684\u5934\u6587\u4ef6\u3002</li> </ul> <p><code>d_sigmoid(\uff09</code>\u7684\u5b9e\u73b0\u663e\u793a\u4e86\u5982\u4f55\u4f7f\u7528 ATen API\u3002PyTorch \u7684\u5f20\u91cf\u548c\u53d8\u91cf\u63a5\u53e3\u662f\u4ece ATen \u5e93\u81ea\u52a8\u751f\u6210\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u6216\u591a\u6216\u5c11\u5730\u5c06\u6211\u4eec\u7684 Python \u8bed\u8a00\u5b9e\u73b01:1\u8f6c\u6362\u4e3a C++ \u8bed\u8a00\u5b9e\u73b0\u3002 \u6211\u4eec\u6240\u6709\u8ba1\u7b97\u7684\u4e3b\u8981\u6570\u636e\u7c7b\u578b\u90fd\u662f<code>at::Tensor</code>\u3002\u53ef\u4ee5\u5728\u6b64\u5904\u67e5\u770b\u5176\u5b8c\u6574\u7684 API\u3002\u53e6\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u53ef\u4ee5\u5f15\u7528<code>&lt;iostream&gt;</code>\u6216\u4efb\u4f55\u5176\u4ed6 C \u6216 C++ \u5934\u6587\u4ef6\u2014\u2014\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 C++ 11 \u7684\u5168\u90e8\u529f\u80fd\u3002</p>"},{"location":"1.0/cpp_extension/#_2","title":"\u524d\u5411\u4f20\u64ad","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6574\u4e2a\u524d\u5411\u4f20\u64ad\u90e8\u5206\u79fb\u690d\u4e3a C++ \u4ee3\u7801\uff1a</p> <pre><code>#include &lt;vector&gt;\n\nstd::vector&lt;at::Tensor&gt; lltm_forward(\n    at::Tensor input,\n    at::Tensor weights,\n    at::Tensor bias,\n    at::Tensor old_h,\n    at::Tensor old_cell) {\n  auto X = at::cat({old_h, input}, /*dim=*/1);\n\n  auto gate_weights = at::addmm(bias, X, weights.transpose(0, 1));\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n\n  auto input_gate = at::sigmoid(gates[0]);\n  auto output_gate = at::sigmoid(gates[1]);\n  auto candidate_cell = at::elu(gates[2], /*alpha=*/1.0);\n\n  auto new_cell = old_cell + candidate_cell * input_gate;\n  auto new_h = at::tanh(new_cell) * output_gate;\n\n  return {new_h,\n          new_cell,\n          input_gate,\n          output_gate,\n          candidate_cell,\n          X,\n          gate_weights};\n}\n\n</code></pre>"},{"location":"1.0/cpp_extension/#_3","title":"\u53cd\u5411\u4f20\u64ad","text":"<p>C++ \u7684\u6269\u5c55 API \u76ee\u524d\u4e0d\u4e3a\u6211\u4eec\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u53cd\u5411\u51fd\u6570\u7684\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8fd8\u5fc5\u987b\u5b9e\u65bd LLTM \u7684\u53cd\u5411\u4f20\u64ad\u90e8\u5206\uff0cLLTM \u8ba1\u7b97\u76f8\u5bf9\u4e8e\u6b63\u5411\u4f20\u64ad\u7684\u6bcf\u4e2a\u8f93\u5165\u7684\u635f\u5931\u7684\u5bfc\u6570\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u5c06\u5411\u524d\u548c\u5411\u540e\u51fd\u6570\u653e\u5165<code>torch.autograd.Function</code>\u4ee5\u521b\u5efa\u4e00\u4e2a\u6f02\u4eae\u7684 Python \u7ed1\u5b9a\u3002 \u5411\u540e\u529f\u80fd\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\uff0c\u6240\u4ee5\u6211\u4eec\u4e0d\u4f1a\u6df1\u5165\u7814\u7a76\u4ee3\u7801(\u5982\u679c\u4f60\u611f\u5174\u8da3\uff0cAlex Graves\u7684\u8bba\u6587\u662f\u4e00\u4e2a\u80fd\u8ba9\u4f60\u4e86\u89e3\u8ddf\u591a\u4fe1\u606f\u7684\u597d\u6587\u7ae0\uff09\uff1a</p> <pre><code>// tanh'(z) = 1 - tanh^2(z)\nat::Tensor d_tanh(at::Tensor z) {\n  return 1 - z.tanh().pow(2);\n}\n\n// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}\nat::Tensor d_elu(at::Tensor z, at::Scalar alpha = 1.0) {\n  auto e = z.exp();\n  auto mask = (alpha * (e - 1)) &lt; 0;\n  return (z &gt; 0).type_as(z) + mask.type_as(z) * (alpha * e);\n}\n\nstd::vector&lt;at::Tensor&gt; lltm_backward(\n    at::Tensor grad_h,\n    at::Tensor grad_cell,\n    at::Tensor new_cell,\n    at::Tensor input_gate,\n    at::Tensor output_gate,\n    at::Tensor candidate_cell,\n    at::Tensor X,\n    at::Tensor gate_weights,\n    at::Tensor weights) {\n  auto d_output_gate = at::tanh(new_cell) * grad_h;\n  auto d_tanh_new_cell = output_gate * grad_h;\n  auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;\n\n  auto d_old_cell = d_new_cell;\n  auto d_candidate_cell = input_gate * d_new_cell;\n  auto d_input_gate = candidate_cell * d_new_cell;\n\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n  d_input_gate *= d_sigmoid(gates[0]);\n  d_output_gate *= d_sigmoid(gates[1]);\n  d_candidate_cell *= d_elu(gates[2]);\n\n  auto d_gates =\n      at::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);\n\n  auto d_weights = d_gates.t().mm(X);\n  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gates.mm(weights);\n  const auto state_size = grad_h.size(1);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell};\n}\n\n</code></pre>"},{"location":"1.0/cpp_extension/#python","title":"\u4e0ePython\u7ed1\u5b9a","text":"<p>\u4e00\u65e6\u4f60\u4f7f\u7528 C++ \u548c ATen \u7f16\u5199\u4e86\u64cd\u4f5c\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528 pybind11 \u4ee5\u975e\u5e38\u7b80\u5355\u7684\u65b9\u5f0f\u5c06 C++ \u51fd\u6570\u6216\u7c7b\u7ed1\u5b9a\u5230 Python \u4e0a\u3002\u5173\u4e8e PyTorch \u7684 C++ \u6269\u5c55\u7684\u8fd9\u4e00\u90e8\u5206\u7684\u95ee\u9898\u6216\u7591\u95ee\u5c06\u4e3b\u8981\u901a\u8fc7pybind11\u6587\u6863\u6765\u89e3\u51b3\u3002</p> <p>\u5bf9\u4e8e\u6211\u4eec\u7684\u6269\u5c55\uff0c\u5fc5\u8981\u7684\u7ed1\u5b9a\u4ee3\u7801\u53ea\u662f\u4ec5\u4ec5\u56db\u884c\uff1a</p> <pre><code>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward\");\n}\n\n</code></pre> <p>\u6709\u4e00\u70b9\u9700\u8981\u6ce8\u610f\u7684\u662f\u5b8f<code>TORCH_EXTENSION_NAME</code>\u3002torch \u7684\u6269\u5c55\u90e8\u5206\u5c06\u4f1a\u628a\u5b83\u5b9a\u4e49\u4e3a\u4f60\u5728<code>setup.py</code>\u811a\u672c\u4e2d\u4e3a\u6269\u5c55\u540d\u547d\u540d\u7684\u540d\u79f0\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>TORCH_EXTENSION_NAME</code>\u7684\u503c\u5c06\u4e3a\u201clltm\u201d\u3002\u8fd9\u662f\u4e3a\u4e86\u907f\u514d\u5fc5\u987b\u5728\u4e24\u4e2a\u5730\u65b9(\u6784\u5efa\u811a\u672c\u548c C++ \u4ee3\u7801\u4e2d\uff09\u7ef4\u62a4\u6269\u5c55\u540d\uff0c\u56e0\u4e3a\u4e24\u8005\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4ee4\u4eba\u8ba8\u538c\u4e14\u96be\u4ee5\u8ddf\u8e2a\u7684\u95ee\u9898\u3002</p>"},{"location":"1.0/cpp_extension/#_4","title":"\u4f7f\u7528\u4f60\u7684\u62d3\u5c55","text":"<p>\u6211\u4eec\u73b0\u5728\u8bbe\u7f6e\u4e3a PyTorch \u5bfc\u5165\u6211\u4eec\u7684\u6269\u5c55\u3002 \u6b64\u65f6\uff0c\u4f60\u7684\u76ee\u5f55\u7ed3\u6784\u53ef\u80fd\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>pytorch/\n  lltm-extension/\n    lltm.cpp\n    setup.py\n\n</code></pre> <p>\u73b0\u5728\uff0c\u8fd0\u884c<code>python setup.py install</code>\u6765\u6784\u5efa\u548c\u5b89\u88c5\u4f60\u7684\u6269\u5c55\u3002 \u8fd0\u884c\u7ed3\u679c\u5e94\u8be5\u662f\u8fd9\u6837\u7684\uff1a</p> <pre><code>running install\nrunning bdist_egg\nrunning egg_info\nwriting lltm.egg-info/PKG-INFO\nwriting dependency_links to lltm.egg-info/dependency_links.txt\nwriting top-level names to lltm.egg-info/top_level.txt\nreading manifest file 'lltm.egg-info/SOURCES.txt'\nwriting manifest file 'lltm.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\nrunning build_ext\nbuilding 'lltm' extension\ngcc -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include/TH -I~/local/miniconda/lib/python3.6/site-packages/torch/lib/include/THC -I~/local/miniconda/include/python3.6m -c lltm.cpp -o build/temp.linux-x86_64-3.6/lltm.o -DTORCH_EXTENSION_NAME=lltm -std=c++11\ncc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\ng++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/lltm.o -o build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so\ncreating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-3.6/lltm_cuda.cpython-36m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-3.6/lltm.cpython-36m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg\ncreating stub loader for lltm.cpython-36m-x86_64-linux-gnu.so\nbyte-compiling build/bdist.linux-x86_64/egg/lltm.py to lltm.cpython-36.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\n__pycache__.lltm.cpython-36: module references __file__\ncreating 'dist/lltm-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\nProcessing lltm-0.0.0-py3.6-linux-x86_64.egg\nremoving '~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg' (and everything under it)\ncreating ~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg\nExtracting lltm-0.0.0-py3.6-linux-x86_64.egg to ~/local/miniconda/lib/python3.6/site-packages\nlltm 0.0.0 is already the active version in easy-install.pth\n\nInstalled ~/local/miniconda/lib/python3.6/site-packages/lltm-0.0.0-py3.6-linux-x86_64.egg\nProcessing dependencies for lltm==0.0.0\nFinished processing dependencies for lltm==0.0.0\n\n</code></pre> <p>\u5173\u4e8e\u7f16\u8bd1\u5668\u7684\u4e00\u4e2a\u5c0f\u6ce8\u610f\u4e8b\u9879\uff1a\u7531\u4e8e ABI \u7248\u672c\u95ee\u9898\uff0c\u7528\u4e8e\u6784\u5efa C++ \u6269\u5c55\u7684\u7f16\u8bd1\u5668\u5fc5\u987b\u4e0e ABI \u517c\u5bb9\uff0c\u5e76\u4e14\u8fd9\u91cc\u7684\u7f16\u8bd1\u5668\u662f\u5fc5\u987b\u662f\u4e0e\u6784\u5efa PyTorch \u65f6\u91c7\u7528\u7684\u7f16\u8bd1\u5668\u4e00\u6837\u7684\u3002\u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740\u4f60\u5fc5\u987b\u5728 Linux \u4e0a\u4f7f\u7528 GCC 4.9 \u53ca\u66f4\u9ad8\u7248\u672c\u3002\u5bf9\u4e8e Ubuntu 16.04 \u548c\u5176\u4ed6\u66f4\u65b0\u7684 Linux \u53d1\u884c\u7248\u6765\u8bf4\uff0c\u8fd9\u5e94\u8be5\u662f\u9ed8\u8ba4\u7684\u7f16\u8bd1\u5668\u3002\u5728MacOS\u4e0a\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528clang(\u6ca1\u6709\u4efb\u4f55\u4e0eABI\u7248\u672c\u76f8\u5173\u7684\u95ee\u9898\uff09\u3002\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u7f16\u8bd1\u5668\u4ece\u6e90\u4ee3\u7801\u6784\u5efa PyTorch\uff0c\u7136\u540e\u4f7f\u7528\u76f8\u540c\u7684\u7f16\u8bd1\u5668\u6784\u5efa\u6269\u5c55\u3002</p> <p>\u6784\u5efa\u6269\u5c55\u540e\uff0c\u4f60\u53ea\u9700\u4f7f\u7528\u5728<code>setup.py</code>\u811a\u672c\u4e2d\u6307\u5b9a\u7684\u540d\u79f0\u5728Python\u4e2d\u5bfc\u5165\u5b83\u3002\u8bf7\u52a1\u5fc5\u9996\u5148\u8fd0\u884c <code>import torch</code> \uff0c\u56e0\u4e3a\u8fd9\u5c06\u89e3\u6790\u52a8\u6001\u94fe\u63a5\u5668\u5fc5\u987b\u770b\u5230\u7684\u4e00\u4e9b\u7b26\u53f7\uff1a</p> <pre><code>In [1]: import torch\nIn [2]: import lltm\nIn [3]: lltm.forward\nOut[3]: &lt;function lltm.PyCapsule.forward&gt;\n\n</code></pre> <p>\u5982\u679c\u6211\u4eec\u5728\u51fd\u6570\u6216\u6a21\u5757\u4e0a\u8c03\u7528<code>help()</code>\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5b83\u7684\u7b7e\u540d(signature\uff09\u4e0e\u6211\u4eec\u7684 C++ \u4ee3\u7801\u5339\u914d\uff1a</p> <pre><code>In[4] help(lltm.forward)\nforward(...) method of builtins.PyCapsule instance\n    forward(arg0: at::Tensor, arg1: at::Tensor, arg2: at::Tensor, arg3: at::Tensor, arg4: at::Tensor) -&gt; List[at::Tensor]\n\n    LLTM forward\n\n</code></pre> <p>\u65e2\u7136\u6211\u4eec\u73b0\u5728\u80fd\u591f\u4ece Python \u4e2d\u8c03\u7528\u6211\u4eec\u7684 C++ \u51fd\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528<code>torch.autograd.Function</code>\u548c<code>torch.nn.Module</code>\u6765\u5305\u88c5(warp\uff09\u5b83\u4eec\uff0c\u4f7f\u5b83\u4eec\u6210\u4e3a PyTorch \u4e2d\u7684\u4e00\u7b49\u516c\u6c11(first class citizens\uff0c\u5173\u952e\u7684\u4e00\u90e8\u5206\uff09\uff1a</p> <pre><code>import math\nimport torch\n\n# Our module!\nimport lltm\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_variables)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state)\n\n</code></pre>"},{"location":"1.0/cpp_extension/#_5","title":"\u6027\u80fd\u6bd4\u8f83","text":"<p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5e76\u8c03\u7528\u6765\u81ea PyTorch \u7684 C++ \u4ee3\u7801\uff0c\u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u4e00\u4e2a\u5c0f\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u770b\u770b\u6211\u4eec\u5728 C++ \u4e2d\u91cd\u5199\u7684\u64cd\u4f5c\u7684\u6027\u80fd\u3002\u6211\u4eec\u5c06\u8fd0\u884c LLTM \u4e2d\u7684\u524d\u5411\u8f6c\u64ad\u4e0e\u53cd\u5411\u4f20\u64ad\u51e0\u6b21\u5e76\u6d4b\u91cf\u8fd0\u884c\u7684\u65f6\u95f4\uff1a</p> <pre><code>import torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))\n\n</code></pre> <p>\u5982\u679c\u8fd0\u884c\u6211\u4eec\u5728\u672c\u6587\u5f00\u5934\u7528\u7eaf Python \u7f16\u5199\u539f\u59cb LLTM \u7684\u4ee3\u7801\uff0c\u6211\u4eec\u5c06\u5f97\u5230\u4ee5\u4e0b\u6570\u5b57(\u5728\u6211\u7684\u673a\u5668\u4e0a\uff09\uff1a</p> <pre><code>\nForward: 506.480 us | Backward 444.694 us\n\n</code></pre> <p>\u7136\u540e\u662f\u8fd0\u884c\u5168\u65b0\u7684 C++ \u7248\u672c\u7684\u4ee3\u7801\uff1a</p> <pre><code>Forward: 349.335 us | Backward 443.523 us\n\n</code></pre> <p>\u6211\u4eec\u5df2\u7ecf\u53ef\u4ee5\u770b\u5230\u524d\u5411\u4f20\u64ad\u51fd\u6570\u7684\u663e\u7740\u52a0\u901f(\u8d85\u8fc730\uff05\uff09\u3002\u5bf9\u4e8e\u53cd\u5411\u4f20\u64ad\u51fd\u6570\u800c\u8a00\uff0c\u6211\u4eec\u4e5f\u662f\u53ef\u4ee5\u770b\u5230\u52a0\u901f\u6548\u679c\u7684\uff0c\u5c3d\u7ba1\u52a0\u901f\u7684\u6548\u679c\u4e0d\u662f\u5f88\u660e\u663e\u3002\u6211\u5728\u4e0a\u9762\u5199\u7684\u53cd\u5411\u4f20\u64ad\u5e76\u6ca1\u6709\u7ecf\u8fc7\u7279\u522b\u4f18\u5316\uff0c\u5b83\u7edd\u5bf9\u8fd8\u53ef\u4ee5\u8fdb\u884c\u6539\u8fdb\u3002\u6b64\u5916\uff0cPyTorch \u7684\u81ea\u52a8\u5dee\u5206\u5f15\u64ce\u53ef\u4ee5\u81ea\u52a8\u5e76\u884c\u5316\u8ba1\u7b97\u56fe\uff0c\u53ef\u4ee5\u4f7f\u7528\u66f4\u9ad8\u6548\u7684\u6574\u4f53\u64cd\u4f5c\u6d41\uff0c\u5e76\u4e14\u8fd9\u4e5f\u662f\u7528 C++ \u5b9e\u73b0\uff0c\u56e0\u6b64\u9884\u8ba1\u8fd0\u884c\u901f\u5ea6\u4f1a\u5f88\u5feb\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u8fd9\u662f\u4e00\u4e2a\u826f\u597d\u7684\u5f00\u7aef\u3002</p>"},{"location":"1.0/cpp_extension/#gpu","title":"\u5728GPU\u8bbe\u5907\u4e0a\u7684\u6027\u80fd","text":"<p>\u5173\u4e8e PyTorch \u7684 ATen \u540e\u7aef\u7684\u4e00\u4e2a\u5f88\u597d\u7684\u4e8b\u5b9e\u662f\u5b83\u62bd\u8c61\u4e86\u4f60\u6b63\u5728\u8fd0\u884c\u4ee3\u7801\u7684\u8ba1\u7b97\u8bbe\u5907\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u4e3aCPU\u7f16\u5199\u7684\u4ee3\u7801\u4e5f\u53ef\u4ee5\u5728GPU\u4e0a\u8fd0\u884c\uff0c\u5e76\u4e14\u5404\u4e2a\u64cd\u4f5c\u5c06\u76f8\u5e94\u5730\u5206\u6d3e\u5230\u4ee5 GPU \u4f18\u5316\u8fc7\u540e\u7684\u5b9e\u73b0\u4e2d\u53bb\u3002\u5bf9\u4e8e\u67d0\u4e9b\u64cd\u4f5c\uff0c\u5982\u77e9\u9635\u4e58\u6cd5(\u5982<code>mm</code>\u6216<code>admm</code>\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f88\u5927\u7684\u80dc\u5229\u3002\u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u4f7f\u7528 CUDA \u5f20\u91cf\u8fd0\u884c C++ \u4ee3\u7801\u53ef\u4ee5\u83b7\u5f97\u591a\u5c11\u7684\u6027\u80fd\u63d0\u5347\u3002\u6211\u4eec\u4e0d\u9700\u8981\u5bf9\u4ee3\u7801\u4f5c\u51fa\u4efb\u4f55\u6539\u53d8\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06\u6211\u4eec\u7684\u5f20\u91cf\u653e\u5728 Python \u4e2d\u7684 GPU \u5185\u5b58\u4e2d\uff0c\u5728\u521b\u5efa\u65f6\u6dfb\u52a0<code>device = cuda_device</code>\u53c2\u6570\u6216\u5728\u521b\u5efa\u540e\u4f7f\u7528<code>.to(cuda_device)</code>\u5373\u53ef\uff1a</p> <pre><code>import torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))\n\n</code></pre> <p>\u518d\u4e00\u6b21\u5c06\u6211\u4eec\u7684\u666e\u901a PyTorch \u4ee3\u7801\u4e0e\u6211\u4eec\u7684 C++ \u7248\u672c\u8fdb\u884c\u6bd4\u8f83\uff0c\u73b0\u5728\u4e24\u8005\u90fd\u8fd0\u884c\u5728 CUDA \u8bbe\u5907\u4e0a\uff0c\u6211\u4eec\u79d1\u6280\u518d\u6b21\u770b\u5230\u6027\u80fd\u5f97\u5230\u4e86\u63d0\u5347\u3002 \u5bf9\u4e8e Python/PyTorch \u6765\u8bf4\uff1a</p> <pre><code>Forward: 187.719 us | Backward 410.815 us\n\n</code></pre> <p>\u7136\u540e\u662f C++ / ATen\uff1a</p> <pre><code>Forward: 149.802 us | Backward 393.458 us\n\n</code></pre> <p>\u4e0e\u975e CUDA \u4ee3\u7801\u76f8\u6bd4\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u6574\u4f53\u52a0\u901f\u3002\u4f46\u662f\uff0c\u901a\u8fc7\u7f16\u5199\u81ea\u5b9a\u4e49 CUDA \u5185\u6838\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece C++ \u4ee3\u7801\u4e2d\u5f97\u5230\u66f4\u591a\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6211\u4eec\u5c06\u5f88\u5feb\u5728\u4e0b\u9762\u4ecb\u7ecd\u8fd9\u4e9b\u5185\u6838\u3002\u5728\u6b64\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u8ba8\u8bba\u6784\u5efa C++ \u6269\u5c55\u7684\u53e6\u4e00\u79cd\u65b9\u6cd5\u3002</p>"},{"location":"1.0/cpp_extension/#jit","title":"JIT \u7f16\u8bd1\u6269\u5c55","text":"<p>\u5728\u4e4b\u524d\uff0c\u6211\u63d0\u5230\u6709\u4e24\u79cd\u6784\u5efa C++ \u6269\u5c55\u7684\u65b9\u6cd5\uff1a\u4f7f\u7528<code>setuptools</code>\u6216\u8005\u662f\u5b9e\u65f6(JIT\uff09\u3002 \u5728\u5bf9\u524d\u8005\u8fdb\u884c\u4e86\u8bf4\u660e\u4e4b\u540e\uff0c\u8ba9\u6211\u4eec\u518d\u8be6\u7ec6\u8bf4\u660e\u4e00\u4e0b\u540e\u8005\u3002JIT \u7f16\u8bd1\u673a\u5236\u901a\u8fc7\u5728 PyTorch \u7684 API \u4e2d\u8c03\u7528\u4e00\u4e2a\u540d\u4e3a<code>torch.utils.cpp_extension.load()</code>\u7684\u7b80\u5355\u51fd\u6570\uff0c\u4e3a\u4f60\u63d0\u4f9b\u4e86\u4e00\u79cd\u7f16\u8bd1\u548c\u52a0\u8f7d\u6269\u5c55\u7684\u65b9\u6cd5\u3002\u5bf9\u4e8e LLTM\uff0c\u8fd9\u770b\u8d77\u6765\u5c31\u50cf\u4e0b\u9762\u8fd9\u6837\u7b80\u5355\uff1a</p> <pre><code>from torch.utils.cpp_extension import load\n\nlltm = load(name=\"lltm\", sources=[\"lltm.cpp\"])\n\n</code></pre> <p>\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4e3a\u51fd\u6570\u63d0\u4f9b\u4e0e<code>setuptools</code>\u76f8\u540c\u7684\u4fe1\u606f\u3002 \u5728\u540e\u53f0\uff0c\u5c06\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a</p> <ol> <li>\u521b\u5efa\u4e34\u65f6\u76ee\u5f55 <code>/tmp/torch_extensions/lltm</code></li> <li>\u5c06\u4e00\u4e2a Ninja \u6784\u5efa\u6587\u4ef6\u53d1\u9001\u5230\u8be5\u4e34\u65f6\u76ee\u5f55\uff0c</li> <li>\u5c06\u6e90\u6587\u4ef6\u7f16\u8bd1\u4e3a\u5171\u4eab\u5e93</li> <li>\u5c06\u6b64\u5171\u4eab\u5e93\u5bfc\u5165\u4e3a Python \u6a21\u5757</li> </ol> <p>\u5b9e\u9645\u4e0a\uff0c\u5982\u679c\u4f60\u5c06<code>verbose = True</code>\u53c2\u6570\u4f20\u9012\u7ed9<code>cpp_extension.load(\uff09</code>\uff0c\u8be5\u8fc7\u7a0b\u5728\u8fdb\u884c\u7684\u8fc7\u7a0b\u4e2d\u5c06\u4f1a\u544a\u77e5\u4f60\uff1a</p> <pre><code>Using /tmp/torch_extensions as PyTorch extensions root...\nCreating extension directory /tmp/torch_extensions/lltm...\nEmitting ninja build file /tmp/torch_extensions/lltm/build.ninja...\nBuilding extension module lltm...\nLoading extension module lltm...\n\n</code></pre> <p>\u751f\u6210\u7684 Python \u6a21\u5757\u4e0e setuptools \u751f\u6210\u7684\u5b8c\u5168\u76f8\u540c\uff0c\u4f46\u4e0d\u9700\u8981\u7ef4\u62a4\u5355\u72ec\u7684<code>setup.py</code>\u6784\u5efa\u6587\u4ef6\u3002\u5982\u679c\u4f60\u7684\u8bbe\u7f6e\u66f4\u590d\u6742\u5e76\u4e14\u4f60\u786e\u5b9e\u9700\u8981<code>setuptools</code>\u7684\u5168\u90e8\u529f\u80fd\uff0c\u90a3\u4e48\u4f60\u53ef\u4ee5\u7f16\u5199\u81ea\u5df1\u7684<code>setup.py</code>\u2014\u2014\u4f46\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\uff0c\u8fd9\u79cdJIT\u7684\u65b9\u5f0f\u5c31\u5df2\u7ecf\u5b8c\u5168\u591f\u7528\u4e86\u3002\u7b2c\u4e00\u6b21\u8fd0\u884c\u6b64\u884c\u4ee3\u7801\u65f6\uff0c\u5c06\u8017\u8d39\u4e00\u4e9b\u65f6\u95f4\uff0c\u56e0\u4e3a\u6269\u5c55\u6b63\u5728\u540e\u53f0\u8fdb\u884c\u7f16\u8bd1\u3002\u7531\u4e8e\u6211\u4eec\u4f7f\u7528 Ninja \u6784\u5efa\u7cfb\u7edf\u6765\u6784\u5efa\u6e90\u4ee3\u7801\uff0c\u56e0\u6b64\u91cd\u65b0\u7f16\u8bd1\u7684\u5de5\u4f5c\u91cf\u662f\u4e0d\u65ad\u589e\u52a0\u7684\uff0c\u800c\u5f53\u4f60\u7b2c\u4e8c\u6b21\u8fd0\u884c Python \u6a21\u5757\u8fdb\u884c\u91cd\u65b0\u52a0\u8f7d\u6269\u5c55\u65f6\u901f\u5ea6\u5c31\u4f1a\u5feb\u5f97\u591a\u4e86\uff0c\u800c\u4e14\u5982\u679c\u4f60\u6ca1\u6709\u5bf9\u6269\u5c55\u7684\u6e90\u6587\u4ef6\u8fdb\u884c\u66f4\u6539\uff0c\u9700\u8981\u7684\u5f00\u9500\u4e5f\u5c06\u4f1a\u5f88\u4f4e\u3002</p>"},{"location":"1.0/cpp_extension/#ccuda","title":"\u7f16\u5199\u4e00\u4e2a C++/CUDA \u6df7\u5408\u7684\u62d3\u5c55","text":"<p>\u4e3a\u4e86\u771f\u6b63\u5c06\u6211\u4eec\u7684\u5b9e\u73b0\u7684\u6027\u80fd\u63d0\u5347\u5230\u4e00\u4e2a\u65b0\u7684\u6c34\u5e73\uff0c\u6211\u4eec\u53ef\u4ee5\u81ea\u5b9a\u4e49 CUDA \u5185\u6838\u5e76\u5168\u624b\u5de5\u7684\u5b8c\u6210\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e2d\u90e8\u5206\u4ee3\u7801\u7684\u7f16\u5199\u3002\u5bf9\u4e8e LLTM \u6765\u8bf4\uff0c\u8fd9\u5177\u6709\u7279\u522b\u6709\u6548\u7684\u524d\u666f\uff0c\u56e0\u4e3a\u5e8f\u5217\u4e2d\u5b58\u5728\u5927\u91cf\u9010\u70b9\u8fd0\u7b97\uff0c\u6240\u6709\u8fd9\u4e9b\u8fd0\u7b97\u90fd\u53ef\u4ee5\u5728\u5355\u4e2a CUDA \u5185\u6838\u4e2d\u878d\u5408\u548c\u5e76\u884c\u5316\u3002\u8ba9\u6211\u4eec\u770b\u770b\u5982\u4f55\u4f7f\u7528\u8fd9\u79cd\u6269\u5c55\u673a\u5236\u7f16\u5199\u8fd9\u6837\u7684 CUDA \u5185\u6838\u5e76\u5c06\u5176\u4e0e PyTorch \u6574\u5408\u5230\u4e00\u8d77\u3002</p> <p>\u7f16\u5199 CUDA \u6269\u5c55\u7684\u4e00\u822c\u7b56\u7565\u662f\u9996\u5148\u7f16\u5199\u4e00\u4e2a C++ \u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5b9a\u4e49\u4e86\u5c06\u4ece Python \u4e2d\u8c03\u7528\u7684\u51fd\u6570\uff0c\u5e76\u4f7f\u7528 pybind11 \u5c06\u8fd9\u4e9b\u51fd\u6570\u7ed1\u5b9a\u5230 Python \u4e0a\u3002\u6b64\u5916\uff0c\u8be5\u6587\u4ef6\u8fd8\u5c06 \u58f0\u660e \u5728 CUDA(<code>.cu</code>\uff09\u6587\u4ef6\u4e2d\u5c06\u5b9a\u4e49\u7684\u51fd\u6570\u3002\u7136\u540e\uff0cC++ \u51fd\u6570\u5c06\u8fdb\u884c\u4e00\u4e9b\u68c0\u67e5\uff0c\u5e76\u6700\u7ec8\u5c06\u5176\u8c03\u7528\u8f6c\u53d1\u7ed9 CUDA \u51fd\u6570\u3002\u5728 CUDA \u6587\u4ef6\u4e2d\uff0c\u6211\u4eec\u7f16\u5199\u4e86\u5b9e\u9645\u7684 CUDA \u5185\u6838\u3002\u63a5\u7740\uff0c<code>cpp_extension</code>\u5305\u5c06\u8d1f\u8d23\u4f7f\u7528 C++ \u7f16\u8bd1\u5668(\u5982<code>gcc</code>\uff09\u548c\u4f7f\u7528 NVIDIA \u7684<code>nvcc</code>\u7f16\u8bd1\u5668\u7684CUDA\u6e90\u7f16\u8bd1 C++ \u6e90\u4ee3\u7801\u3002\u4ee5\u6b64\u6765\u786e\u4fdd\u6bcf\u4e2a\u7f16\u8bd1\u5668\u5904\u7406\u5b83\u6700\u597d\u7f16\u8bd1\u7684\u6587\u4ef6\u3002\u6700\u7ec8\uff0c\u5b83\u4eec\u5c06\u94fe\u63a5\u5230\u4e00\u4e2a\u53ef\u4ece Python \u4ee3\u7801\u4e2d\u8fdb\u884c\u8bbf\u95ee\u7684\u5171\u4eab\u5e93\u3002</p> <p>\u6211\u4eec\u5c06\u4ece C++ \u6587\u4ef6\u5f00\u59cb\uff0c\u6211\u4eec\u5c06\u5176\u79f0\u4e3a<code>lltm_cuda.cpp</code>\uff0c\u4f8b\u5982\uff1a</p> <pre><code>#include &lt;torch/torch.h&gt;\n\n#include &lt;vector&gt;\n\n// CUDA forward declarations\n\nstd::vector&lt;at::Tensor&gt; lltm_cuda_forward(\n    at::Tensor input,\n    at::Tensor weights,\n    at::Tensor bias,\n    at::Tensor old_h,\n    at::Tensor old_cell);\n\nstd::vector&lt;at::Tensor&gt; lltm_cuda_backward(\n    at::Tensor grad_h,\n    at::Tensor grad_cell,\n    at::Tensor new_cell,\n    at::Tensor input_gate,\n    at::Tensor output_gate,\n    at::Tensor candidate_cell,\n    at::Tensor X,\n    at::Tensor gate_weights,\n    at::Tensor weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector&lt;at::Tensor&gt; lltm_forward(\n    at::Tensor input,\n    at::Tensor weights,\n    at::Tensor bias,\n    at::Tensor old_h,\n    at::Tensor old_cell) {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);\n}\n\nstd::vector&lt;at::Tensor&gt; lltm_backward(\n    at::Tensor grad_h,\n    at::Tensor grad_cell,\n    at::Tensor new_cell,\n    at::Tensor input_gate,\n    at::Tensor output_gate,\n    at::Tensor candidate_cell,\n    at::Tensor X,\n    at::Tensor gate_weights,\n    at::Tensor weights) {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return lltm_cuda_backward(\n      grad_h,\n      grad_cell,\n      new_cell,\n      input_gate,\n      output_gate,\n      candidate_cell,\n      X,\n      gate_weights,\n      weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward (CUDA)\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward (CUDA)\");\n}\n\n</code></pre> <p>\u6b63\u5982\u4f60\u6240\u770b\u5230\u7684\uff0c\u5b83\u4e3b\u8981\u662f\u4e00\u4e2a\u6837\u677f(boilerplate\uff09\uff0c\u68c0\u67e5\u548c\u8f6c\u53d1\u5230\u6211\u4eec\u5c06\u5728 CUDA \u6587\u4ef6\u4e2d\u5b9a\u4e49\u7684\u51fd\u6570\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u6587\u4ef6\u547d\u540d\u4e3a<code>lltm_cuda_kernel.cu</code>(\u6ce8\u610f<code>.cu</code>\u6269\u5c55\u540d\uff01\uff09\u3002NVCC \u53ef\u4ee5\u5408\u7406\u5730\u7f16\u8bd1 C++ 11\uff0c\u56e0\u6b64\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u4f7f\u7528 ATen \u548c C++ \u6807\u51c6\u5e93(\u4f46<code>torch.h</code>\u4e0d\u884c\uff09\u3002 \u8bf7\u6ce8\u610f\uff0c<code>setuptools</code>\u65e0\u6cd5\u5904\u7406\u5177\u6709\u76f8\u540c\u540d\u79f0\u4f46\u6269\u5c55\u540d\u4e0d\u540c\u7684\u6587\u4ef6\uff0c\u56e0\u6b64\u5982\u679c\u4f7f\u7528<code>setup.py</code>\u65b9\u6cd5\u800c\u4e0d\u662f JIT \u65b9\u6cd5\uff0c\u5219\u5fc5\u987b\u4e3a CUDA \u6587\u4ef6\u6307\u5b9a\u4e0e C++ \u6587\u4ef6\u4e0d\u540c\u7684\u540d\u79f0(\u5bf9\u4e8eJIT\uff09 \u65b9\u6cd5\uff0c<code>lltm.cpp</code>\u548c<code>lltm.cu</code>\u4f1a\u6b63\u5e38\u5de5\u4f5c\uff09\u3002 \u6211\u4eec\u6765\u770b\u770b\u8fd9\u4e2a\u6587\u4ef6\u7684\u6837\u5b50\uff1a</p> <pre><code>#include &lt;ATen/ATen.h&gt;\n\n#include &lt;cuda.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#include &lt;vector&gt;\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t sigmoid(scalar_t z) {\n  return 1.0 / (1.0 + exp(-z));\n}\n\n</code></pre> <p>\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6211\u521a\u521a\u63cf\u8ff0\u7684\u5934\u6587\u4ef6\uff0c\u4ee5\u53ca\u6211\u4eec\u4f7f\u7528 CUDA \u7279\u5b9a\u7684\u58f0\u660e\uff0c\u5982<code>__device__</code>\u548c<code>__forceinline__</code>\u4ee5\u53ca\u50cf<code>exp</code>\u8fd9\u6837\u7684\u51fd\u6570\u3002\u8ba9\u6211\u4eec\u7ee7\u7eed\u4f7f\u7528\u6211\u4eec\u5c06\u9700\u8981\u7528\u5230\u7684\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\uff1a</p> <pre><code>template &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) {\n  const auto s = sigmoid(z);\n  return (1.0 - s) * s;\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_tanh(scalar_t z) {\n  const auto t = tanh(z);\n  return 1 - (t * t);\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) {\n  return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0));\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) {\n  const auto e = exp(z);\n  const auto d_relu = z &lt; 0.0 ? 0.0 : 1.0;\n  return d_relu + (((alpha * (e - 1.0)) &lt; 0.0) ? (alpha * e) : 0.0);\n}\n\n</code></pre> <p>\u73b0\u5728\u5b9e\u9645\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2a\u51fd\u6570\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u4e24\u4ef6\u4e8b\uff1a\u4e00\u4e2a\u51fd\u6570\u6267\u884c\u6211\u4eec\u4e0d\u5e0c\u671b\u624b\u52a8\u663e\u5f0f\u5199\u5165\u7684\u64cd\u4f5c\u5e76\u8c03\u7528 CUDA \u5185\u6838\uff0c\u7136\u540e\u5b9e\u9645\u7684 CUDA \u5185\u6838\u7528\u4e8e\u6211\u4eec\u60f3\u8981\u52a0\u901f\u7684\u90e8\u5206\u3002\u5bf9\u4e8e\u524d\u5411\u8f6c\u64ad\u6765\u8bf4\uff0c\u7b2c\u4e00\u4e2a\u51fd\u6570\u5e94\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>std::vector&lt;at::Tensor&gt; lltm_cuda_forward(\n    at::Tensor input,\n    at::Tensor weights,\n    at::Tensor bias,\n    at::Tensor old_h,\n    at::Tensor old_cell) {\n  auto X = at::cat({old_h, input}, /*dim=*/1);\n  auto gates = at::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto new_h = at::zeros_like(old_cell);\n  auto new_cell = at::zeros_like(old_cell);\n  auto input_gate = at::zeros_like(old_cell);\n  auto output_gate = at::zeros_like(old_cell);\n  auto candidate_cell = at::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&amp;] {\n    lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        gates.data&lt;scalar_t&gt;(),\n        old_cell.data&lt;scalar_t&gt;(),\n        new_h.data&lt;scalar_t&gt;(),\n        new_cell.data&lt;scalar_t&gt;(),\n        input_gate.data&lt;scalar_t&gt;(),\n        output_gate.data&lt;scalar_t&gt;(),\n        candidate_cell.data&lt;scalar_t&gt;(),\n        state_size);\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}\n\n</code></pre> <p>\u8fd9\u91cc\u4e3b\u8981\u5173\u6ce8\u7684\u662f<code>AT_DISPATCH_FLOATING_TYPES</code>\u5b8f\u548c\u5185\u6838\u542f\u52a8(\u7531<code>&lt;&lt;&lt;...&gt;&gt;&gt;</code>\u8fdb\u884c\u8868\u793a\uff09\u3002\u867d\u7136 ATen \u4f1a\u5bf9\u6211\u4eec\u6240\u5904\u7406\u7684\u5f20\u91cf\u7684\u8bbe\u5907\u548c\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u62bd\u8c61\u5316\uff0c\u4f46\u662f\u5728\u8fd0\u884c\u65f6\uff0c\u5f20\u91cf\u4ecd\u5c06\u7531\u5177\u4f53\u8bbe\u5907\u4e0a\u7684\u5177\u4f53\u7c7b\u578b\u7684\u5185\u5b58\u652f\u6301\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u5728\u8fd0\u884c\u65f6\u786e\u5b9a\u5f20\u91cf\u662f\u4ec0\u4e48\u7c7b\u578b\u7684\u65b9\u6cd5\uff0c\u7136\u540e\u9009\u62e9\u6027\u5730\u8c03\u7528\u76f8\u5e94\u7684\u5177\u6709\u6b63\u786e\u7c7b\u578b\u7b7e\u540d(signature\uff09\u51fd\u6570\u3002\u624b\u52a8\u5b8c\u6210\u8fd9\u4e9b\u90e8\u5206\uff0c\u8fd9\u5c06(\u6982\u5ff5\u4e0a\uff09\u770b\u8d77\u6765\u50cf\u8fd9\u6837\uff1a</p> <pre><code>switch (tensor.type().scalarType()) {\n  case at::ScalarType::Double:\n    return function&lt;double&gt;(tensor.data&lt;double&gt;());\n  case at::ScalarType::Float:\n    return function&lt;float&gt;(tensor.data&lt;float&gt;());\n  ...\n}\n\n</code></pre> <p><code>AT_DISPATCH_FLOATING_TYPES</code> \u7684\u76ee\u7684\u662f\u4e3a\u6211\u4eec\u5904\u7406\u6b64\u6b21\u8c03\u5ea6\u3002\u5b83\u9700\u8981\u4e00\u4e2a\u7c7b\u578b(\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u662f<code>gates.type()</code>\uff09\uff0c\u4e00\u4e2a\u540d\u79f0(\u7528\u4e8e\u9519\u8bef\u6d88\u606f\uff09\u548c\u4e00\u4e2a lambda \u51fd\u6570\u3002\u5728\u8fd9\u4e2a lambda \u51fd\u6570\u4e2d\uff0c\u7c7b\u578b\u522b\u540d<code>scalar_t</code>\u662f\u53ef\u7528\u7684\uff0c\u5e76\u4e14\u88ab\u5b9a\u4e49\u4e3a\u5f20\u91cf\u5728\u8be5\u4e0a\u4e0b\u6587\u4e2d\u5b9e\u9645\u5904\u4e8e\u8fd0\u884c\u65f6\u7684\u7c7b\u578b\u3002\u56e0\u6b64\uff0c\u5982\u679c\u6211\u4eec\u6709\u4e00\u4e2a\u6a21\u677f\u51fd\u6570(\u6211\u4eec\u7684 CUDA \u5185\u6838\u5c06\u4f5c\u4e3a\u6a21\u677f\u51fd\u6570\uff09\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u8fd9\u4e2a<code>scalar_t</code>\u522b\u540d\u5b9e\u4f8b\u5316\u5b83\uff0c\u7136\u540e\u6b63\u786e\u7684\u51fd\u6570\u5c31\u4f1a\u88ab\u8c03\u7528\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8fd8\u5e0c\u671b\u68c0\u7d22\u5f20\u91cf\u7684\u6570\u636e\u6307\u9488\u4f5c\u4e3a<code>scalar_t</code>\u7c7b\u578b\u7684\u6307\u9488\u3002 \u5982\u679c\u4f60\u60f3\u8c03\u5ea6\u6240\u6709\u7c7b\u578b\u800c\u4e0d\u4ec5\u4ec5\u662f\u6d6e\u70b9\u7c7b\u578b(<code>Float</code>\u548c<code>Double</code>\uff09\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528<code>AT_DISPATCH_ALL_TYPES</code>\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u4f7f\u7528\u666e\u901a ATen \u6267\u884c\u7684\u4e00\u4e9b\u64cd\u4f5c\u3002\u8fd9\u4e9b\u64cd\u4f5c\u4ecd\u5c06\u5728 GPU \u4e0a\u8fd0\u884c\uff0c\u4f46\u4f7f\u7528\u7684\u662f ATen \u7684\u9ed8\u8ba4\u5b9e\u73b0\u3002\u8fd9\u662f\u6709\u9053\u7406\u7684\uff0c\u56e0\u4e3a ATen \u5c06\u4f7f\u7528\u9ad8\u5ea6\u4f18\u5316\u7684\u4f8b\u7a0b\u6765\u5904\u7406\u8bf8\u5982\u77e9\u9635\u4e58\u6cd5(\u4f8b\u5982<code>addmm</code>\uff09\u6216\u8005\u662f\u4e00\u4e9b\u6211\u4eec\u81ea\u5df1\u5341\u5206\u96be\u4ee5\u5b9e\u73b0\u4ee5\u53ca\u5b8c\u6210\u6027\u80fd\u63d0\u5347\u7684\u64cd\u4f5c\uff0c\u5982\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>\u81f3\u4e8e\u5185\u6838\u542f\u52a8\u672c\u8eab\uff0c\u6211\u4eec\u5728\u8fd9\u91cc\u6307\u5b9a\u6bcf\u4e2a CUDA \u5757\u5c06\u5177\u67091024\u4e2a\u7ebf\u7a0b\uff0c\u5e76\u4e14\u6574\u4e2a GPU \u7f51\u683c\u88ab\u5206\u6210\u5c3d\u53ef\u80fd\u591a\u7684 <code>1 x 1024</code> \u7ebf\u7a0b\u5757\uff0c\u5e76\u4ee5\u4e00\u7ec4\u4e00\u4e2a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u586b\u5145\u6211\u4eec\u7684\u77e9\u9635\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6211\u4eec\u7684\u72b6\u6001(state\uff09\u5927\u5c0f\u4e3a2048\u4e14\u6279\u91cf\u5927\u5c0f\u4e3a4\uff0c\u90a3\u4e48\u6211\u4eec\u5c06\u4ee5\u6bcf\u4e2a\u57571024\u4e2a\u7ebf\u7a0b\u5b8c\u6210\u542f\u52a8\uff0c\u603b\u5171 <code>4 x 2 = 8</code> \u4e2a\u5757\u3002\u5982\u679c\u4f60\u4e4b\u524d\u4ece\u672a\u542c\u8bf4\u8fc7 CUDA \u201c\u5757\u201d\u6216\u201c\u7f51\u683c\u201d\uff0c\u90a3\u4e48\u5173\u4e8e CUDA \u7684\u4ecb\u7ecd\u6027\u9605\u8bfb\u53ef\u80fd\u4f1a\u6709\u6240\u5e2e\u52a9\u3002</p> <p>\u5b9e\u9645\u7684 CUDA \u5185\u6838\u975e\u5e38\u7b80\u5355(\u5982\u679c\u4f60\u4ee5\u524d\u8fdb\u884c\u8fc7 GPU \u7f16\u7a0b\uff09\uff1a</p> <pre><code>template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_forward_kernel(\n    const scalar_t* __restrict__ gates,\n    const scalar_t* __restrict__ old_cell,\n    scalar_t* __restrict__ new_h,\n    scalar_t* __restrict__ new_cell,\n    scalar_t* __restrict__ input_gate,\n    scalar_t* __restrict__ output_gate,\n    scalar_t* __restrict__ candidate_cell,\n    size_t state_size) {\n  const int column = blockIdx.x * blockDim.x + threadIdx.x;\n  const int index = blockIdx.y * state_size + column;\n  const int gates_row = blockIdx.y * (state_size * 3);\n  if (column &lt; state_size) {\n    input_gate[index] = sigmoid(gates[gates_row + column]);\n    output_gate[index] = sigmoid(gates[gates_row + state_size + column]);\n    candidate_cell[index] = elu(gates[gates_row + 2 * state_size + column]);\n    new_cell[index] =\n        old_cell[index] + candidate_cell[index] * input_gate[index];\n    new_h[index] = tanh(new_cell[index]) * output_gate[index];\n  }\n}\n\n</code></pre> <p>\u8fd9\u91cc\u6700\u611f\u5174\u8da3\u7684\u662f\uff0c\u6211\u4eec\u80fd\u591f\u5b8c\u5168\u5e76\u884c\u5730\u4e3a\u95e8\u77e9\u9635\u4e2d\u7684\u6bcf\u4e2a\u5355\u72ec\u7ec4\u4ef6\u8ba1\u7b97\u6240\u6709\u7684\u8fd9\u4e9b\u9010\u70b9\u8fd0\u7b97\u3002\u5982\u679c\u4f60\u80fd\u60f3\u8c61\u5fc5\u987b\u7528\u4e00\u4e2a\u5de8\u5927\u7684<code>for</code>\u5faa\u73af\u6765\u8fde\u7eed\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u5143\u7d20\u7684\u60c5\u51b5\uff0c\u4f60\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u4ec0\u4e48\u6539\u8fdb\u4e4b\u540e\u901f\u5ea6\u4f1a\u66f4\u5feb\u4e86\u3002</p> <p>\u53cd\u5411\u4f20\u64ad\u9075\u5faa\u76f8\u540c\u7684\u6a21\u5f0f\uff0c\u5728\u8fd9\u91cc\u5c06\u4e0d\u518d\u8be6\u7ec6\u8bf4\u660e\uff1a</p> <pre><code>template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_backward_kernel(\n    scalar_t* __restrict__ d_old_cell,\n    scalar_t* __restrict__ d_gates,\n    const scalar_t* __restrict__ grad_h,\n    const scalar_t* __restrict__ grad_cell,\n    const scalar_t* __restrict__ new_cell,\n    const scalar_t* __restrict__ input_gate,\n    const scalar_t* __restrict__ output_gate,\n    const scalar_t* __restrict__ candidate_cell,\n    const scalar_t* __restrict__ gate_weights,\n    size_t state_size) {\n  const int column = blockIdx.x * blockDim.x + threadIdx.x;\n  const int index = blockIdx.y * state_size + column;\n  const int gates_row = blockIdx.y * (state_size * 3);\n  if (column &lt; state_size) {\n    const auto d_output_gate = tanh(new_cell[index]) * grad_h[index];\n    const auto d_tanh_new_cell = output_gate[index] * grad_h[index];\n    const auto d_new_cell =\n        d_tanh(new_cell[index]) * d_tanh_new_cell + grad_cell[index];\n\n    d_old_cell[index] = d_new_cell;\n    const auto d_candidate_cell = input_gate[index] * d_new_cell;\n    const auto d_input_gate = candidate_cell[index] * d_new_cell;\n\n    const auto input_gate_index = gates_row + column;\n    const auto output_gate_index = gates_row + state_size + column;\n    const auto candidate_cell_index = gates_row + 2 * state_size + column;\n\n    d_gates[input_gate_index] =\n        d_input_gate * d_sigmoid(gate_weights[input_gate_index]);\n    d_gates[output_gate_index] =\n        d_output_gate * d_sigmoid(gate_weights[output_gate_index]);\n    d_gates[candidate_cell_index] =\n        d_candidate_cell * d_elu(gate_weights[candidate_cell_index]);\n  }\n}\n\nstd::vector&lt;at::Tensor&gt; lltm_cuda_backward(\n    at::Tensor grad_h,\n    at::Tensor grad_cell,\n    at::Tensor new_cell,\n    at::Tensor input_gate,\n    at::Tensor output_gate,\n    at::Tensor candidate_cell,\n    at::Tensor X,\n    at::Tensor gate_weights,\n    at::Tensor weights) {\n  auto d_old_cell = at::zeros_like(new_cell);\n  auto d_gates = at::zeros_like(gate_weights);\n\n  const auto batch_size = new_cell.size(0);\n  const auto state_size = new_cell.size(1);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_backward_cuda\", ([&amp;] {\n    lltm_cuda_backward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        d_old_cell.data&lt;scalar_t&gt;(),\n        d_gates.data&lt;scalar_t&gt;(),\n        grad_h.contiguous().data&lt;scalar_t&gt;(),\n        grad_cell.contiguous().data&lt;scalar_t&gt;(),\n        new_cell.contiguous().data&lt;scalar_t&gt;(),\n        input_gate.contiguous().data&lt;scalar_t&gt;(),\n        output_gate.contiguous().data&lt;scalar_t&gt;(),\n        candidate_cell.contiguous().data&lt;scalar_t&gt;(),\n        gate_weights.contiguous().data&lt;scalar_t&gt;(),\n        state_size);\n  }));\n\n  auto d_weights = d_gates.t().mm(X);\n  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gates.mm(weights);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};\n}\n\n</code></pre>"},{"location":"1.0/cpp_extension/#ccuda-pytorch","title":"\u5c06 C++/CUDA \u64cd\u4f5c\u4e0e PyTorch \u96c6\u6210","text":"<p>\u6211\u4eec\u652f\u6301 CUDA \u7684\u64cd\u4f5c\u4e0e PyTorch \u7684\u96c6\u6210\u540c\u6837\u5341\u5206\u7b80\u5355\u3002\u5982\u679c\u4f60\u60f3\u5199\u4e00\u4e2a<code>setup.py</code>\u811a\u672c\uff0c\u5b83\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u8fd9\u6837\uff1a</p> <pre><code>from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n\n</code></pre> <p>\u6211\u4eec\u73b0\u5728\u4f7f\u7528<code>CUDAExtension()</code>\u800c\u4e0d\u662f<code>CppExtension()</code>\u3002\u6211\u4eec\u53ef\u4ee5\u53ea\u6307\u5b9a<code>.cu</code>\u6587\u4ef6\u548c<code>.cpp</code>\u6587\u4ef6\u2014\u2014\u5e93\u53ef\u4ee5\u89e3\u51b3\u6240\u6709\u9ebb\u70e6\u3002JIT \u673a\u5236\u5219\u66f4\u7b80\u5355\uff1a</p> <pre><code>from torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu'])\n\n</code></pre>"},{"location":"1.0/cpp_extension/#_6","title":"\u6027\u80fd\u6bd4\u8f83","text":"<p>\u6211\u4eec\u5e0c\u671b\u5e76\u884c\u5316\u4e0e\u878d\u5408\u6211\u4eec\u4ee3\u7801\u4e0e CUDA \u7684\u9010\u70b9\u64cd\u4f5c\u5c06\u6539\u5584\u6211\u4eec\u7684 LLTM \u7684\u6027\u80fd\u3002\u8ba9\u6211\u4eec\u770b\u770b\u662f\u5426\u6210\u7acb\u3002\u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u5728\u524d\u9762\u5217\u51fa\u7684\u4ee3\u7801\u6765\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u6211\u4eec\u4e4b\u524d\u7684\u6700\u5feb\u7248\u672c\u662f\u57fa\u4e8e CUDA \u7684 C++ \u4ee3\u7801\uff1a</p> <pre><code>Forward: 149.802 us | Backward 393.458 us\n\n</code></pre> <p>\u73b0\u5728\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49 CUDA \u5185\u6838\uff1a</p> <pre><code>Forward: 129.431 us | Backward 304.641 us\n\n</code></pre> <p>\u6027\u80fd\u5f97\u5230\u4e86\u66f4\u591a\u7684\u63d0\u5347\uff01</p>"},{"location":"1.0/cpp_extension/#_7","title":"\u7ed3\u8bba","text":"<p>\u4f60\u73b0\u5728\u5e94\u8be5\u5bf9 PyTorch \u7684 C++ \u6269\u5c55\u673a\u5236\u4ee5\u53ca\u4f7f\u7528\u5b83\u4eec\u7684\u52a8\u673a\u6709\u4e00\u4e2a\u5f88\u597d\u7684\u5927\u81f4\u4e0a\u7684\u4e86\u89e3\u4e86\u3002\u4f60\u53ef\u4ee5\u5728\u6b64\u5904\u4e2d\u627e\u5230\u672c\u6587\u4e2d\u663e\u793a\u7684\u4ee3\u7801\u793a\u4f8b\u3002\u5982\u679c\u4f60\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u8bf7\u4f7f\u7528\u8bba\u575b\u3002\u5982\u679c\u4f60\u9047\u5230\u4efb\u4f55\u95ee\u9898\uff0c\u8bf7\u52a1\u5fc5\u67e5\u770b\u6211\u4eec\u7684FAQ\u3002</p>"},{"location":"1.0/cpp_frontend/","title":"\u4f7f\u7528 PyTorch C++ \u524d\u7aef","text":"<p>\u8bd1\u8005\uff1asolerji</p> <p>PyTorch C++ \u524d\u7aef \u662fPyTorch\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7684\u4e00\u4e2a\u7eafC++\u63a5\u53e3\u3002PyTorch\u7684\u4e3b\u63a5\u53e3\u662fPython\uff0cPython API\u4f4d\u4e8e\u4e00\u4e2a\u57fa\u7840\u7684C++\u4ee3\u7801\u5e93\u4e4b\u4e0a\uff0c\u63d0\u4f9b\u4e86\u57fa\u672c\u7684\u6570\u636e\u7ed3\u6784\u548c\u529f\u80fd\uff0c\u4f8b\u5982\u5f20\u91cf\u548c\u81ea\u52a8\u6c42\u5bfc\u3002C++\u524d\u7aef\u66b4\u9732\u4e86\u4e00\u4e2a\u7eaf\u7684C++11\u7684API\uff0c\u5728C++\u5e95\u5c42\u4ee3\u7801\u5e93\u4e4b\u4e0a\u6269\u5c55\u4e86\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u6240\u9700\u7684\u5de5\u5177\u6269\u5c55\u3002\u8fd9\u5305\u62ec\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u7684\u5185\u7f6e\u7ec4\u4ef6\u96c6\u5408\uff1b\u6269\u5c55\u6b64\u96c6\u5408\u7684\u81ea\u5b9a\u4e49\u6a21\u5757API\uff1b\u6d41\u884c\u7684\u4f18\u5316\u7b97\u6cd5\u5e93(\u5982\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff09\uff1b\u4f7f\u7528API\u5b9a\u4e49\u548c\u52a0\u8f7d\u6570\u636e\u96c6\u7684\u5e76\u884c\u6570\u636e\u52a0\u8f7d\u7a0b\u5e8f\uff1b\u5e8f\u5217\u5316\u4f8b\u884c\u7a0b\u5e8f\u7b49\u7b49\u3002</p> <p>\u672c\u6559\u7a0b\u5c06\u4e3a\u60a8\u4ecb\u7ecd\u4e00\u4e2a\u7528C++ \u524d\u7aef\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u793a\u4f8b\u3002\u5177\u4f53\u5730\u8bf4\uff0c\u6211\u4eec\u5c06\u8bad\u7ec3\u4e00\u4e2a DCGAN\u2014\u2014\u4e00\u79cd\u751f\u6210\u6a21\u578b\u2014\u2014\u6765\u751f\u6210 MNIST\u6570\u5b57\u7684\u56fe\u50cf\u3002\u867d\u7136\u770b\u8d77\u6765\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u4f46\u5b83\u8db3\u4ee5\u8ba9\u4f60\u5bf9 PyTorch C++ frontend\u6709\u4e00\u4e2a\u6df1\u523b\u7684\u8ba4\u8bc6\uff0c\u5e76\u52fe\u8d77\u4f60\u5bf9\u8bad\u7ec3\u66f4\u590d\u6742\u6a21\u578b\u7684\u5174\u8da3\u3002\u6211\u4eec\u5c06\u4ece\u8bbe\u8ba1\u5b83\u7684\u539f\u56e0\u5f00\u59cb\uff0c\u544a\u8bc9\u4f60\u4e3a\u4ec0\u4e48\u4f60\u5e94\u8be5\u4f7f\u7528C++\u524d\u7aef\uff0c\u7136\u540e\u76f4\u63a5\u6df1\u5165\u89e3\u91ca\u548c\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u53ef\u4ee5\u5728 this lightning talk from CppCon 2018 \u7f51\u7ad9\u89c2\u770b\u6709\u5173C++\u524d\u7aef\u7684\u5feb\u901f\u4ecb\u7ecd\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u8fd9\u4efd\u7b14\u8bb0\u63d0\u4f9b\u4e86C++\u524d\u7aef\u7ec4\u4ef6\u548c\u8bbe\u8ba1\u7406\u5ff5\u7684\u5168\u9762\u6982\u8ff0\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u5728 https://pytorch.org/cppdocs\u4f60\u53ef\u4ee5\u627e\u5230\u5de5\u4f5c\u4eba\u5458\u7684API\u8bf4\u660e\u6587\u6863\uff0c\u8fd9\u4e9bPyTorch C++ \u751f\u6001\u7cfb\u7edf\u7684\u6587\u6863\u662f\u5f88\u6709\u7528\u7684\u3002</p>"},{"location":"1.0/cpp_frontend/#_1","title":"\u52a8\u673a","text":"<p>\u5728\u6211\u4eec\u5f00\u59cb\u4ee4\u4eba\u5174\u594b\u7684GANs\u548cMNIST\u6570\u5b57\u7684\u65c5\u7a0b\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u5f80\u56de\u770b\uff0c\u8ba8\u8bba\u4e00\u4e0b\u4e3a\u4ec0\u4e48\u6211\u4eec\u4e00\u5f00\u59cb\u8981\u4f7f\u7528C++\u524d\u7aef\u800c\u4e0d\u662fPython\u3002\u6211\u4eec(the PyTorch team\uff09\u521b\u5efa\u4e86C++\u524d\u7aef\uff0c\u4ee5\u4fbf\u5728\u4e0d\u80fd\u4f7f\u7528Python\u7684\u73af\u5883\u4e2d\u6216\u8005\u662f\u6ca1\u6709\u9002\u5408\u8be5\u4f5c\u4e1a\u7684\u5de5\u5177\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u7814\u7a76\u3002\u6b64\u7c7b\u73af\u5883\u7684\u793a\u4f8b\u5305\u62ec\uff1a</p> <ul> <li>\u4f4e\u5ef6\u8fdf\u7cfb\u7edf\uff1a\u60a8\u53ef\u80fd\u5e0c\u671b\u5728\u5177\u6709\u9ad8\u5e27/\u79d2\u548c\u4f4e\u5ef6\u8fdf\u7684\u8981\u6c42\u7684\u7eafC++\u6e38\u620f\u5f15\u64ce\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u3002\u7531\u4e8ePython\u89e3\u91ca\u5668\u7684\u901f\u5ea6\u6162\uff0cPython\u53ef\u80fd\u6839\u672c\u65e0\u6cd5\u88ab\u8ddf\u8e2a\uff0c\u4f7f\u7528\u7eafC++\u5e93\u8fd9\u6837\u7684\u73af\u5883\u6bd4Python\u5e93\u66f4\u5408\u9002\u3002</li> <li>\u9ad8\u5ea6\u591a\u7ebf\u7a0b\u73af\u5883\uff1a\u7531\u4e8e\u5168\u5c40\u89e3\u91ca\u5668\u9501(GIL\uff09\uff0c\u4e00\u6b21\u4e0d\u80fd\u8fd0\u884c\u591a\u4e2a\u7cfb\u7edf\u7ebf\u7a0b\u3002\u591a\u9053\u5904\u7406\u662f\u53e6\u4e00\u79cd\u9009\u62e9\uff0c\u4f46\u5b83\u4e0d\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u6709\u663e\u8457\u7684\u7f3a\u70b9\u3002C++\u6ca1\u6709\u8fd9\u6837\u7684\u7ea6\u675f\uff0c\u7ebf\u7a0b\u6613\u4e8e\u4f7f\u7528\u548c\u521b\u5efa\u3002\u9700\u8981\u5927\u91cf\u5e76\u884c\u5316\u7684\u6a21\u578b\uff0c\u50cf\u90a3\u4e9b\u7528\u4e8e\u6df1\u5ea6\u795e\u7ecf\u8fdb\u5316 Deep Neuroevolution\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u4ece\u4e2d\u53d7\u76ca\u3002</li> <li>\u73b0\u6709\u7684C++\u4ee3\u7801\u5e93\uff1a\u60a8\u53ef\u80fd\u662f\u4e00\u4e2a\u73b0\u6709\u7684C++\u5e94\u7528\u7a0b\u5e8f\u7684\u6240\u6709\u8005\uff0c\u5728\u540e\u53f0\u670d\u52a1\u5668\u4e0a\u4e3aWeb\u9875\u9762\u63d0\u4f9b\u670d\u52a1\uff0c\u4ee5\u5728\u7167\u7247\u7f16\u8f91\u8f6f\u4ef6\u4e2d\u7ed8\u52363D\u56fe\u5f62\uff0c\u5e76\u5e0c\u671b\u5c06\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96c6\u6210\u5230\u60a8\u7684\u7cfb\u7edf\u4e2d\u3002C++\u524d\u7aef\u5141\u8bb8\u60a8\u4fdd\u7559\u5728C++\u4e2d\uff0c\u514d\u9664\u4e86\u5728Python\u548cC++\u4e4b\u95f4\u6765\u56de\u7ed1\u5b9a\u7684\u9ebb\u70e6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4f20\u7edf PyTorch(Python\uff09\u4f53\u9a8c\u7684\u5927\u90e8\u5206\u7075\u6d3b\u6027\u548c\u76f4\u89c2\u6027\u3002</li> </ul> <p>C++\u524d\u7aef\u4e0d\u6253\u7b97\u4e0ePython\u524d\u7aef\u7ade\u4e89\uff0c\u5b83\u662f\u4e3a\u4e86\u8865\u5145Python\u524d\u7aef\u3002\u6211\u4eec\u77e5\u9053\u7531\u4e8e\u5b83\u7b80\u5355\u3001\u7075\u6d3b\u548c\u76f4\u89c2\u7684API\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u90fd\u559c\u6b22PyTorch\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u786e\u4fdd\u60a8\u53ef\u4ee5\u5728\u6bcf\u4e2a\u53ef\u80fd\u7684\u73af\u5883\u4e2d\u5229\u7528\u8fd9\u4e9b\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u5305\u62ec\u4e0a\u9762\u63cf\u8ff0\u7684\u90a3\u4e9b\u3002\u5982\u679c\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u4e00\u4e2a\u63cf\u8ff0\u4e86\u4f60\u7684\u7528\u4f8b\uff0c\u6216\u8005\u5982\u679c\u4f60\u53ea\u662f\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u8ddf\u7740\u6211\u4eec\u5728\u4e0b\u9762\u7684\u6587\u7ae0\u4e2d\u8be6\u7ec6\u63a2\u7a76C++\u524d\u7aef\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>C++\u524d\u7aef\u8bd5\u56fe\u63d0\u4f9b\u5c3d\u53ef\u80fd\u63a5\u8fd1Python\u524d\u7aef\u7684API\u3002\u5982\u679c\u4f60\u5bf9Python\u524d\u7aef\u6709\u7ecf\u9a8c\uff0c\u5e76\u4e14\u60f3\u77e5\u9053\uff1a\u201c\u6211\u5982\u4f55\u7528C++\u524d\u7aef\u505a\u8fd9\u4e2a\u4e1c\u897f\uff1f\u201d\u4f60\u53ef\u4ee5\u4ee5Python\u7684\u65b9\u5f0f\u7f16\u5199\u4ee3\u7801\uff0c\u5728Python\u4e2d\uff0c\u901a\u5e38\u53ef\u4ee5\u4f7f\u7528\u4e0eC++\u76f8\u540c\u7684\u51fd\u6570\u548c\u65b9\u6cd5(\u53ea\u8981\u8bb0\u4f4f\u7528\u53cc\u5192\u53f7\u66ff\u6362\u70b9\uff09\u3002</p>"},{"location":"1.0/cpp_frontend/#_2","title":"\u7f16\u5199\u57fa\u672c\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u8ba9\u6211\u4eec\u5f00\u59cb\u7f16\u5199\u4e00\u4e2a\u5c0f\u7684C++\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u9a8c\u8bc1\u6211\u4eec\u5728\u5b89\u88c5\u548c\u6784\u5efa\u73af\u5883\u4e0a\u662f\u4e00\u81f4\u7684\u3002\u9996\u5148\uff0c\u60a8\u9700\u8981\u83b7\u53d6 LibTorch\u5206\u53d1\u7684\u526f\u672c\u2014\u2014\u6211\u4eec\u5df2\u7ecf\u51c6\u5907\u597d\u4e86ZIP\u5b58\u6863\uff0c\u5b83\u5c01\u88c5\u4e86\u4f7f\u7528C++\u524d\u7aef\u6240\u9700\u7684\u6240\u6709\u76f8\u5173\u7684\u5934\u6587\u4ef6\u3001\u5e93\u548c CMake \u6784\u5efa\u6587\u4ef6\u3002Libtorch\u53d1\u884c\u7248\u53ef\u5728Linux, MacOS \u548c Windows\u7684PyTorch website\u4e0a\u4e0b\u8f7d\u3002\u672c\u6559\u7a0b\u7684\u5176\u4f59\u90e8\u5206\u5c06\u5047\u8bbe\u4e00\u4e2a\u57fa\u672c\u7684Ubuntu Linux\u73af\u5883\uff0c\u60a8\u4e5f\u53ef\u4ee5\u5728MacOS\u6216Windows\u4e0a\u7ee7\u7eed\u81ea\u7531\u5730\u5b66\u4e60\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u5173\u4e8e\u5b89\u88c5PyTrac C++  \u5728 Installing C++ Distributions of PyTorch \u7684\u6587\u6863\u66f4\u8be6\u7ec6\u5730\u63cf\u8ff0\u4e86\u4ee5\u4e0b\u6b65\u9aa4\u3002</p> <p>\u7b2c\u4e00\u6b65\u662f\u901a\u8fc7\u4ecePyTorch\u7f51\u7ad9\u68c0\u7d22\u5230\u7684\u94fe\u63a5\u5728\u672c\u5730\u4e0b\u8f7d LibTorch\u53d1\u884c\u7248\u3002\u5bf9\u4e8e\u666e\u901a\u7684Ubuntu Linux\u73af\u5883\uff0c\u8fd9\u610f\u5473\u7740\u8fd0\u884c\uff1a</p> <pre><code>wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip\n\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u540d\u4e3a <code>dcgan.cpp</code> \u7684\u5c0f\u578bC++\u6587\u4ef6\uff0c\u5b83\u5305\u62ec <code>torch/torch.h</code> \uff0c\u73b0\u5728\u53ea\u9700\u6253\u5370\u51fa\u4e09*\u4e09\u7684\u8eab\u4efd\u77e9\u9635\uff1a</p> <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n  torch::Tensor tensor = torch::eye(3);\n  std::cout &lt;&lt; tensor &lt;&lt; std::endl;\n}\n\n</code></pre> <p>\u6211\u4eec\u5c06\u4f7f\u7528<code>CMakeLists.txt</code>\u6587\u4ef6\u6784\u5efa\u8fd9\u4e2a\u5c0f\u5e94\u7528\u7a0b\u5e8f\u4ee5\u53ca\u6211\u4eec\u7a0d\u540e\u7684\u5b8c\u6574\u8bad\u7ec3\u811a\u672c\uff1a</p> <pre><code>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(dcgan)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(dcgan dcgan.cpp)\ntarget_link_libraries(dcgan \"${TORCH_LIBRARIES}\")\nset_property(TARGET dcgan PROPERTY CXX_STANDARD 11)\n\n</code></pre> <p>\u7b14\u8bb0</p> <p>\u867d\u7136CMake\u662fLibTorch\u63a8\u8350\u7684\u6784\u5efa\u7cfb\u7edf\uff0c\u4f46\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u786c\u6027\u8981\u6c42\u3002\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528Visual Studio\u9879\u76ee\u6587\u4ef6\u3001Qmake\u3001plain Makefiles\u6216\u4efb\u4f55\u5176\u4ed6\u60a8\u89c9\u5f97\u5408\u9002\u7684\u6784\u5efa\u73af\u5883\u3002\u4f46\u662f\uff0c\u6211\u4eec\u4e0d\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u7684\u652f\u6301\u3002</p> <p>\u8bb0\u4e0b\u4e0a\u8ff0CMake\u6587\u4ef6\u4e2d\u7684\u7b2c4\u884c\uff1a <code>find_package(Torch REQUIRED)</code>.\u3002\u8fd9\u5c06\u6307\u793aCMake\u67e5\u627eLibTorch\u5e93\u7684\u6784\u5efa\u914d\u7f6e\u3002\u4e3a\u4e86\u8ba9CMake\u77e5\u9053\u5728\u54ea\u91cc\u627e\u5230\u8fd9\u4e9b\u6587\u4ef6\uff0c\u6211\u4eec\u5fc5\u987b\u5728\u8c03\u7528 <code>cmake</code>\u65f6\u8bbe\u7f6e   <code>CMAKE_PREFIX_PATH</code>  \u3002\u5728\u8fdb\u884c\u6b64\u64cd\u4f5c\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u5c31 <code>dcgan</code>\u5e94\u7528\u7a0b\u5e8f\u7684\u4ee5\u4e0b\u76ee\u5f55\u7ed3\u6784\u8fbe\u6210\u4e00\u81f4\uff1a</p> <pre><code>dcgan/\n  CMakeLists.txt\n  dcgan.cpp\n</code></pre> <p>\u6b64\u5916\uff0c\u6211\u5c06\u7279\u522b\u6307\u51fa\u89e3\u538bLibTorch\u5206\u53d1\u7684\u8def\u5f84 <code>/path/to/libtorch</code>\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u5fc5\u987b\u662f\u7edd\u5bf9\u8def\u5f84\u3002\u6211\u4eec\u7528\u7f16\u5199 <code>$PWD/../../libtorch</code> \u7684\u505a\u6cd5\u83b7\u53d6\u76f8\u5e94\u7684\u7edd\u5bf9\u8def\u5f84\uff1b\u5982\u679c\u5c06 <code>CMAKE_PREFIX_PATH</code> \u8bbe\u7f6e\u4e3a<code>../../libtorch</code>\u5b83\u5c06\u4ee5\u610f\u60f3\u4e0d\u5230\u7684\u65b9\u5f0f\u4e2d\u65ad\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5df2\u7ecf\u51c6\u5907\u597d\u6784\u5efa\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\uff1a</p> <pre><code>root@fa350df05ecf:/home# mkdir build\nroot@fa350df05ecf:/home# cd build\nroot@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /path/to/libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/build\nroot@fa350df05ecf:/home/build# make -j\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\n</code></pre> <p>\u5728\u4e0a\u6587\uff0c\u6211\u4eec\u9996\u5148\u5728 <code>dcgan</code> \u76ee\u5f55\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a <code>build</code> \u6587\u4ef6\u5939\uff0c\u7136\u540e\u8fdb\u5165\u8fd9\u4e2a\u6587\u4ef6\u5939\uff0c\u8fd0\u884c <code>cmake</code> \u547d\u4ee4\u751f\u6210\u5fc5\u8981\u7684build(Make\uff09\u6587\u4ef6\uff0c\u6700\u540e\u901a\u8fc7\u8fd0\u884c <code>make -j</code>.\u6210\u529f\u7f16\u8bd1\u4e86\u9879\u76ee\u3002\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u9879\u76ee\u8bbe\u7f6e\u4e3a\u6267\u884c\u6700\u5c0f\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u57fa\u672c\u9879\u76ee\u914d\u7f6e\u8fd9\u4e00\u90e8\u5206\u5c31\u5b8c\u6210\u4e86\uff1a</p> <pre><code>root@fa350df05ecf:/home/build# ./dcgan\n1  0  0\n0  1  0\n0  0  1\n[ Variable[CPUFloatType]{3,3} ]\n\n</code></pre> <p>\u5728\u6211\u770b\u6765\u5b83\u5c31\u50cf\u4e00\u4e2a\u8eab\u4efd\u77e9\u9635\uff01</p>"},{"location":"1.0/cpp_frontend/#_3","title":"\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u6a21\u578b","text":"<p>\u65e2\u7136\u6211\u4eec\u5df2\u7ecf\u914d\u7f6e\u4e86\u57fa\u672c\u73af\u5883\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u672c\u6559\u7a0b\u4e2d\u66f4\u6709\u8da3\u7684\u90e8\u5206\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u5982\u4f55\u5728C++\u524d\u7aef\u4e2d\u5b9a\u4e49\u548c\u4ea4\u4e92\u6a21\u5757\u3002\u6211\u4eec\u5c06\u4ece\u57fa\u672c\u7684\u3001\u5c0f\u89c4\u6a21\u7684\u793a\u4f8b\u6a21\u5757\u5f00\u59cb\uff0c\u7136\u540e\u4f7f\u7528C++\u524d\u7aef\u63d0\u4f9b\u7684\u5185\u7f6e\u6a21\u5757\u7684\u5e7f\u6cdb\u5e93\u6765\u5b9e\u73b0\u4e00\u4e2a\u6210\u719f\u7684GAN\u3002</p>"},{"location":"1.0/cpp_frontend/#api","title":"\u6a21\u5757API\u57fa\u7840\u77e5\u8bc6","text":"<p>\u4f9d\u636ePython\u63a5\u53e3\uff0c\u57fa\u4e8eC++\u524d\u7aef\u7684\u795e\u7ecf\u7f51\u7edc\u7531\u53ef\u91cd\u7528\u7684\u6a21\u5757\u7ec4\u6210\uff0c\u79f0\u4e3a\u6a21\u5757\u3002\u5b83\u6709\u4e00\u4e2a\u57fa\u672c\u6a21\u5757\u7c7b\uff0c\u4ece\u4e2d\u6d3e\u751f\u6240\u6709\u5176\u4ed6\u6a21\u5757\u3002\u5728Python\u4e2d\uff0c\u8fd9\u4e2a\u7c7b\u662f <code>torch.nn.Module</code> \uff0c\u5728C++\u4e2d\u662f <code>torch::nn::Module</code>\u6a21\u5757\u3002\u9664\u4e86\u5b9e\u73b0\u6a21\u5757\u5c01\u88c5\u7684\u7b97\u6cd5\u7684 <code>forward()</code> \u65b9\u6cd5\u5916\uff0c\u6a21\u5757\u901a\u5e38\u8fd8\u5305\u542b\u4e09\u79cd\u5b50\u5bf9\u8c61\uff1a\u53c2\u6570\u3001\u7f13\u51b2\u533a\u548c\u5b50\u6a21\u5757\u3002</p> <p>\u53c2\u6570\u548c\u7f13\u51b2\u533a\u4ee5\u5f20\u91cf\u7684\u5f62\u5f0f\u5b58\u50a8\u72b6\u6001\u3002\u53c2\u6570\u8bb0\u5f55\uff0c\u800c\u7f13\u51b2\u533a\u4e0d\u8bb0\u5f55\u3002\u53c2\u6570\u901a\u5e38\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6743\u91cd\u3002\u7f13\u51b2\u533a\u7684\u793a\u4f8b\u5305\u62ec\u7528\u4e8e\u6279\u5904\u7406\u89c4\u8303\u5316\u7684\u5e73\u5747\u503c\u548c\u65b9\u5dee\u3002\u4e3a\u4e86\u91cd\u7528\u7279\u5b9a\u7684\u903b\u8f91\u5757\u548c\u72b6\u6001\u5757\uff0cPyTorch API\u5141\u8bb8\u5d4c\u5957\u6a21\u5757\u3002\u5d4c\u5957\u6a21\u5757\u79f0\u4e3a_\u5b50\u6a21\u5757_\u3002</p> <p>\u5fc5\u987b\u663e\u5f0f\u6ce8\u518c\u53c2\u6570\u3001\u7f13\u51b2\u533a\u548c\u5b50\u6a21\u5757\u3002\u6ce8\u518c\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528<code>parameters()</code> or <code>buffers()</code>\u7b49\u65b9\u6cd5\u6765\u68c0\u7d22\u6574\u4e2a(\u5d4c\u5957\uff09\u6a21\u5757\u5c42\u6b21\u7ed3\u6784\u4e2d\u6240\u6709\u53c2\u6570\u7684\u5bb9\u5668\u3002\u7c7b\u4f3c\u5730\uff0c\u7c7b\u4f3c\u4e8e <code>to(...)</code>\u7684\u65b9\u6cd5(\u4f8b\u5982 <code>to(torch::kCUDA)</code> \u5c06\u6240\u6709\u53c2\u6570\u548c\u7f13\u51b2\u533a\u4eceCPU\u79fb\u52a8\u5230CUDA\u5185\u5b58\uff09\u5728\u6574\u4e2a\u6a21\u5757\u5c42\u6b21\u7ed3\u6784\u4e0a\u5de5\u4f5c\u3002</p>"},{"location":"1.0/cpp_frontend/#_4","title":"\u5b9a\u4e49\u6a21\u5757\u5e76\u6ce8\u518c\u53c2\u6570","text":"<p>\u4e3a\u4e86\u5c06\u8fd9\u4e9b\u968f\u673a\u6570\u653e\u5165\u4ee3\u7801\u4e2d\uff0c\u8ba9\u6211\u4eec\u8003\u8651\u4e00\u4e0b\u5728Python\u63a5\u53e3\u4e2d\u7f16\u5199\u8fd9\u4e2a\u7b80\u5355\u6a21\u5757\uff1a</p> <pre><code>import torch\n\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n    super(Net, self).__init__()\n    self.W = torch.nn.Parameter(torch.randn(N, M))\n    self.b = torch.nn.Parameter(torch.randn(M))\n\n  def forward(self, input):\n    return torch.addmm(self.b, input, self.W)\n\n</code></pre> <p>\u5728C++\u4e2d\u5b83\u957f\u8fd9\u6837\uff1a</p> <pre><code>#include &lt;torch/torch.h&gt;\n\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    W = register_parameter(\"W\", torch::randn({N, M}));\n    b = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return torch::admm(b, input, W);\n  }\n  torch::Tensor W, b;\n};\n\n</code></pre> <p>\u5c31\u50cf\u5728Python\u4e2d\u4e00\u6837\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7c7b <code>Net</code>  (\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u8fd9\u91cc\u662f <code>struct</code> \u800c\u4e0d\u662f\u4e00\u4e2a <code>class</code>\uff09\u5e76\u4ece\u6a21\u5757\u57fa\u7c7b\u6d3e\u751f\u5b83\u3002\u5728\u6784\u9020\u51fd\u6570\u5185\u90e8\uff0c\u6211\u4eec\u4f7f\u7528 <code>torch::randn</code> \u521b\u5efa\u5f20\u91cf\uff0c\u5c31\u50cf\u5728Python\u4e2d\u4f7f\u7528<code>torch.randn</code>\u4e00\u6837\u3002\u4e00\u4e2a\u6709\u8da3\u7684\u533a\u522b\u662f\u6211\u4eec\u5982\u4f55\u6ce8\u518c\u53c2\u6570\u3002\u5728Python\u4e2d\uff0c\u6211\u4eec\u7528<code>torch.nn.Parameter</code>\u7c7b\u6765\u5305\u88c5\u5f20\u91cf\uff0c\u800c\u5728C++\u4e2d\uff0c\u6211\u4eec\u5fc5\u987b\u901a\u8fc7 <code>register_parameter</code> \u53c2\u6570\u65b9\u6cd5\u6765\u4f20\u9012\u5f20\u91cf\u3002\u539f\u56e0\u662fPython API\u53ef\u4ee5\u68c0\u6d4b\u5230\u5c5e\u6027\u7684\u7c7b\u578b\u4e3a <code>torch.nn.Parameter</code> \uff0c\u5e76\u81ea\u52a8\u6ce8\u518c\u8fd9\u4e9b\u5f20\u91cf\u3002\u5728C++\u4e2d\uff0c\u53cd\u5c04\u662f\u975e\u5e38\u6709\u9650\u7684\uff0c\u56e0\u6b64\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4f20\u7edf\u7684(\u548c\u4e0d\u592a\u795e\u5947\u7684\uff09\u65b9\u6cd5\u3002</p>"},{"location":"1.0/cpp_frontend/#_5","title":"\u6ce8\u518c\u5b50\u6a21\u5757\u5e76\u904d\u5386\u6a21\u5757\u5c42\u6b21\u7ed3\u6784","text":"<p>\u540c\u6837\uff0c\u6211\u4eec\u53ef\u4ee5\u6ce8\u518c\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u6ce8\u518c\u5b50\u6a21\u5757\u3002\u5728Python\u4e2d\uff0c\u5f53\u5b50\u6a21\u5757\u88ab\u6307\u5b9a\u4e3a\u6a21\u5757\u7684\u5c5e\u6027\u65f6\uff0c\u5c06\u81ea\u52a8\u68c0\u6d4b\u548c\u6ce8\u518c\u5b50\u6a21\u5757\uff1a</p> <pre><code>class Net(torch.nn.Module):\n  def __init__(self, N, M):\n      super(Net, self).__init__()\n      # Registered as a submodule behind the scenes\n      self.linear = torch.nn.Linear(N, M)\n      self.another_bias = torch.nn.Parameter(torch.rand(M))\n\n  def forward(self, input):\n    return self.linear(input) + self.another_bias\n\n</code></pre> <p>\u4f8b\u5982\uff0c\u8fd9\u5141\u8bb8\u4f7f\u7528 <code>parameters()</code> \u65b9\u6cd5\u9012\u5f52\u8bbf\u95ee\u6a21\u5757\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u6240\u6709\u53c2\u6570\uff1a</p> <pre><code>&gt;&gt;&gt; net = Net(4, 5)\n&gt;&gt;&gt; print(list(net.parameters()))\n[Parameter containing:\ntensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing:\ntensor([[-0.3740, -0.0976, -0.4786, -0.4928],\n [-0.1434,  0.4713,  0.1735, -0.3293],\n [-0.3467, -0.3858,  0.1980,  0.1986],\n [-0.1975,  0.4278, -0.1831, -0.2709],\n [ 0.3730,  0.4307,  0.3236, -0.0629]], requires_grad=True), Parameter containing:\ntensor([ 0.2038,  0.4638, -0.2023,  0.1230, -0.0516], requires_grad=True)]\n\n</code></pre> <p>\u4e3a\u4e86\u5728C++\u4e2d\u6ce8\u518c\u5b50\u6a21\u5757\uff0c\u4f7f\u7528\u6070\u5f53\u547d\u540d\u7684 <code>register_module()</code> \u65b9\u6cd5\u6ce8\u518c\u4e00\u4e2a\u5c31\u50cf <code>torch::nn::Linear</code>:\u7684\u6a21\u5757\u3002</p> <pre><code>struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M))) {\n    another_bias = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return linear(input) + another_bias;\n  }\n  torch::nn::Linear linear;\n  torch::Tensor another_bias;\n};\n\n</code></pre> <p>\u5c0f\u8d34\u58eb</p> <p>\u60a8\u53ef\u4ee5\u5728\u8fd9\u91cc\u7684 <code>torch::nn</code> \u547d\u540d\u7a7a\u95f4\u6587\u6863\u4e2d\u627e\u5230\u53ef\u7528\u5185\u7f6e\u6a21\u5757\u7684\u5b8c\u6574\u5217\u8868\uff0c\u5982 <code>torch::nn::Linear</code>, <code>torch::nn::Dropout</code> \u548c <code>torch::nn::Conv2d</code> \u3002</p> <p>\u4e0a\u9762\u4ee3\u7801\u7684\u4e00\u4e2a\u5fae\u5999\u4e4b\u5904\u5c31\u662f\uff0c\u4e3a\u4ec0\u4e48\u6211\u4eec\u8981\u5728\u6784\u9020\u51fd\u6570\u7684\u521d\u59cb\u503c\u8bbe\u5b9a\u9879\u5217\u8868\u4e2d\u521b\u5efa\u5b50\u6a21\u5757\uff0c\u800c\u5728\u6784\u9020\u51fd\u6570\u4e3b\u4f53\u4e2d\u521b\u5efa\u53c2\u6570\u3002\u8fd9\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u7406\u7531\uff0c\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u8fdb\u4e00\u6b65\u8ba8\u8bbaC++\u524d\u7aef\u7684 ownership model \u3002\u6700\u7ec8\uff0c\u6211\u4eec\u53ef\u4ee5\u50cf\u5728Python\u4e2d\u90a3\u6837\u9012\u5f52\u5730\u8bbf\u95ee\u6811\u7684\u6a21\u5757\u7684\u53c2\u6570\u3002\u8c03\u7528\u53c2\u6570 <code>parameters()</code> \u8fd4\u56de\u4e00\u4e2a\u6211\u4eec\u53ef\u4ee5\u8fed\u4ee3\u7684 <code>std::vector&amp;lt;torch::Tensor&amp;gt;</code>\uff1a</p> <pre><code>int main() {\n  Net net(4, 5);\n  for (const auto&amp; p : net.parameters()) {\n    std::cout &lt;&lt; p &lt;&lt; std::endl;\n  }\n}\n\n</code></pre> <p>\u8f93\u51fa\u7684\u7ed3\u679c\u662f\uff1a</p> <pre><code>root@fa350df05ecf:/home/build# ./dcgan\n0.0345\n1.4456\n-0.6313\n-0.3585\n-0.4008\n[ Variable[CPUFloatType]{5} ]\n-0.1647  0.2891  0.0527 -0.0354\n0.3084  0.2025  0.0343  0.1824\n-0.4630 -0.2862  0.2500 -0.0420\n0.3679 -0.1482 -0.0460  0.1967\n0.2132 -0.1992  0.4257  0.0739\n[ Variable[CPUFloatType]{5,4} ]\n0.01 *\n3.6861\n-10.1166\n-45.0333\n7.9983\n-20.0705\n[ Variable[CPUFloatType]{5} ]\n\n</code></pre> <p>\u5c31\u50cf\u5728Python\u4e2d\u4e00\u6837\u8fd9\u91cc\u6709\u4e09\u4e2a\u53c2\u6570\u3002\u4e3a\u4e86\u770b\u5230\u8fd9\u4e9b\u53c2\u6570\u7684\u540d\u79f0\uff0cC++ API\u63d0\u4f9b\u4e86\u4e00\u4e2a <code>named_parameters()</code>\u53c2\u6570\u65b9\u6cd5\uff0c\u5b83\u50cfPython\u4e00\u6837\u8fd4\u56de <code>named_parameters()</code>\uff1a</p> <pre><code>Net net(4, 5);\nfor (const auto&amp; pair : net.named_parameters()) {\n  std::cout &lt;&lt; pair.key() &lt;&lt; \": \" &lt;&lt; pair.value() &lt;&lt; std::endl;\n}\n\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u518d\u6b21\u6267\u884c\u6765\u67e5\u770b\u8f93\u51fa\uff1a</p> <pre><code>root@fa350df05ecf:/home/build# make &amp;&amp; ./dcgan                                                                                                                                            11:13:48\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nb: -0.1863\n-0.8611\n-0.1228\n1.3269\n0.9858\n[ Variable[CPUFloatType]{5} ]\nlinear.weight:  0.0339  0.2484  0.2035 -0.2103\n-0.0715 -0.2975 -0.4350 -0.1878\n-0.3616  0.1050 -0.4982  0.0335\n-0.1605  0.4963  0.4099 -0.2883\n0.1818 -0.3447 -0.1501 -0.0215\n[ Variable[CPUFloatType]{5,4} ]\nlinear.bias: -0.0250\n0.0408\n0.3756\n-0.2149\n-0.3636\n[ Variable[CPUFloatType]{5} ]\n\n</code></pre> <p>\u7b14\u8bb0</p> <p><code>torch::nn::Module</code> \u7684\u6587\u6863 \u5305\u542b\u5728\u6a21\u5757\u5c42\u6b21\u7ed3\u6784\u4e0a\u64cd\u4f5c\u7684\u65b9\u6cd5\u7684\u5b8c\u6574\u6e05\u5355\u3002</p>"},{"location":"1.0/cpp_frontend/#_6","title":"\u5728\u6b63\u5411\u6a21\u5f0f\u4e2d\u8fd0\u884c\u7f51\u7edc","text":"<p>\u4e3a\u4e86\u5728C++\u4e2d\u8fd0\u884c\u7f51\u7edc\uff0c\u6211\u4eec\u53ea\u9700\u8c03\u7528\u6211\u4eec\u5b9a\u4e49\u7684 <code>forward()</code> \u65b9\u6cd5\uff1a</p> <pre><code>int main() {\n  Net net(4, 5);\n  std::cout &lt;&lt; net.forward(torch::ones({2, 4})) &lt;&lt; std::endl;\n}\n\n</code></pre> <p>\u8f93\u51fa\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>root@fa350df05ecf:/home/build# ./dcgan\n0.8559  1.1572  2.1069 -0.1247  0.8060\n0.8559  1.1572  2.1069 -0.1247  0.8060\n[ Variable[CPUFloatType]{2,5} ]\n\n</code></pre>"},{"location":"1.0/cpp_frontend/#_7","title":"\u6a21\u5757\u6240\u6709\u6743","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u77e5\u9053\u5982\u4f55\u5b9a\u4e49C++\u4e2d\u7684\u6a21\u5757\u3001\u5bc4\u5b58\u5668\u53c2\u6570\u3001\u5bc4\u5b58\u5668\u5b50\u6a21\u5757\u3001\u901a\u8fc7\u53c2\u6570 <code>parameters()</code> \u7b49\u65b9\u6cd5\u904d\u5386\u6a21\u5757\u5c42\u6b21\u7ed3\u6784\uff0c\u548c\u6700\u540e\u8fd0\u884c\u6a21\u5757\u7684 <code>forward()</code> \u65b9\u6cd5\u3002\u5728C++ API\u4e2d\u6709\u66f4\u591a\u7684\u65b9\u6cd5\u3001\u7c7b\u548c\u4e3b\u9898\u8981\u6211\u4eec\u601d\u8003\uff0c\u4f46\u63a5\u4e0b\u6765\u6211\u4f1a\u5411\u4f60\u4ecb\u7ecd\u5b8c\u6574\u6e05\u5355 \u6587\u6863 \u3002\u6211\u4eec\u5728\u4e00\u79d2\u949f\u5185\u5b9e\u73b0 DCGAN\u6a21\u578b\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u7ba1\u9053\u7684\u540c\u65f6\uff0c\u8fd8\u5c06\u6d89\u53ca\u66f4\u591a\u7684\u6982\u5ff5\u3002\u5728\u6211\u4eec\u8fd9\u6837\u505a\u4e4b\u524d\uff0c\u8ba9\u6211\u7b80\u5355\u5730\u4ecb\u7ecd\u4e00\u4e0bC++\u524d\u7aef\u7684\u6240\u6709\u6743\u6a21\u578b\uff0c\u5b83\u63d0\u4f9b\u4e86 <code>torch::nn::Module</code>.\u6a21\u5757\u7684\u5b50\u7c7b\u3002</p> <p>\u5bf9\u4e8e\u8fd9\u4e2a\u8bba\u8ff0\uff0c\u6240\u6709\u6743\u6a21\u578b\u6307\u7684\u662f\u6a21\u5757\u7684\u5b58\u50a8\u548c\u4f20\u9012\u65b9\u5f0f\uff0c\u5b83\u51b3\u5b9a\u4e86\u8c01\u6216\u4ec0\u4e48\u62e5\u6709\u4e00\u4e2a\u7279\u5b9a\u7684\u6a21\u5757\u5b9e\u4f8b\u3002\u5728Python\u4e2d\uff0c\u5bf9\u8c61\u603b\u662f\u52a8\u6001\u5206\u914d(\u5728\u5806\u4e0a\uff09\u5e76\u5177\u6709\u5f15\u7528\u8bed\u4e49\u3002\u8fd9\u5f88\u5bb9\u6613\u64cd\u4f5c\uff0c\u4e5f\u5f88\u5bb9\u6613\u7406\u89e3\u3002\u4e8b\u5b9e\u4e0a\uff0c\u5728Python\u4e2d\uff0c\u60a8\u5927\u53ef\u4ee5\u5fd8\u8bb0\u5bf9\u8c61\u7684\u4f4d\u7f6e\u4ee5\u53ca\u5b83\u4eec\u662f\u5982\u4f55\u88ab\u5f15\u7528\u7684\uff0c\u800c\u66f4\u4e13\u6ce8\u4e8e\u5b8c\u6210\u5de5\u4f5c\u3002</p> <p>C++\u662f\u4e00\u79cd\u8fd9\u4e2a\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u591a\u7684\u9009\u62e9\u7684\u4f4e\u7ea7\u8bed\u8a00\u3002\u5b83\u66f4\u52a0\u4e86\u590d\u6742\uff0c\u5e76\u4e25\u91cd\u5f71\u54cd\u4e86C++\u524d\u7aef\u7684\u8bbe\u8ba1\u548c\u4eba\u673a\u5de5\u7a0b\u5b66\u3002\u7279\u522b\u5730\uff0c\u5bf9\u4e8eC++\u524d\u7aef\u4e2d\u7684\u6a21\u5757\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u503c\u8bed\u4e49\u6216\u5f15\u7528\u8bed\u4e49\u3002\u7b2c\u4e00\u79cd\u60c5\u51b5\u662f\u6700\u7b80\u5355\u7684\uff0c\u5e76\u5728\u8fc4\u4eca\u4e3a\u6b62\u7684\u793a\u4f8b\u4e2d\u663e\u793a\uff1a\u5f53\u4f20\u9012\u7ed9\u51fd\u6570\u65f6\uff0c\u5728\u5806\u6808\u4e0a\u5206\u914d\u7684\u6a21\u5757\u5bf9\u8c61\uff0c\u53ef\u4ee5\u590d\u5236\u3001\u79fb\u52a8(\u4f7f\u7528 <code>std::move</code>)\uff09\u6216\u901a\u8fc7\u5f15\u7528\u548c\u6307\u9488\u83b7\u53d6\uff1a</p> <pre><code>struct Net : torch::nn::Module { };\n\nvoid a(Net net) { }\nvoid b(Net&amp; net) { }\nvoid c(Net* net) { }\n\nint main() {\n  Net net;\n  a(net);\n  a(std::move(net));\n  b(net);\n  c(&amp;net);\n}\n\n</code></pre> <p>\u5bf9\u4e8e\u7b2c\u4e8c\u79cd\u60c5\u51b5\u2014\u2014\u5f15\u7528\u8bed\u4e49\u2014\u2014\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 <code>std::shared_ptr</code>.\u3002\u5f15\u7528\u8bed\u4e49\u7684\u4f18\u70b9\u5728\u4e8e\uff0c\u4e0ePython\u4e00\u6837\uff0c\u5b83\u51cf\u5c11\u4e86\u8ba4\u77e5\u6a21\u5757\u5982\u4f55\u4f20\u9012\u7ed9\u51fd\u6570\u4ee5\u53ca\u5982\u4f55\u58f0\u660e\u53c2\u6570(\u5047\u8bbe\u5728\u4efb\u4f55\u5730\u65b9\u90fd\u4f7f\u7528<code>shared_ptr</code> )\u3002</p> <pre><code>struct Net : torch::nn::Module {};\n\nvoid a(std::shared_ptr&lt;Net&gt; net) { }\n\nint main() {\n  auto net = std::make_shared&lt;Net&gt;();\n  a(net);\n}\n\n</code></pre> <p>\u636e\u4ee5\u5f80\u7ecf\u9a8c\uff0c\u6765\u81ea\u52a8\u6001\u8bed\u8a00\u7684\u7814\u7a76\u4eba\u5458\u66f4\u503e\u5411\u4e8e\u5f15\u7528\u8bed\u4e49\u800c\u4e0d\u662f\u503c\u8bed\u4e49\uff0c\u5373\u4f7f\u540e\u8005\u5bf9\u4e8e\u800c\u8a00C++\u66f4\u4e3a\u201c\u672c\u571f\u201d\u3002\u8fd8\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e3a\u4e86\u63a5\u8fd1PythonAPI\u7684\u4eba\u673a\u5de5\u7a0b\u5b66\uff0c<code>torch::nn::Module</code>\u7684\u8bbe\u8ba1\u4f9d\u8d56\u4e8e\u6240\u6709\u6743\u7684\u5171\u4eab\u3002\u4f8b\u5982\uff0c\u4ee5\u6211\u4eec\u4e4b\u524d(\u6b64\u5904\u7b80\u79f0\uff09\u5bf9<code>Net</code>\u7684\u5b9a\u4e49\u4e3a\u4f8b\uff1a</p> <pre><code>struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};\n\n</code></pre> <p>\u4e3a\u4e86\u4f7f\u7528 <code>linear</code> \u5b50\u6a21\u5757\uff0c\u6211\u4eec\u5e0c\u671b\u5c06\u5176\u76f4\u63a5\u5b58\u50a8\u5728\u6211\u4eec\u7684\u7c7b\u4e2d\u3002\u4f46\u662f\uff0c\u6211\u4eec\u4e5f\u5e0c\u671b\u6a21\u5757\u57fa\u7c7b\u4e86\u89e3\u5e76\u80fd\u591f\u8bbf\u95ee\u8fd9\u4e2a\u5b50\u6a21\u5757\u3002\u4e3a\u6b64\uff0c\u5b83\u5fc5\u987b\u5b58\u50a8\u5bf9\u6b64\u5b50\u6a21\u5757\u7684\u5f15\u7528\u3002\u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u5df2\u7ecf\u8fbe\u5230\u4e86\u6240\u6709\u6743\u5171\u4eab\u7684\u9700\u6c42\u3002 <code>torch::nn::Module</code> \u7c7b\u548c \u5177\u4f53\u7c7b <code>Net</code> \u90fd\u9700\u8981\u5f15\u7528\u5b50\u6a21\u5757\u3002\u56e0\u6b64\uff0c\u57fa\u7c7b\u5c06\u6a21\u5757\u5b58\u50a8\u4e3a<code>shared_ptr</code>\uff0c\u5177\u4f53\u7684\u7c7b\u4e5f\u5fc5\u987b\u5b58\u50a8\u3002</p> <p>\u7b49\u7b49\uff01\u5728\u4e0a\u9762\u7684\u4ee3\u7801\u4e2d\u6211\u6ca1\u6709\u770b\u5230\u5b83\u63d0\u53ca\u5171\u4eab\u8d44\u6e90\uff01\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\uff1f\u56e0\u4e3a<code>std::shared_ptr&amp;lt;MyModule&amp;gt;</code>\u662f\u4e00\u4e2a\u5f88\u96be\u8f93\u5165\u7684\u7c7b\u578b\u3002\u4e3a\u4e86\u4fdd\u6301\u7814\u7a76\u4eba\u5458\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65b9\u6848\u6765\u9690\u85cf\u5e94\u8be5\u63d0\u53ca\u7684\u5171\u4eab\u8d44\u6e90\u2014\u2014\u8fd9\u662f\u4fdd\u7559\u503c\u8bed\u4e49\u7684\u597d\u5904\uff0c\u5b83\u540c\u65f6\u4fdd\u7559\u4e86\u5f15\u7528\u8bed\u4e49\u3002\u8981\u4e86\u89e3\u8fd9\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u6838\u5fc3\u5e93\u4e2d<code>torch::nn::Linear</code>\u6a21\u5757\u7684\u7b80\u5316\u5b9a\u4e49(\u5b8c\u6574\u5b9a\u4e49\u5982\u4e0b\uff09\uff1a</p> <pre><code>struct LinearImpl : torch::nn::Module {\n  LinearImpl(int64_t in, int64_t out);\n\n  Tensor forward(const Tensor&amp; input);\n\n  Tensor weight, bias;\n};\n\nTORCH_MODULE(Linear);\n\n</code></pre> <p>\u7b80\u800c\u8a00\u4e4b\uff1a\u6a21\u5757\u4e0d\u662f <code>Linear</code>,\u800c\u662f <code>LinearImpl</code>.\u3002\u5b83\u662f\u4e00\u4e2a\u5b8f\u5b9a\u4e49\uff0c\u5373 <code>TORCH_MODULE</code> \u5b9a\u4e49\u7684\u771f\u6b63\u7684  <code>Linear</code> \u3002\u8fd9\u4e2a\u201c\u751f\u6210\u7684\u201d\u7c7b\u5b9e\u9645\u4e0a\u662f<code>std::shared_ptr&amp;lt;LinearImpl&amp;gt;</code>\u7684\u5c01\u88c5\u3002\u5b83\u662f\u4e00\u4e2a\u5c01\u88c5\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u7c7b\u578b\u5b9a\u4e49\uff0c\u56e0\u6b64\uff0c\u6784\u9020\u51fd\u6570\u4ecd\u7136\u53ef\u4ee5\u6309\u9884\u671f\u5de5\u4f5c\uff0c\u5373\u60a8\u4ecd\u7136\u53ef\u4ee5\u7f16\u5199 <code>torch::nn::Linear(3, 4)</code>\u800c\u4e0d\u9700\u8981\u5199 <code>std::make_shared&amp;lt;LinearImpl&amp;gt;(3, 4)</code>\u3002\u6211\u4eec\u5c06\u5b8f\u521b\u5efa\u7684\u7c7b\u79f0\u4e3a\u6a21\u5757\u5bb9\u5668\u3002\u4e0e(\u5171\u4eab\uff09\u6307\u9488\u7c7b\u4f3c\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u7bad\u5934\u64cd\u4f5c\u7b26(\u5982 <code>model-&amp;gt;forward(...)</code>)\u8bbf\u95ee\u57fa\u7840\u5bf9\u8c61\u3002\u6700\u7ec8\u7684\u7ed3\u679c\u662f\u4e00\u4e2a\u4e0ePythonAPI\u975e\u5e38\u76f8\u4f3c\u7684\u6240\u6709\u6743\u6a21\u578b\u3002\u5f15\u7528\u8bed\u4e49\u6210\u4e3a\u9ed8\u8ba4\u8bed\u4e49\uff0c\u4f46\u4e0d\u9700\u8981\u989d\u5916\u8f93\u5165<code>std::shared_ptr</code> \u6216\u8005 <code>std::make_shared</code>\u3002\u5bf9\u4e8e\u6211\u4eec\u7684\u7f51\u7edc\uff0c\u4f7f\u7528\u6a21\u5757\u4fdd\u6301\u5668API\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>struct NetImpl : torch::nn::Module {};\nTORCH_MODULE(Net);\n\nvoid a(Net net) { }\n\nint main() {\n  Net net;\n  a(net);\n}\n\n</code></pre> <p>\u8fd9\u91cc\u6709\u4e00\u4e2a\u5fae\u5999\u7684\u95ee\u9898\u503c\u5f97\u4e00\u63d0\u3002\u9ed8\u8ba4\u6784\u9020\u7684 <code>std::shared_ptr</code> \u4e3a\u201c\u7a7a\u201d\uff0c\u5373\u5305\u542b\u7a7a\u6307\u9488\u3002\u4ec0\u4e48\u662f\u9ed8\u8ba4\u6784\u9020\u7684 <code>Linear</code> \u6216\u8005<code>Net</code>\uff1f\u55ef\uff0c\u8fd9\u662f\u4e00\u4e2a\u68d8\u624b\u7684\u9009\u62e9\u3002\u6211\u4eec\u53ef\u4ee5\u8bf4\u5b83\u5e94\u8be5\u662f\u4e00\u4e2a\u7a7a\u7684(\u7a7a\uff09 <code>std::shared_ptr&amp;lt;LinearImpl&amp;gt</code>\u3002\u4f46\u662f\uff0c\u8bf7\u8bb0\u4f4f\uff0c <code>Linear(3, 4)</code> \u4e0e <code>std::make_shared&amp;lt;LinearImpl&amp;gt;(3, 4)</code>\u76f8\u540c\u3002\u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u6211\u4eec\u5df2\u7ecf\u51b3\u5b9a <code>Linear linear</code>\uff1b\u5e94\u8be5\u662f\u4e00\u4e2a\u7a7a\u6307\u9488\uff0c\u90a3\u4e48\u5c31\u6ca1\u6709\u529e\u6cd5\u6784\u9020\u4e00\u4e2a\u4e0d\u63a5\u53d7\u4efb\u4f55\u6784\u9020\u51fd\u6570\u53c2\u6570\u7684\u6a21\u5757\uff0c\u6216\u8005\u9ed8\u8ba4\u6240\u6709\u8fd9\u4e9b\u53c2\u6570\u3002\u56e0\u6b64\uff0c\u5728\u5f53\u524dAPI\u4e2d\uff0c\u9ed8\u8ba4\u6784\u9020\u7684\u6a21\u5757\u6301\u6709\u8005(\u5982 <code>Linear()</code>))\u8c03\u7528\u5e95\u5c42\u6a21\u5757\u7684\u9ed8\u8ba4\u6784\u9020\u51fd\u6570(<code>LinearImpl()</code>\uff09\u3002\u5982\u679c\u5e95\u5c42\u6a21\u5757\u6ca1\u6709\u9ed8\u8ba4\u7684\u6784\u9020\u51fd\u6570\uff0c\u5219\u4f1a\u5f97\u5230\u4e00\u4e2a\u7f16\u8bd1\u5668\u9519\u8bef\u3002\u8981\u6784\u9020\u7a7a\u5bb9\u5668\uff0c\u53ef\u4ee5\u5c06<code>nullptr</code>\u4f20\u9012\u7ed9\u5bb9\u5668\u7684\u6784\u9020\u51fd\u6570\u3002</p> <p>\u5b9e\u9645\u4e0a\uff0c\u8fd9\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u4f7f\u7528\u524d\u9762\u6240\u793a\u7684\u5b50\u6a21\u5757\uff0c\u5176\u4e2d\u6a21\u5757\u5728\u521d\u59cb\u503c _initializer list_\u4e2d\u6ce8\u518c\u548c\u6784\u9020\uff1a</p> <pre><code>struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};\n\n</code></pre> <p>\u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u5148\u7528\u4e00\u4e2a\u7a7a\u6307\u9488\u6784\u9020\u6240\u6709\u8005\uff0c\u7136\u540e\u5728\u6784\u9020\u51fd\u6570\u4e2d\u5206\u914d\u7ed9\u5b83(\u5bf9Pythonistas\u66f4\u719f\u6089\uff09\uff1a</p> <pre><code>struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    linear = register_module(\"linear\", torch::nn::Linear(N, M));\n  }\n  torch::nn::Linear linear{nullptr}; // construct an empty holder\n};\n</code></pre> <p>\u603b\u4e4b\uff1a\u60a8\u5e94\u8be5\u4f7f\u7528\u54ea\u79cd\u6240\u6709\u6743\u6a21\u578b\u2014\u2014\u54ea\u79cd\u8bed\u4e49\uff1fC++\u524d\u7aef\u7684API\u6700\u4f18\u5316\u652f\u6301\u6a21\u5757\u6301\u6709\u8005\u63d0\u4f9b\u7684\u6240\u6709\u6743\u6a21\u578b\u3002\u8fd9\u79cd\u673a\u5236\u7684\u552f\u4e00\u7f3a\u70b9\u662f\u5728\u6a21\u5757\u58f0\u660e\u4e0b\u9762\u591a\u4e86\u4e00\u884c\u6837\u677f\u6587\u4ef6\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6700\u7b80\u5355\u7684\u6a21\u578b\u4ecd\u7136\u662f\u5728C++\u6a21\u5757\u7684\u4ecb\u7ecd\u4e2d\u6240\u663e\u793a\u7684\u503c\u8bed\u4e49\u6a21\u578b\u3002\u5bf9\u4e8e\u5c0f\u7684\u3001\u7b80\u5355\u7684\u811a\u672c\uff0c\u60a8\u4e5f\u53ef\u4ee5\u6446\u8131\u5b83\u3002\u4f46\u4f60\u8fdf\u65e9\u4f1a\u53d1\u73b0\uff0c\u51fa\u4e8e\u6280\u672f\u539f\u56e0\uff0c\u5e76\u4e0d\u603b\u662f\u652f\u6301\u5b83\u3002\u4f8b\u5982\uff0c\u5e8f\u5217\u5316API(<code>torch::save</code> \u548c <code>torch::load</code>)\u53ea\u652f\u6301\u6a21\u5757\u6301\u6709\u8005(\u6216\u7eaf <code>shared_ptr</code>\uff09\u3002\u56e0\u6b64\uff0c\u6a21\u5757\u6301\u6709\u8005API\u662f\u7528C++\u524d\u7aef\u5b9a\u4e49\u6a21\u5757\u7684\u63a8\u8350\u65b9\u5f0f\uff0c\u4eca\u540e\u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u8be5API\u3002</p>"},{"location":"1.0/cpp_frontend/#dcgan","title":"\u5b9a\u4e49DCGAN\u6a21\u5757","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u6709\u4e86\u5fc5\u8981\u7684\u80cc\u666f\u548c\u4ecb\u7ecd\uff0c\u6765\u4e3a\u6211\u4eec\u5728\u672c\u7bc7\u6587\u7ae0\u4e2d\u8981\u89e3\u51b3\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u5b9a\u4e49\u6a21\u5757\u3002\u56de\u987e\u4e00\u4e0b\uff1a\u6211\u4eec\u7684\u4efb\u52a1\u662f\u4eceMNIST \u6570\u636e\u96c6\u4e2d\u751f\u6210\u6570\u5b57\u56fe\u50cf\u3002\u6211\u4eec\u60f3\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN) \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a DCGAN \u4f53\u7cfb\u7ed3\u6784\u2014\u2014\u5b83\u662f\u7b2c\u4e00\u4e2a\u4e5f\u662f\u6700\u7b80\u5355\u7684\u4f53\u7cfb\u7ed3\u6784\u4e4b\u4e00\uff0c\u4f46\u5bf9\u4e8e\u8fd9\u4e2a\u4efb\u52a1\u6765\u8bf4\u5df2\u7ecf\u5b8c\u5168\u8db3\u591f\u4e86\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u60a8\u53ef\u4ee5\u5728\u6b64 \u5b58\u50a8\u5e93\u4e2d\u627e\u5230\u672c\u6559\u7a0b\u4e2d\u4ecb\u7ecd\u7684\u5b8c\u6574\u6e90\u4ee3\u7801\u3002</p>"},{"location":"1.0/cpp_frontend/#gan-agan","title":"\u4ec0\u4e48\u662f GAN aGAN\uff1f","text":"<p>GAN\u7531\u4e24\u4e2a\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7ec4\u6210\uff1a\u53d1\u751f\u5668\u548c\u9274\u522b\u5668\u3002\u751f\u6210\u5668\u63a5\u6536\u6765\u81ea\u566a\u58f0\u5206\u5e03\u7684\u6837\u672c\uff0c\u5176\u76ee\u7684\u662f\u5c06\u6bcf\u4e2a\u566a\u58f0\u6837\u672c\u8f6c\u6362\u4e3a\u7c7b\u4f3c\u4e8e\u76ee\u6807\u5206\u5e03\u7684\u56fe\u50cf\u2014\u2014\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\u662fMNIST\u6570\u636e\u96c6\u3002\u9274\u522b\u5668\u53cd\u8fc7\u6765\u63a5\u6536\u6765\u81eaMNIST\u6570\u636e\u96c6\u7684\u771f\u5b9e\u56fe\u50cf\u6216\u6765\u81ea\u751f\u6210\u5668\u7684\u5047\u56fe\u50cf\u3002\u5b83\u88ab\u8981\u6c42\u53d1\u51fa\u4e00\u4e2a\u6982\u7387\u6765\u5224\u65ad\u4e00\u4e2a\u7279\u5b9a\u56fe\u50cf\u662f\u771f\u5b9e\u7684(\u63a5\u8fd1 <code>1</code>)\uff09\u8fd8\u662f\u865a\u5047\u7684(\u63a5\u8fd1 <code>0</code>)\uff09\u3002\u4ece\u9274\u522b\u5668\u4e0a\u5f97\u5230\u7684\u751f\u6210\u5668\u751f\u6210\u56fe\u7247\u7684\u771f\u5b9e\u5ea6\u7684\u53cd\u9988\u88ab\u7528\u6765\u8bad\u7ec3\u751f\u6210\u5668\uff1b\u9274\u522b\u5668\u7684\u8fa8\u8bc6\u5ea6\u7684\u53cd\u9988\u5df2\u7ecf\u88ab\u7528\u6765\u4f18\u5316\u9274\u522b\u5668\u3002\u7406\u8bba\u4e0a\uff0c\u53d1\u751f\u5668\u548c\u9274\u522b\u5668\u4e4b\u95f4\u7684\u5fae\u5999\u5e73\u8861\u4f7f\u5b83\u4eec\u4e32\u8054\u6539\u8fdb\uff0c\u5bfc\u81f4\u53d1\u751f\u5668\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u76ee\u6807\u5206\u5e03\u4e0d\u53ef\u533a\u5206\uff0c\u4ece\u800c\u611a\u5f04\u9274\u522b\u5668\u7684\u8fa8\u8bc6\uff0c\u4f7f\u771f\u5b9e\u548c\u865a\u5047\u56fe\u50cf\u7684\u6982\u7387\u5747\u4e3a <code>0.5</code> \u3002\u5bf9\u4e8e\u6211\u4eec\u6765\u8bf4\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u662f\u4e00\u53f0\u673a\u5668\uff0c\u5b83\u63a5\u6536\u566a\u58f0\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u751f\u6210\u6570\u5b57\u7684\u771f\u5b9e\u56fe\u50cf\u4f5c\u4e3a\u8f93\u51fa\u3002</p>"},{"location":"1.0/cpp_frontend/#_8","title":"\u751f\u6210\u5668\u6a21\u5757","text":"<p>\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u751f\u6210\u5668\u6a21\u5757\uff0c\u5b83\u7531\u4e00\u7cfb\u5217\u8f6c\u7f6e\u7684\u4e8c\u7ef4\u5377\u79ef\u3001\u6279\u5904\u7406\u89c4\u8303\u5316\u548cReLU\u6fc0\u6d3b\u5355\u5143\u7ec4\u6210\u3002\u4e0ePython\u4e00\u6837\uff0c\u8fd9\u91cc\u7684PyTorch\u4e3a\u6a21\u578b\u5b9a\u4e49\u63d0\u4f9b\u4e86\u4e24\u4e2aAPI\uff1a\u4e00\u4e2a\u529f\u80fd\u6027\u7684API\uff0c\u8f93\u5165\u901a\u8fc7\u8fde\u7eed\u7684\u51fd\u6570\u4f20\u9012\uff0c\u53e6\u4e00\u4e2a\u9762\u5411\u5bf9\u8c61\u7684API\uff0c\u6211\u4eec\u5728\u5176\u4e2d\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u6574\u4e2a\u6a21\u578b\u4f5c\u4e3a\u5b50\u6a21\u5757\u7684 <code>Sequential</code> \u6a21\u5757\u3002\u8ba9\u6211\u4eec\u770b\u770b\u6211\u4eec\u7684\u751f\u6210\u5668\u5982\u4f55\u4f7f\u7528\u8fd9\u4e24\u79cdAPI\uff0c\u60a8\u53ef\u4ee5\u81ea\u5df1\u51b3\u5b9a\u60a8\u559c\u6b22\u54ea\u4e00\u79cd\u3002\u9996\u5148\uff0c\u4f7f\u7528 <code>Sequential</code>:\uff1a</p> <pre><code>using namespace torch;\n\nnn::Sequential generator(\n    // Layer 1\n    nn::Conv2d(nn::Conv2dOptions(kNoiseSize, 256, 4)\n                   .with_bias(false)\n                   .transposed(true)),\n    nn::BatchNorm(256),\n    nn::Functional(torch::relu),\n    // Layer 2\n    nn::Conv2d(nn::Conv2dOptions(256, 128, 3)\n                   .stride(2)\n                   .padding(1)\n                   .with_bias(false)\n                   .transposed(true)),\n    nn::BatchNorm(128),\n    nn::Functional(torch::relu),\n    // Layer 3\n    nn::Conv2d(nn::Conv2dOptions(128, 64, 4)\n                   .stride(2)\n                   .padding(1)\n                   .with_bias(false)\n                   .transposed(true)),\n    nn::BatchNorm(64),\n    nn::Functional(torch::relu),\n    // Layer 4\n    nn::Conv2d(nn::Conv2dOptions(64, 1, 4)\n                   .stride(2)\n                   .padding(1)\n                   .with_bias(false)\n                   .transposed(true)),\n    nn::Functional(torch::tanh));\n\n</code></pre> <p>\u5c0f\u8d34\u58eb</p> <p><code>Sequential</code> \u6a21\u5757\u53ea\u6267\u884c\u51fd\u6570\u7ec4\u5408\u3002\u7b2c\u4e00\u4e2a\u5b50\u6a21\u5757\u7684\u8f93\u51fa\u6210\u4e3a\u7b2c\u4e8c\u4e2a\u5b50\u6a21\u5757\u7684\u8f93\u5165\uff0c\u7b2c\u4e09\u4e2a\u5b50\u6a21\u5757\u7684\u8f93\u51fa\u6210\u4e3a\u7b2c\u56db\u4e2a\u5b50\u6a21\u5757\u7684\u8f93\u5165\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002</p> <p>\u6240\u9009\u7684\u7279\u5b9a\u6a21\u5757(\u5982 <code>nn::Conv2d</code> \u548c<code>nn::BatchNorm</code>\uff09\u9075\u5faa\u524d\u9762\u6982\u8ff0\u7684\u7ed3\u6784\u3002 <code>kNoiseSize</code>\u5e38\u91cf\u786e\u5b9a\u8f93\u5165\u566a\u58f0\u77e2\u91cf\u7684\u5927\u5c0f\uff0c\u5e76\u8bbe\u7f6e\u4e3a <code>100</code>.\u3002\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5728\u6fc0\u6d3b\u51fd\u6570\u4e2d\u4f7f\u7528\u4e86<code>torch::nn::Functional</code>\u6a21\u5757\uff0c\u5c06\u5185\u90e8\u5c42\u7684<code>torch::relu</code>\u4f20\u9012\u7ed9\u5b83\uff0c\u6700\u540e\u6fc0\u6d3b\u7684\u662f <code>torch::tanh</code> \u3002\u5f53\u7136\uff0c\u8d85\u53c2\u6570\u662f\u901a\u8fc7\u68af\u5ea6\u7684\u4e0b\u964d\u53d1\u73b0\u7684\u3002</p> <p>\u7b14\u8bb0</p> <p>Python\u524d\u7aef\u4e3a\u6bcf\u4e2a\u6fc0\u6d3b\u529f\u80fd\u90fd\u6709\u4e00\u4e2a\u6a21\u5757\uff0c\u6bd4\u5982 <code>torch.nn.ReLU</code> \u6216<code>torch.nn.Tanh</code>\u3002\u5728C++\u4e2d\uff0c\u6211\u4eec\u53ea\u63d0\u4f9b <code>Functional</code> \u6a21\u5757\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7 <code>Functional</code>\u7684\u8f6c\u53d1<code>forward()</code>\u4e2d\u8c03\u7528\u7684\u4efb\u4f55C++\u51fd\u6570\u3002</p> <p>\u6ce8\u610f</p> <p>\u5bf9\u4e8e\u7b2c\u4e8c\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u5728\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u5757\u7684<code>forward()</code>\u65b9\u6cd5\u4e2d\u663e\u5f0f\u5730\u5728\u6a21\u5757\u4e4b\u95f4\u4f20\u9012\u8f93\u5165(\u4ee5\u51fd\u6570\u65b9\u5f0f\uff09\uff1a</p> <pre><code>struct GeneratorImpl : nn::Module {\n  GeneratorImpl()\n      : conv1(nn::Conv2dOptions(kNoiseSize, 512, 4)\n                  .with_bias(false)\n                  .transposed(true)),\n        batch_norm1(512),\n        conv2(nn::Conv2dOptions(512, 256, 4)\n                  .stride(2)\n                  .padding(1)\n                  .with_bias(false)\n                  .transposed(true)),\n        batch_norm2(256),\n        conv3(nn::Conv2dOptions(256, 128, 4)\n                  .stride(2)\n                  .padding(1)\n                  .with_bias(false)\n                  .transposed(true)),\n        batch_norm3(128),\n        conv4(nn::Conv2dOptions(128, 64, 4)\n                  .stride(2)\n                  .padding(1)\n                  .with_bias(false)\n                  .transposed(true)),\n        batch_norm4(64),\n        conv5(nn::Conv2dOptions(64, 1, 4)\n                  .stride(2)\n                  .padding(1)\n                  .with_bias(false)\n                  .transposed(true)) {}\n\n  torch::Tensor forward(torch::Tensor x) {\n    x = torch::relu(batch_norm1(conv1(x)));\n    x = torch::relu(batch_norm2(conv2(x)));\n    x = torch::relu(batch_norm3(conv3(x)));\n    x = torch::relu(batch_norm4(conv4(x)));\n    x = torch::tanh(conv5(x));\n    return x;\n  }\n\n  nn::Conv2d conv1, conv2, conv3, conv4, conv5;\n  nn::BatchNorm batch_norm1, batch_norm2, batch_norm3, batch_norm4;\n};\nTORCH_MODULE(Generator);\n\nGenerator generator;\n\n</code></pre> <p>\u65e0\u8bba\u4f7f\u7528\u54ea\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u73b0\u5728\u90fd\u53ef\u4ee5\u5728\u751f\u6210\u5668\u4e0a\u8c03\u7528 <code>forward()</code> \u6765\u5c06<code>Generator</code>\u566a\u58f0\u6837\u672c\u6620\u5c04\u5230\u56fe\u50cf\u3002</p> <p>\u7b14</p> <p>\u4e00\u4e2a\u7b80\u77ed\u7684\u5173\u4e8e\u8def\u5f84\u9009\u62e9\u7684\u9009\u9879\u88ab\u4f20\u9012\u5230C++\u6a21\u5757\u4e2d\u7684\u50cf <code>Conv2d</code> \u8fd9\u6837\u7684\u5185\u7f6e\u6a21\u5757\uff1a\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u4e00\u4e9b\u5fc5\u9700\u7684\u9009\u9879\uff0c\u6bd4\u5982 <code>BatchNorm</code>.\u7684\u7279\u5f81\u6570\u3002\u5982\u679c\u53ea\u9700\u8981\u914d\u7f6e\u6240\u9700\u7684\u9009\u9879\uff0c\u5219\u53ef\u4ee5\u5c06\u5b83\u4eec\u76f4\u63a5\u4f20\u9012\u7ed9\u6a21\u5757\u7684\u6784\u9020\u51fd\u6570\uff0c\u5982<code>BatchNorm(128)</code> \u6216 <code>Dropout(0.5)</code> \u6216 <code>Conv2d(8, 4, 2)</code> (\u7528\u4e8e\u8f93\u5165\u901a\u9053\u8ba1\u6570\u3001\u8f93\u51fa\u901a\u9053\u8ba1\u6570\u548c\u5185\u6838\u5927\u5c0f\uff09\u3002\u4f46\u662f\uff0c\u5982\u679c\u60a8\u9700\u8981\u4fee\u6539\u5176\u4ed6\u9009\u9879(\u901a\u5e38\u662f\u9ed8\u8ba4\u7684\uff09\uff0c\u4f8b\u5982\u4f7f\u7528 <code>Conv2d</code>\u7684 <code>with_bias</code> \uff0c\u5219\u9700\u8981\u6784\u9020\u5e76\u4f20\u9012\u4e00\u4e2a\u9009\u9879\u5bf9\u8c61\u3002C++\u524d\u7aef\u4e2d\u7684\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u4e00\u4e2a\u76f8\u5173\u7684\u9009\u9879\u7ed3\u6784\uff0c\u79f0\u4e3a\u6a21\u5757\u9009\u9879\uff0c\u5176\u4e2d  <code>Module</code>  \u662f<code>ModuleOptions</code>  \uff0c\u6bd4\u5982 <code>Linear</code> \u7684<code>LinearOptions</code>  \u3002\u8fd9\u662f\u6211\u4eec\u4e3a\u4e0a\u9762\u7684 <code>Conv2d</code> \u6a21\u5757\u6240\u505a\u7684\u3002</p>"},{"location":"1.0/cpp_frontend/#_9","title":"\u9274\u522b\u5668\u6a21\u5757","text":"<p>\u9274\u522b\u5668\u7c7b\u4f3c\u4e8e\u4e00\u7cfb\u5217\u5377\u79ef\u3001\u6279\u91cf\u89c4\u8303\u5316\u548c\u6fc0\u6d3b\u3002\u7136\u800c\uff0c\u73b0\u5728\u5377\u79ef\u662f\u5e38\u89c4\u7684\u800c\u4e0d\u662f\u8f6c\u7f6e\u7684\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u4e2aalpha\u503c\u4e3a0.2\u7684leaky ReLU\u800c\u4e0d\u662fvanilla ReLU\u3002\u800c\u4e14\uff0c\u6700\u7ec8\u7684\u6fc0\u6d3b\u53d8\u6210\u4e86\u4e00\u4e2aSigmoid\uff0c\u5b83\u5c06\u503c\u538b\u7f29\u52300\u52301\u4e4b\u95f4\u7684\u8303\u56f4\u3002\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u538b\u7f29\u503c\u89e3\u91ca\u4e3a\u9274\u522b\u5668\u5206\u914d\u7ed9\u56fe\u50cf\u771f\u5b9e\u7684\u6982\u7387\uff1a</p> <pre><code>nn::Sequential discriminator(\n  // Layer 1\n  nn::Conv2d(\n      nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).with_bias(false)),\n  nn::Functional(torch::leaky_relu, 0.2),\n  // Layer 2\n  nn::Conv2d(\n      nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).with_bias(false)),\n  nn::BatchNorm(128),\n  nn::Functional(torch::leaky_relu, 0.2),\n  // Layer 3\n  nn::Conv2d(\n      nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).with_bias(false)),\n  nn::BatchNorm(256),\n  nn::Functional(torch::leaky_relu, 0.2),\n  // Layer 4\n  nn::Conv2d(\n      nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).with_bias(false)),\n  nn::Functional(torch::sigmoid));\n\n</code></pre> <p>\u7b14\u8bb0</p> <p>\u5f53\u6211\u4eec\u4f20\u9012\u7ed9 <code>Functional</code> \u51fd\u6570\u63a5\u53d7\u7684\u53c2\u6570\u591a\u4e8e\u4e00\u4e2atensor\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u4f20\u9012\u7ed9 <code>Functional</code> \u6784\u9020\u51fd\u6570\uff0c\u540e\u8005\u5c06\u628a\u5b83\u4eec\u8f6c\u53d1\u7ed9\u6bcf\u4e2a\u51fd\u6570\u8c03\u7528\u3002\u5bf9\u4e8e\u4e0a\u9762\u7684leaky ReLU\uff0c\u8fd9\u610f\u5473\u7740\u8c03\u7528\u4e86<code>torch::leaky_relu(previous_output_tensor, 0.2)</code> \u3002</p>"},{"location":"1.0/cpp_frontend/#_10","title":"\u52a0\u8f7d\u6570\u636e","text":"<p>\u65e2\u7136\u6211\u4eec\u5df2\u7ecf\u5b9a\u4e49\u4e86\u751f\u6210\u5668\u548c\u9274\u522b\u5668\u6a21\u578b\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e9b\u53ef\u4ee5\u7528\u6765\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u7684\u6570\u636e\u3002C++\u524d\u7aef\u4e0ePython\u4e00\u6837\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5e76\u884c\u6570\u636e\u52a0\u8f7d\u7a0b\u5e8f\u3002\u8fd9\u4e2a\u6570\u636e\u52a0\u8f7d\u5668\u53ef\u4ee5\u4ece\u6570\u636e\u96c6(\u60a8\u53ef\u4ee5\u81ea\u5df1\u5b9a\u4e49\uff09\u4e2d\u8bfb\u53d6\u6279\u91cf\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u8bb8\u591a\u914d\u7f6e\u3002</p> <p>\u7b14\u8bb0</p> <p>Python\u6570\u636e\u88c5\u8f7d\u5668\u4f7f\u7528\u5e76\u884c\u5904\u7406\u3002C++\u6570\u636e\u88c5\u8f7d\u5668\u662f\u591a\u7ebf\u7a0b\u7684\uff0c\u5e76\u4e14\u4e0d\u542f\u52a8\u4efb\u4f55\u65b0\u8fdb\u7a0b\u3002</p> <p>\u6570\u636e\u52a0\u8f7d\u5668\u662fC++\u524d\u7aef <code>data</code> API\u7684\u4e00\u90e8\u5206\uff0c\u5305\u542b\u5728 <code>torch::data::</code> \u547d\u540d\u7a7a\u95f4\u4e2d\u3002\u6b64API\u7531\u51e0\u4e2a\u4e0d\u540c\u7684\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ul> <li>\u6570\u636e\u52a0\u8f7d\u5668\u7c7b\uff0c</li> <li>\u7528\u4e8e\u5b9a\u4e49\u6570\u636e\u96c6\u7684API\uff0c</li> <li>\u7528\u4e8e\u5b9a\u4e49\u8f6c\u6362\u7684API\uff0c\u53ef\u5e94\u7528\u4e8e\u6570\u636e\u96c6\uff0c</li> <li>\u7528\u4e8e\u5b9a\u4e49\u91c7\u6837\u5668\u7684API\uff0c\u8be5\u91c7\u6837\u5668\u751f\u6210\u7528\u4e8e\u7d22\u5f15\u6570\u636e\u96c6\u7684\u7d22\u5f15\uff0c</li> <li>\u73b0\u6709\u6570\u636e\u96c6\u3001\u8f6c\u6362\u548c\u91c7\u6837\u5668\u7684\u5e93\u3002</li> </ul> <p>\u5bf9\u4e8e\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5e26\u6709C++\u524d\u7aef\u7684 <code>MNIST</code> \u6570\u636e\u96c6\u3002\u8ba9\u6211\u4eec\u4e3a\u6b64\u5b9e\u4f8b\u5316\u4e00\u4e2a<code>torch::data::datasets::MNIST</code>\uff0c\u5e76\u5e94\u7528\u4e24\u79cd\u8f6c\u6362\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5bf9\u56fe\u50cf\u8fdb\u884c\u89c4\u683c\u5316\uff0c\u4f7f\u5176\u5728 <code>-1</code> \u5230 <code>+1</code> \u7684\u8303\u56f4\u5185(\u4ece\u539f\u59cb\u8303\u56f4 <code>0</code> \u5230<code>1</code>\uff09\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u5806\u6808\u6392\u5e8f\u89c4\u5219\uff0c\u5b83\u5c06 a batch of tensors\u6cbf\u7740\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5806\u53e0\u6210\u4e00\u4e2atensor\uff1a</p> <pre><code>auto dataset = torch::data::datasets::MNIST(\"./mnist\")\n    .map(torch::data::transforms::Normalize(0.5, 0.5))\n    .map(torch::data::transforms::Stack&lt;&gt;());\n\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c MNIST\u6570\u636e\u96c6\u5e94\u8be5\u4f4d\u4e8e<code>./mnist</code>\u76ee\u5f55\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u6267\u884c\u8bad\u7ec3\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u4f4d\u7f6e\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u6b64\u811a\u672c \u4e0b\u8f7dMNIST\u6570\u636e\u96c6\u3002</p> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u52a0\u8f7d\u5668\u5e76\u5c06\u8fd9\u4e2a\u6570\u636e\u96c6\u4f20\u9012\u7ed9\u5b83\u3002\u8981\u521b\u5efa\u65b0\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u6211\u4eec\u4f7f\u7528<code>torch::data::make_data_loader</code>\uff0c\u5b83\u8fd4\u56de\u6b63\u786e\u7c7b\u578b\u7684<code>std::unique_ptr</code>(\u8fd9\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\u7684\u7c7b\u578b\u3001\u91c7\u6837\u5668\u7684\u7c7b\u578b\u548c\u4e00\u4e9b\u5176\u4ed6\u5b9e\u73b0\u7ec6\u8282\uff09\uff1a</p> <pre><code>auto dataloader = torch::data::make_data_loader(std::move(dataset));\n</code></pre> <p>\u6570\u636e\u52a0\u8f7d\u5668\u786e\u5b9e\u6709\u5f88\u591a\u9009\u9879\u3002\u4f60\u53ef\u4ee5\u5728 \u8fd9\u91cc\u68c0\u67e5\u6574\u5957\u8bbe\u5907\u3002\u4f8b\u5982\uff0c\u4e3a\u4e86\u52a0\u901f\u6570\u636e\u52a0\u8f7d\uff0c\u6211\u4eec\u53ef\u4ee5\u589e\u52a0\u5de5\u4eba\u7684\u6570\u91cf\u3002\u9ed8\u8ba4\u503c\u4e3a\u96f6\uff0c\u8fd9\u610f\u5473\u7740\u5c06\u4f7f\u7528\u4e3b\u7ebf\u7a0b\u3002\u5982\u679c\u5c06 <code>workers</code> \u8bbe\u7f6e\u4e3a <code>2</code>,\uff0c\u5219\u4f1a\u540c\u65f6\u751f\u6210\u4e24\u4e2a\u7ebf\u7a0b\u6765\u52a0\u8f7d\u6570\u636e\u3002\u6211\u4eec\u8fd8\u5e94\u8be5\u5c06\u6279\u5927\u5c0f\u4ece\u9ed8\u8ba4\u503c <code>1</code> \u589e\u52a0\u5230\u66f4\u5408\u7406\u7684\u503c\uff0c\u6bd4\u5982 <code>64</code>  (<code>kBatchSize</code>)\u7684\u503c\uff09\u3002\u56e0\u6b64\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a <code>DataLoaderOptions</code> \u5bf9\u8c61\u5e76\u8bbe\u7f6e\u9002\u5f53\u7684\u5c5e\u6027:</p> <pre><code>auto dataloader = torch::data::make_data_loader(\n    std::move(dataset),\n    torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));\n\n</code></pre> <p>\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u7f16\u5199\u4e00\u4e2a\u5faa\u73af\u6765\u52a0\u8f7d\u6279\u6570\u636e\uff0c\u73b0\u5728\u53ea\u8f93\u51fa\u5230\u63a7\u5236\u53f0\uff1a</p> <pre><code>for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n  std::cout &lt;&lt; \"Batch size: \" &lt;&lt; batch.data.size(0) &lt;&lt; \" | Labels: \";\n  for (int64_t i = 0; i &lt; batch.data.size(0); ++i) {\n    std::cout &lt;&lt; batch.target[i].item&lt;int64_t&gt;() &lt;&lt; \" \";\n  }\n  std::cout &lt;&lt; std::endl;\n}\n\n</code></pre> <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u8fd4\u56de\u7684\u7c7b\u578b\u662f <code>torch::data::Example</code>.\u3002\u6b64\u7c7b\u578b\u662f\u4e00\u4e2a\u7b80\u5355\u7ed3\u6784\uff0c\u5177\u6709 <code>data</code> \u5b57\u6bb5\u548c\u6807\u7b7e <code>target</code> \u5b57\u6bb5\u3002\u56e0\u4e3a\u6211\u4eec\u4e4b\u524d\u5e94\u7528\u4e86 <code>Stack</code> \u6392\u5e8f\u89c4\u5219\uff0c\u6240\u4ee5\u6570\u636e\u52a0\u8f7d\u5668\u53ea\u8fd4\u56de\u4e00\u4e2a\u8fd9\u6837\u7684\u793a\u4f8b\u3002\u5982\u679c\u6211\u4eec\u6ca1\u6709\u5e94\u7528\u6392\u5e8f\u89c4\u5219\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u5c06\u751f\u6210 <code>std::vector&amp;lt;torch::data::Example&amp;lt;&amp;gt;&amp;gt;</code> \uff0c\u5728\u6279\u5904\u7406\u4e2d\u6bcf\u4e2a\u793a\u4f8b\u6709\u4e00\u4e2a\u5143\u7d20\u3002</p> <p>\u5982\u679c\u91cd\u65b0\u751f\u6210\u5e76\u8fd0\u884c\u6b64\u4ee3\u7801\uff0c\u5219\u5e94\u8be5\u770b\u5230\u5982\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>root@fa350df05ecf:/home/build# make\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# make\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# ./dcgan\nBatch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9\nBatch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2\nBatch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0\nBatch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7\nBatch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0\nBatch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2\nBatch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7\nBatch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3\nBatch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7\nBatch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4\nBatch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6\n...\n\n</code></pre> <p>\u8fd9\u610f\u5473\u7740\u6211\u4eec\u80fd\u591f\u6210\u529f\u5730\u4ece MNIST \u6570\u636e\u96c6\u4e2d\u52a0\u8f7d\u6570\u636e\u3002</p>"},{"location":"1.0/cpp_frontend/#_11","title":"\u7f16\u5199\u8fed\u4ee3\u8bad\u7ec3","text":"<p>\u73b0\u5728\u8ba9\u6211\u4eec\u5b8c\u6210\u793a\u4f8b\u4e2d\u7684\u7b97\u6cd5\u90e8\u5206\uff0c\u5e76\u5b9e\u73b0\u751f\u6210\u5668\u548c\u9274\u522b\u5668\u4e4b\u95f4\u7684\u5fae\u5999\u8df3\u8dc3\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c06\u521b\u5efa\u4e24\u4e2a\u4f18\u5316\u5668\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5668\uff0c\u4e00\u4e2a\u7528\u4e8e\u9274\u522b\u5668\u3002\u6211\u4eec\u4f7f\u7528\u7684\u4f18\u5316\u5668\u5b9e\u73b0\u4e86 Adam \u7b97\u6cd5\uff1a</p> <pre><code>torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5));\n\n</code></pre> <p>\u7b14\u8bb0</p> <p>\u5728\u672c\u6587\u4e2d\uff0cC++\u524d\u7aef\u63d0\u4f9b\u4e86\u5b9e\u73b0 Adagrad, Adam, LBFGS, RMSprop \u548c SGD\u7684\u4f18\u5316\u5668\u3002\u8fd9\u4e2a\u6587\u6863\u662f\u6700\u65b0\u7684\u6e05\u5355\u3002</p> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u66f4\u65b0\u6211\u4eec\u7684\u8fed\u4ee3\u8bad\u7ec3\u3002\u5728\u6bcf\u4e2a\u5468\u671f\uff0c\u6211\u4eec\u5c06\u6dfb\u52a0\u4e00\u4e2a\u5916\u5faa\u73af\u6765\u4f7f\u7528\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u7136\u540e\u7f16\u5199GAN\u7684\u8bad\u7ec3\u4ee3\u7801\uff1a</p> <pre><code>for (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n    // Train discriminator with real images.\n    discriminator-&gt;zero_grad();\n    torch::Tensor real_images = batch.data;\n    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);\n    torch::Tensor real_output = discriminator-&gt;forward(real_images);\n    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);\n    d_loss_real.backward();\n\n    // Train discriminator with fake images.\n    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});\n    torch::Tensor fake_images = generator-&gt;forward(noise);\n    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));\n    torch::Tensor fake_output = discriminator-&gt;forward(fake_images.detach());\n    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);\n    d_loss_fake.backward();\n\n    torch::Tensor d_loss = d_loss_real + d_loss_fake;\n    discriminator_optimizer.step();\n\n    // Train generator.\n    generator-&gt;zero_grad();\n    fake_labels.fill_(1);\n    fake_output = discriminator-&gt;forward(fake_images);\n    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);\n    g_loss.backward();\n    generator_optimizer.step();\n\n    std::printf(\n        \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\",\n        epoch,\n        kNumberOfEpochs,\n        ++batch_index,\n        batches_per_epoch,\n        d_loss.item&lt;float&gt;(),\n        g_loss.item&lt;float&gt;());\n  }\n}\n\n</code></pre> <p>\u4ee5\u4e0a\uff0c\u6211\u4eec\u9996\u5148\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u5bf9\u9274\u522b\u5668\u8fdb\u884c\u8bc4\u4f30\uff0cAdam\u5e94\u8be5\u4e3a\u5176\u5206\u914d\u9ad8\u8870\u51cf\u7387\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4f7f\u7528 <code>torch::empty(batch.data.size(0)).uniform_(0.8, 1.0)</code> \u4f5c\u4e3a\u76ee\u6807\u6982\u7387\u3002</p> <p>\u7b14\u8bb0</p> <p>\u6211\u4eec\u9009\u53d60.8\u52301.0\u4e4b\u95f4\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u503c\uff0c\u800c\u4e0d\u662f1.0\uff0c\u4ee5\u4f7f\u9274\u522b\u5668\u8bad\u7ec3\u66f4\u52a0\u5065\u58ee\u3002\u8fd9\u4e2a\u6280\u5de7\u53eb\u505alabel smoothing\u3002</p> <p>\u5728\u8bc4\u4f30\u9274\u522b\u5668\u4e4b\u524d\uff0c\u6211\u4eec\u5c06\u5176\u53c2\u6570\u7684\u68af\u5ea6\u5f52\u96f6\u3002\u8ba1\u7b97\u5b8c\u635f\u5931\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u8c03\u7528 <code>d_loss.backward()</code> \u6765\u8ba1\u7b97\u65b0\u7684\u68af\u5ea6\uff0c\u4ece\u800c\u5728\u7f51\u7edc\u4e2d\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\u6211\u4eec\u5728\u5047\u56fe\u50cf\u4e0a\u4e0d\u65ad\u91cd\u590d\u3002\u6211\u4eec\u4e0d\u4f7f\u7528\u6765\u81ea\u6570\u636e\u96c6\u7684\u56fe\u50cf\uff0c\u800c\u662f\u8ba9\u751f\u6210\u5668\u901a\u8fc7\u5411\u5176\u63d0\u4f9b\u4e00\u6279\u968f\u673a\u566a\u58f0\u6765\u4e3a\u6b64\u521b\u5efa\u5047\u56fe\u50cf\u3002\u7136\u540e\u6211\u4eec\u628a\u8fd9\u4e9b\u5047\u56fe\u50cf\u8f6c\u53d1\u7ed9\u9274\u522b\u5668\u3002\u8fd9\u4e00\u6b21\uff0c\u6211\u4eec\u5e0c\u671b\u9274\u522b\u5668\u53d1\u51fa\u4f4e\u6982\u7387\uff0c\u7406\u60f3\u60c5\u51b5\u4e0b\u5168\u90e8\u4e3a0\u3002\u4e00\u65e6\u6211\u4eec\u8ba1\u7b97\u4e86\u4e00\u6279\u771f\u5b9e\u56fe\u50cf\u548c\u4e00\u6279\u5047\u56fe\u50cf\u7684\u9274\u522b\u5668\u635f\u5931\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4e00\u6b65\u4e00\u6b65\u5730\u5bf9\u9274\u522b\u5668\u7684\u4f18\u5316\u5668\u8fdb\u884c\u5347\u7ea7\uff0c\u4ee5\u66f4\u65b0\u5176\u53c2\u6570\u3002</p> <p>\u4e3a\u4e86\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u6211\u4eec\u518d\u6b21\u5c06\u5176\u68af\u5ea6\u8c03\u96f6\uff0c\u7136\u540e\u5728\u4f2a\u56fe\u50cf\u4e0a\u91cd\u65b0\u8bc4\u4f30\u9274\u522b\u5668\u3002\u7136\u800c\uff0c\u8fd9\u6b21\u6211\u4eec\u5e0c\u671b\u9274\u522b\u5668\u5206\u914d\u7684\u6982\u7387\u975e\u5e38\u63a5\u8fd11.0\uff0c\u8fd9\u5c06\u8868\u660e\u751f\u6210\u5668\u53ef\u4ee5\u751f\u6210\u56fe\u50cf\uff0c\u6b3a\u9a97\u9274\u522b\u5668\u8ba4\u4e3a\u5b83\u4eec\u662f\u771f\u5b9e\u7684(\u4ece\u6570\u636e\u96c6\uff09\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06<code>fake_labels</code>  tensor\u586b\u5145\u4e3a\u6240\u6709 tensor\u3002\u6700\u540e\uff0c\u6211\u4eec\u5bf9\u751f\u6210\u5668\u7684\u4f18\u5316\u5668\u6267\u884c\u6b65\u9aa4\uff0c\u4ee5\u66f4\u65b0\u5176\u53c2\u6570\u3002</p> <p>\u6211\u4eec\u73b0\u5728\u5e94\u8be5\u51c6\u5907\u597d\u5728CPU\u4e0a\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u4e86\u3002\u6211\u4eec\u8fd8\u6ca1\u6709\u4efb\u4f55\u4ee3\u7801\u6765\u6355\u83b7\u72b6\u6001\u6216\u793a\u4f8b\u8f93\u51fa\uff0c\u4f46\u6211\u4eec\u5c06\u5728\u7a0d\u540e\u6dfb\u52a0\u6b64\u4ee3\u7801\u3002\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u89c2\u5bdf\u4e00\u4e0b\u6211\u4eec\u7684\u6a21\u578b\u5728\u505a\u4ec0\u4e48\u2014\u2014\u7a0d\u540e\u6211\u4eec\u5c06\u6839\u636e\u751f\u6210\u7684\u56fe\u50cf\u6765\u9a8c\u8bc1\u8fd9\u4ef6\u4e8b\u662f\u5426\u6709\u610f\u4e49\u3002\u91cd\u5efa\u548c\u8fd0\u884c\u5e94\u8f93\u51fa\u5982\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcga\n[ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304\n[ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101\n[ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626\n[ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795\n[ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452\n[ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811\n[ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623\n[ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385\n[ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333\n[ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242\n[ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886\n[ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103\n[ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544\n[ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545\n...\n\n</code></pre>"},{"location":"1.0/cpp_frontend/#gpugpu","title":"GPU\u79fb\u52a8\u5230GPU","text":"<p>\u867d\u7136\u6211\u4eec\u5f53\u524d\u7684\u811a\u672c\u53ef\u4ee5\u5728CPU\u4e0a\u8fd0\u884c\u5f97\u5f88\u597d\uff0c\u4f46\u6211\u4eec\u90fd\u77e5\u9053\u5728GPU\u4e0a\u5377\u79ef\u8981\u5feb\u5f97\u591a\u3002\u8ba9\u6211\u4eec\u5feb\u901f\u8ba8\u8bba\u4e00\u4e0b\u5982\u4f55\u5c06\u6211\u4eec\u7684\u8bad\u7ec3\u8f6c\u79fb\u5230GPU\u4e0a\u3002\u6211\u4eec\u9700\u8981\u4e3a\u6b64\u505a\u4e24\u4ef6\u4e8b\uff1a\u5c06GPU\u8bbe\u5907\u6307\u5b9a\u7684\u4f20\u9012\u7ed9\u6211\u4eec\u5206\u914d\u7684 tensors \uff0c\u5e76\u4e14\u901a\u8fc7 <code>to()</code> \u65b9\u6cd5\u5c06\u5176\u4ed6tensors\u663e\u5f0f\u590d\u5236\u5230\u6240\u6709tensors\u548c\u6a21\u5757\u90fd\u5177\u6709\u7684GPU\u4e0a\u3002\u5b9e\u73b0\u8fd9\u4e24\u8005\u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u5728\u6211\u4eec\u7684\u8bad\u7ec3\u811a\u672c\u9876\u5c42\u521b\u5efa\u4e00\u4e2a <code>torch::Device</code> \u7684\u5b9e\u4f8b\uff0c\u7136\u540e\u5c06\u8be5\u8bbe\u5907\u4f20\u9012\u7ed9tensors\u5de5\u5382\u65b9\u6cd5\uff0c\u5982 <code>torch::zeros</code> \u548c <code>to()</code> \u65b9\u6cd5\u3002\u6211\u4eec\u53ef\u4ee5\u4eceCPU\u8bbe\u5907\u5f00\u59cb\uff1a</p> <pre><code>// Place this somewhere at the top of your training script.\ntorch::Device device(torch::kCPU);\n\n</code></pre> <p>\u50cf\u8fd9\u6837\u5206\u914d\u65b0tensor</p> <pre><code>torch::Tensor fake_labels = torch::zeros(batch.data.size(0));\n\n</code></pre> <p>\u5e94\u66f4\u65b0\u4ee5\u5c06<code>device</code>\u4f5c\u4e3a\u6700\u540e\u4e00\u4e2a\u53c2\u6570\uff1a</p> <pre><code>torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device);\n\n</code></pre> <p>\u5bf9\u4e8e\u521b\u5efa\u4e0d\u5728\u6211\u4eec\u624b\u4e2d\u7684 tensors\uff0c\u6bd4\u5982\u6765\u81ea MNIST\u6570\u636e\u96c6\u7684 tensors\uff0c\u6211\u4eec\u5fc5\u987b\u63d2\u5165\u663e\u5f0f<code>to()</code>\u8c03\u7528\u3002\u8fd9\u610f\u5473\u7740</p> <pre><code>torch::Tensor real_images = batch.data;\n\n</code></pre> <p>\u53d8\u6210\u5982\u4e0b</p> <pre><code>torch::Tensor real_images = batch.data.to(device);\n\n</code></pre> <p>\u6211\u4eec\u7684\u6a21\u578b\u53c2\u6570\u5e94\u8be5\u88ab\u79fb\u52a8\u5230\u5408\u9002\u7684\u8bbe\u5907\u4e0a</p> <pre><code>generator-&gt;to(device);\ndiscriminator-&gt;to(device);\n\n</code></pre> <p>\u7b14\u8bb0</p> <p>\u5982\u679c\u4e00\u4e2a tensor\u5df2\u7ecf\u5b58\u5728\u4e8e\u63d0\u4f9b<code>to()</code>\u7ed9\u7684\u8bbe\u5907\u4e0a\uff0c\u5219\u8c03\u7528\u662fno-op\u3002\u4e0d\u8fdb\u884c\u989d\u5916\u7684\u590d\u5236\u3002</p> <p>\u5728\u8fd9\u4e00\u70b9\u4e0a\uff0c\u6211\u4eec\u521a\u521a\u4f7f\u4ee5\u524d\u7684CPU\u9a7b\u7559\u4ee3\u7801\u66f4\u52a0\u660e\u786e\u3002\u4f46\u662f\uff0c\u73b0\u5728\u4e5f\u5f88\u5bb9\u6613\u5c06\u8bbe\u5907\u66f4\u6539\u4e3aCUDA\u8bbe\u5907\uff1a</p> <pre><code>torch::Device device(torch::kCUDA)\n\n</code></pre> <p>\u73b0\u5728\u6240\u6709\u7684 tensors\u90fd\u5c06\u6d3b\u52a8\u5728GPU\u4e0a\uff0c\u4e3a\u6240\u6709\u7684\u64cd\u4f5c\u8c03\u7528\u5feb\u901f\u7684CUDA\u5185\u6838\uff0c\u800c\u4e0d\u9700\u8981\u6211\u4eec\u66f4\u6539\u4efb\u4f55\u4e0b\u6e38\u4ee3\u7801\u3002\u5982\u679c\u6211\u4eec\u60f3\u8981\u6307\u5b9a\u4e00\u4e2a\u7279\u5b9a\u7684\u8bbe\u5907\u7d22\u5f15\uff0c\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u7b2c\u4e8c\u4e2a\u53c2\u6570\u4f20\u9012\u7ed9<code>Device</code>\u6784\u9020\u51fd\u6570\u3002\u5982\u679c\u6211\u4eec\u5e0c\u671b\u4e0d\u540c\u7684tensors\u5b58\u5728\u4e8e\u4e0d\u540c\u7684\u8bbe\u5907\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u4f20\u9012\u5355\u72ec\u7684\u8bbe\u5907\u5b9e\u4f8b(\u4f8b\u5982\uff0c\u4e00\u4e2a\u5728CUDA\u8bbe\u59070\u4e0a\uff0c\u53e6\u4e00\u4e2a\u5728CUDA\u8bbe\u59071\u4e0a\uff09\u3002\u6211\u4eec\u751a\u81f3\u53ef\u4ee5\u52a8\u6001\u5730\u8fdb\u884c\u6b64\u914d\u7f6e\uff0c\u8fd9\u901a\u5e38\u6709\u52a9\u4e8e\u4f7f\u6211\u4eec\u7684\u8bad\u7ec3\u811a\u672c\u66f4\u6613\u4e8e\u79fb\u690d\uff1a</p> <pre><code>torch::Device device = torch::kCPU;\nif (torch::cuda::is_available()) {\n  std::cout &lt;&lt; \"CUDA is available! Training on GPU.\" &lt;&lt; std::endl;\n  device = torch::kCUDA;\n}\n\n</code></pre> <p>\u751a\u81f3\u662f\u8fd9\u6837</p> <pre><code>torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);\n\n</code></pre>"},{"location":"1.0/cpp_frontend/#_12","title":"\u68c0\u67e5\u70b9\u548c\u6062\u590d\u8bad\u7ec3\u72b6\u6001","text":"<p>\u6211\u4eec\u5bf9\u8bad\u7ec3\u811a\u672c\u7684\u6700\u540e\u4e00\u4e2a\u589e\u5f3a\u70b9\u662f\u5b9a\u671f\u4fdd\u5b58\u6a21\u578b\u53c2\u6570\u7684\u72b6\u6001\u3001\u4f18\u5316\u5668\u7684\u72b6\u6001\u4ee5\u53ca\u4e00\u4e9b\u751f\u6210\u7684\u56fe\u50cf\u6837\u672c\u3002\u5982\u679c\u6211\u4eec\u7684\u8ba1\u7b97\u673a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5d29\u6e83\uff0c\u524d\u4e24\u4e2a\u5c06\u5141\u8bb8\u6211\u4eec\u6062\u590d\u8bad\u7ec3\u72b6\u6001\u3002\u5bf9\u4e8e\u957f\u671f\u7684\u8bad\u7ec3\uff0c\u8fd9\u662f\u7edd\u5bf9\u5fc5\u8981\u7684\u3002\u5e78\u8fd0\u7684\u662f\uff0cC++\u524d\u7aef\u63d0\u4f9b\u4e86\u4e00\u4e2aAPI\u6765\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u4ee5\u53ca\u72ec\u7acb\u7684 tensors\u3002</p> <p>\u5b83\u7684\u6838\u5fc3API\u662f <code>torch::save(thing,filename)</code> \u548c <code>torch::load(thing,filename)</code>\uff0c\u5176\u4e2d<code>thing</code>\u53ef\u4ee5\u662f <code>torch::nn::Module</code> \u5b50\u7c7b\u6216\u4f18\u5316\u7a0b\u5e8f\u5b9e\u4f8b\uff0c\u5982\u6211\u4eec\u8bad\u7ec3\u811a\u672c\u4e2d\u7684 <code>Adam</code> \u5bf9\u8c61\u3002\u8ba9\u6211\u4eec\u66f4\u65b0\u6211\u4eec\u7684\u8bad\u7ec3\u5faa\u73af\uff0c\u4ee5\u68c0\u67e5\u6a21\u578b\u548c\u4f18\u5316\u5668\u5728\u7279\u5b9a\u65f6\u95f4\u95f4\u9694\u7684\u72b6\u6001\uff1a</p> <pre><code>if (batch_index % kCheckpointEvery == 0) {\n  // Checkpoint the model and optimizer state.\n  torch::save(generator, \"generator-checkpoint.pt\");\n  torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::save(discriminator, \"discriminator-checkpoint.pt\");\n  torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n  // Sample the generator and save the images.\n  torch::Tensor samples = generator-&gt;forward(torch::randn({8, kNoiseSize, 1, 1}, device));\n  torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n  std::cout &lt;&lt; \"\\n-&gt; checkpoint \" &lt;&lt; ++checkpoint_counter &lt;&lt; '\\n';\n}\n\n</code></pre> <p>\u5176\u4e2d <code>kCheckpointEvery</code> \u662f\u4e00\u4e2a\u6574\u6570\uff0c\u8bbe\u7f6e\u4e3a <code>100</code> \uff0c\u6bcf <code>100</code> \u6279\u68c0\u67e5\u4e00\u6b21\uff0c <code>checkpoint_counter</code>\u662f\u4e00\u4e2a\u8ba1\u6570\u5668\uff0c\u6bcf\u5f53\u6211\u4eec\u5efa\u7acb\u4e00\u4e2a\u68c0\u67e5\u70b9\u65f6\u90fd\u4f1a\u51b2\u649e\u5230\u5b83\u3002</p> <p>\u8981\u6062\u590d\u8bad\u7ec3\u72b6\u6001\uff0c\u53ef\u4ee5\u5728\u521b\u5efa\u6240\u6709\u6a21\u578b\u548c\u4f18\u5316\u5668\u4e4b\u540e\u3002\u4f46\u5728\u8bad\u7ec3\u5faa\u73af\u4e4b\u524d\u6dfb\u52a0\u7c7b\u4f3c\u8fd9\u6837\u7684\u884c\uff1a</p> <pre><code>torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\n\nif (kRestoreFromCheckpoint) {\n  torch::load(generator, \"generator-checkpoint.pt\");\n  torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::load(discriminator, \"discriminator-checkpoint.pt\");\n  torch::load(\n      discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n}\n\nint64_t checkpoint_counter = 0;\nfor (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n\n</code></pre>"},{"location":"1.0/cpp_frontend/#_13","title":"\u68c0\u9a8c\u751f\u6210\u7684\u56fe\u50cf","text":"<p>\u6211\u4eec\u7684\u8bad\u7ec3\u811a\u672c\u73b0\u5728\u5b8c\u6210\u4e86\u3002\u6211\u4eec\u5728CPU\u6216\u8005GPU\u4e0a\u51c6\u5907\u597d\u8bad\u7ec3\u6211\u4eec\u7684GAN\u3002\u4e3a\u4e86\u68c0\u67e5\u6211\u4eec\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e2d\u95f4\u8f93\u51fa\uff0c\u6211\u4eec\u6dfb\u52a0\u4e86\u4ee3\u7801\u4ee5\u5b9a\u671f\u5c06\u56fe\u50cf\u6837\u672c\u4fdd\u5b58\u5230 <code>\"dcgan-sample-xxx.pt\"</code>\u6587\u4ef6\u4e2d\u3002\u6211\u4eec\u53ef\u4ee5\u7f16\u5199\u4e00\u4e2a\u5c0f\u7684python\u811a\u672c\u6765\u52a0\u8f7dtensors\u5e76\u7528matplotlib\u663e\u793a\u5b83\u4eec\uff1a</p> <pre><code>from __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)\n\n</code></pre> <p>\u73b0\u5728\u8ba9\u6211\u4eec\u628a\u8fd9\u4e2a\u6a21\u578b\u8bad\u7ec3\u5927\u698230\u6b21\uff1a</p> <pre><code>root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan                                                                                                                                10:17:57\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nCUDA is available! Training on GPU.\n[ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195\n-&gt; checkpoint 1\n[ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148\n-&gt; checkpoint 2\n[ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760\n-&gt; checkpoint 3\n[ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250\n-&gt; checkpoint 4\n[ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790\n-&gt; checkpoint 5\n[ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315\n...\n-&gt; checkpoint 120\n[30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084\n\n</code></pre> <p>\u5e76\u5728\u7ed8\u56fe\u4e2d\u663e\u793a\u56fe\u50cf\uff1a</p> <pre><code>root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt\nSaved out.png\n\n</code></pre> <p>\u5b83\u770b\u8d77\u6765\u5e94\u8be5\u662f\u8fd9\u6837\uff1a</p> <p></p> <p>\u6570\u5b57\uff01\u4e07\u5c81\uff01\u73b0\u5728\u8f6e\u5230\u4f60\u4e86\uff1a\u4f60\u80fd\u6539\u8fdb\u6a21\u578b\u4f7f\u6570\u5b57\u770b\u8d77\u6765\u66f4\u597d\u5417\uff1f</p>"},{"location":"1.0/cpp_frontend/#_14","title":"\u7ed3\u8bba","text":"<p>\u672c\u6559\u7a0b\u5e0c\u671b\u7ed9\u60a8\u4e00\u4e2a\u6613\u4e86\u89e3\u7684PyTrac C++\u524d\u7aef\u7684\u6458\u8981\u3002\u50cfPyTorch\u8fd9\u6837\u7684\u673a\u5668\u5b66\u4e60\u5e93\u5fc5\u7136\u5177\u6709\u6570\u91cf\u5e9e\u5927\u7684API\u3002\u56e0\u6b64\uff0c\u8fd9\u91cc\u6709\u8bb8\u591a\u6982\u5ff5\u6211\u4eec\u6ca1\u6709\u65f6\u95f4\u6765\u8ba8\u8bba\u3002\u4f46\u662f\uff0c\u6211\u9f13\u52b1\u60a8\u5c1d\u8bd5\u4f7f\u7528API\uff0c\u5e76\u5728\u9047\u5230\u56f0\u96be\u65f6\u53c2\u8003 \u6211\u4eec\u7684\u6587\u6863 \uff0c\u5c24\u5176\u662f\u5e93API \u90e8\u5206\u3002\u6b64\u5916\uff0c\u8bf7\u8bb0\u4f4f\uff0c\u6211\u4eec\u4f1a\u5c3d\u53ef\u80fd\u4f7fC++\u524d\u7aef\u53ef\u4ee5\u9075\u5faaPython\u524d\u7aef\u7684\u8bed\u4e49\u548c\u8bbe\u8ba1\uff0c\u8fd9\u6837\u60a8\u5c31\u53ef\u4ee5\u5229\u7528\u8fd9\u4e2a\u7279\u70b9\u6765\u63d0\u9ad8\u5b66\u4e60\u901f\u5ea6\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u60a8\u53ef\u4ee5\u5728\u6b64\u5b58\u50a8\u5e93\u4e2d\u627e\u5230\u672c\u6559\u7a0b\u4e2d\u4ecb\u7ecd\u7684\u5b8c\u6574\u6e90\u4ee3\u7801\u3002</p> <p>\u548c\u5f80\u5e38\u4e00\u6837\uff0c\u5982\u679c\u4f60\u9047\u5230\u4efb\u4f55\u95ee\u9898\u6216\u6709\u4efb\u4f55\u7591\u95ee\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684 \u8bba\u575b \u6216GitHub issues\u95ee\u9898\u8054\u7cfb\u3002</p>"},{"location":"1.0/cuda/","title":"torch.cuda","text":"<p>\u8bd1\u8005\uff1abdqfork</p> <p>\u8fd9\u4e2a\u5305\u6dfb\u52a0\u4e86\u5bf9CUDA\u5f20\u91cf\u7c7b\u578b\u7684\u652f\u6301\uff0c\u5b83\u5b9e\u73b0\u4e86\u4e0eCPU\u5f20\u91cf\u540c\u6837\u7684\u529f\u80fd\uff0c\u4f46\u662f\u5b83\u4f7f\u7528GPU\u8fdb\u8ba1\u7b97\u3002</p> <p>\u5b83\u662f\u61d2\u52a0\u8f7d\u7684\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u968f\u65f6\u5bfc\u5165\u5b83\uff0c\u5e76\u4f7f\u7528 <code>is_available()</code> \u6765\u51b3\u5b9a\u662f\u5426\u8ba9\u4f60\u7684\u7cfb\u7edf\u652f\u6301CUDA\u3002</p> <p>CUDA semantics \u6709\u5173\u4e8e\u4f7f\u7528CUDA\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.current_blas_handle()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2acublasHandle_t\u6307\u9488\u7ed9\u5f53\u524d\u7684cuBLAS\u5904\u7406\u3002</p> <pre><code>torch.cuda.current_device()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u9009\u62e9\u5730\u8bbe\u5907\u7d22\u5f15\u3002</p> <pre><code>torch.cuda.current_stream()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u9009\u62e9\u5730 <code>Stream</code>\u3002</p> <pre><code>class torch.cuda.device(device)\n</code></pre> <p>Context-manager \u7528\u6765\u6539\u53d8\u9009\u62e9\u7684\u8bbe\u5907\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int) \u2013 \u8981\u9009\u62e9\u7684\u8bbe\u5907\u7d22\u5f15\u3002\u5982\u679c\u8fd9\u4e2a\u53c2\u6570\u662f\u8d1f\u6570\u6216\u8005\u662f <code>None</code>\uff0c\u90a3\u4e48\u5b83\u4e0d\u4f1a\u8d77\u4efb\u4f55\u4f5c\u7528\u3002 <pre><code>torch.cuda.device_count()\n</code></pre> <p>\u8fd4\u56de\u53ef\u7528\u7684GPU\u6570\u91cf\u3002</p> <pre><code>torch.cuda.device_ctx_manager\n</code></pre> <p><code>torch.cuda.device</code> \u7684\u522b\u540d\u3002</p> <pre><code>class torch.cuda.device_of(obj)\n</code></pre> <p>Context-manager \u5c06\u5f53\u524d\u7684\u8bbe\u5907\u6539\u53d8\u6210\u4f20\u5165\u7684\u5bf9\u8c61\u3002.</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u5f20\u91cf\u6216\u8005\u5b58\u50a8\u4f5c\u4e3a\u53c2\u6570\u3002\u5982\u679c\u4f20\u5165\u7684\u5bf9\u8c61\u6ca1\u6709\u5206\u914d\u5728GPU\u4e0a\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u662f\u65e0\u6548\u7684\u3002</p> \u53c2\u6570: obj (Tensor \u6216\u8005 Storage) \u2013 \u5206\u914d\u5728\u5df2\u9009\u62e9\u7684\u8bbe\u5907\u4e0a\u7684\u5bf9\u8c61\u3002 <pre><code>torch.cuda.empty_cache()\n</code></pre> <p>\u91ca\u653e\u7f13\u5b58\u5206\u914d\u5668\u5f53\u524d\u6301\u6709\u7684\u6240\u6709\u672a\u5360\u7528\u7684\u7f13\u5b58\u663e\u5b58\uff0c\u4f7f\u5176\u53ef\u4ee5\u7528\u5728\u5176\u4ed6GPU\u5e94\u7528\u4e14\u53ef\u4ee5\u5728 <code>nvidia-smi</code>\u53ef\u89c6\u5316\u3002</p> <p>\u6ce8\u610f</p> <p><code>empty_cache()</code> \u5e76\u4e0d\u4f1a\u589e\u52a0PyTorch\u53ef\u4ee5\u4f7f\u7528\u7684GPU\u663e\u5b58\u7684\u5927\u5c0f\u3002 \u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u6765\u83b7\u53d6\u66f4\u591a\u7684GPU\u663e\u5b58\u7ba1\u7406\u7684\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.get_device_capability(device)\n</code></pre> <p>\u83b7\u53d6\u4e00\u4e2a\u8bbe\u5907\u7684cuda\u5bb9\u91cf\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9700\u8981\u8fd4\u56de\u5bb9\u91cf\u7684\u8bbe\u5907\u3002\u5982\u679c\u8fd9\u4e2a\u53c2\u6570\u4f20\u5165\u7684\u662f\u8d1f\u6570\uff0c\u90a3\u4e48\u8fd9\u4e2a\u65b9\u6cd5\u4e0d\u4f1a\u8d77\u4efb\u4f55\u4f5c\u7528\u3002\u5982\u679c<code>device</code>\u662f<code>None</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u4f1a\u901a\u8fc7 <code>current_device()</code>\u4f20\u5165\u5f53\u524d\u8bbe\u5907\u3002 \u8fd4\u56de: \u8bbe\u5907\u7684\u6700\u5927\u548c\u6700\u5c0f\u7684cuda\u5bb9\u91cf\u3002 --- --- \u8fd4\u56de \u7c7b\u578b: tuple(int, int) --- --- <pre><code>torch.cuda.get_device_name(device)\n</code></pre> <p>\u83b7\u53d6\u8bbe\u5907\u540d\u79f0\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9700\u8981\u8fd4\u56de\u540d\u79f0\u7684\u8bbe\u5907\u3002\u5982\u679c\u53c2\u6570\u662f\u8d1f\u6570\uff0c\u90a3\u4e48\u5c06\u4e0d\u8d77\u4f5c\u7528\u3002\u5982\u679c<code>device</code>\u662f<code>None</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u4f1a\u901a\u8fc7 <code>current_device()</code>\u4f20\u5165\u5f53\u524d\u8bbe\u5907\u3002 <pre><code>torch.cuda.init()\n</code></pre> <p>\u521d\u59cb\u5316PyTorch\u7684CUDA\u72b6\u6001\u3002\u5982\u679c\u4f60\u901a\u8fc7C API\u4e0ePyTorch\u8fdb\u884c\u4ea4\u4e92\uff0c\u4f60\u53ef\u80fd\u9700\u8981\u663e\u5f0f\u8c03\u7528\u8fd9\u4e2a\u65b9\u6cd5\u3002\u53ea\u6709CUDA\u7684\u521d\u59cb\u5316\u5b8c\u6210\uff0cCUDA\u7684\u529f\u80fd\u624d\u4f1a\u7ed1\u5b9a\u5230Python\u3002\u7528\u6237\u4e00\u822c\u4e0d\u5e94\u8be5\u9700\u8981\u8fd9\u4e2a\uff0c\u56e0\u4e3a\u6240\u6709PyTorch\u7684CUDA\u65b9\u6cd5\u90fd\u4f1a\u81ea\u52a8\u5728\u9700\u8981\u7684\u65f6\u5019\u521d\u59cb\u5316CUDA\u3002</p> <p>\u5982\u679cCUDA\u7684\u72b6\u6001\u5df2\u7ecf\u521d\u59cb\u5316\u4e86\uff0c\u5c06\u4e0d\u8d77\u4efb\u4f55\u4f5c\u7528\u3002</p> <pre><code>torch.cuda.is_available()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2abool\u503c\uff0c\u8868\u793a\u5f53\u524dCUDA\u662f\u5426\u53ef\u7528\u3002</p> <pre><code>torch.cuda.max_memory_allocated(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u5f20\u91cf\u7684\u6700\u5927GPU\u663e\u5b58\u4f7f\u7528\u91cf(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device or int, optional) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.max_memory_cached(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u7f13\u5b58\u5206\u914d\u5668\u7ba1\u7406\u7684\u6700\u5927GPU\u663e\u5b58(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.memory_allocated(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u5f53\u524dGPU\u663e\u5b58\u4f7f\u7528\u91cf(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u8fd9\u53ef\u80fd\u6bd4 <code>nvidia-smi</code> \u663e\u793a\u7684\u6570\u91cf\u5c11\uff0c\u56e0\u4e3a\u4e00\u4e9b\u6ca1\u6709\u4f7f\u7528\u7684\u663e\u5b58\u4f1a\u88ab\u7f13\u5b58\u5206\u914d\u5668\u6301\u6709\uff0c\u4e14\u4e00\u4e9b\u4e0a\u4e0b\u6587\u9700\u8981\u5728GPU\u4e2d\u521b\u5efa\u3002\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.memory_cached(device=None)\n</code></pre> <p>\u8fd4\u56de\u7531\u7f13\u5b58\u5206\u914d\u5668\u7ba1\u7406\u7684\u5f53\u524dGPU\u663e\u5b58(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.set_device(device)\n</code></pre> <p>\u8bbe\u7f6e\u5f53\u524d\u8bbe\u5907\u3002</p> <p>\u4e0d\u9f13\u52b1\u4f7f\u7528\u6b64\u529f\u80fd\u4ee5\u652f\u6301 <code>device</code>.\u3002\u5728\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6700\u597d\u4f7f\u7528<code>CUDA_VISIBLE_DEVICES</code>\u73af\u5883\u53d8\u91cf\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c\u53c2\u6570\u662f\u8d1f\u6570\uff0c\u5c06\u4e0d\u4f1a\u8d77\u4efb\u4f55\u4f5c\u7528\u3002 <pre><code>torch.cuda.stream(stream)\n</code></pre> <p>\u7ed9\u5b9a\u6d41\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <p>\u6240\u6709CUDA\u5728\u4e0a\u4e0b\u6587\u4e2d\u6392\u961f\u7684\u5185\u6838\u5c06\u4f1a\u88ab\u6dfb\u52a0\u5230\u9009\u62e9\u7684\u6d41\u4e2d\u3002</p> \u53c2\u6570: stream (Stream) \u2013 \u9009\u62e9\u7684\u6d41\u3002\u5982\u679c\u4e3a<code>None</code>\uff0c\u8fd9\u4e2a\u7ba1\u7406\u5668\u5c06\u4e0d\u8d77\u4efb\u4f55\u4f5c\u7528\u3002 <p>\u6ce8\u610f</p> <p>\u6d41\u662f\u9488\u5bf9\u6bcf\u4e2a\u8bbe\u5907\u7684\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u53ea\u66f4\u6539\u5f53\u524d\u9009\u62e9\u8bbe\u5907\u7684\u201c\u5f53\u524d\u6d41\u201d\u3002\u9009\u62e9\u4e00\u4e2a\u4e0d\u540c\u7684\u8bbe\u5907\u6d41\u662f\u4e0d\u5141\u8bb8\u7684\u3002</p> <pre><code>torch.cuda.synchronize()\n</code></pre> <p>\u7b49\u5f85\u6240\u6709\u5f53\u524d\u8bbe\u5907\u7684\u6240\u6709\u6d41\u5b8c\u6210\u3002</p>"},{"location":"1.0/cuda/#_1","title":"\u968f\u673a\u6570\u751f\u6210\u5668","text":"<pre><code>torch.cuda.get_rng_state(device=-1)\n</code></pre> <p>\u4ee5ByteTensor\u7684\u5f62\u5f0f\u8fd4\u56de\u5f53\u524dGPU\u7684\u968f\u673a\u6570\u751f\u6210\u5668\u7684\u72b6\u6001\u3002</p> \u53c2\u6570: device (int, \u53ef\u9009\u7684) \u2013 \u9700\u8981\u8fd4\u56deRNG\u72b6\u6001\u7684\u76ee\u6807\u8bbe\u5907\u3002\u9ed8\u8ba4\uff1a-1 (\u4f8b\u5982\uff0c\u4f7f\u7528\u5f53\u524d\u8bbe\u5907)\u3002 <p>\u8b66\u544a</p> <p>\u6b64\u51fd\u6570\u4f1a\u7acb\u5373\u521d\u59cb\u5316CUDA\u3002</p> <pre><code>torch.cuda.set_rng_state(new_state, device=-1)\n</code></pre> <p>\u8bbe\u7f6e\u5f53\u524dGPU\u7684\u968f\u673a\u6570\u751f\u6210\u5668\u72b6\u6001\u3002</p> \u53c2\u6570: new_state (torch.ByteTensor) \u2013 \u76ee\u6807\u72b6\u6001 <pre><code>torch.cuda.manual_seed(seed)\n</code></pre> <p>\u8bbe\u7f6e\u4e3a\u5f53\u524dGPU\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u3002\u5982\u679cCUDA\u4e0d\u53ef\u7528\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u8c03\u7528\u6b64\u51fd\u6570\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u88ab\u9759\u9ed8\u5730\u5ffd\u7565\u3002</p> \u53c2\u6570: seed (int) \u2013 \u76ee\u6807\u79cd\u5b50\u3002 <p>\u8b66\u544a</p> <p>\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u591aGPU\u6a21\u578b\uff0c\u90a3\u4e48\u8fd9\u4e2a\u51fd\u6570\u4e0d\u5177\u6709\u786e\u5b9a\u6027\u3002\u8bbe\u7f6e\u7528\u4e8e\u5728\u6240\u6709GPU\u4e0a\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\uff0c\u4f7f\u7528 <code>manual_seed_all()</code>.</p> <pre><code>torch.cuda.manual_seed_all(seed)\n</code></pre> <p>\u8bbe\u7f6e\u7528\u4e8e\u5728\u6240\u6709GPU\u4e0a\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u3002 \u5982\u679cCUDA\u4e0d\u53ef\u7528\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u8c03\u7528\u6b64\u51fd\u6570\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u88ab\u9759\u9ed8\u5730\u5ffd\u7565\u3002</p> \u53c2\u6570: seed (int) \u2013 \u76ee\u6807\u79cd\u5b50\u3002 <pre><code>torch.cuda.seed()\n</code></pre> <p>\u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u5f53\u524dGPU\u7684\u968f\u673a\u6570\u3002 \u5982\u679cCUDA\u4e0d\u53ef\u7528\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u8c03\u7528\u6b64\u51fd\u6570\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u88ab\u9759\u9ed8\u5730\u5ffd\u7565\u3002</p> <p>\u8b66\u544a</p> <p>\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u591aGPU\u6a21\u578b\uff0c\u6b64\u51fd\u6570\u5c06\u53ea\u521d\u59cb\u5316\u4e00\u4e2aGPU\u4e0a\u7684\u79cd\u5b50\u3002\u5728\u6240\u6709GPU\u4e0a\u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u968f\u673a\u6570\uff0c \u4f7f\u7528 <code>seed_all()</code>.</p> <pre><code>torch.cuda.seed_all()\n</code></pre> <p>\u5728\u6240\u6709GPU\u4e0a\u5c06\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u8bbe\u7f6e\u4e3a\u968f\u673a\u6570\u3002 \u5982\u679cCUDA\u4e0d\u53ef\u7528\uff0c\u53ef\u4ee5\u5b89\u5168\u5730\u8c03\u7528\u6b64\u51fd\u6570\uff1b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5c06\u88ab\u9759\u9ed8\u5730\u5ffd\u7565\u3002</p> <pre><code>torch.cuda.initial_seed()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524dGPU\u7684\u5f53\u524d\u968f\u673a\u79cd\u5b50\u3002</p> <p>\u8b66\u544a</p> <p>\u6b64\u51fd\u6570\u4f1a\u7acb\u5373\u521d\u59cb\u5316CUDA\u3002</p>"},{"location":"1.0/cuda/#_2","title":"\u901a\u4fe1\u96c6\u5408","text":"<pre><code>torch.cuda.comm.broadcast(tensor, devices)\n</code></pre> <p>\u5c06\u5f20\u91cf\u5e7f\u64ad\u5230\u591a\u4e2aGPU\u3002</p> <p>| \u53c2\u6570: | </p> <ul> <li>tensor (Tensor) \u2013 \u9700\u8981\u5e7f\u64ad\u7684\u5f20\u91cf\u3002</li> <li>devices (Iterable) \u2013 \u4e00\u4e2a\u8981\u88ab\u5e7f\u64ad\u7684\u53ef\u8fed\u4ee3\u7684\u5f20\u91cf\u96c6\u5408\u3002\u6ce8\u610f\uff0c\u5b83\u5e94\u8be5\u662f\u8fd9\u6837\u7684\u5f62\u5f0f (src, dst1, dst2, \u2026)\uff0c\u5176\u4e2d\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5e7f\u64ad\u7684\u6e90\u8bbe\u5907\u3002</li> </ul> \u8fd4\u56de: \u4e00\u4e2a\u5305\u542b<code>tensor</code>\u526f\u672c\u7684\u5143\u7ec4\uff0c\u653e\u7f6e\u5728\u4e0e\u8bbe\u5907\u7d22\u5f15\u76f8\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a\u3002 <pre><code>torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)\n</code></pre> <p>\u5c06\u5e8f\u5217\u5f20\u91cf\u5e7f\u64ad\u5230\u6307\u5b9a\u7684GPU\u3002 \u9996\u5148\u5c06\u5c0f\u578b\u5f20\u91cf\u5408\u5e76\u5230\u7f13\u51b2\u533a\u4e2d\u4ee5\u51cf\u5c11\u540c\u6b65\u6b21\u6570\u3002</p> <p>| \u53c2\u6570: | </p> <ul> <li>tensors (sequence) \u2013 \u8981\u88ab\u5e7f\u64ad\u7684\u5f20\u91cf\u3002</li> <li>devices (Iterable) \u2013 \u4e00\u4e2a\u8981\u88ab\u5e7f\u64ad\u7684\u53ef\u8fed\u4ee3\u7684\u5f20\u91cf\u96c6\u5408\u3002\u6ce8\u610f\uff0c\u5b83\u5e94\u8be5\u662f\u8fd9\u6837\u7684\u5f62\u5f0f (src, dst1, dst2, \u2026)\uff0c\u5176\u4e2d\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5e7f\u64ad\u7684\u6e90\u8bbe\u5907\u3002</li> <li>buffer_size (int) \u2013 \u7528\u4e8e\u5408\u5e76\u7684\u7f13\u51b2\u533a\u7684\u6700\u5927\u5927\u5c0f</li> </ul> \u8fd4\u56de: \u4e00\u4e2a\u5305\u542b<code>tensor</code>\u526f\u672c\u7684\u5143\u7ec4\uff0c\u653e\u7f6e\u5728\u4e0e\u8bbe\u5907\u7d22\u5f15\u76f8\u5bf9\u5e94\u7684\u8bbe\u5907\u4e0a\u3002 <pre><code>torch.cuda.comm.reduce_add(inputs, destination=None)\n</code></pre> <p>\u4ece\u591a\u4e2aGPU\u4e0a\u5bf9\u5f20\u91cf\u8fdb\u884c\u6c42\u548c\u3002</p> <p>\u6240\u6709\u8f93\u5165\u5fc5\u987b\u6709\u76f8\u540c\u7684\u5f62\u72b6\u3002</p> <p>| \u53c2\u6570: | </p> <ul> <li>inputs (Iterable__[Tensor]) \u2013 \u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u8981\u6dfb\u52a0\u7684\u5f20\u91cf\u96c6\u5408\u3002</li> <li>destination (int, \u53ef\u9009\u7684) \u2013 \u8f93\u51fa\u6240\u5728\u7684\u8bbe\u5907\u3002(\u9ed8\u8ba4\u503c: \u5f53\u524d\u8bbe\u5907)\u3002</li> </ul> \u8fd4\u56de: \u4e00\u4e2a\u5305\u542b\u6309\u5143\u7d20\u76f8\u52a0\u7684\u6240\u6709\u8f93\u5165\u7684\u548c\u7684\u5f20\u91cf\uff0c\u5728 <code>destination</code> \u8bbe\u5907\u4e0a\u3002 <pre><code>torch.cuda.comm.scatter(tensor, devices, chunk_sizes=None, dim=0, streams=None)\n</code></pre> <p>\u5c06\u5f20\u91cf\u5206\u6563\u5728\u591a\u4e2aGPU\u4e0a\u3002</p> <p>| \u53c2\u6570: | </p> <ul> <li>tensor (Tensor) \u2013 \u8981\u5206\u6563\u7684\u5f20\u91cf.</li> <li>devices (Iterable__[int]) \u2013 \u53ef\u8fed\u4ee3\u7684\u6570\u5b57\u96c6\u5408\uff0c\u6307\u660e\u5728\u54ea\u4e2a\u8bbe\u5907\u4e0a\u7684\u5f20\u91cf\u8981\u88ab\u5206\u6563\u3002</li> <li>chunk_sizes (Iterable__[int]__, \u53ef\u9009\u7684) \u2013 \u6bcf\u4e2a\u8bbe\u5907\u4e0a\u653e\u7f6e\u7684\u5757\u7684\u5927\u5c0f\u3002\u5b83\u5e94\u8be5\u548c<code>devices</code>\u7684\u957f\u5ea6\u76f8\u7b49\uff0c\u5e76\u76f8\u52a0\u7b49\u4e8e<code>tensor.size(dim)</code>\u3002\u5982\u679c\u6ca1\u6709\u6307\u5b9a\uff0c\u5f20\u91cf\u5c06\u4f1a\u88ab\u5206\u6563\u6210\u76f8\u540c\u7684\u5757\u3002</li> <li>dim (int, \u53ef\u9009\u7684) \u2013 \u5206\u5757\u5f20\u91cf\u6240\u5728\u7684\u7ef4\u5ea6\u3002</li> </ul> \u8fd4\u56de: \u4e00\u4e2a\u5305\u542b<code>tensor</code>\u5757\u7684\u5143\u7ec4\uff0c\u5206\u6563\u5728\u7ed9\u5b9a\u7684<code>devices</code>\u4e0a\u3002 <pre><code>torch.cuda.comm.gather(tensors, dim=0, destination=None)\n</code></pre> <p>\u4ece\u591a\u4e2aGPU\u6536\u96c6\u5f20\u91cf\u3002</p> <p>\u5728\u6240\u6709\u7ef4\u5ea6\u4e2d\u4e0e<code>dim</code>\u4e0d\u540c\u7684\u5f20\u91cf\u5c3a\u5bf8\u5fc5\u987b\u5339\u914d\u3002</p> <p>| \u53c2\u6570: | </p> <ul> <li>tensors (Iterable__[Tensor]) \u2013 \u53ef\u8fed\u4ee3\u7684\u5f20\u91cf\u96c6\u5408\u3002</li> <li>dim (int) \u2013 \u7eac\u5ea6\uff0c\u5f20\u91cf\u5c06\u4f1a\u5728\u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u88ab\u8fde\u63a5\u3002</li> <li>destination (int, \u53ef\u9009\u7684) \u2013 \u8f93\u51fa\u8bbe\u5907(-1 \u8868\u793a CPU, \u9ed8\u8ba4\u503c: \u5f53\u524d\u8bbe\u5907)</li> </ul> \u8fd4\u56de: \u5728<code>destination</code> \u8bbe\u5907\u4e0a\u7684\u5f20\u91cf\uff0c\u8fd9\u662f\u6cbf\u7740<code>dim</code>\u8fde\u63a5\u5f20\u91cf\u7684\u7ed3\u679c\u3002"},{"location":"1.0/cuda/#_3","title":"\u6d41\u548c\u4e8b\u4ef6","text":"<pre><code>class torch.cuda.Stream\n</code></pre> <p>\u56f4\u7ed5CUDA\u6d41\u7684\u5305\u88c5\u5668\u3002</p> <p>CUDA\u6d41\u662f\u5c5e\u4e8e\u7279\u5b9a\u8bbe\u5907\u7684\u7ebf\u6027\u6267\u884c\u5e8f\u5217\uff0c\u72ec\u7acb\u4e8e\u5176\u4ed6\u6d41\u3002 \u67e5\u770b CUDA semantics \u83b7\u53d6\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002</p> <p>\u53c2\u6570:</p> <ul> <li>device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u8981\u5728\u5176\u4e0a\u5206\u914d\u6d41\u7684\u8bbe\u5907\u3002 \u5982\u679c <code>device</code> \u4e3a<code>None</code>(\u9ed8\u8ba4\u503c\uff09\u6216\u8d1f\u6574\u6570\uff0c\u5219\u5c06\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u3002</li> <li>priority (int, \u53ef\u9009\u7684) \u2013 \u6d41\u7684\u4f18\u5148\u7ea7\u3002\u6570\u5b57\u8d8a\u5c0f\uff0c\u4f18\u5148\u7ea7\u8d8a\u9ad8\u3002</li> </ul> <pre><code>query()\n</code></pre> <p>\u68c0\u67e5\u63d0\u4ea4\u7684\u6240\u6709\u5de5\u4f5c\u662f\u5426\u5df2\u5b8c\u6210\u3002</p> \u8fd4\u56de: \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u6b64\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u662f\u5426\u90fd\u5df2\u5b8c\u6210\u3002 <pre><code>record_event(event=None)\n</code></pre> <p>\u8bb0\u5f55\u4e00\u4e2a\u4e8b\u4ef6\u3002</p> \u53c2\u6570: event (Event, \u53ef\u9009\u7684) \u2013 \u9700\u8981\u8bb0\u5f55\u7684\u4e8b\u4ef6\u3002\u5982\u679c\u6ca1\u6709\u7ed9\u51fa\uff0c\u5c06\u5206\u914d\u4e00\u4e2a\u65b0\u7684\u3002 \u8fd4\u56de: \u8bb0\u5f55\u7684\u4e8b\u4ef6\u3002 --- --- <pre><code>synchronize()\n</code></pre> <p>\u7b49\u5f85\u6b64\u6d41\u4e2d\u7684\u6240\u6709\u5185\u6838\u5b8c\u6210\u3002</p> <p>\u6ce8\u610f</p> <p>\u8fd9\u662f\u4e00\u4e2a\u56f4\u7ed5 <code>cudaStreamSynchronize()</code>\u7684\u5305\u88c5\uff1a \u67e5\u770b CUDA \u6587\u6863 \u83b7\u53d6\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002</p> <pre><code>wait_event(event)\n</code></pre> <p>\u4f7f\u63d0\u4ea4\u7ed9\u6d41\u7684\u6240\u6709\u672a\u6765\u5de5\u4f5c\u7b49\u5f85\u4e8b\u4ef6\u3002</p> \u53c2\u6570: event (Event) \u2013 \u9700\u8981\u7b49\u5f85\u7684\u4e8b\u4ef6\u3002 <p>\u6ce8\u610f</p> <p>\u8fd9\u662f\u4e00\u4e2a\u56f4\u7ed5 <code>cudaStreamWaitEvent()</code>\u7684\u5305\u88c5\uff1a \u67e5\u770b CUDA \u6587\u6863 \u83b7\u53d6\u66f4\u8be6\u7ec6\u7684\u4fe1\u606f\u3002</p> <p>\u6b64\u51fd\u6570\u8fd4\u56de\u65f6\u65e0\u9700\u7b49\u5f85<code>event</code>\uff1a \u53ea\u6709\u672a\u6765\u7684\u64cd\u4f5c\u53d7\u5230\u5f71\u54cd\u3002</p> <pre><code>wait_stream(stream)\n</code></pre> <p>\u4e0e\u53e6\u4e00\u4e2a\u6d41\u540c\u6b65\u3002</p> <p>\u63d0\u4ea4\u7ed9\u6b64\u6d41\u7684\u6240\u6709\u672a\u6765\u5de5\u4f5c\u5c06\u7b49\u5230\u6240\u6709\u5185\u6838\u5728\u547c\u53eb\u5b8c\u6210\u65f6\u63d0\u4ea4\u7ed9\u7ed9\u5b9a\u6d41\u3002</p> \u53c2\u6570: stream (Stream) \u2013 \u8981\u540c\u6b65\u7684\u6d41\u3002 <p>\u6ce8\u610f</p> <p>\u6b64\u51fd\u6570\u8fd4\u56de\u65f6\u4e0d\u7b49\u5f85<code>stream</code>\u4e2d\u5f53\u524d\u6392\u961f\u7684\u5185\u6838 \uff1a \u53ea\u6709\u672a\u6765\u7684\u64cd\u4f5c\u53d7\u5230\u5f71\u54cd\u3002</p> <pre><code>class torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False, _handle=None)\n</code></pre> <p>\u56f4\u7ed5CUDA\u4e8b\u4ef6\u7684\u5305\u88c5\u3002</p> <p>\u53c2\u6570:</p> <ul> <li>enable_timing (bool) \u2013 \u8868\u793a\u4e8b\u4ef6\u662f\u5426\u5e94\u8be5\u6d4b\u91cf\u65f6\u95f4(\u9ed8\u8ba4\u503c\uff1a<code>False</code>\uff09</li> <li>blocking (bool) \u2013 \u5982\u679c\u662f<code>True</code>\uff0c <code>wait()</code> \u5c06\u4f1a\u963b\u585e (\u9ed8\u8ba4\u503c: <code>False</code>)</li> <li>interprocess (bool) \u2013 \u5982\u679c\u662f <code>True</code>\uff0c\u4e8b\u4ef6\u5c06\u4f1a\u5728\u8fdb\u7a0b\u4e2d\u5171\u4eab (\u9ed8\u8ba4\u503c: <code>False</code>)</li> </ul> <pre><code>elapsed_time(end_event)\n</code></pre> <p>\u8fd4\u56de\u8bb0\u5f55\u4e8b\u4ef6\u4e4b\u524d\u7ecf\u8fc7\u7684\u65f6\u95f4\u3002</p> <pre><code>ipc_handle()\n</code></pre> <p>\u8fd4\u56de\u6b64\u4e8b\u4ef6\u7684IPC\u53e5\u67c4\u3002</p> <pre><code>query()\n</code></pre> <p>\u68c0\u6d4b\u4e8b\u4ef6\u662f\u5426\u88ab\u8bb0\u5f55\u3002</p> \u8fd4\u56de: \u4e00\u4e2a\u5e03\u5c14\u503c\uff0c\u8868\u793a\u4e8b\u4ef6\u662f\u5426\u88ab\u8bb0\u5f55\u3002 <pre><code>record(stream=None)\n</code></pre> <p>\u8bb0\u5f55\u7ed9\u5b9a\u6d41\u7684\u4e00\u4e2a\u4e8b\u4ef6\u3002</p> <pre><code>synchronize()\n</code></pre> <p>\u548c\u4e00\u4e2a\u4e8b\u4ef6\u540c\u6b65\u3002</p> <pre><code>wait(stream=None)\n</code></pre> <p>\u4f7f\u7ed9\u5b9a\u7684\u6d41\u7b49\u5f85\u4e00\u4e2a\u4e8b\u4ef6\u3002</p>"},{"location":"1.0/cuda/#_4","title":"\u663e\u5b58\u7ba1\u7406","text":"<pre><code>torch.cuda.empty_cache()\n</code></pre> <p>\u91ca\u653e\u5f53\u524d\u7531\u7f13\u5b58\u5206\u914d\u5668\u4fdd\u5b58\u7684\u6240\u6709\u672a\u5360\u7528\u7684\u7f13\u5b58\u663e\u5b58\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5728\u5176\u4ed6GPU\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u7f13\u5b58\u5e76\u5728<code>nvidia-smi</code>\u4e2d\u53ef\u89c1\u3002</p> <p>\u6ce8\u610f</p> <p><code>empty_cache()</code> \u4e0d\u4f1a\u589e\u52a0PyTorch\u53ef\u7528\u7684GPU\u663e\u5b58\u91cf\u3002 \u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u4ee5\u4e86\u89e3\u66f4\u591aGPU\u663e\u5b58\u7ba1\u7406\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.memory_allocated(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u5f53\u524dGPU\u663e\u5b58\u4f7f\u7528\u91cf(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9009\u5b9a\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u8fd9\u53ef\u80fd\u6bd4 <code>nvidia-smi</code> \u663e\u793a\u7684\u6570\u91cf\u5c11\uff0c\u56e0\u4e3a\u4e00\u4e9b\u6ca1\u6709\u4f7f\u7528\u7684\u663e\u5b58\u4f1a\u88ab\u7f13\u5b58\u5206\u914d\u5668\u6301\u6709\uff0c\u4e14\u4e00\u4e9b\u4e0a\u4e0b\u6587\u9700\u8981\u5728GPU\u4e2d\u521b\u5efa\u3002\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.max_memory_allocated(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u5f20\u91cf\u7684\u6700\u5927GPU\u663e\u5b58\u4f7f\u7528\u91cf(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013  \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.memory_cached(device=None)\n</code></pre> <p>\u8fd4\u56de\u7531\u7f13\u5b58\u5206\u914d\u5668\u7ba1\u7406\u7684\u5f53\u524dGPU\u663e\u5b58(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device or int, \u53ef\u9009\u7684) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p> <pre><code>torch.cuda.max_memory_cached(device=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8bbe\u5907\u7684\u7f13\u5b58\u5206\u914d\u5668\u7ba1\u7406\u7684\u6700\u5927GPU\u663e\u5b58(\u4ee5\u5b57\u8282\u4e3a\u5355\u4f4d\uff09\u3002</p> \u53c2\u6570: device (torch.device \u6216\u8005 int, \u53ef\u9009\u7684) \u2013 \u9009\u62e9\u7684\u8bbe\u5907\u3002\u5982\u679c <code>device</code> \u662f<code>None</code>(\u9ed8\u8ba4\u7684\uff09\uff0c\u5c06\u8fd4\u56de <code>current_device()</code>\u8fd4\u56de\u7684\u5f53\u524d\u8bbe\u5907\u7684\u6570\u636e\u3002 <p>\u6ce8\u610f</p> <p>\u67e5\u770b \u663e\u5b58\u7ba1\u7406 \u90e8\u5206\u4e86\u89e3\u66f4\u591a\u5173\u4e8eGPU\u663e\u5b58\u7ba1\u7406\u90e8\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"1.0/cuda/#nvidia-tools-extension-nvtx","title":"NVIDIA Tools Extension (NVTX)","text":"<pre><code>torch.cuda.nvtx.mark(msg)\n</code></pre> <p>\u63cf\u8ff0\u67d0\u4e2a\u65f6\u523b\u53d1\u751f\u7684\u77ac\u65f6\u4e8b\u4ef6\u3002</p> \u53c2\u6570: msg (string) \u2013 \u4e0e\u65f6\u95f4\u76f8\u5173\u7684ASCII\u4fe1\u606f\u3002 <pre><code>torch.cuda.nvtx.range_push(msg)\n</code></pre> <p>\u5c06\u8303\u56f4\u63a8\u5230\u5d4c\u5957\u8303\u56f4\u8de8\u5ea6\u7684\u5806\u6808\u4e0a\u3002 \u8fd4\u56de\u542f\u52a8\u8303\u56f4\u7684\u4ece\u96f6\u5f00\u59cb\u7684\u6df1\u5ea6\u3002</p> \u53c2\u6570: msg (string) \u2013 \u4e0e\u65f6\u95f4\u76f8\u5173\u7684ASCII\u4fe1\u606f\u3002 <pre><code>torch.cuda.nvtx.range_pop()\n</code></pre> <p>\u4ece\u4e00\u5806\u5d4c\u5957\u8303\u56f4\u8de8\u5ea6\u4e2d\u5f39\u51fa\u4e00\u4e2a\u8303\u56f4\u3002 \u8fd4\u56de\u7ed3\u675f\u8303\u56f4\u7684\u4ece\u96f6\u5f00\u59cb\u7684\u6df1\u5ea6\u3002</p>"},{"location":"1.0/data/","title":"torch.utils.data","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <pre><code>class torch.utils.data.Dataset\n</code></pre> <p>\u8868\u793a\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b\u3002</p> <p>\u6240\u6709\u7528\u5230\u7684\u6570\u636e\u96c6\u90fd\u5fc5\u987b\u662f\u5176\u5b50\u7c7b\u3002\u8fd9\u4e9b\u5b50\u7c7b\u90fd\u5fc5\u987b\u91cd\u5199\u4ee5\u4e0b\u65b9\u6cd5\uff1a<code>__len__</code>\uff1a\u5b9a\u4e49\u4e86\u6570\u636e\u96c6\u7684\u89c4\u6a21\uff1b<code>__getitem__</code>\uff1a\u652f\u63010\u5230len(self)\u8303\u56f4\u5185\u7684\u6574\u6570\u7d22\u5f15\u3002</p> <pre><code>class torch.utils.data.TensorDataset(*tensors)\n</code></pre> <p>\u7528\u4e8e\u5f20\u91cf\u5c01\u88c5\u7684Dataset\u7c7b\u3002</p> <p>\u5f20\u91cf\u53ef\u4ee5\u6cbf\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5212\u5206\u4e3a\u6837\u4f8b\u4e4b\u540e\u8fdb\u884c\u68c0\u7d22\u3002</p> \u53c2\u6570\uff1a *tensors (Tensor) \u2013 \u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u76f8\u540c\u7684\u5f20\u91cf\u3002 <pre><code>class torch.utils.data.ConcatDataset(datasets)\n</code></pre> <p>\u7528\u4e8e\u878d\u5408\u4e0d\u540c\u6570\u636e\u96c6\u7684Dataset\u7c7b\u3002\u76ee\u7684\uff1a\u7ec4\u5408\u4e0d\u540c\u7684\u73b0\u6709\u6570\u636e\u96c6\uff0c\u9274\u4e8e\u878d\u5408\u64cd\u4f5c\u662f\u540c\u65f6\u6267\u884c\u7684\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u53ef\u4ee5\u5f88\u5927\u3002</p> \u53c2\u6570\uff1a datasets(\u5e8f\u5217\uff09\u2013 \u8981\u878d\u5408\u7684\u6570\u636e\u96c6\u5217\u8868\u3002 <pre><code>class torch.utils.data.Subset(dataset, indices)\n</code></pre> <p>\u7528\u7d22\u5f15\u6307\u5b9a\u7684\u6570\u636e\u96c6\u5b50\u96c6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>dataset(Dataset\uff09\u2013 \u539f\u6570\u636e\u96c6\u3002</li> <li>indices(\u5e8f\u5217\uff09\u2013 \u5168\u96c6\u4e2d\u9009\u62e9\u4f5c\u4e3a\u5b50\u96c6\u7684\u7d22\u5f15\u3002</li> </ul> <pre><code>class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)\n</code></pre> <p>\u6570\u636e\u52a0\u8f7d\u5668\u3002\u7ec4\u5408\u6570\u636e\u96c6\u548c\u91c7\u6837\u5668\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u5355\u8fdb\u7a0b\u6216\u591a\u8fdb\u7a0b\u8fed\u4ee3\u5668\u3002</p> <p>\u53c2\u6570\uff1a  *   dataset(Dataset) \u2013 \u8981\u52a0\u8f7d\u6570\u636e\u7684\u6570\u636e\u96c6\u3002 *   batch_size(int, \u53ef\u9009) \u2013 \u6bcf\u4e00\u6279\u8981\u52a0\u8f7d\u591a\u5c11\u6570\u636e(\u9ed8\u8ba4\uff1a<code>1</code>\uff09\u3002 *   shuffle(bool, \u53ef\u9009) \u2013 \u5982\u679c\u6bcf\u4e00\u4e2aepoch\u5185\u8981\u6253\u4e71\u6570\u636e\uff0c\u5c31\u8bbe\u7f6e\u4e3a<code>True</code>(\u9ed8\u8ba4\uff1a<code>False</code>\uff09\u3002 *   sampler(Sampler, \u53ef\u9009\uff09\u2013 \u5b9a\u4e49\u4e86\u4ece\u6570\u636e\u96c6\u91c7\u6570\u636e\u7684\u7b56\u7565\u3002\u5982\u679c\u8fd9\u4e00\u9009\u9879\u6307\u5b9a\u4e86\uff0c<code>shuffle</code>\u5fc5\u987b\u662fFalse\u3002 *   batch_sampler(Sampler, \u53ef\u9009\uff09\u2013 \u7c7b\u4f3c\u4e8esampler\uff0c\u4f46\u662f\u6bcf\u6b21\u8fd4\u56de\u4e00\u6279\u7d22\u5f15\u3002\u548c<code>batch_size</code>\uff0c<code>shuffle</code>\uff0c<code>sampler</code>\uff0c<code>drop_last</code>\u4e92\u76f8\u51b2\u7a81\u3002 *   num_workers(int, \u53ef\u9009) \u2013 \u52a0\u8f7d\u6570\u636e\u7684\u5b50\u8fdb\u7a0b\u6570\u91cf\u30020\u8868\u793a\u4e3b\u8fdb\u7a0b\u52a0\u8f7d\u6570\u636e(\u9ed8\u8ba4\uff1a<code>0</code>\uff09\u3002 *   collate_fn(\u53ef\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u5f52\u5e76\u6837\u4f8b\u5217\u8868\u6765\u7ec4\u6210\u5c0f\u6279\u3002 *   pin_memory(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u4f1a\u5728\u8fd4\u56de\u524d\u5c06\u5f20\u91cf\u62f7\u8d1d\u5230CUDA\u9501\u9875\u5185\u5b58\u3002 *   drop_last(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u80fd\u4e0d\u80fd\u88ab\u6279\u5927\u5c0f\u6574\u9664\uff0c\u8be5\u9009\u9879\u8bbe\u4e3a<code>True</code>\u540e\u4e0d\u4f1a\u628a\u6700\u540e\u7684\u6b8b\u7f3a\u6279\u4f5c\u4e3a\u8f93\u5165\uff1b\u5982\u679c\u8bbe\u7f6e\u4e3a<code>False</code>\uff0c\u6700\u540e\u4e00\u4e2a\u6279\u5c06\u4f1a\u7a0d\u5fae\u5c0f\u4e00\u70b9\u3002(\u9ed8\u8ba4\uff1a<code>False</code>\uff09 *   timeout(\u6570\u503c , \u53ef\u9009\uff09 \u2013 \u5982\u679c\u662f\u6b63\u6570\uff0c\u5373\u4e3a\u6536\u96c6\u4e00\u4e2a\u6279\u6570\u636e\u7684\u65f6\u95f4\u9650\u5236\u3002\u5fc5\u987b\u975e\u8d1f\u3002(\u9ed8\u8ba4\uff1a<code>0</code>\uff09 *   worker_init_fn(\u53ef\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u5982\u679c\u4e0d\u662f<code>None</code>\uff0c\u6bcf\u4e2aworker\u5b50\u8fdb\u7a0b\u90fd\u4f1a\u4f7f\u7528worker id(\u5728<code>[0, num_workers - 1]</code>\u5185\u7684\u6574\u6570\uff09\u8fdb\u884c\u8c03\u7528\u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u53d1\u751f\u5728\u8bbe\u7f6e\u79cd\u5b50\u4e4b\u540e\u3001\u52a0\u8f7d\u6570\u636e\u4e4b\u524d\u3002(\u9ed8\u8ba4\uff1a<code>None</code>\uff09</p> <p>\u6ce8\u610f\uff1a</p> <p>\u9ed8\u8ba4\u5730\uff0c\u6bcf\u4e2aworker\u90fd\u4f1a\u6709\u5404\u81ea\u7684PyTorch\u79cd\u5b50\uff0c\u8bbe\u7f6e\u65b9\u6cd5\u662f<code>base_seed + worker_id</code>\uff0c\u5176\u4e2d<code>base_seed</code>\u662f\u4e3b\u8fdb\u7a0b\u901a\u8fc7\u968f\u673a\u6570\u751f\u6210\u5668\u751f\u6210\u7684long\u578b\u6570\u3002\u800c\u5176\u5b83\u5e93(\u5982NumPy\uff09\u7684\u79cd\u5b50\u53ef\u80fd\u7531\u521d\u59cbworker\u590d\u5236\u5f97\u5230, \u4f7f\u5f97\u6bcf\u4e00\u4e2aworker\u8fd4\u56de\u76f8\u540c\u7684\u79cd\u5b50\u3002(\u89c1FAQ\u4e2d\u7684My data loader workers return identical random numbers\u90e8\u5206\u3002\uff09\u4f60\u53ef\u4ee5\u7528<code>torch.initial_seed()</code>\u67e5\u770b<code>worker_init_fn</code>\u4e2d\u6bcf\u4e2aworker\u7684PyTorch\u79cd\u5b50\uff0c\u4e5f\u53ef\u4ee5\u5728\u52a0\u8f7d\u6570\u636e\u4e4b\u524d\u8bbe\u7f6e\u5176\u4ed6\u79cd\u5b50\u3002</p> <p>\u8b66\u544a\uff1a</p> <p>\u5982\u679c\u4f7f\u7528\u4e86<code>spawn</code>\u65b9\u6cd5\uff0c\u90a3\u4e48<code>worker_init_fn</code>\u4e0d\u80fd\u662f\u4e0d\u53ef\u5e8f\u5217\u5316\u5bf9\u8c61\uff0c\u5982lambda\u51fd\u6570\u3002</p> <pre><code>torch.utils.data.random_split(dataset, lengths)\n</code></pre> <p>\u4ee5\u7ed9\u5b9a\u7684\u957f\u5ea6\u5c06\u6570\u636e\u96c6\u968f\u673a\u5212\u5206\u4e3a\u4e0d\u91cd\u53e0\u7684\u5b50\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a *   dataset (Dataset) \u2013 \u8981\u5212\u5206\u7684\u6570\u636e\u96c6\u3002 *   lengths(\u5e8f\u5217\uff09\u2013 \u8981\u5212\u5206\u7684\u957f\u5ea6\u3002</p> <pre><code>class torch.utils.data.Sampler(data_source)\n</code></pre> <p>\u6240\u6709\u91c7\u6837\u5668\u7684\u57fa\u7c7b\u3002</p> <p>\u6bcf\u4e2aSampler\u5b50\u7c7b\u5fc5\u987b\u63d0\u4f9b__iter__\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u57fa\u4e8e\u7d22\u5f15\u8fed\u4ee3\u6570\u636e\u96c6\u5143\u7d20\uff0c\u540c\u65f6__len__\u65b9\u6cd5\u53ef\u4ee5\u8fd4\u56de\u6570\u636e\u96c6\u5927\u5c0f\u3002</p> <pre><code>class torch.utils.data.SequentialSampler(data_source)\n</code></pre> <p>\u4ee5\u76f8\u540c\u7684\u987a\u5e8f\u4f9d\u6b21\u91c7\u6837\u3002</p> \u53c2\u6570\uff1a data_source (Dataset) \u2013 \u8981\u4ece\u4e2d\u91c7\u6837\u7684\u6570\u636e\u96c6\u3002 <pre><code>class torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None)\n</code></pre> <p>\u968f\u673a\u91c7\u6837\u5143\u7d20\u3002\u5982\u679creplacement\u4e0d\u8bbe\u7f6e\uff0c\u5219\u4ece\u6253\u4e71\u4e4b\u540e\u7684\u6570\u636e\u96c6\u91c7\u6837\u3002\u5982\u679creplacement\u8bbe\u7f6e\u4e86\uff0c\u90a3\u4e48\u7528\u6237\u53ef\u4ee5\u6307\u5b9a<code>num_samples</code>\u6765\u91c7\u6837\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>data_source (Dataset) \u2013 \u8981\u4ece\u4e2d\u91c7\u6837\u7684\u6570\u636e\u96c6\u3002</li> <li>num_samples (int) \u2013 \u91c7\u6837\u7684\u6837\u672c\u6570\uff0c\u9ed8\u8ba4\u4e3alen(dataset)\u3002</li> <li>replacement (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u66ff\u6362\u91c7\u6837\u3002\u9ed8\u8ba4False\u3002</li> </ul> <pre><code>class torch.utils.data.SubsetRandomSampler(indices)\n</code></pre> <p>\u4ece\u7ed9\u5b9a\u7684\u7d22\u5f15\u5217\u8868\u4e2d\u91c7\u6837\uff0c\u4e0d\u66ff\u6362\u3002</p> \u53c2\u6570\uff1a indices(\u5e8f\u5217\uff09\u2013 \u7d22\u5f15\u5e8f\u5217 <pre><code>class torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True)\n</code></pre> <p>\u6837\u672c\u5143\u7d20\u6765\u81ea[0,..,len(weights)-1]\uff0c\uff0c\u7ed9\u5b9a\u6982\u7387(\u6743\u91cd)\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>weights(\u5e8f\u5217) \u2013 \u6743\u91cd\u5e8f\u5217\uff0c\u4e0d\u9700\u8981\u548c\u4e3a1\u3002</li> <li>num_samples (int) \u2013 \u91c7\u6837\u6570\u3002</li> <li>replacement (bool) \u2013 \u5982\u679c\u662f<code>True</code>\uff0c\u66ff\u6362\u91c7\u6837\u3002\u5426\u5219\u4e0d\u66ff\u6362\uff0c\u5373\uff1a\u5982\u679c\u67d0\u4e2a\u6837\u672c\u7d22\u5f15\u5df2\u7ecf\u91c7\u8fc7\u4e86\uff0c\u90a3\u4e48\u4e0d\u4f1a\u7ee7\u7eed\u88ab\u91c7\u3002</li> </ul> <pre><code>class torch.utils.data.BatchSampler(sampler, batch_size, drop_last)\n</code></pre> <p>\u6253\u5305\u91c7\u6837\u5668\u6765\u83b7\u5f97\u5c0f\u6279\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>sampler(Sampler\uff09\u2013 \u57fa\u91c7\u6837\u5668\u3002</li> <li>batch_size(int\uff09\u2013 \u5c0f\u6279\u7684\u89c4\u6a21\u3002</li> <li>drop_last(bool\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u91c7\u6837\u5668\u4f1a\u4e22\u5f03\u6700\u540e\u4e00\u4e2a\u4e0d\u591f<code>batch_size</code>\u7684\u5c0f\u6279(\u5982\u679c\u5b58\u5728\u7684\u8bdd\uff09\u3002</li> </ul> <p>\u793a\u4f8b</p> <pre><code>&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n</code></pre> <pre><code>class torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None)\n</code></pre> <p>\u5c06\u6570\u636e\u52a0\u8f7d\u9650\u5236\u5230\u6570\u636e\u96c6\u5b50\u96c6\u7684\u91c7\u6837\u5668\u3002</p> <p>\u548c<code>torch.nn.parallel.DistributedDataParallel</code>\u540c\u65f6\u4f7f\u7528\u65f6\u5c24\u5176\u6709\u6548\u3002\u5728\u8fd9\u4e2d\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u4f1a\u4f20\u9012\u4e00\u4e2aDistributedSampler\u5b9e\u4f8b\u4f5c\u4e3aDataLoader\u91c7\u6837\u5668\uff0c\u5e76\u52a0\u8f7d\u72ec\u5360\u7684\u539f\u59cb\u6570\u636e\u96c6\u7684\u5b50\u96c6\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u5047\u8bbe\u6570\u636e\u96c6\u7684\u5927\u5c0f\u4e0d\u53d8\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>dataset \u2013 \u91c7\u6837\u7684\u6570\u636e\u96c6\u3002</li> <li>num_replicas(\u53ef\u9009\uff09\u2013 \u53c2\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u8fdb\u7a0b\u6570\u3002</li> <li>rank(\u53ef\u9009\uff09\u2013 num_replicas\u4e2d\u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7\u3002</li> </ul>"},{"location":"1.0/data_loading_tutorial/","title":"\u6570\u636e\u52a0\u8f7d\u548c\u5904\u7406\u6559\u7a0b","text":"<p>\u8bd1\u8005\uff1ayportne13</p> <p>\u4f5c\u8005\uff1aSasank Chilamkurthy</p> <p>\u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u65f6\u5019\uff0c\u4eba\u4eec\u82b1\u4e86\u5927\u91cf\u7cbe\u529b\u51c6\u5907\u6570\u636e\u3002pytorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u8ba9\u8f7d\u5165\u6570\u636e\u66f4\u7b80\u5355\u5e76\u5c3d\u91cf\u8ba9\u4f60\u7684\u4ee3\u7801\u7684\u53ef\u8bfb\u6027\u66f4\u9ad8\u3002\u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ece\u4e00\u4e2a\u5bb9\u6613\u5904\u7406\u7684\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5982\u4f55\u52a0\u8f7d\u548c\u9884\u5904\u7406/\u589e\u5f3a\u6570\u636e\u3002</p> <p>\u5728\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\u524d\u8bf7\u5148\u786e\u4fdd\u4f60\u5df2\u5b89\u88c5\u4ee5\u4e0b\u7684\u5305:</p> <ul> <li><code>scikit-image</code>: \u56fe\u5f62\u63a5\u53e3\u4ee5\u53ca\u53d8\u6362</li> <li><code>pandas</code>: \u4fbf\u4e8e\u5904\u7406csv\u6587\u4ef6</li> </ul> <pre><code>from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode\n\n</code></pre> <p>\u6211\u4eec\u8981\u5904\u7406\u7684\u662f\u4e00\u4e2a\u9762\u90e8\u59ff\u6001\u7684\u6570\u636e\u96c6\u3002\u4e5f\u5c31\u662f\u6309\u5982\u4e0b\u65b9\u5f0f\u6807\u6ce8\u7684\u4eba\u8138:</p> <p></p> <p>\u6bcf\u5f20\u8138\u6807\u6ce8\u4e8668\u4e2a\u4e0d\u540c\u7684\u7279\u5f81\u70b9\u3002</p> <p>\u6ce8\u610f</p> <p>\u4ece\u8fd9\u91cc\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u628a\u5b83\u653e\u7f6e\u5728 'data/faces/'\u8def\u5f84\u4e0b\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u5b9e\u9645\u4e0a\u662f\u5bf9ImageNet\u4e2d\u7684\u4eba\u8138\u56fe\u50cf\u4f7f\u7528\u8868\u73b0\u51fa\u8272\u7684DLIB\u59ff\u52bf\u4f30\u8ba1\u6a21\u578b(dlib's pose estimation) \u751f\u6210\u7684\u3002</p> <p>\u6570\u636e\u96c6\u662f\u6309\u5982\u4e0b\u89c4\u5219\u6253\u5305\u6210\u7684csv\u6587\u4ef6:</p> <pre><code>image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n0805personali01.jpg,27,83,27,98, ... 84,134\n1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n\n</code></pre> <p>\u5feb\u901f\u8bfb\u53d6csv\u5e76\u5c06\u6807\u6ce8\u70b9\u6570\u636e\u5199\u5165(N\uff0c2\uff09\u6570\u7ec4\u4e2d\uff0c\u5176\u4e2dN\u662f\u7279\u5f81\u70b9\u7684\u6570\u91cf\u3002</p> <pre><code>landmarks_frame = pd.read_csv('data/faces/face_landmarks.csv')\n\nn = 65\nimg_name = landmarks_frame.iloc[n, 0]\nlandmarks = landmarks_frame.iloc[n, 1:].as_matrix()\nlandmarks = landmarks.astype('float').reshape(-1, 2)\n\nprint('Image name: {}'.format(img_name))\nprint('Landmarks shape: {}'.format(landmarks.shape))\nprint('First 4 Landmarks: {}'.format(landmarks[:4]))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Image name: person-7.jpg\nLandmarks shape: (68, 2)\nFirst 4 Landmarks: [[32\\. 65.]\n [33\\. 76.]\n [34\\. 86.]\n [34\\. 97.]]\n\n</code></pre> <p>\u5199\u4e00\u4e2a\u7b80\u5355\u7684\u8f85\u52a9\u51fd\u6570\u6765\u5c55\u793a\u4e00\u5f20\u56fe\u7247\u548c\u5b83\u5bf9\u5e94\u7684\u6807\u6ce8\u70b9\u4f5c\u4e3a\u4f8b\u5b50\u3002</p> <pre><code>def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('data/faces/', img_name)),\n               landmarks)\nplt.show()\n\n</code></pre> <p></p>"},{"location":"1.0/data_loading_tutorial/#dataset-class","title":"\u6570\u636e\u96c6\u7c7b Dataset class","text":"<p><code>torch.utils.data.Dataset</code> \u662f\u4e00\u4e2a\u4ee3\u8868\u6570\u636e\u96c6\u7684\u62bd\u8c61\u7c7b\u3002\u4f60\u81ea\u5b9a\u7684\u6570\u636e\u96c6\u7c7b\u5e94\u8be5\u7ee7\u627f\u81ea <code>Dataset</code> \u7c7b\u5e76\u91cd\u65b0\u5b9e\u73b0\u4ee5\u4e0b\u65b9\u6cd5:</p> <ul> <li><code>__len__</code> \u5b9e\u73b0 <code>len(dataset)</code> \u8fd4\u8fd8\u6570\u636e\u96c6\u7684\u5c3a\u5bf8\u3002</li> <li><code>__getitem__</code> \u7528\u6765\u83b7\u53d6\u4e00\u4e9b\u7d22\u5f15\u6570\u636e\uff0c\u4f8b\u5982 \u4f7f\u7528<code>dataset[i]</code> \u83b7\u5f97\u7b2ci\u4e2a\u6837\u672c\u3002</li> </ul> <p>\u8ba9\u6211\u4eec\u6765\u4e3a\u6211\u4eec\u7684\u6570\u636e\u96c6\u521b\u5efa\u4e00\u4e2a\u7c7b\u3002\u6211\u4eec\u5c06\u5728 <code>__init__</code> \u4e2d\u8bfb\u53d6csv\u7684\u6587\u4ef6\u5185\u5bb9\uff0c\u5728 <code>__getitem__</code>\u4e2d\u8bfb\u53d6\u56fe\u7247\u3002\u8fd9\u4e48\u505a\u662f\u4e3a\u4e86\u8282\u7701\u5185\u5b58\u7a7a\u95f4\u3002\u53ea\u6709\u5728\u9700\u8981\u7528\u5230\u56fe\u7247\u7684\u65f6\u5019\u624d\u8bfb\u53d6\u5b83\u800c\u4e0d\u662f\u4e00\u5f00\u59cb\u5c31\u628a\u56fe\u7247\u5168\u90e8\u5b58\u8fdb\u5185\u5b58\u91cc\u3002</p> <p>\u6211\u4eec\u7684\u6570\u636e\u6837\u672c\u5c06\u6309\u8fd9\u6837\u4e00\u4e2a\u5b57\u5178 <code>{'image': image, 'landmarks': landmarks}</code>\u7ec4\u7ec7\u3002 \u6211\u4eec\u7684\u6570\u636e\u96c6\u7c7b\u5c06\u6dfb\u52a0\u4e00\u4e2a\u53ef\u9009\u53c2\u6570 <code>transform</code> \u4ee5\u65b9\u4fbf\u5bf9\u6837\u672c\u8fdb\u884c\u9884\u5904\u7406\u3002\u4e0b\u4e00\u8282\u6211\u4eec\u4f1a\u770b\u5230\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u7528\u5230 <code>transform</code> \u53c2\u6570\u3002</p> <pre><code>class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n Args:\n csv_file (string): Path to the csv file with annotations.\n root_dir (string): Directory with all the images.\n transform (callable, optional): Optional transform to be applied\n on a sample.\n \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n</code></pre> <p>\u8ba9\u6211\u4eec\u5b9e\u4f8b\u5316\u8fd9\u4e2a\u7c7b\u5e76\u521b\u5efa\u51e0\u4e2a\u6570\u636e\u3002\u6211\u4eec\u5c06\u4f1a\u6253\u5370\u51fa\u524d\u56db\u4e2a\u4f8b\u5b50\u7684\u5c3a\u5bf8\u5e76\u5c55\u793a\u6807\u6ce8\u7684\u7279\u5f81\u70b9\u3002</p> <pre><code>face_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n                                    root_dir='data/faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break\n\n</code></pre> <p></p> <p>\u8f93\u51fa:</p> <pre><code>0 (324, 215, 3) (68, 2)\n1 (500, 333, 3) (68, 2)\n2 (250, 258, 3) (68, 2)\n3 (434, 290, 3) (68, 2)\n\n</code></pre>"},{"location":"1.0/data_loading_tutorial/#transforms","title":"\u8f6c\u6362 Transforms","text":"<p>\u901a\u8fc7\u4e0a\u9762\u7684\u4f8b\u5b50\u6211\u4eec\u4f1a\u53d1\u73b0\u56fe\u7247\u5e76\u4e0d\u662f\u540c\u6837\u7684\u5c3a\u5bf8\u3002\u7edd\u5927\u591a\u6570\u795e\u7ecf\u7f51\u7edc\u90fd\u5047\u5b9a\u56fe\u7247\u7684\u5c3a\u5bf8\u76f8\u540c\u3002\u56e0\u6b64\u6211\u4eec\u9700\u8981\u505a\u4e00\u4e9b\u9884\u5904\u7406\u3002\u8ba9\u6211\u4eec\u521b\u5efa\u4e09\u4e2a\u8f6c\u6362:</p> <ul> <li><code>Rescale</code>: \u7f29\u653e\u56fe\u7247</li> <li><code>RandomCrop</code>: \u5bf9\u56fe\u7247\u8fdb\u884c\u968f\u673a\u88c1\u526a\u3002\u8fd9\u662f\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u64cd\u4f5c</li> <li><code>ToTensor</code>: \u628a numpy \u683c\u5f0f\u56fe\u7247\u8f6c\u4e3a torch \u683c\u5f0f\u56fe\u7247 (\u6211\u4eec\u9700\u8981\u4ea4\u6362\u5750\u6807\u8f74).</li> </ul> <p>\u6211\u4eec\u4f1a\u628a\u5b83\u4eec\u5199\u6210\u53ef\u8c03\u7528\u7684\u7c7b\u7684\u5f62\u5f0f\u800c\u4e0d\u662f\u7b80\u5355\u7684\u51fd\u6570\uff0c\u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u6bcf\u6b21\u8c03\u7528\u65f6\u4f20\u9012\u4e00\u904d\u53c2\u6570\u3002\u6211\u4eec\u53ea\u9700\u8981\u5b9e\u73b0 <code>__call__</code> \u65b9\u6cd5\uff0c\u5fc5\u8981\u7684\u65f6\u5019\u5b9e\u73b0 <code>__init__</code> \u65b9\u6cd5\u3002\u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u8c03\u7528\u8fd9\u4e9b\u8f6c\u6362:</p> <pre><code>tsfm = Transform(params)\ntransformed_sample = tsfm(sample)\n\n</code></pre> <p>\u89c2\u5bdf\u4e0b\u9762\u8fd9\u4e9b\u8f6c\u6362\u662f\u5982\u4f55\u5e94\u7528\u5728\u56fe\u50cf\u548c\u6807\u7b7e\u4e0a\u7684\u3002</p> <pre><code>class Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n Args:\n output_size (tuple or int): Desired output size. If tuple, output is\n matched to output_size. If int, smaller of image edges is matched\n to output_size keeping aspect ratio the same.\n \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h &gt; w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        # h and w are swapped for landmarks because for images,\n        # x and y axes are axis 1 and 0 respectively\n        landmarks = landmarks * [new_w / w, new_h / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n Args:\n output_size (tuple or int): Desired output size. If int, square crop\n is made.\n \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left, top]\n\n        return {'image': image, 'landmarks': landmarks}\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'landmarks': torch.from_numpy(landmarks)}\n\n</code></pre>"},{"location":"1.0/data_loading_tutorial/#compose-transforms","title":"\u7ec4\u5408\u8f6c\u6362 Compose transforms","text":"<p>\u63a5\u4e0b\u6765\u6211\u4eec\u628a\u8fd9\u4e9b\u8f6c\u6362\u5e94\u7528\u5230\u4e00\u4e2a\u4f8b\u5b50\u4e0a\u3002</p> <p>\u6211\u4eec\u60f3\u8981\u628a\u56fe\u50cf\u7684\u77ed\u8fb9\u8c03\u6574\u4e3a256\uff0c\u7136\u540e\u968f\u673a\u88c1\u526a (randomcrop) \u4e3a224\u5927\u5c0f\u7684\u6b63\u65b9\u5f62\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u6253\u7b97\u7ec4\u5408\u4e00\u4e2a <code>Rescale</code> \u548c <code>RandomCrop</code> \u7684\u53d8\u6362\u3002 \u6211\u4eec\u53ef\u4ee5\u8c03\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u7c7b <code>torchvision.transforms.Compose</code> \u6765\u5b9e\u73b0\u8fd9\u4e00\u64cd\u4f5c\u3002</p> <pre><code>scale = Rescale(256)\ncrop = RandomCrop(128)\ncomposed = transforms.Compose([Rescale(256),\n                               RandomCrop(224)])\n\n# Apply each of the above transforms on sample.\nfig = plt.figure()\nsample = face_dataset[65]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tsfrm).__name__)\n    show_landmarks(**transformed_sample)\n\nplt.show()\n\n</code></pre> <p></p>"},{"location":"1.0/data_loading_tutorial/#iterating-through-the-dataset","title":"\u8fed\u4ee3\u6570\u636e\u96c6 Iterating through the dataset","text":"<p>\u8ba9\u6211\u4eec\u628a\u8fd9\u4e9b\u6574\u5408\u8d77\u6765\u4ee5\u521b\u5efa\u4e00\u4e2a\u5e26\u7ec4\u5408\u8f6c\u6362\u7684\u6570\u636e\u96c6\u3002 \u603b\u7ed3\u4e00\u4e0b\uff0c\u6bcf\u6b21\u8fd9\u4e2a\u6570\u636e\u96c6\u88ab\u91c7\u6837\u65f6:</p> <ul> <li>\u53ca\u65f6\u5730\u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u7247</li> <li>\u5bf9\u8bfb\u53d6\u7684\u56fe\u7247\u5e94\u7528\u8f6c\u6362</li> <li>\u7531\u4e8e\u5176\u4e2d\u4e00\u6b65\u64cd\u4f5c\u662f\u968f\u673a\u7684 (randomcrop) , \u6570\u636e\u88ab\u589e\u5f3a\u4e86</li> </ul> <p>\u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u90a3\u6837\u4f7f\u7528 <code>for i in range</code> \u5faa\u73af\u6765\u5bf9\u6240\u6709\u521b\u5efa\u7684\u6570\u636e\u96c6\u6267\u884c\u540c\u6837\u7684\u64cd\u4f5c\u3002</p> <pre><code>transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n                                           root_dir='data/faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>0 torch.Size([3, 224, 224]) torch.Size([68, 2])\n1 torch.Size([3, 224, 224]) torch.Size([68, 2])\n2 torch.Size([3, 224, 224]) torch.Size([68, 2])\n3 torch.Size([3, 224, 224]) torch.Size([68, 2])\n\n</code></pre> <p>\u4f46\u662f\uff0c\u5bf9\u6240\u6709\u6570\u636e\u96c6\u7b80\u5355\u7684\u4f7f\u7528 <code>for</code> \u5faa\u73af\u727a\u7272\u4e86\u8bb8\u591a\u529f\u80fd\uff0c\u5c24\u5176\u662f:</p> <ul> <li>\u6279\u5904\u7406\u6570\u636e(Batching the data\uff09</li> <li>\u6253\u4e71\u6570\u636e(Shuffling the data\uff09</li> <li>\u4f7f\u7528\u591a\u7ebf\u7a0b <code>multiprocessing</code> \u5e76\u884c\u52a0\u8f7d\u6570\u636e\u3002</li> </ul> <p><code>torch.utils.data.DataLoader</code> \u8fd9\u4e2a\u8fed\u4ee3\u5668\u63d0\u4f9b\u4e86\u4ee5\u4e0a\u6240\u6709\u529f\u80fd\u3002 \u4e0b\u9762\u4f7f\u7528\u7684\u53c2\u6570\u5fc5\u987b\u662f\u6e05\u695a\u7684\u3002 \u4e00\u4e2a\u503c\u5f97\u5173\u6ce8\u7684\u53c2\u6570\u662f <code>collate_fn</code>. \u4f60\u53ef\u4ee5\u901a\u8fc7 <code>collate_fn</code> \u6765\u51b3\u5b9a\u5982\u4f55\u5bf9\u6570\u636e\u8fdb\u884c\u6279\u5904\u7406\u3002 \u4f46\u662f\u7edd\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u9ed8\u8ba4\u503c\u5c31\u80fd\u8fd0\u884c\u826f\u597d\u3002</p> <pre><code>dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break\n\n</code></pre> <p></p> <p>\u8f93\u51fa:</p> <pre><code>0 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n1 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n2 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n3 torch.Size([4, 3, 224, 224]) torch.Size([4, 68, 2])\n\n</code></pre>"},{"location":"1.0/data_loading_tutorial/#torchvision","title":"\u540e\u8bb0: torchvision","text":"<p>\u5728\u8fd9\u7bc7\u6559\u7a0b\u4e2d\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u6784\u9020\u548c\u4f7f\u7528\u6570\u636e\u96c6\u7c7b (datasets), \u8f6c\u6362 (transforms) \u548c\u6570\u636e\u52a0\u8f7d\u5668 (dataloader)\u3002 <code>torchvision</code> \u5305\u63d0\u4f9b\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u7c7b (datasets) \u548c\u8f6c\u6362 (transforms)\u3002  \u4f60\u53ef\u80fd\u4e0d\u9700\u8981\u81ea\u5df1\u6784\u9020\u8fd9\u4e9b\u7c7b\u3002 torchvision \u4e2d\u8fd8\u6709\u4e00\u4e2a\u66f4\u5e38\u7528\u7684\u6570\u636e\u96c6\u7c7b <code>ImageFolder</code>. \u5b83\u5047\u5b9a\u4e86\u6570\u636e\u96c6\u662f\u4ee5\u5982\u4e0b\u65b9\u5f0f\u6784\u9020\u7684:</p> <pre><code>root/ants/xxx.png\nroot/ants/xxy.jpeg\nroot/ants/xxz.png\n.\n.\n.\nroot/bees/123.jpg\nroot/bees/nsdf3.png\nroot/bees/asd932_.png\n\n</code></pre> <p>\u5176\u4e2d 'ants', 'bees' \u7b49\u662f\u5206\u7c7b\u6807\u7b7e\u3002 \u5728 <code>PIL.Image</code> \u4e2d\u4f60\u4e5f\u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c\u7684\u8f6c\u6362 (transforms) \u4f8b\u5982 <code>RandomHorizontalFlip</code>, <code>Scale</code>\u3002\u5229\u7528\u8fd9\u4e9b\u4f60\u53ef\u4ee5\u6309\u5982\u4e0b\u7684\u65b9\u5f0f\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u52a0\u8f7d\u5668 (dataloader) :</p> <pre><code>import torch\nfrom torchvision import transforms, datasets\n\ndata_transform = transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\nhymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n                                           transform=data_transform)\ndataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n                                             batch_size=4, shuffle=True,\n                                             num_workers=4)\n\n</code></pre> <p>\u5e26\u8bad\u7ec3\u90e8\u5206\u7684\u4f8b\u7a0b\u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc Transfer Learning Tutorial.</p>"},{"location":"1.0/dcgan_faces_tutorial/","title":"DCGAN \u6559\u7a0b","text":"<p>\u4f5c\u8005: Nathan Inkawhich</p> <p>\u8bd1\u8005\uff1awangshuai9517</p> <p>\u6821\u9a8c: \u7247\u523b</p>"},{"location":"1.0/dcgan_faces_tutorial/#_1","title":"\u4ecb\u7ecd","text":"<p>\u672c\u6559\u7a0b\u5c06\u901a\u8fc7\u4e00\u4e2a\u4f8b\u5b50\u6765\u4ecb\u7ecdDCGAN\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u5f88\u591a\u771f\u6b63\u7684\u540d\u4eba\u7167\u7247\u8bad\u7ec3\u4e00\u4e2a\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN\uff09\u540e\uff0c\u751f\u6210\u65b0\u7684\u5047\u540d\u4eba\u7167\u7247\u3002\u8fd9\u91cc\u7684\u5927\u591a\u6570\u4ee3\u7801\u6765\u81ea\u4e8epytorch/examples\u4e2d\u5bf9DCGAN\u7684\u5b9e\u73b0\uff0c\u5e76\u4e14\u672c\u6587\u6863\u5c06\u5bf9DCGAN\u7684\u5b9e\u73b0\u8fdb\u884c\u5168\u9762\u89e3\u91ca\uff0c\u5e76\u9610\u660e\u8be5\u6a21\u578b\u662f\u600e\u6837\u5de5\u4f5c\u7684\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u80fd\u5de5\u4f5c\u3002\u4f46\u662f\u4e0d\u8981\u62c5\u5fc3\uff0c\u6211\u4eec\u5e76\u4e0d\u9700\u8981\u4f60\u4e8b\u5148\u4e86\u89e3GAN\uff0c\u4f46\u662f\u53ef\u80fd\u9700\u8981\u5148\u82b1\u4e00\u4e9b\u65f6\u95f4\u6765\u5f04\u660e\u767d\u5b9e\u9645\u53d1\u751f\u4e86\u4ec0\u4e48\u3002 \u6b64\u5916\uff0c\u62e5\u6709\u4e00\u4e24\u4e2aGPU\u5c06\u5bf9\u8282\u7701\u8fd0\u884c\u65f6\u95f4\u5f88\u6709\u5e2e\u52a9\u3002 \u8ba9\u6211\u4eec\u4ece\u5934\u5f00\u59cb\u5427\u3002</p>"},{"location":"1.0/dcgan_faces_tutorial/#_2","title":"\u5bf9\u6297\u751f\u6210\u7f51\u7edc","text":""},{"location":"1.0/dcgan_faces_tutorial/#gan","title":"\u4ec0\u4e48\u662f\u5bf9\u6297\u751f\u6210\u7f51\u7edc(GAN)?","text":"<p>\u5bf9\u6297\u751f\u6210\u7f51\u7edc(GAN\uff09\u662f\u4e00\u4e2a\u6559\u6df1\u5ea6\u6a21\u578b\u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u4e00\u79cd\u6846\u67b6\uff0c\u56e0\u6b64\u6211\u4eec\u80fd\u591f\u4f7f\u7528\u7c7b\u4f3c\u7684\u5206\u5e03\u6765\u751f\u6210\u65b0\u7684\u6570\u636e\u3002\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u662fIan Goodfellow\u57282014\u5e74\u53d1\u660e\u7684\uff0c\u5e76\u9996\u6b21\u53d1\u8868\u5728\u6587\u7ae0 Generative Adversarial Nets\u4e2d\u3002\u5b83\u4eec\u7531\u4e24\u79cd\u4e0d\u540c\u7684\u6a21\u5757\u7ec4\u6210\uff0c\u4e00\u4e2a\u751f\u6210\u5668  generator \u4ee5\u53ca\u4e00\u4e2a\u5224\u522b\u5668 discriminator \u3002\u751f\u6210\u5668\u7684\u5de5\u4f5c\u662f\u4ea7\u751f\u770b\u8d77\u6765\u50cf\u8bad\u7ec3\u56fe\u50cf\u7684\u201c\u5047\u201d\u56fe\u50cf\u3002 \u5224\u522b\u5668\u7684\u5de5\u4f5c\u662f\u67e5\u770b\u56fe\u50cf\u5e76\u8f93\u51fa\u5b83\u662f\u5426\u662f\u6765\u81ea\u771f\u5b9e\u8bad\u7ec3\u56fe\u50cf\u6216\u751f\u6210\u5668\u7684\u4f2a\u56fe\u50cf\u3002\u5728\u8bad\u7ec3\u671f\u95f4\uff0c\u751f\u6210\u5668\u4e0d\u65ad\u5c1d\u8bd5\u901a\u8fc7\u4ea7\u751f\u8d8a\u6765\u8d8a\u597d\u7684\u5047\u56fe\u7247\u6765\u8d85\u8d8a\u5224\u522b\u5668\uff0c\u4e0e\u6b64\u540c\u65f6\u5224\u522b\u5668\u9010\u6e10\u66f4\u597d\u7684\u68c0\u6d4b\u5e76\u6b63\u786e\u5206\u7c7b\u771f\u5047\u56fe\u7247\u3002 \u8fd9\u4e2a\u8fc7\u7a0b\u6700\u540e\u9010\u6e10\u7684\u53d8\u5f97\u5e73\u8861\uff0c\u751f\u6210\u5668\u751f\u6210\u5b8c\u7f8e\u7684\u5047\u56fe\u7247\uff0c\u8fd9\u4e9b\u5047\u56fe\u7247\u770b\u8d77\u6765\u597d\u50cf\u5b83\u4eec\u76f4\u63a5\u6765\u81ea\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u5224\u522b\u5668\u603b\u662f\u731c\u6d4b\u751f\u6210\u5668\u8f93\u51fa\u7684\u56fe\u7247\u771f\u5047\u90fd\u662f50%\u3002</p> <p>\u73b0\u5728\uff0c\u6211\u4eec\u5148\u5b9a\u4e49\u4e00\u4e9b\u6574\u4e2a\u6559\u7a0b\u4e2d\u8981\u4f7f\u7528\u7684\u7b26\u53f7\uff0c\u9996\u5148\u4ece\u5224\u522b\u5668\u5f00\u59cb\u3002 \\(\\(x\\)\\) \u8868\u793a\u56fe\u50cf\u6570\u636e\u3002\\(\\(D(x)\\)\\) \u8868\u793a\u5224\u522b\u7f51\u7edc\uff0c\u5b83\u7684\u8f93\u51fa\u8868\u793a\u6570\u636e \\(\\(x\\)\\) \u6765\u81ea\u4e0e\u8bad\u7ec3\u6570\u636e\u800c\u4e0d\u662f\u751f\u6210\u6570\u636e\u7684\u6982\u7387\u3002\u8fd9\u91cc \\(\\(D(x)\\)\\) \u7684\u8f93\u5165\u56fe\u50cf\u662f\u5927\u5c0f\u4e3a3x64x64\u3002 \u76f4\u89c2\u5730\u8bf4\uff0c\u5f53 \\(\\(x\\)\\) \u6765\u81ea\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\\(\\(D(x)\\)\\)\u7684\u503c\u5e94\u5f53\u662f\u5927\u7684\uff1b\u800c\u5f53 \\(\\(x\\)\\) \u6765\u81ea\u53d1\u751f\u5668\u65f6\uff0c\\(\\(D(x)\\)\\) \u7684\u503c\u5e94\u4e3a\u5c0f\u7684\u3002 \\(\\(D(x)\\)\\) \u4e5f\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4f20\u7edf\u7684\u4e8c\u5143\u5206\u7c7b\u5668\u3002</p> <p>\u5bf9\u4e8e\u751f\u6210\u5668 \\(\\(z\\)\\) \u8868\u793a\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u7a7a\u95f4\u77e2\u91cf(\u672c\u5f81\u5411\u91cf\uff09\u3002 \\(\\(G(z)\\)\\) \u8868\u793a\u5c06\u672c\u5f81\u5411\u91cf \\(\\(z\\)\\) \u6620\u5c04\u5230\u6570\u636e\u7a7a\u95f4\u7684\u751f\u6210\u5668\u51fd\u6570\u3002 \\(\\(G\\)\\) \u7684\u76ee\u6807\u662f\u4f30\u8ba1\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u7684\u5206\u5e03 \\(\\(p_{data}\\)\\) \uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u4ece\u4f30\u8ba1\u7684\u5206\u5e03 \\(\\(p_g\\)\\) \u4e2d\u751f\u6210\u5047\u6837\u672c\u3002</p> <p>\u56e0\u6b64\uff0c\\(\\(D(G(z))\\)\\) \u8868\u793a\u751f\u6210\u5668\u8f93\u51fa\\(\\(G\\)\\)\u662f\u771f\u5b9e\u56fe\u7247\u7684\u6982\u7387\u3002\u5c31\u50cf\u5728 Goodfellow's paper\u4e2d\u63cf\u8ff0\u7684\u90a3\u6837\uff0c\\(\\(D\\)\\) \u548c \\(\\(G\\)\\) \u5728\u73a9\u4e00\u4e2a\u6781\u5927\u6781\u5c0f\u6e38\u620f\u3002\u5728\u8fd9\u4e2a\u6e38\u620f\u4e2d \\(\\(D\\)\\) \u8bd5\u56fe\u6700\u5927\u5316\u6b63\u786e\u5206\u7c7b\u771f\u5047\u56fe\u7247\u7684\u6982\u7387 \\(\\(logD(x)\\)\\) \uff0c\\(\\(G\\)\\) \u8bd5\u56fe\u6700\u5c0f\u5316 \\(\\(D\\)\\) \u9884\u6d4b\u5176\u8f93\u51fa\u4e3a\u5047\u56fe\u7247\u7684\u6982\u7387  \\(\\(log(1-D(G(x)))\\)\\) \u3002\u6587\u7ae0\u4e2dGAN\u7684\u635f\u5931\u51fd\u6570\u662f</p> <p>\\( \\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}{x\\sim p{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}{z\\sim p{z}(z)}\\big[log(1-D(G(x)))\\big] \\)</p> <p>\u7406\u8bba\u4e0a\uff0c\u8fd9\u4e2a\u6781\u5c0f\u6781\u5927\u6e38\u620f\u7684\u76ee\u6807\u662f \\(\\(p_g=p_{data}\\)\\)\uff0c\u5982\u679c\u8f93\u5165\u662f\u771f\u5b9e\u7684\u6216\u5047\u7684\uff0c\u5219\u5224\u522b\u5668\u4f1a\u968f\u673a\u731c\u6d4b\u3002 \u7136\u800c\uff0cGAN\u7684\u6536\u655b\u7406\u8bba\u4ecd\u5728\u79ef\u6781\u7814\u7a76\u4e2d\uff0c\u5b9e\u9645\u4e0a\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u8bad\u7ec3\u5230\u8fd9\u4e00\u70b9\u3002</p>"},{"location":"1.0/dcgan_faces_tutorial/#dcgan_1","title":"\u4ec0\u4e48\u662fDCGAN?","text":"<p>DCGAN\u662f\u5bf9\u4e0a\u9762\u63cf\u8ff0\u7684GAN\u7684\u76f4\u63a5\u6269\u5c55\uff0c\u9664\u4e86\u5b83\u5206\u522b\u5728\u5224\u522b\u5668\u548c\u751f\u6210\u5668\u4e2d\u660e\u786e\u5730\u4f7f\u7528\u5377\u79ef\u548c\u5377\u79ef\u8f6c\u7f6e\u5c42\u3002 DCGAN\u662f\u5728Radford\u7b49\u7684\u6587\u7ae0Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks\u4e2d\u9996\u6b21\u88ab\u63d0\u51fa\u7684\u3002\u5224\u522b\u5668\u7531\u5377\u79ef\u5c42\u3001\u6279\u6807\u51c6\u5316 \u5c42\u4ee5\u53caLeakyReLU \u6fc0\u6d3b\u5c42\u7ec4\u6210\u3002\u8f93\u5165\u662f3x64x64\u7684\u56fe\u50cf\uff0c\u8f93\u51fa\u662f\u8f93\u5165\u56fe\u50cf\u6765\u81ea\u5b9e\u9645\u6570\u636e\u7684\u6982\u7387\u3002\u751f\u6210\u5668\u7531\u8f6c\u7f6e\u5377\u79ef\u5c42\uff0c\u6279\u6807\u51c6\u5316\u5c42\u4ee5\u53caReLU \u6fc0\u6d3b\u5c42\u7ec4\u6210\u3002 \u8f93\u5165\u662f\u4e00\u4e2a\u672c\u5f81\u5411\u91cf(latent vector\uff09 \\(\\(z\\)\\)\uff0c\u5b83\u662f\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\u5f97\u5230\u7684\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a3x64x64 \u7684RGB\u56fe\u50cf\u3002 \u8f6c\u7f6e\u5377\u79ef\u5c42\u80fd\u591f\u628a\u672c\u5f81\u5411\u91cf\u8f6c\u6362\u6210\u548c\u56fe\u50cf\u5177\u6709\u76f8\u540c\u5927\u5c0f\u3002 \u5728\u672c\u6587\u4e2d\uff0c\u4f5c\u8005\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6709\u5173\u5982\u4f55\u8bbe\u7f6e\u4f18\u5316\u5668\uff0c\u5982\u4f55\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u5982\u4f55\u521d\u59cb\u5316\u6a21\u578b\u6743\u91cd\u7684\u5efa\u8bae\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8fdb\u884c\u8bf4\u660e\u3002</p> <pre><code>from __future__ import print_function\n#%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# \u4e3a\u4e86\u53ef\u91cd\u590d\u6027\u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # \u5982\u679c\u4f60\u60f3\u6709\u4e00\u4e2a\u4e0d\u540c\u7684\u7ed3\u679c\u4f7f\u7528\u8fd9\u884c\u4ee3\u7801\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Random Seed:  999\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_3","title":"\u53d8\u91cf","text":"<p>\u4e3a\u4e86\u80fd\u591f\u8fd0\u884c\uff0c\u5b9a\u4e49\u4e00\u4e9b\u53d8\u91cf\uff1a</p> <ul> <li>dataroot - \u6570\u636e\u96c6\u6587\u4ef6\u5939\u7684\u8def\u5f84\u3002\u6211\u4eec\u5c06\u5728\u540e\u9762\u7684\u7ae0\u8282\u4e2d\u8ba8\u8bba\u66f4\u591a\u5173\u4e8e\u6570\u636e\u96c6\u7684\u5185\u5bb9 </li> <li>workers - \u6570\u636e\u52a0\u8f7d\u5668DataLoader\u52a0\u8f7d\u6570\u636e\u65f6\u80fd\u591f\u4f7f\u7528\u7684\u8fdb\u7a0b\u6570</li> <li>batch_size - \u8bad\u7ec3\u65f6\u7684\u6279\u5927\u5c0f\u3002\u5728DCGAN\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u6279\u5927\u5c0f\u662f128</li> <li>image_size - \u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u56fe\u7247\u5927\u5c0f\u3002 \u8fd9\u91cc\u8bbe\u7f6e\u9ed8\u8ba4\u503c\u4e3a 64x64\\ \u3002\u5982\u679c\u60f3\u4f7f\u7528\u522b\u7684\u5927\u5c0f\uff0c\u751f\u6210\u5668G\u548c\u5224\u522b\u5668D\u7684\u7ed3\u6784\u4e5f\u8981\u6539\u53d8\u3002 \u60f3\u770b\u66f4\u591a\u8be6\u7ec6\u5185\u5bb9\u8bf7\u70b9\u51fb\u8fd9\u91cc</li> <li>nc - \u8f93\u5165\u56fe\u7247\u7684\u989c\u8272\u901a\u9053\u4e2a\u6570\u3002\u5f69\u8272\u56fe\u7247\u662f3</li> <li>nz - \u672c\u5f81\u5411\u91cf\u7684\u957f\u5ea6</li> <li>ngf - \u751f\u6210\u5668\u4f7f\u7528\u7684\u7279\u5f81\u56fe\u6df1\u5ea6</li> <li>ndf - \u8bbe\u7f6e\u5224\u522b\u5668\u4f7f\u7528\u7684\u7279\u5f81\u56fe\u7684\u6df1\u5ea6</li> <li>num_epochs - \u4e00\u5171\u8bad\u7ec3\u591a\u5c11\u6b21\u3002\u8bad\u7ec3\u6b21\u6570\u591a\u5f88\u53ef\u80fd\u4ea7\u751f\u66f4\u597d\u7684\u7ed3\u679c\u4f46\u662f\u9700\u8981\u8bad\u7ec3\u66f4\u957f\u7684\u65f6\u95f4</li> <li>lr - \u8bad\u7ec3\u65f6\u7684\u5b66\u4e60\u7387\uff0cDCGAN\u6587\u7ae0\u4e2d\u4f7f\u7528\u7684\u662f0.0002</li> <li>beta1 - Adam\u4f18\u5316\u7b97\u6cd5\u7684beta1\u8d85\u53c2\u6570\u3002\u6587\u7ae0\u7528\u4f7f\u7528\u7684\u662f0.5 </li> <li>ngpu - \u53ef\u5229\u7528\u7684GPU\u6570\u91cf\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3a0\u5219\u8fd0\u884c\u5728CPU\u6a21\u5f0f\u3002\u5982\u679c\u8bbe\u7f6e\u7684\u5927\u4e8e0\u5219\u518d\u884c\u5728\u90a3\u4e9b\u6570\u91cf\u7684GPU</li> </ul> <pre><code># \u6570\u636e\u96c6\u6839\u76ee\u5f55\ndataroot = \"data/celeba\"\n\n# \u6570\u636e\u52a0\u8f7d\u5668\u80fd\u591f\u4f7f\u7528\u7684\u8fdb\u7a0b\u6570\u91cf\nworkers = 2\n\n# \u8bad\u7ec3\u65f6\u7684\u6279\u5927\u5c0f\nbatch_size = 128\n\n# \u8bad\u7ec3\u56fe\u7247\u7684\u5927\u5c0f\uff0c\u6240\u6709\u7684\u56fe\u7247\u7ed9\u90fd\u5c06\u6539\u53d8\u5230\u8be5\u5927\u5c0f\n# \u8f6c\u6362\u5668\u4f7f\u7528\u7684\u5927\u5c0f.\nimage_size = 64\n\n# \u8bad\u7ec3\u56fe\u7247\u7684\u901a\u9053\u6570\uff0c\u5f69\u8272\u56fe\u7247\u662f3\nnc = 3\n\n# \u672c\u5f81\u5411\u91cfz\u7684\u5927\u5c0f(\u751f\u6210\u5668\u7684\u8f93\u5165\u5927\u5c0f)\nnz = 100\n\n# \u751f\u6210\u5668\u4e2d\u7279\u5f81\u56fe\u5927\u5c0f\nngf = 64\n\n# \u5224\u522b\u5668\u4e2d\u7279\u5f81\u56fe\u5927\u5c0f\nndf = 64\n\n# \u8bad\u7ec3\u6b21\u6570\nnum_epochs = 5\n\n# \u4f18\u5316\u5668\u5b66\u4e60\u7387\nlr = 0.0002\n\n# Adam\u4f18\u5316\u5668\u7684Beta1\u8d85\u53c2\nbeta1 = 0.5\n\n# \u53ef\u5229\u7528\u7684GPU\u6570\u91cf\uff0c\u4f7f\u75280\u5c06\u8fd0\u884c\u5728CPU\u6a21\u5f0f\u3002\nngpu = 1\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_4","title":"\u6570\u636e","text":"<p>\u672c\u6559\u7a0b\u4e2d\u6211\u5c06\u4f7f\u7528 Celeb-A Faces \u6570\u636e\u96c6 \u53ef\u4ee5\u5728\u94fe\u63a5\u4e2d\u4e0b\u8f7d\uff0c\u6216\u8005\u5728  \u8c37\u6b4c\u7f51\u76d8\u4e2d\u4e0b\u8f7d\u3002\u4e0b\u8f7d\u8be5\u6570\u636e\u96c6\u5c06\u4ea7\u751f\u4e00\u4e2a\u540d\u4e3a img_align_celeba.zip \u7684\u6587\u4ef6\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a celeba \u7684\u6587\u4ef6\u5939\u89e3\u538b\u4e0b\u8f7d\u7684\u6570\u636e\u96c6\u5230\u8be5\u76ee\u5f55\u4e0b\u3002\u7136\u540e\uff0c\u5728\u672c\u7b14\u8bb0\u4e2d\u8bbe\u7f6e dataroot \u5230\u4f60\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939 celeba \u3002\u6700\u540e\u5f97\u5230\u7684\u6587\u4ef6\u5939\u7ed3\u6784\u5982\u4e0b\uff1a</p> <pre><code>/path/to/celeba\n    -&gt; img_align_celeba\n        -&gt; 188242.jpg\n        -&gt; 173822.jpg\n        -&gt; 284702.jpg\n        -&gt; 537394.jpg\n           ...\n\n</code></pre> <p>\u8fd9\u662f\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u6b65\u9aa4\uff0c\u56e0\u4e3a\u6211\u4eec\u5c06\u4f7f\u7528ImageFolder\u6570\u636e\u96c6\u7c7b\u9700\u8981\u4f7f\u7528\u5728\u6570\u636e\u96c6\u6839\u76ee\u5f55\u4e0b\u7684\u5b50\u6587\u4ef6\u5939\u3002\u73b0\u5728\uff0c\u6211\u4eec\u80fd\u591f\u521b\u5efa\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\u4ee5\u53ca\u8bbe\u7f6e\u5728\u54ea\u8fd0\u884c\uff0c\u6700\u540e\u53ef\u89c6\u5316\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e\u3002</p> <pre><code># \u6211\u4eec\u80fd\u591f\u4f7f\u7528\u6211\u4eec\u521b\u5efa\u7684\u6570\u636e\u96c6\u56fe\u7247\u6587\u4ef6\u5939\u4e86\n# \u521b\u5efa\u6570\u636e\u96c6\ndataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# \u51b3\u5b9a\u6211\u4eec\u5728\u54ea\u4e2a\u8bbe\u5907\u4e0a\u8fd0\u884c\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu &gt; 0) else \"cpu\")\n\n# \u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u56fe\u7247\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\n</code></pre> <p></p>"},{"location":"1.0/dcgan_faces_tutorial/#implementation","title":"\u5b9e\u73b0\u65b9\u6cd5(implementation\uff09","text":"<p>\u968f\u7740\u6211\u4eec\u53d8\u91cf\u7684\u8bbe\u7f6e\u4ee5\u53ca\u6570\u636e\u96c6\u7684\u51c6\u5907\uff0c\u6211\u4eec\u5c06\u5f00\u59cb\u8be6\u7ec6\u7684\u4ecb\u7ecd\u6743\u91cd\u521d\u59cb\u5316\u7b56\u7565\u3001\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u3001\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u3002</p>"},{"location":"1.0/dcgan_faces_tutorial/#_5","title":"\u6743\u91cd\u521d\u59cb\u5316","text":"<p>\u5728DCGAN\u8bba\u6587\u4e2d\uff0c\u4f5c\u8005\u6307\u51fa\u6240\u6709\u6a21\u578b\u6743\u91cd\u5e94\u4ece\u5747\u503c\u4e3a0\u65b9\u5dee\u4e3a0.2\u7684\u6b63\u6001\u5206\u5e03\u968f\u673a\u521d\u59cb\u5316\u3002 <code>weights_init</code>\u51fd\u6570\u5c06\u672a\u521d\u59cb\u5316\u6a21\u578b\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u521d\u59cb\u5316\u6240\u6709\u5377\u79ef\uff0c\u5377\u79ef\u8f6c\u7f6e\u548c\u6279\u6807\u51c6\u5316\u5c42\u4ee5\u6ee1\u8db3\u6b64\u6807\u51c6\u3002 \u521d\u59cb\u5316\u540e\u7acb\u5373\u5c06\u6b64\u529f\u80fd\u5e94\u7528\u4e8e\u6a21\u578b\u3002</p> <pre><code># \u5728netG\u548cnetD\u4e0a\u8c03\u7528\u7684\u81ea\u5b9a\u4e49\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_6","title":"\u751f\u6210\u5668","text":"<p>\u751f\u6210\u5668 \\(\\(G\\)\\) \u7528\u4e8e\u5c06\u672c\u5f81\u5411\u91cf \\(\\(z\\)\\) \u6620\u5c04\u5230\u6570\u636e\u7a7a\u95f4\u3002 \u7531\u4e8e\u6211\u4eec\u7684\u6570\u636e\u662f\u56fe\u50cf\uff0c\u56e0\u6b64\u5c06 \\(\\(z\\)\\) \u8f6c\u6362\u4e3a\u6570\u636e\u7a7a\u95f4\u610f\u5473\u7740\u6700\u7ec8\u521b\u5efa\u4e00\u4e2a\u4e0e\u8bad\u7ec3\u56fe\u50cf\u5927\u5c0f\u76f8\u540c\u7684RGB\u56fe\u50cf(\u53733x64x64\uff09\u3002 \u5b9e\u9645\u4e0a\uff0c\u8fd9\u662f\u901a\u8fc7\u4e00\u7cfb\u5217\u8de8\u6b65\u7684\u4e8c\u7ef4\u5377\u79ef\u8f6c\u7f6e\u5c42\u5b9e\u73b0\u7684\uff0c\u6bcf\u4e2a\u8f6c\u6362\u5c42\u4e0e\u4e8c\u7ef4\u6279\u6807\u51c6\u5316\u5c42\u548crelu\u6fc0\u6d3b\u5c42\u914d\u5bf9\u3002 \u751f\u6210\u5668\u7684\u8f93\u51fa\u901a\u8fc7tanh\u5c42\uff0c\u4f7f\u5176\u8f93\u51fa\u6570\u636e\u8303\u56f4\u548c\u8f93\u5165\u56fe\u7247\u4e00\u6837\uff0c\u5728 \\(\\([-1, 1]\\)\\) \u4e4b\u95f4\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728\u8f6c\u6362\u5c42\u4e4b\u540e\u5b58\u5728\u6279\u6807\u51c6\u5316\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u662fDCGAN\u8bba\u6587\u7684\u5173\u952e\u8d21\u732e\u3002 \u8fd9\u4e9b\u5c42\u6709\u52a9\u4e8e\u8bad\u7ec3\u671f\u95f4\u7684\u68af\u5ea6\u4f20\u64ad\u3002 DCGAN\u8bba\u6587\u4e2d\u7684\u751f\u6210\u5668\u56fe\u7247\u5982\u4e0b\u6240\u793a\u3002</p> <p></p> <p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5728\u53d8\u91cf\u5b9a\u4e49\u90e8\u5206 <code>(nz, ngf \u548c nc)</code> \u4e2d\u8bbe\u7f6e\u7684\u8f93\u5165\u5982\u4f55\u5f71\u54cd\u4ee3\u7801\u4e2d\u7684\u751f\u6210\u5668\u4f53\u7cfb\u7ed3\u6784\u3002 <code>nz</code> \u662fz\u8f93\u5165\u5411\u91cf\u7684\u957f\u5ea6\uff0c<code>ngf</code> \u751f\u6210\u5668\u8981\u751f\u6210\u7684\u7279\u5f81\u56fe\u4e2a\u6570\u5927\u5c0f\uff0c<code>nc</code> \u662f\u8f93\u51fa\u56fe\u50cf\u4e2d\u7684\u901a\u9053\u6570(\u5bf9\u4e8eRGB\u56fe\u50cf\uff0c\u8bbe\u7f6e\u4e3a3\uff09\u3002 \u4e0b\u9762\u662f\u751f\u6210\u5668\u7684\u4ee3\u7801\u3002</p> <pre><code># \u751f\u6210\u5668\u4ee3\u7801\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # \u8f93\u5165\u662f Z, \u5bf9Z\u8fdb\u884c\u5377\u79ef\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # \u8f93\u5165\u7279\u5f81\u56fe\u5927\u5c0f. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # \u8f93\u5165\u7279\u5f81\u56fe\u5927\u5c0f. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # \u8f93\u5165\u7279\u5f81\u56fe\u5927\u5c0f. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # \u8f93\u5165\u7279\u5f81\u56fe\u5927\u5c0f. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # \u8f93\u5165\u7279\u5f81\u56fe\u5927\u5c0f. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n</code></pre> <p>\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u4f8b\u5316\u751f\u6210\u5668\u5e76\u5bf9\u5176\u4f7f\u7528 <code>weights_init</code> \u51fd\u6570\u3002\u6253\u5370\u51fa\u751f\u6210\u5668\u6a21\u578b\uff0c\u7528\u4ee5\u67e5\u770b\u751f\u6210\u5668\u7684\u7ed3\u6784\u3002</p> <pre><code># \u521b\u5efa\u751f\u6210\u5668\nnetG = Generator(ngpu).to(device)\n\n# \u5982\u679c\u671f\u671b\u4f7f\u7528\u591a\u4e2aGPU\uff0c\u8bbe\u7f6e\u4e00\u4e0b\u3002\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# \u4f7f\u7528\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570 weights_init \u53bb\u968f\u673a\u521d\u59cb\u5316\u6240\u6709\u6743\u91cd\n#  mean=0, stdev=0.2.\nnetG.apply(weights_init)\n\n# \u8f93\u51fa\u8be5\u6a21\u578b\nprint(netG)\n\n</code></pre> <p>Out:</p> <pre><code>Generator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_7","title":"\u5224\u522b\u5668","text":"<p>\u5982\u4e0a\u6240\u8ff0\uff0c\u5224\u522b\u5668 \\(\\(D\\)\\) \u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u7f51\u7edc\uff0c\u5b83\u5c06\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u5e76\u8f93\u51fa\u8f93\u5165\u56fe\u50cf\u662f\u771f\u5b9e\u7684\u6982\u7387(\u800c\u4e0d\u662f\u5047\u7684\uff09\u3002 \u8fd9\u91cc\uff0c\\(\\(D\\)\\) \u91c7\u75283x64x64\u8f93\u5165\u56fe\u50cf\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217Conv2d\uff0cBatchNorm2d\u548cLeakyReLU\u5c42\u5904\u7406\u5b83\uff0c\u5e76\u901a\u8fc7Sigmoid\u6fc0\u6d3b\u51fd\u6570\u8f93\u51fa\u6700\u7ec8\u6982\u7387\u3002 \u5982\u679c\u95ee\u9898\u9700\u8981\uff0c\u53ef\u4ee5\u4f7f\u7528\u66f4\u591a\u5c42\u6269\u5c55\u6b64\u4f53\u7cfb\u7ed3\u6784\uff0c\u4f46\u4f7f\u7528\u8de8\u6b65\u5377\u79ef\uff0cBatchNorm\u548cLeakyReLU\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 DCGAN\u8bba\u6587\u63d0\u5230\u4f7f\u7528\u8de8\u6b65\u5377\u79ef\u800c\u4e0d\u662f\u4f7f\u7528pooling\u4e0b\u91c7\u6837\u662f\u4e00\u79cd\u5f88\u597d\u7684\u505a\u6cd5\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u8ba9\u7f51\u7edc\u5b66\u4e60\u81ea\u5df1\u7684pooling\u529f\u80fd\u3002\u6279\u6807\u51c6\u5316\u548cLeakyReLU\u51fd\u6570\u4e5f\u4fc3\u8fdb\u4e86\u5065\u5eb7\u7684\u68af\u5ea6\u6d41\u52a8\uff0c\u8fd9\u5bf9\u4e8e \\(\\(G\\)\\) \u548c \\(\\(D\\)\\) \u7684\u5b66\u4e60\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002</p> <p>\u5224\u522b\u5668\u4ee3\u7801</p> <pre><code>class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # \u8f93\u5165\u5927\u5c0f (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \u8f93\u5165\u5927\u5c0f. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \u8f93\u5165\u5927\u5c0f. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \u8f93\u5165\u5927\u5c0f. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n</code></pre> <p>\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9e\u4f8b\u5316\u5224\u522b\u5668\u5e76\u5bf9\u5176\u5e94\u7528<code>weights_init</code>\u51fd\u6570\u3002\u67e5\u770b\u6253\u5370\u7684\u6a21\u578b\u4ee5\u67e5\u770b\u5224\u522b\u5668\u5bf9\u8c61\u7684\u7ed3\u6784\u3002</p> <pre><code># \u521b\u5efa\u5224\u522b\u5668\nnetD = Discriminator(ngpu).to(device)\n\n# \u5982\u679c\u671f\u671b\u4f7f\u7528\u591aGPU\uff0c\u8bbe\u7f6e\u4e00\u4e0b\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n\n# \u4f7f\u7528\u6743\u91cd\u521d\u59cb\u5316\u51fd\u6570 weights_init \u53bb\u968f\u673a\u521d\u59cb\u5316\u6240\u6709\u6743\u91cd\n#  mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# \u8f93\u51fa\u8be5\u6a21\u578b\nprint(netD)\n\n</code></pre> <p>Out:</p> <pre><code>Discriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_8","title":"\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668","text":"<p>\u968f\u7740\u5bf9\u5224\u522b\u5668 \\(\\(D\\)\\) \u548c\u751f\u6210\u5668 \\(\\(G\\)\\) \u5b8c\u6210\u4e86\u8bbe\u7f6e\uff0c \u6211\u4eec\u80fd\u591f\u8be6\u7ec6\u7684\u53d9\u8ff0\u5b83\u4eec\u600e\u4e48\u901a\u8fc7\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\u6765\u8fdb\u884c\u5b66\u4e60\u7684\u3002\u6211\u4eec\u5c06\u4f7f\u7528Binary Cross Entropy loss (BCELoss) \u51fd\u6570\uff0c\u5176\u5728pyTorch\u4e2d\u7684\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <p>\\( \\ell(x, y) = L = {l_1,\\dots,l_N}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right] \\)</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\u76ee\u6807\u51fd\u6570\u4e2d\u4e24\u4e2alog\u90e8\u5206\u662f\u600e\u4e48\u63d0\u4f9b\u8ba1\u7b97\u7684(i.e. \\(\\(log(D(x))\\)\\) and \\(\\(log(1-D(G(z)))\\)\\) \u3002 \u5373\u5c06\u4ecb\u7ecd\u7684\u8bad\u7ec3\u5faa\u73af\u4e2d\u6211\u4eec\u5c06\u8be6\u7ec6\u7684\u4ecb\u7ecdBCE\u516c\u5f0f\u7684\u600e\u4e48\u4f7f\u7528\u8f93\u5165 \\(\\(y\\)\\) \u7684\u3002\u4f46\u91cd\u8981\u7684\u662f\u8981\u4e86\u89e3\u6211\u4eec\u5982\u4f55\u901a\u8fc7\u6539\u53d8 \\(\\(y\\)\\)(\u5373GT\u6807\u7b7e\uff09\u6765\u9009\u62e9\u6211\u4eec\u60f3\u8981\u8ba1\u7b97\u7684\u90e8\u5206\u635f\u5931\u3002</p> <p>\u4e0b\u4e00\u6b65\uff0c\u6211\u4eec\u5b9a\u4e49\u771f\u5b9e\u56fe\u7247\u6807\u8bb0\u4e3a1\uff0c\u5047\u56fe\u7247\u6807\u8bb0\u4e3a0\u3002\u8fd9\u4e2a\u6807\u8bb0\u5c06\u5728\u8ba1\u7b97 \\(\\(D\\)\\) \u548c \\(\\(G\\)\\) \u7684\u635f\u5931\u51fd\u6570\u7684\u65f6\u5019\u4f7f\u7528\uff0c\u8fd9\u662f\u5728\u539f\u59cb\u7684GAN\u6587\u732e\u4e2d\u4f7f\u7528\u7684\u60ef\u4f8b\u3002\u6700\u540e\u6211\u4eec\u8bbe\u7f6e\u4e24\u4e2a\u5355\u72ec\u7684\u4f18\u5316\u5668\uff0c\u4e00\u4e2a\u7ed9\u5224\u522b\u5668 \\(\\(D\\)\\) \u4f7f\u7528\uff0c\u4e00\u4e2a\u7ed9\u751f\u6210\u5668 \\(\\(G\\)\\) \u4f7f\u7528\u3002 \u5c31\u50cfDCGAN\u6587\u7ae0\u4e2d\u8bf4\u7684\u90a3\u6837\uff0c\u4e24\u4e2aAdam\u4f18\u5316\u7b97\u6cd5\u90fd\u662f\u7528\u5b66\u4e60\u7387\u4e3a0.0002\u4ee5\u53caBeta1\u53c2\u6570\u4e3a0.5\u3002\u4e3a\u4e86\u4fdd\u5b58\u8ffd\u8e2a\u751f\u6210\u5668\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u6211\u4eec\u5c06\u751f\u6210\u4e00\u4e2a\u6279\u56fa\u5b9a\u4e0d\u53d8\u7684\u6765\u81ea\u4e8e\u9ad8\u65af\u5206\u5e03\u7684\u672c\u5f81\u5411\u91cf(\u4f8b\u5982\u00a0fixed_noise)\u3002\u5728\u8bad\u7ec3\u7684\u5faa\u73af\u4e2d\uff0c\u6211\u4eec\u5c06\u5468\u671f\u6027\u7684\u8f93\u5165\u8fd9\u4e2afixed_noise\u5230\u751f\u6210\u5668 \\(\\(G\\)\\) \u4e2d\uff0c \u5728\u8bad\u7ec3\u90fd\u5b8c\u6210\u540e\u6211\u4eec\u5c06\u770b\u4e00\u4e0b\u7531fixed_noise\u751f\u6210\u7684\u56fe\u7247\u3002 </p> <pre><code># \u521d\u59cb\u5316 BCE\u635f\u5931\u51fd\u6570\ncriterion = nn.BCELoss()\n\n# \u521b\u5efa\u4e00\u4e2a\u6279\u6b21\u7684\u672c\u5f81\u5411\u91cf\u7528\u4e8e\u53ef\u89c6\u5316\u751f\u6210\u5668\u8bad\u7ec3\u7684\u8fc7\u7a0b\u3002\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# \u5efa\u7acb\u4e00\u4e2a\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u771f\u5b9e\u548c\u5047\u7684\u6807\u8bb0\nreal_label = 1\nfake_label = 0\n\n# \u4e3aG\u548cD\u90fd\u8bbe\u7f6eAdam\u4f18\u5316\u5668\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_9","title":"\u8bad\u7ec3","text":"<p>\u6700\u540e\uff0c\u65e2\u7136\u5df2\u7ecf\u5b9a\u4e49\u4e86GAN\u6846\u67b6\u7684\u6240\u6709\u90e8\u5206\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c\u8bad\u7ec3\u3002 \u8bf7\u6ce8\u610f\uff0c\u8bad\u7ec3GAN\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u662f\u4e00\u79cd\u827a\u672f\u5f62\u5f0f\uff0c\u56e0\u4e3a\u4e0d\u6b63\u786e\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u4f1a\u5bfc\u81f4mode collapse\uff0c\u800c\u5bf9\u9519\u8bef\u7684\u89e3\u91ca\u5f88\u5c11\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u5bc6\u5207\u5173\u6ce8Goodfellow\u7684\u8bba\u6587\u4e2d\u7684\u7b97\u6cd51\uff0c\u540c\u65f6\u9075\u5b88ganhacks\u4e2d\u663e\u793a\u7684\u4e00\u4e9b\u6700\u4f73\u5b9e\u8df5\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u5c06\u201c\u4e3a\u771f\u5b9e\u548c\u5047\u5192\u201d\u56fe\u50cf\u6784\u5efa\u4e0d\u540c\u7684\u5c0f\u6279\u91cf\uff0c\u5e76\u8c03\u6574G\u7684\u76ee\u6807\u51fd\u6570\u4ee5\u6700\u5927\u5316\\(\\(logD(G(z))\\)\\)\u3002 \u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\u3002 \u7b2c1\u90e8\u5206\u66f4\u65b0\u5224\u522b\u5668Discriminator\uff0c\u7b2c2\u90e8\u5206\u66f4\u65b0\u751f\u6210\u5668Generator\u3002 </p> <p>Part 1 - \u8bad\u7ec3\u5224\u522b\u5668</p> <p>\u56de\u60f3\u4e00\u4e0b\uff0c\u8bad\u7ec3\u5224\u522b\u5668\u7684\u76ee\u7684\u662f\u6700\u5927\u5316\u5c06\u7ed9\u5b9a\u8f93\u5165\u6b63\u786e\u5206\u7c7b\u4e3a\u771f\u5b9e\u6216\u5047\u7684\u6982\u7387\u3002 \u5c31Goodfellow\u800c\u8a00\uff0c\u6211\u4eec\u5e0c\u671b\u201c\u901a\u8fc7\u63d0\u5347\u5176\u968f\u673a\u68af\u5ea6\u6765\u66f4\u65b0\u5224\u522b\u5668\u201d\u3002 \u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u60f3\u8981\u6700\u5927\u5316\u635f\u5931\\(\\(log(D(x))+ log(1-D(G(z)))\\)\\)\u3002 \u7531\u4e8eganhacks\u7684\u5355\u72ec\u5c0f\u6279\u91cf\u5efa\u8bae\uff0c\u6211\u4eec\u5c06\u5206\u4e24\u6b65\u8ba1\u7b97\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5c06\u4ece\u8bad\u7ec3\u96c6\u4e2d\u6784\u9020\u4e00\u6279\u5b9e\u9645\u6837\u672c\uff0c\u5411\u524d\u901a\u8fc7 \\(\\(D\\)\\)\uff0c\u8ba1\u7b97\u635f\u5931(\\(\\(log(D(x))\\)\\)\uff09\uff0c\u7136\u540e\u8ba1\u7b97\u68af\u5ea6 \u5411\u540e\u4f20\u9012\u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5c06\u7528\u5f53\u524d\u7684\u751f\u6210\u5668\u6784\u9020\u4e00\u6279\u5047\u6837\u672c\uff0c\u901a\u8fc7\\((D (\\(\u8f6c\u53d1\u8be5\u6279\u6b21\uff0c\u8ba1\u7b97\u635f\u5931(\\)\\)log(1-D(G(z)))\\)\\)\uff09\u548c  accumulate \u5e26\u6709\u5411\u540e\u4f20\u9012\u3002 \u73b0\u5728\uff0c\u968f\u7740\u4ece\u5168\u771f\u5b9e\u548c\u5168\u5047\u6279\u91cf\u7d2f\u79ef\u7684\u68af\u5ea6\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aDiscriminator\u4f18\u5316\u5668\u7684\u4e00\u6b65\u3002 </p> <p>Part 2 - \u8bad\u7ec3\u751f\u6210\u5668</p> <p>\u6b63\u5982\u539f\u59cb\u8bba\u6587\u6240\u8ff0\uff0c\u6211\u4eec\u5e0c\u671b\u901a\u8fc7\u6700\u5c0f\u5316 \\(\\(log(1-D(G(z)))\\)\\) \u6765\u8bad\u7ec3\u751f\u6210\u5668Generator\uff0c\u4ee5\u4fbf\u4ea7\u751f\u66f4\u597d\u7684\u5047\u6837\u672c\u3002 \u5982\u4e0a\u6240\u8ff0\uff0cGoodfellow\u8868\u660e\u8fd9\u4e0d\u4f1a\u63d0\u4f9b\u8db3\u591f\u7684\u68af\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5b66\u4e60\u8fc7\u7a0b\u7684\u65e9\u671f\u9636\u6bb5\u3002 \u4f5c\u4e3a\u4fee\u6539\uff0c\u6211\u4eec\u5e0c\u671b\u6700\u5927\u5316 \\(\\(log(D(G(z)))\\)\\)\u3002 \u5728\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u73b0\u6b64\u76ee\u7684\uff1a\u4f7f\u7528 Discriminator \u5bf9\u7b2c1\u90e8\u5206\u7684 Generator \u8f93\u51fa\u8fdb\u884c\u5206\u7c7b\uff0c\u4f7f\u7528\u771f\u5b9e\u6807\u7b7e\u4f5c\u4e3a <code>GT</code>\u8ba1\u7b97<code>G</code>\u7684\u635f\u5931\uff0c\u5728\u53cd\u5411\u4f20\u9012\u4e2d\u8ba1\u7b97<code>G</code>\u7684\u68af\u5ea6\uff0c\u6700\u540e\u4f7f\u7528\u4f18\u5316\u5668\u6b65\u9aa4\u66f4\u65b0<code>G</code>\u7684\u53c2\u6570\u3002\u4f7f\u7528\u771f\u5b9e\u6807\u7b7e\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u7684<code>GT</code>\u6807\u7b7e\u4f3c\u4e4e\u662f\u8fdd\u53cd\u76f4\u89c9\u7684\uff0c\u4f46\u8fd9\u5141\u8bb8\u6211\u4eec\u4f7f\u7528BCELoss\u7684 \\(\\(log(x)\\)\\) \u90e8\u5206(\u800c\u4e0d\u662f \\(\\(log(1-x)\\)\\) \u8fd9\u90e8\u5206\uff09\u8fd9\u6b63\u662f\u6211\u4eec\u60f3\u8981\u7684\u3002</p> <p>\u6700\u540e\uff0c\u6211\u4eec\u5c06\u8fdb\u884c\u4e00\u4e9b\u7edf\u8ba1\u62a5\u544a\uff0c\u5728\u6bcf\u4e2a\u5faa\u73af\u7ed3\u675f\u65f6\uff0c\u6211\u4eec\u5c06\u901a\u8fc7\u751f\u6210\u5668\u63a8\u9001\u6211\u4eec\u7684fixed_noise\u6279\u6b21\uff0c\u4ee5\u76f4\u89c2\u5730\u8ddf\u8e2aG\u8bad\u7ec3\u7684\u8fdb\u5ea6\u3002 \u62a5\u544a\u7684\u8bad\u7ec3\u7edf\u8ba1\u6570\u636e\u662f\uff1a</p> <ul> <li>Loss_D - \u5224\u522b\u5668\u635f\u5931\u662f\u6240\u6709\u771f\u5b9e\u6837\u672c\u6279\u6b21\u548c\u6240\u6709\u5047\u6837\u672c\u6279\u6b21\u7684\u635f\u5931\u4e4b\u548c  \\(\\(log(D(x)) + log(D(G(z)))\\)\\) .</li> <li>Loss_G - \u751f\u6210\u5668\u635f\u5931 \\(\\(log(D(G(z)))\\)\\)</li> <li>D(x) - \u6240\u6709\u771f\u5b9e\u6279\u6b21\u7684\u5224\u522b\u5668\u7684\u5e73\u5747\u8f93\u51fa(\u6574\u6279\uff09\u3002 \u8fd9\u5e94\u8be5\u4ece\u63a5\u8fd11\u5f00\u59cb\uff0c\u7136\u540e\u5f53G\u53d8\u597d\u65f6\u7406\u8bba\u4e0a\u6536\u655b\u52300.5\u3002 \u60f3\u60f3\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\u3002</li> <li>D(G(z)) - \u6240\u6709\u5047\u6279\u6b21\u7684\u5e73\u5747\u5224\u522b\u5668\u8f93\u51fa\u3002 \u7b2c\u4e00\u4e2a\u6570\u5b57\u662f\u5728D\u66f4\u65b0\u4e4b\u524d\uff0c\u7b2c\u4e8c\u4e2a\u6570\u5b57\u662f\u5728D\u66f4\u65b0\u4e4b\u540e\u3002 \u5f53G\u53d8\u597d\u65f6\uff0c\u8fd9\u4e9b\u6570\u5b57\u5e94\u8be5\u4ece0\u5f00\u59cb\u5e76\u6536\u655b\u52300.5\u3002 \u60f3\u60f3\u4e3a\u4ec0\u4e48\u4f1a\u8fd9\u6837\u3002</li> </ul> <p>Note: \u6b64\u6b65\u9aa4\u53ef\u80fd\u9700\u8981\u4e00\u6bb5\u65f6\u95f4\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u60a8\u8fd0\u884c\u7684\u5faa\u73af\u6570\u4ee5\u53ca\u662f\u5426\u4ece\u6570\u636e\u96c6\u4e2d\u5220\u9664\u4e86\u4e00\u4e9b\u6570\u636e\u3002</p> <pre><code># \u8bad\u7ec3\u5faa\u73af\n\n# \u4fdd\u5b58\u8ddf\u8e2a\u8fdb\u5ea6\u7684\u5217\u8868\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# \u6bcf\u4e2aepoh\nfor epoch in range(num_epochs):\n    # \u6570\u636e\u52a0\u8f7d\u5668\u4e2d\u7684\u6bcf\u4e2a\u6279\u6b21\n    for i, data in enumerate(dataloader, 0):\n\n        ############################\n        # (1) \u66f4\u65b0 D \u7f51\u7edc: \u6700\u5927\u5316 log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## \u4f7f\u7528\u6240\u6709\u771f\u5b9e\u6837\u672c\u6279\u6b21\u8bad\u7ec3\n        netD.zero_grad()\n        # \u683c\u5f0f\u5316\u6279\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n        # \u901a\u8fc7D\u5411\u524d\u4f20\u9012\u771f\u5b9e\u6279\u6b21\n        output = netD(real_cpu).view(-1)\n        # \u5bf9\u6240\u6709\u771f\u5b9e\u6837\u672c\u6279\u6b21\u8ba1\u7b97\u635f\u5931\n        errD_real = criterion(output, label)\n        # \u8ba1\u7b97\u540e\u5411\u4f20\u9012\u4e2dD\u7684\u68af\u5ea6\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## \u4f7f\u7528\u6240\u6709\u5047\u6837\u672c\u6279\u6b21\u8bad\u7ec3\n        # \u751f\u6210\u672c\u5f81\u5411\u91cf\u6279\u6b21\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # \u4f7f\u7528\u751f\u6210\u5668G\u751f\u6210\u5047\u56fe\u7247\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # \u4f7f\u7528\u5224\u522b\u5668\u5206\u7c7b\u6240\u6709\u7684\u5047\u6279\u6b21\u6837\u672c\n        output = netD(fake.detach()).view(-1)\n        # \u8ba1\u7b97\u5224\u522b\u5668D\u7684\u635f\u5931\u5bf9\u6240\u6709\u7684\u5047\u6837\u672c\u6279\u6b21\n        errD_fake = criterion(output, label)\n        # \u5bf9\u8fd9\u4e2a\u6279\u6b21\u8ba1\u7b97\u68af\u5ea6\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # \u628a\u6240\u6709\u771f\u6837\u672c\u548c\u5047\u6837\u672c\u6279\u6b21\u7684\u68af\u5ea6\u52a0\u8d77\u6765\n        errD = errD_real + errD_fake\n        # \u66f4\u65b0\u5224\u522b\u5668D\n        optimizerD.step()\n\n        ############################\n        # (2) \u66f4\u65b0 G \u7f51\u7edc: \u6700\u5927\u5316 log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # \u5047\u6837\u672c\u7684\u6807\u7b7e\u5bf9\u4e8e\u751f\u6210\u5668\u6210\u672c\u662f\u771f\u7684\n        # \u56e0\u4e3a\u6211\u4eec\u4e4b\u66f4\u65b0\u4e86D\uff0c\u901a\u8fc7D\u6267\u884c\u6240\u6709\u5047\u6837\u672c\u6279\u6b21\u7684\u6b63\u5411\u4f20\u9012\n        output = netD(fake).view(-1)\n        # \u57fa\u4e8e\u8fd9\u4e2a\u8f93\u51fa\u8ba1\u7b97G\u7684\u635f\u5931\n        errG = criterion(output, label)\n        # \u4e3a\u751f\u6210\u5668\u8ba1\u7b97\u68af\u5ea6\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # \u66f4\u65b0\u751f\u6210\u5668G\n        optimizerG.step()\n\n        # \u8f93\u51fa\u8bad\u7ec3\u72b6\u6001\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # \u4e3a\u4ee5\u540e\u753b\u635f\u5931\u56fe\uff0c\u4fdd\u5b58\u635f\u5931\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # \u68c0\u67e5\u751f\u6210\u5668generator\u505a\u4e86\u4ec0\u4e48\uff0c\u901a\u8fc7\u4fdd\u5b58\u7684fixed_noise\u901a\u8fc7G\u7684\u8f93\u51fa\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n\n</code></pre> <p>Out:</p> <pre><code>Starting Training Loop...\n[0/5][0/1583]   Loss_D: 1.7410  Loss_G: 4.7761  D(x): 0.5343    D(G(z)): 0.5771 / 0.0136\n[0/5][50/1583]  Loss_D: 1.7332  Loss_G: 25.4829 D(x): 0.9774    D(G(z)): 0.7441 / 0.0000\n[0/5][100/1583] Loss_D: 1.6841  Loss_G: 11.6585 D(x): 0.4728    D(G(z)): 0.0000 / 0.0000\n[0/5][150/1583] Loss_D: 1.2547  Loss_G: 8.7245  D(x): 0.9286    D(G(z)): 0.5209 / 0.0044\n[0/5][200/1583] Loss_D: 0.7563  Loss_G: 8.9600  D(x): 0.9525    D(G(z)): 0.4514 / 0.0003\n[0/5][250/1583] Loss_D: 1.0221  Loss_G: 2.5713  D(x): 0.5274    D(G(z)): 0.0474 / 0.1177\n[0/5][300/1583] Loss_D: 0.3387  Loss_G: 3.8185  D(x): 0.8431    D(G(z)): 0.1066 / 0.0461\n[0/5][350/1583] Loss_D: 0.5054  Loss_G: 3.6141  D(x): 0.7289    D(G(z)): 0.0758 / 0.0535\n[0/5][400/1583] Loss_D: 0.8758  Loss_G: 6.5680  D(x): 0.8097    D(G(z)): 0.4017 / 0.0031\n[0/5][450/1583] Loss_D: 0.2486  Loss_G: 3.5121  D(x): 0.9035    D(G(z)): 0.1054 / 0.0717\n[0/5][500/1583] Loss_D: 1.5792  Loss_G: 4.3590  D(x): 0.3457    D(G(z)): 0.0053 / 0.0379\n[0/5][550/1583] Loss_D: 0.8897  Loss_G: 3.9447  D(x): 0.5350    D(G(z)): 0.0349 / 0.0386\n[0/5][600/1583] Loss_D: 0.5292  Loss_G: 4.4346  D(x): 0.8914    D(G(z)): 0.2768 / 0.0233\n[0/5][650/1583] Loss_D: 0.3779  Loss_G: 4.7253  D(x): 0.7868    D(G(z)): 0.0627 / 0.0174\n[0/5][700/1583] Loss_D: 0.7512  Loss_G: 2.6246  D(x): 0.6112    D(G(z)): 0.0244 / 0.1493\n[0/5][750/1583] Loss_D: 0.4378  Loss_G: 5.0045  D(x): 0.8614    D(G(z)): 0.2028 / 0.0108\n[0/5][800/1583] Loss_D: 0.5795  Loss_G: 6.0537  D(x): 0.8693    D(G(z)): 0.2732 / 0.0066\n[0/5][850/1583] Loss_D: 0.8980  Loss_G: 6.5355  D(x): 0.8465    D(G(z)): 0.4226 / 0.0048\n[0/5][900/1583] Loss_D: 0.5776  Loss_G: 7.7162  D(x): 0.9756    D(G(z)): 0.3707 / 0.0009\n[0/5][950/1583] Loss_D: 0.5593  Loss_G: 5.6692  D(x): 0.9560    D(G(z)): 0.3494 / 0.0080\n[0/5][1000/1583]        Loss_D: 0.5036  Loss_G: 5.1312  D(x): 0.7775    D(G(z)): 0.0959 / 0.0178\n[0/5][1050/1583]        Loss_D: 0.5192  Loss_G: 4.5706  D(x): 0.8578    D(G(z)): 0.2605 / 0.0222\n[0/5][1100/1583]        Loss_D: 0.5645  Loss_G: 3.1618  D(x): 0.7133    D(G(z)): 0.1138 / 0.0768\n[0/5][1150/1583]        Loss_D: 0.2790  Loss_G: 4.5294  D(x): 0.8541    D(G(z)): 0.0909 / 0.0207\n[0/5][1200/1583]        Loss_D: 0.5334  Loss_G: 4.3445  D(x): 0.8567    D(G(z)): 0.2457 / 0.0245\n[0/5][1250/1583]        Loss_D: 0.7318  Loss_G: 2.2779  D(x): 0.6846    D(G(z)): 0.1485 / 0.1497\n[0/5][1300/1583]        Loss_D: 0.6939  Loss_G: 6.1172  D(x): 0.9123    D(G(z)): 0.3853 / 0.0041\n[0/5][1350/1583]        Loss_D: 0.4653  Loss_G: 3.7054  D(x): 0.8208    D(G(z)): 0.1774 / 0.0404\n[0/5][1400/1583]        Loss_D: 1.9711  Loss_G: 3.1569  D(x): 0.2704    D(G(z)): 0.0108 / 0.1390\n[0/5][1450/1583]        Loss_D: 0.4427  Loss_G: 5.8683  D(x): 0.9230    D(G(z)): 0.2600 / 0.0056\n[0/5][1500/1583]        Loss_D: 0.4432  Loss_G: 3.3681  D(x): 0.8001    D(G(z)): 0.1510 / 0.0633\n[0/5][1550/1583]        Loss_D: 0.4852  Loss_G: 3.2790  D(x): 0.7532    D(G(z)): 0.1100 / 0.0661\n[1/5][0/1583]   Loss_D: 0.3536  Loss_G: 4.5358  D(x): 0.8829    D(G(z)): 0.1714 / 0.0173\n[1/5][50/1583]  Loss_D: 0.4717  Loss_G: 4.7728  D(x): 0.8973    D(G(z)): 0.2750 / 0.0142\n[1/5][100/1583] Loss_D: 0.4702  Loss_G: 2.3528  D(x): 0.7847    D(G(z)): 0.1468 / 0.1385\n[1/5][150/1583] Loss_D: 0.4833  Loss_G: 2.9645  D(x): 0.7893    D(G(z)): 0.1607 / 0.0867\n[1/5][200/1583] Loss_D: 0.6035  Loss_G: 2.0728  D(x): 0.6646    D(G(z)): 0.0852 / 0.1806\n[1/5][250/1583] Loss_D: 0.3822  Loss_G: 3.1946  D(x): 0.7969    D(G(z)): 0.1024 / 0.0656\n[1/5][300/1583] Loss_D: 0.3892  Loss_G: 3.3337  D(x): 0.7848    D(G(z)): 0.0969 / 0.0525\n[1/5][350/1583] Loss_D: 1.7989  Loss_G: 7.5798  D(x): 0.9449    D(G(z)): 0.7273 / 0.0011\n[1/5][400/1583] Loss_D: 0.4765  Loss_G: 3.0655  D(x): 0.7479    D(G(z)): 0.1116 / 0.0687\n[1/5][450/1583] Loss_D: 0.3649  Loss_G: 3.1674  D(x): 0.8603    D(G(z)): 0.1619 / 0.0627\n[1/5][500/1583] Loss_D: 0.6922  Loss_G: 4.5841  D(x): 0.9235    D(G(z)): 0.4003 / 0.0175\n[1/5][550/1583] Loss_D: 0.6126  Loss_G: 4.6642  D(x): 0.8761    D(G(z)): 0.3199 / 0.0180\n[1/5][600/1583] Loss_D: 0.7032  Loss_G: 4.6221  D(x): 0.9463    D(G(z)): 0.4365 / 0.0154\n[1/5][650/1583] Loss_D: 0.4707  Loss_G: 3.3616  D(x): 0.7664    D(G(z)): 0.1280 / 0.0617\n[1/5][700/1583] Loss_D: 0.3393  Loss_G: 2.4236  D(x): 0.9120    D(G(z)): 0.1771 / 0.1280\n[1/5][750/1583] Loss_D: 0.6828  Loss_G: 4.4585  D(x): 0.8647    D(G(z)): 0.3546 / 0.0191\n[1/5][800/1583] Loss_D: 0.7958  Loss_G: 3.6708  D(x): 0.8386    D(G(z)): 0.3987 / 0.0403\n[1/5][850/1583] Loss_D: 0.4651  Loss_G: 2.7477  D(x): 0.7602    D(G(z)): 0.1334 / 0.0900\n[1/5][900/1583] Loss_D: 0.8799  Loss_G: 4.7930  D(x): 0.9050    D(G(z)): 0.4710 / 0.0201\n[1/5][950/1583] Loss_D: 0.3909  Loss_G: 2.7973  D(x): 0.7730    D(G(z)): 0.0902 / 0.0838\n[1/5][1000/1583]        Loss_D: 0.3822  Loss_G: 3.0223  D(x): 0.8699    D(G(z)): 0.1837 / 0.0709\n[1/5][1050/1583]        Loss_D: 0.4689  Loss_G: 2.2831  D(x): 0.7096    D(G(z)): 0.0536 / 0.1448\n[1/5][1100/1583]        Loss_D: 0.6676  Loss_G: 2.2773  D(x): 0.6669    D(G(z)): 0.1386 / 0.1443\n[1/5][1150/1583]        Loss_D: 0.5970  Loss_G: 4.1558  D(x): 0.9166    D(G(z)): 0.3554 / 0.0240\n[1/5][1200/1583]        Loss_D: 0.3622  Loss_G: 3.5782  D(x): 0.8590    D(G(z)): 0.1547 / 0.0481\n[1/5][1250/1583]        Loss_D: 0.5234  Loss_G: 2.5915  D(x): 0.7811    D(G(z)): 0.1990 / 0.1037\n[1/5][1300/1583]        Loss_D: 1.3243  Loss_G: 5.5428  D(x): 0.9882    D(G(z)): 0.6572 / 0.0088\n[1/5][1350/1583]        Loss_D: 0.4891  Loss_G: 1.9552  D(x): 0.7686    D(G(z)): 0.1540 / 0.1910\n[1/5][1400/1583]        Loss_D: 0.5639  Loss_G: 3.7796  D(x): 0.9137    D(G(z)): 0.3390 / 0.0343\n[1/5][1450/1583]        Loss_D: 1.7329  Loss_G: 5.0373  D(x): 0.9760    D(G(z)): 0.7332 / 0.0161\n[1/5][1500/1583]        Loss_D: 0.7999  Loss_G: 3.7268  D(x): 0.9029    D(G(z)): 0.4550 / 0.0384\n[1/5][1550/1583]        Loss_D: 0.4740  Loss_G: 2.3220  D(x): 0.7824    D(G(z)): 0.1625 / 0.1327\n[2/5][0/1583]   Loss_D: 0.8693  Loss_G: 3.8890  D(x): 0.9376    D(G(z)): 0.4822 / 0.0339\n[2/5][50/1583]  Loss_D: 0.3742  Loss_G: 2.5041  D(x): 0.8148    D(G(z)): 0.1310 / 0.1151\n[2/5][100/1583] Loss_D: 1.1134  Loss_G: 1.5167  D(x): 0.4248    D(G(z)): 0.0335 / 0.3023\n[2/5][150/1583] Loss_D: 0.5987  Loss_G: 3.2047  D(x): 0.8536    D(G(z)): 0.3121 / 0.0555\n[2/5][200/1583] Loss_D: 2.0846  Loss_G: 1.5473  D(x): 0.1919    D(G(z)): 0.0054 / 0.2899\n[2/5][250/1583] Loss_D: 0.5017  Loss_G: 3.0225  D(x): 0.8965    D(G(z)): 0.2986 / 0.0626\n[2/5][300/1583] Loss_D: 1.3296  Loss_G: 4.1927  D(x): 0.9444    D(G(z)): 0.6574 / 0.0270\n[2/5][350/1583] Loss_D: 0.4905  Loss_G: 2.7693  D(x): 0.8049    D(G(z)): 0.2090 / 0.0863\n[2/5][400/1583] Loss_D: 0.4668  Loss_G: 2.1790  D(x): 0.7160    D(G(z)): 0.0815 / 0.1529\n[2/5][450/1583] Loss_D: 0.4877  Loss_G: 2.4190  D(x): 0.6943    D(G(z)): 0.0693 / 0.1254\n[2/5][500/1583] Loss_D: 0.7856  Loss_G: 2.2362  D(x): 0.6148    D(G(z)): 0.1698 / 0.1489\n[2/5][550/1583] Loss_D: 0.6371  Loss_G: 1.3879  D(x): 0.6164    D(G(z)): 0.0852 / 0.3041\n[2/5][600/1583] Loss_D: 0.6409  Loss_G: 2.8623  D(x): 0.7658    D(G(z)): 0.2684 / 0.0790\n[2/5][650/1583] Loss_D: 0.6454  Loss_G: 1.5708  D(x): 0.6293    D(G(z)): 0.0944 / 0.2706\n[2/5][700/1583] Loss_D: 0.8472  Loss_G: 2.0847  D(x): 0.5071    D(G(z)): 0.0181 / 0.1937\n[2/5][750/1583] Loss_D: 1.2356  Loss_G: 0.3673  D(x): 0.3606    D(G(z)): 0.0328 / 0.7270\n[2/5][800/1583] Loss_D: 0.4852  Loss_G: 2.7325  D(x): 0.8670    D(G(z)): 0.2630 / 0.0877\n[2/5][850/1583] Loss_D: 0.6494  Loss_G: 4.5357  D(x): 0.8899    D(G(z)): 0.3756 / 0.0158\n[2/5][900/1583] Loss_D: 0.5184  Loss_G: 2.7194  D(x): 0.8377    D(G(z)): 0.2540 / 0.0871\n[2/5][950/1583] Loss_D: 0.9771  Loss_G: 4.6200  D(x): 0.9596    D(G(z)): 0.5432 / 0.0176\n[2/5][1000/1583]        Loss_D: 0.7509  Loss_G: 2.2864  D(x): 0.5861    D(G(z)): 0.1021 / 0.1539\n[2/5][1050/1583]        Loss_D: 0.4512  Loss_G: 3.2484  D(x): 0.8649    D(G(z)): 0.2313 / 0.0542\n[2/5][1100/1583]        Loss_D: 0.6856  Loss_G: 2.2425  D(x): 0.6405    D(G(z)): 0.1333 / 0.1508\n[2/5][1150/1583]        Loss_D: 0.5271  Loss_G: 3.0327  D(x): 0.8385    D(G(z)): 0.2552 / 0.0639\n[2/5][1200/1583]        Loss_D: 0.4058  Loss_G: 2.9557  D(x): 0.8769    D(G(z)): 0.2169 / 0.0694\n[2/5][1250/1583]        Loss_D: 0.5564  Loss_G: 2.9065  D(x): 0.8409    D(G(z)): 0.2835 / 0.0695\n[2/5][1300/1583]        Loss_D: 0.4703  Loss_G: 2.7865  D(x): 0.7825    D(G(z)): 0.1680 / 0.0850\n[2/5][1350/1583]        Loss_D: 0.5352  Loss_G: 3.1362  D(x): 0.8260    D(G(z)): 0.2582 / 0.0606\n[2/5][1400/1583]        Loss_D: 0.5281  Loss_G: 2.7742  D(x): 0.7970    D(G(z)): 0.2275 / 0.0835\n[2/5][1450/1583]        Loss_D: 0.6558  Loss_G: 1.8152  D(x): 0.6103    D(G(z)): 0.0795 / 0.2030\n[2/5][1500/1583]        Loss_D: 0.9446  Loss_G: 1.1492  D(x): 0.4593    D(G(z)): 0.0356 / 0.3947\n[2/5][1550/1583]        Loss_D: 0.9269  Loss_G: 0.7383  D(x): 0.5226    D(G(z)): 0.1333 / 0.5205\n[3/5][0/1583]   Loss_D: 0.4855  Loss_G: 2.1548  D(x): 0.7157    D(G(z)): 0.1059 / 0.1568\n[3/5][50/1583]  Loss_D: 0.7259  Loss_G: 1.1093  D(x): 0.5804    D(G(z)): 0.0797 / 0.3894\n[3/5][100/1583] Loss_D: 0.7367  Loss_G: 1.0389  D(x): 0.5515    D(G(z)): 0.0405 / 0.4190\n[3/5][150/1583] Loss_D: 0.5942  Loss_G: 3.4803  D(x): 0.9290    D(G(z)): 0.3709 / 0.0432\n[3/5][200/1583] Loss_D: 1.3464  Loss_G: 0.6549  D(x): 0.3261    D(G(z)): 0.0242 / 0.5949\n[3/5][250/1583] Loss_D: 0.5110  Loss_G: 2.2086  D(x): 0.7263    D(G(z)): 0.1327 / 0.1457\n[3/5][300/1583] Loss_D: 1.4272  Loss_G: 3.3018  D(x): 0.9230    D(G(z)): 0.6654 / 0.0635\n[3/5][350/1583] Loss_D: 0.6491  Loss_G: 3.0766  D(x): 0.8124    D(G(z)): 0.3127 / 0.0607\n[3/5][400/1583] Loss_D: 0.5583  Loss_G: 2.9363  D(x): 0.8233    D(G(z)): 0.2759 / 0.0666\n[3/5][450/1583] Loss_D: 0.9496  Loss_G: 0.6436  D(x): 0.4958    D(G(z)): 0.1367 / 0.5538\n[3/5][500/1583] Loss_D: 0.4463  Loss_G: 2.2234  D(x): 0.7776    D(G(z)): 0.1545 / 0.1371\n[3/5][550/1583] Loss_D: 0.5874  Loss_G: 3.6688  D(x): 0.8478    D(G(z)): 0.2930 / 0.0348\n[3/5][600/1583] Loss_D: 0.3724  Loss_G: 2.6326  D(x): 0.8673    D(G(z)): 0.1854 / 0.0891\n[3/5][650/1583] Loss_D: 0.7292  Loss_G: 4.4254  D(x): 0.9081    D(G(z)): 0.4234 / 0.0200\n[3/5][700/1583] Loss_D: 0.4728  Loss_G: 2.8665  D(x): 0.8189    D(G(z)): 0.2115 / 0.0774\n[3/5][750/1583] Loss_D: 0.5845  Loss_G: 3.3046  D(x): 0.8977    D(G(z)): 0.3490 / 0.0463\n[3/5][800/1583] Loss_D: 0.5597  Loss_G: 2.2564  D(x): 0.7088    D(G(z)): 0.1497 / 0.1300\n[3/5][850/1583] Loss_D: 0.6518  Loss_G: 2.5048  D(x): 0.7195    D(G(z)): 0.2183 / 0.1053\n[3/5][900/1583] Loss_D: 0.7340  Loss_G: 1.4263  D(x): 0.6285    D(G(z)): 0.1806 / 0.2818\n[3/5][950/1583] Loss_D: 1.4633  Loss_G: 4.9204  D(x): 0.9792    D(G(z)): 0.7093 / 0.0143\n[3/5][1000/1583]        Loss_D: 0.6643  Loss_G: 2.8332  D(x): 0.8548    D(G(z)): 0.3597 / 0.0751\n[3/5][1050/1583]        Loss_D: 0.7741  Loss_G: 2.9355  D(x): 0.7281    D(G(z)): 0.3064 / 0.0712\n[3/5][1100/1583]        Loss_D: 0.7279  Loss_G: 3.2299  D(x): 0.8867    D(G(z)): 0.4193 / 0.0544\n[3/5][1150/1583]        Loss_D: 0.6049  Loss_G: 1.9150  D(x): 0.6917    D(G(z)): 0.1645 / 0.1912\n[3/5][1200/1583]        Loss_D: 0.7431  Loss_G: 3.8188  D(x): 0.9334    D(G(z)): 0.4500 / 0.0306\n[3/5][1250/1583]        Loss_D: 0.5061  Loss_G: 1.9905  D(x): 0.7393    D(G(z)): 0.1531 / 0.1653\n[3/5][1300/1583]        Loss_D: 0.6979  Loss_G: 3.0183  D(x): 0.8182    D(G(z)): 0.3421 / 0.0616\n[3/5][1350/1583]        Loss_D: 0.9133  Loss_G: 4.0629  D(x): 0.9198    D(G(z)): 0.5131 / 0.0261\n[3/5][1400/1583]        Loss_D: 0.7075  Loss_G: 4.0061  D(x): 0.9188    D(G(z)): 0.4216 / 0.0266\n[3/5][1450/1583]        Loss_D: 0.7704  Loss_G: 2.3802  D(x): 0.7555    D(G(z)): 0.3348 / 0.1114\n[3/5][1500/1583]        Loss_D: 0.6055  Loss_G: 1.8402  D(x): 0.7011    D(G(z)): 0.1643 / 0.1995\n[3/5][1550/1583]        Loss_D: 0.7240  Loss_G: 3.2589  D(x): 0.8747    D(G(z)): 0.4069 / 0.0528\n[4/5][0/1583]   Loss_D: 0.8162  Loss_G: 2.8040  D(x): 0.8827    D(G(z)): 0.4435 / 0.0870\n[4/5][50/1583]  Loss_D: 0.5859  Loss_G: 2.2796  D(x): 0.6782    D(G(z)): 0.1312 / 0.1309\n[4/5][100/1583] Loss_D: 0.6655  Loss_G: 3.5365  D(x): 0.8178    D(G(z)): 0.3262 / 0.0394\n[4/5][150/1583] Loss_D: 1.8662  Loss_G: 5.4950  D(x): 0.9469    D(G(z)): 0.7590 / 0.0113\n[4/5][200/1583] Loss_D: 0.7060  Loss_G: 3.6253  D(x): 0.9215    D(G(z)): 0.4316 / 0.0364\n[4/5][250/1583] Loss_D: 0.5589  Loss_G: 2.1394  D(x): 0.7108    D(G(z)): 0.1513 / 0.1548\n[4/5][300/1583] Loss_D: 0.7278  Loss_G: 1.2391  D(x): 0.5757    D(G(z)): 0.0987 / 0.3454\n[4/5][350/1583] Loss_D: 0.7597  Loss_G: 2.8481  D(x): 0.7502    D(G(z)): 0.3094 / 0.0843\n[4/5][400/1583] Loss_D: 0.6167  Loss_G: 2.2143  D(x): 0.6641    D(G(z)): 0.1315 / 0.1405\n[4/5][450/1583] Loss_D: 0.6234  Loss_G: 1.7961  D(x): 0.7303    D(G(z)): 0.2208 / 0.2007\n[4/5][500/1583] Loss_D: 0.6098  Loss_G: 4.9416  D(x): 0.9442    D(G(z)): 0.3978 / 0.0104\n[4/5][550/1583] Loss_D: 0.6570  Loss_G: 3.6935  D(x): 0.9180    D(G(z)): 0.4015 / 0.0312\n[4/5][600/1583] Loss_D: 0.4195  Loss_G: 2.3446  D(x): 0.7798    D(G(z)): 0.1319 / 0.1211\n[4/5][650/1583] Loss_D: 0.5291  Loss_G: 2.5303  D(x): 0.7528    D(G(z)): 0.1875 / 0.1075\n[4/5][700/1583] Loss_D: 0.5187  Loss_G: 2.0350  D(x): 0.7174    D(G(z)): 0.1431 / 0.1547\n[4/5][750/1583] Loss_D: 0.8208  Loss_G: 1.0780  D(x): 0.5665    D(G(z)): 0.1128 / 0.3844\n[4/5][800/1583] Loss_D: 0.5223  Loss_G: 3.0140  D(x): 0.8708    D(G(z)): 0.2871 / 0.0612\n[4/5][850/1583] Loss_D: 2.9431  Loss_G: 1.0175  D(x): 0.0914    D(G(z)): 0.0162 / 0.4320\n[4/5][900/1583] Loss_D: 0.5456  Loss_G: 1.7923  D(x): 0.7489    D(G(z)): 0.1972 / 0.2038\n[4/5][950/1583] Loss_D: 0.4718  Loss_G: 2.3825  D(x): 0.7840    D(G(z)): 0.1772 / 0.1172\n[4/5][1000/1583]        Loss_D: 0.5174  Loss_G: 2.5070  D(x): 0.8367    D(G(z)): 0.2556 / 0.1074\n[4/5][1050/1583]        Loss_D: 0.8214  Loss_G: 0.8055  D(x): 0.5181    D(G(z)): 0.0694 / 0.4963\n[4/5][1100/1583]        Loss_D: 1.3243  Loss_G: 0.7562  D(x): 0.3284    D(G(z)): 0.0218 / 0.5165\n[4/5][1150/1583]        Loss_D: 0.9334  Loss_G: 5.1260  D(x): 0.8775    D(G(z)): 0.4817 / 0.0088\n[4/5][1200/1583]        Loss_D: 0.5141  Loss_G: 2.7230  D(x): 0.8067    D(G(z)): 0.2188 / 0.0872\n[4/5][1250/1583]        Loss_D: 0.6007  Loss_G: 1.9893  D(x): 0.6968    D(G(z)): 0.1667 / 0.1748\n[4/5][1300/1583]        Loss_D: 0.4025  Loss_G: 2.3066  D(x): 0.8101    D(G(z)): 0.1471 / 0.1412\n[4/5][1350/1583]        Loss_D: 0.5979  Loss_G: 3.2825  D(x): 0.8248    D(G(z)): 0.3003 / 0.0509\n[4/5][1400/1583]        Loss_D: 0.7430  Loss_G: 3.6521  D(x): 0.8888    D(G(z)): 0.4243 / 0.0339\n[4/5][1450/1583]        Loss_D: 1.0814  Loss_G: 5.4255  D(x): 0.9647    D(G(z)): 0.5842 / 0.0070\n[4/5][1500/1583]        Loss_D: 1.7211  Loss_G: 0.7875  D(x): 0.2588    D(G(z)): 0.0389 / 0.5159\n[4/5][1550/1583]        Loss_D: 0.5871  Loss_G: 2.1340  D(x): 0.7332    D(G(z)): 0.1982 / 0.1518\n\n</code></pre>"},{"location":"1.0/dcgan_faces_tutorial/#_10","title":"\u7ed3\u679c","text":"<p>\u6700\u540e\uff0c\u8ba9\u6211\u4eec\u770b\u770b\u6211\u4eec\u505a\u7684\u600e\u4e48\u6837\u3002 \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u770b\u770b\u4e09\u4e2a\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u9996\u5148\uff0c\u6211\u4eec\u5c06\u770b\u5230\u5224\u522b\u5668D\u548c\u751f\u6210\u5668G\u7684\u635f\u5931\u5728\u8bad\u7ec3\u671f\u95f4\u662f\u5982\u4f55\u53d8\u5316\u7684\u3002 \u5176\u6b21\uff0c\u6211\u4eec\u5c06\u5728\u6bcf\u4e2a\u6279\u6b21\u53ef\u89c6\u5316\u751f\u6210\u5668G\u7684\u8f93\u51fa\u3002 \u7b2c\u4e09\uff0c\u6211\u4eec\u5c06\u67e5\u770b\u4e00\u6279\u5b9e\u9645\u6570\u636e\u4ee5\u53ca\u6765\u81ea\u751f\u6210\u5668G\u4e00\u6279\u5047\u6570\u636e\u3002 </p> <p>\u635f\u5931\u4e0e\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u5173\u7cfb\u56fe</p> <p>\u4e0b\u9762\u5c06\u7ed8\u5236\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u7684\u635f\u5931\u548c\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u5173\u7cfb\u56fe\u3002</p> <pre><code>plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n</code></pre> <p></p> <p>\u751f\u6210\u5668G\u7684\u8bad\u7ec3\u8fdb\u5ea6</p> <p>\u6211\u4eec\u5728\u6bcf\u4e00\u4e2a\u6279\u6b21\u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\u90fd\u4fdd\u5b58\u4e86\u751f\u6210\u5668\u7684\u8f93\u51fa\u3002 \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u52a8\u753b\u53ef\u89c6\u5316\u751f\u6210\u5668G\u7684\u8bad\u7ec3\u8fdb\u5ea6\u3002\u70b9\u51fb\u64ad\u653e\u6309\u94ae\u5f00\u59cb\u52a8\u753b.</p> <pre><code>#%%capture\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n\n</code></pre> <p></p> <p>\u771f\u5b9e\u56fe\u50cf vs.\u00a0\u5047\u56fe\u50cf</p> <p>\u6700\u540e\uff0c\u8ba9\u6211\u4eec\u4e00\u8d77\u770b\u770b\u4e00\u4e9b\u771f\u5b9e\u7684\u56fe\u50cf\u548c\u5047\u56fe\u50cf\u3002</p> <pre><code># \u4ece\u6570\u636e\u52a0\u8f7d\u5668\u4e2d\u83b7\u53d6\u4e00\u6279\u771f\u5b9e\u56fe\u50cf\nreal_batch = next(iter(dataloader))\n\n# \u753b\u51fa\u771f\u5b9e\u56fe\u50cf\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# \u753b\u51fa\u6765\u81ea\u6700\u540e\u4e00\u6b21\u8bad\u7ec3\u7684\u5047\u56fe\u50cf\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()\n\n</code></pre> <p></p>"},{"location":"1.0/dcgan_faces_tutorial/#_11","title":"\u4e0b\u4e00\u6b65\u8ba1\u5212","text":"<p>\u6211\u4eec\u5df2\u7ecf\u5230\u4e86\u6559\u7a0b\u7684\u6700\u540e\uff0c\u4f46\u662f\u4f60\u53ef\u4ee5\u6839\u636e\u6b64\u6559\u7a0b\u7814\u7a76\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <ul> <li>\u8bad\u7ec3\u66f4\u957f\u7684\u65f6\u95f4\u770b\u770b\u80fd\u591f\u8fbe\u5230\u591a\u597d\u7684\u7ed3\u679c</li> <li>\u8c03\u6574\u6b64\u6a21\u578b\u4ee5\u9002\u5408\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff0c\u5982\u679c\u53ef\u80fd\u4f60\u53ef\u4ee5\u66f4\u6539\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4ee5\u53ca\u6a21\u578b\u7684\u67b6\u6784</li> <li>\u770b\u770b\u8fd9\u91cc\u5176\u4ed6\u4e00\u4e9b\u5f88\u9177\u7684GAN\u9879\u76ee</li> <li>\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u4ea7\u751f\u97f3\u4e50\u7684GAN\u6a21\u578b</li> </ul>"},{"location":"1.0/deep_learning_60min_blitz/","title":"PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005\uff1aSoumith Chintala</p> <p>\u6b64\u6559\u7a0b\u7684\u76ee\u6807\uff1a</p> <ul> <li>\u66f4\u9ad8\u5c42\u6b21\u5730\u7406\u89e3PyTorch\u7684Tensor\u5e93\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc\u3002</li> <li>\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7528\u4e8e\u5206\u7c7b\u56fe\u50cf\u3002</li> </ul> <p>\u672c\u6559\u7a0b\u5047\u8bbe\u8bfb\u8005\u5bf9<code>numpy</code>\u6709\u57fa\u672c\u7684\u4e86\u89e3</p> <p>\u6ce8\u610f:\u786e\u4fdd\u4f60\u5b89\u88c5\u4e86 torch \u548c torchvision \u5305\u3002</p> <p></p> <p>PyTorch \u662f\u4ec0\u4e48\uff1f</p> <p></p> <p>Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc</p> <p></p> <p>\u795e\u7ecf\u7f51\u7edc</p> <p></p> <p>\u8bad\u7ec3\u5206\u7c7b\u5668</p> <p></p> <p>\u53ef\u9009\uff1a\u6570\u636e\u5e76\u884c</p>"},{"location":"1.0/deep_learning_nlp_tutorial/","title":"\u5728\u6df1\u5ea6\u5b66\u4e60\u548c NLP \u4e2d\u4f7f\u7528 Pytorch","text":"<p>\u8bd1\u8005 bruce1408</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005: Robert Guthrie</p> <p>\u672c\u6587\u5e26\u60a8\u8fdb\u5165pytorch\u6846\u67b6\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u7684\u6838\u5fc3\u601d\u60f3\u3002Pytorch\u7684\u5f88\u591a\u6982\u5ff5(\u6bd4\u5982\u8ba1\u7b97\u56fe\u62bd\u8c61\u548c\u81ea\u52a8\u6c42\u5bfc)\u5e76\u975e\u5b83\u6240\u72ec\u6709\u7684,\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u76f8\u5173\u3002</p> <p>\u6211\u5199\u8fd9\u7bc7\u6559\u7a0b\u662f\u4e13\u95e8\u9488\u5bf9\u90a3\u4e9b\u4ece\u672a\u7528\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6(\u4f8b\u5982\uff1aTensorflow, Theano, Keras, Dynet)\u7f16\u5199\u4ee3\u7801\u800c\u4ece\u4e8bNLP\u9886\u57df\u7684\u4eba\u3002\u6211\u5047\u8bbe\u4f60\u5df2\u7ecf\u77e5\u9053NLP\u9886\u57df\u8981\u89e3\u51b3\u7684\u6838\u5fc3\u95ee\u9898\uff1a\u8bcd\u6027\u6807\u6ce8\u3001\u8bed\u8a00\u6a21\u578b\u7b49\u7b49\u3002\u6211\u4e5f\u8ba4\u4e3a\u4f60\u901a\u8fc7AI\u8fd9\u672c\u4e66\u4e2d\u6240\u8bb2\u7684\u77e5\u8bc6\u719f\u6089\u4e86\u795e\u7ecf\u7f51\u7edc\u8fbe\u5230\u4e86\u5165\u95e8\u7684\u7ea7\u522b\u3002\u901a\u5e38\u8fd9\u4e9b\u8bfe\u7a0b\u90fd\u4f1a\u4ecb\u7ecd\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u6307\u51fa\u5b83\u4eec\u662f\u7ebf\u6027\u7ec4\u5408\u548c\u975e\u7ebf\u6027\u7ec4\u5408\u6784\u6210\u7684\u94fe\u3002\u672c\u6587\u5728\u5047\u8bbe\u4f60\u5df2\u7ecf\u6709\u4e86\u8fd9\u4e9b\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u6559\u4f60\u5982\u4f55\u5f00\u59cb\u5199\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7801\u3002</p> <p>\u6ce8\u610f\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u5173\u4e8e_models_\uff0c\u800c\u4e0d\u662f\u6570\u636e\u3002\u5bf9\u4e8e\u6240\u6709\u7684\u6a21\u578b\uff0c\u6211\u53ea\u521b\u5efa\u4e00\u4e9b\u6570\u636e\u7ef4\u5ea6\u8f83\u5c0f\u7684\u6d4b\u8bd5\u793a\u4f8b\u4ee5\u4fbf\u4f60\u53ef\u4ee5\u770b\u5230\u6743\u91cd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u4f55\u53d8\u5316\u3002\u5982\u679c\u4f60\u60f3\u8981\u5c1d\u8bd5\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\uff0c\u60a8\u6709\u80fd\u529b\u5220\u9664\u672c\u793a\u4f8b\u4e2d\u7684\u6a21\u578b\u5e76\u91cd\u65b0\u8bad\u7ec3\u4ed6\u4eec\u3002</p> <p></p> <p>Introduction to PyTorch</p> <p></p> <p>Deep Learning with PyTorch</p> <p></p> <p>Word Embeddings: Encoding Lexical Semantics</p> <p></p> <p>Sequence Models and Long-Short Term Memory Networks</p> <p></p> <p>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/","title":"\u6df7\u5408\u524d\u7aef\u7684seq2seq\u6a21\u578b\u90e8\u7f72","text":"<p>\u8bd1\u8005\uff1acangyunye</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005: Matthew Inkawhich</p> <p>\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u5982\u4f55\u662f<code>seq2seq</code>\u6a21\u578b\u8f6c\u6362\u4e3aPyTorch\u53ef\u7528\u7684\u524d\u7aef\u6df7\u5408Torch\u811a\u672c\u3002 \u6211\u4eec\u8981\u8f6c\u6362\u7684\u6a21\u578b\u662f\u6765\u81ea\u4e8e\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b Chatbot tutorial. \u4f60\u53ef\u4ee5\u628a\u8fd9\u4e2a\u6559\u7a0b\u5f53\u505aChatbot tutorial\u7684\u7b2c\u4e8c\u7bc7\u7ae0,\u5e76\u4e14\u90e8\u7f72\u4f60\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6216\u8005\u4f60\u4e5f\u53ef\u4ee5\u4f9d\u636e\u672c\u6587\u4f7f\u7528\u6211\u4eec\u91c7\u53d6\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u5c31\u540e\u8005\u800c\u8a00\uff0c\u4f60\u53ef\u4ee5\u4ece\u539f\u59cb\u7684Chatbot tutorial\u53c2\u8003\u66f4\u8be6\u7ec6\u7684\u6570\u636e\u9884\u5904\u7406\uff0c\u6a21\u578b\u7406\u8bba\u548c\u5b9a\u4e49\u4ee5\u53ca\u6a21\u578b\u8bad\u7ec3\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#hybrid-frontend","title":"\u4ec0\u4e48\u662f\u6df7\u5408\u524d\u7aef(Hybrid Frontend\uff09?","text":"<p>\u5728\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u7684\u7814\u53d1\u9636\u6bb5, \u4f7f\u7528\u50cfPyTorch\u8fd9\u6837\u5373\u65f6<code>eager</code>\u3001\u547d\u4ee4\u5f0f\u7684\u754c\u9762\u8fdb\u884c\u4ea4\u4e92\u80fd\u5e26\u6765\u5f88\u5927\u4fbf\u5229\u3002 \u8fd9\u4f7f\u7528\u6237\u80fd\u591f\u5728\u4f7f\u7528Python\u6570\u636e\u7ed3\u6784\u3001\u63a7\u5236\u6d41\u64cd\u4f5c\u3001\u6253\u5370\u8bed\u53e5\u548c\u8c03\u8bd5\u5b9e\u7528\u7a0b\u5e8f\u65f6\u901a\u8fc7\u719f\u6089\u7684\u3001\u60ef\u7528\u7684Python\u811a\u672c\u7f16\u5199\u3002\u5c3d\u7ba1\u5373\u65f6\u6027\u754c\u9762\u5bf9\u4e8e\u7814\u7a76\u548c\u8bd5\u9a8c\u5e94\u7528\u7a0b\u5e8f\u662f\u4e00\u4e2a\u6709\u7528\u7684\u5de5\u5177\uff0c\u4f46\u662f\u5bf9\u4e8e\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u57fa\u4e8e\u56fe\u5f62<code>graph-based</code>\u7684\u6a21\u578b\u8868\u793a\u5c06\u66f4\u52a0\u9002\u7528\u7684\u3002 \u4e00\u4e2a\u5ef6\u8fdf\u7684\u56fe\u578b\u5c55\u793a\u610f\u5473\u7740\u53ef\u4ee5\u4f18\u5316\uff0c\u6bd4\u5982\u65e0\u5e8f\u6267\u884c\u64cd\u4f5c\uff0c\u4ee5\u53ca\u9488\u5bf9\u9ad8\u5ea6\u4f18\u5316\u7684\u786c\u4ef6\u67b6\u6784\u7684\u80fd\u529b\u3002 \u6b64\u5916\uff0c\u57fa\u4e8e\u56fe\u5f62\u7684\u8868\u793a\u652f\u6301\u6846\u67b6\u65e0\u5173\u7684\u6a21\u578b\u5bfc\u51fa\u3002PyTorch\u63d0\u4f9b\u4e86\u5c06\u5373\u65f6\u6a21\u5f0f\u7684\u4ee3\u7801\u589e\u91cf\u8f6c\u6362\u4e3aTorch\u811a\u672c\u7684\u673a\u5236\uff0cTorch\u811a\u672c\u662f\u4e00\u4e2a\u5728Python\u4e2d\u7684\u9759\u6001\u53ef\u5206\u6790\u548c\u53ef\u4f18\u5316\u7684\u5b50\u96c6\uff0cTorch\u4f7f\u7528\u5b83\u6765\u5728Python\u8fd0\u884c\u65f6\u72ec\u7acb\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u3002</p> <p>\u5728Torch\u4e2d\u7684<code>torch.jit</code>\u6a21\u5757\u53ef\u4ee5\u627e\u5230\u5c06\u5373\u65f6\u6a21\u5f0f\u7684PyTorch\u7a0b\u5e8f\u8f6c\u6362\u4e3aTorch\u811a\u672c\u7684API\u3002 \u8fd9\u4e2a\u6a21\u5757\u6709\u4e24\u4e2a\u6838\u5fc3\u6a21\u5f0f\u7528\u4e8e\u5c06\u5373\u65f6\u6a21\u5f0f\u6a21\u578b\u8f6c\u6362\u4e3aTorch\u811a\u672c\u56fe\u5f62\u8868\u793a: \u8ddf\u8e2a<code>tracing</code> \u4ee5\u53ca \u811a\u672c\u5316<code>scripting</code>\u3002<code>torch.jit.trace</code> \u51fd\u6570\u63a5\u53d7\u4e00\u4e2a\u6a21\u5757\u6216\u8005\u4e00\u4e2a\u51fd\u6570\u548c\u4e00\u7ec4\u793a\u4f8b\u7684\u8f93\u5165\uff0c\u7136\u540e\u901a\u8fc7\u51fd\u6570\u6216\u6a21\u5757\u8fd0\u884c\u8f93\u5165\u793a\u4f8b\uff0c\u540c\u65f6\u8ddf\u8ddf\u8e2a\u9047\u5230\u7684\u8ba1\u7b97\u6b65\u9aa4\uff0c\u7136\u540e\u8f93\u51fa\u4e00\u4e2a\u53ef\u4ee5\u5c55\u793a\u8ddf\u8e2a\u6d41\u7a0b\u7684\u57fa\u4e8e\u56fe\u5f62\u7684\u51fd\u6570\u3002\u8ddf\u8e2a<code>Tracing</code>\u5bf9\u4e8e\u4e0d\u6d89\u53ca\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u63a7\u5236\u6d41\u7684\u76f4\u63a5\u7684\u6a21\u5757\u548c\u51fd\u6570\u975e\u5e38\u6709\u7528\uff0c\u5c31\u6bd4\u5982\u6807\u51c6\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u7136\u800c\uff0c\u5982\u679c\u4e00\u4e2a\u6709\u6570\u636e\u4f9d\u8d56\u7684if\u8bed\u53e5\u548c\u5faa\u73af\u7684\u51fd\u6570\u88ab\u8ddf\u8e2a\uff0c\u5219\u53ea\u8bb0\u5f55\u793a\u4f8b\u8f93\u5165\u6cbf\u6267\u884c\u8def\u5f84\u8c03\u7528\u7684\u64cd\u4f5c\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u63a7\u5236\u6d41\u672c\u8eab\u5e76\u6ca1\u6709\u88ab\u6355\u83b7\u3002\u8981\u5c06\u5e26\u6709\u6570\u636e\u4f9d\u8d56\u63a7\u5236\u6d41\u7684\u6a21\u5757\u548c\u51fd\u6570\u8fdb\u884c\u8f6c\u5316\uff0c\u5df2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u811a\u672c\u5316\u673a\u5236\u3002\u811a\u672c\u663e\u5f0f\u5730\u5c06\u6a21\u5757\u6216\u51fd\u6570\u4ee3\u7801\u8f6c\u6362\u4e3aTorch\u811a\u672c\uff0c\u5305\u62ec\u6240\u6709\u53ef\u80fd\u7684\u63a7\u5236\u6d41\u8def\u5f84\u3002 \u5982\u9700\u4f7f\u7528\u811a\u672c\u6a21\u5f0f<code>script mode</code>\uff0c \u8981\u786e\u5b9a\u7ee7\u627f\u4e86 <code>torch.jit.ScriptModule</code>\u57fa\u672c\u7c7b (\u53d6\u4ee3<code>torch.nn.Module</code>) \u5e76\u4e14\u589e\u52a0 <code>torch.jit.script</code> \u88c5\u9970\u5668\u5230\u4f60\u7684Python\u51fd\u6570\u6216\u8005 <code>torch.jit.script_method</code> \u88c5\u9970\u5668\u5230\u4f60\u7684\u6a21\u5757\u65b9\u6cd5\u3002\u4f7f\u7528\u811a\u672c\u5316\u7684\u4e00\u4e2a\u8b66\u544a\u662f\uff0c\u5b83\u53ea\u652f\u6301Python\u7684\u4e00\u4e2a\u53d7\u9650\u5b50\u96c6\u3002\u8981\u83b7\u53d6\u4e0e\u652f\u6301\u7684\u7279\u6027\u76f8\u5173\u7684\u6240\u6709\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003 Torch Script language reference\u3002\u4e3a\u4e86\u8fbe\u5230\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u53ef\u4ee5\u7ec4\u5408Torch\u811a\u672c\u7684\u6a21\u5f0f\u6765\u8868\u793a\u6574\u4e2a\u7a0b\u5e8f\uff0c\u5e76\u4e14\u53ef\u4ee5\u589e\u91cf\u5730\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u3002</p> <p></p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_1","title":"\u81f4\u8c22","text":"<p>\u672c\u7bc7\u6559\u7a0b\u7075\u611f\u6765\u81ea\u5982\u4e0b\u8d44\u6e90\uff1a</p> <ol> <li>Yuan-Kuei Wu's pytorch-chatbot implementation: https://github.com/ywk991112/pytorch-chatbot</li> <li>Sean Robertson's practical-pytorch seq2seq-translation example: https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation</li> <li>FloydHub's Cornell Movie Corpus preprocessing code: https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus</li> </ol>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_2","title":"\u9884\u5907\u73af\u5883","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u5e94\u8be5\u8981\u5bfc\u5165\u6240\u9700\u7684\u6a21\u5757\u4ee5\u53ca\u8bbe\u7f6e\u4e00\u4e9b\u5e38\u91cf\u3002\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u9700\u8981\u4fdd\u8bc1<code>MAX_LENGTH</code>\u5e38\u91cf\u8bbe\u7f6e\u6b63\u786e\u3002\u63d0\u9192\u4e00\u4e0b\uff0c\u8fd9\u4e2a\u5e38\u91cf\u5b9a\u4e49\u4e86\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5141\u8bb8\u7684\u6700\u5927\u53e5\u5b50\u957f\u5ea6\u4ee5\u53ca\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u7684\u6700\u5927\u53e5\u5b50\u957f\u5ea6\u8f93\u51fa\u3002</p> <pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport re\nimport os\nimport unicodedata\nimport numpy as np\n\ndevice = torch.device(\"cpu\")\n\nMAX_LENGTH = 10 # Maximum sentence length\n\n# Default word tokens\nPAD_token = 0 # Used for padding short sentences\nSOS_token = 1 # Start-of-sentence token\nEOS_token = 2 # End-of-sentence token\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_3","title":"\u6a21\u578b\u6982\u89c8","text":"<p>\u6b63\u5982\u524d\u6587\u6240\u8a00\uff0c\u6211\u4eec\u4f7f\u7528\u7684sequence-to-sequence (seq2seq) \u6a21\u578b\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u6a21\u578b\u7528\u4e8e\u8f93\u5165\u662f\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u7684\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u53ef\u53d8\u957f\u5ea6\u5e8f\u5217\u5b83\u4e0d\u4e00\u5b9a\u662f\u4e00\u5bf9\u4e00\u8f93\u5165\u6620\u5c04\u3002<code>seq2seq</code> \u6a21\u578b\u7531\u4e24\u4e2a\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNNs)\u7ec4\u6210\uff1a\u7f16\u7801\u5668 encoder\u548c\u89e3\u7801\u5668decoder.</p> <p></p> <p>\u56fe\u7247\u6765\u6e90: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#encoder","title":"\u7f16\u7801\u5668(Encoder)","text":"<p>\u7f16\u7801\u5668RNN\u5728\u8f93\u5165\u8bed\u53e5\u4e2d\u6bcf\u6b21\u8fed\u4ee3\u4e00\u4e2a\u6807\u8bb0(\u4f8b\u5982\u5355\u8bcd)\uff0c\u6bcf\u6b21\u6b65\u9aa4\u8f93\u51fa\u4e00\u4e2a\u201c\u8f93\u51fa\u201d\u5411\u91cf\u548c\u4e00\u4e2a\u201c\u9690\u85cf\u72b6\u6001\u201d\u5411\u91cf\u3002\u201d\u9690\u85cf\u72b6\u6001\u201c\u5411\u91cf\u5728\u4e4b\u540e\u5219\u4f20\u9012\u5230\u4e0b\u4e00\u4e2a\u6b65\u9aa4\uff0c\u540c\u65f6\u8bb0\u5f55\u8f93\u51fa\u5411\u91cf\u3002\u7f16\u7801\u5668\u5c06\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u5750\u6807\u4ee3\u8868\u7684\u6587\u672c\u8f6c\u6362\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4e00\u7ec4\u5750\u6807\uff0c\u89e3\u7801\u5668\u5c06\u4f7f\u7528\u8fd9\u4e9b\u5750\u6807\u4e3a\u7ed9\u5b9a\u7684\u4efb\u52a1\u751f\u6210\u6709\u610f\u4e49\u7684\u8f93\u51fa\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#decoder","title":"\u89e3\u7801\u5668(Decoder)","text":"<p>\u89e3\u7801\u5668RNN\u4ee5\u9010\u4e2a\u4ee4\u724c\u7684\u65b9\u5f0f\u751f\u6210\u54cd\u5e94\u8bed\u53e5\u3002\u5b83\u4f7f\u7528\u6765\u81ea\u4e8e\u7f16\u7801\u5668\u7684\u6587\u672c\u5411\u91cf\u548c\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6765\u751f\u6210\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\u5b83\u7ee7\u7eed\u751f\u6210\u5355\u8bcd\uff0c\u76f4\u5230\u8f93\u51fa\u8868\u793a\u53e5\u5b50\u7ed3\u675f\u7684EOS\u8bed\u53e5\u3002\u6211\u4eec\u5728\u89e3\u7801\u5668\u4e2d\u4f7f\u7528\u4e13\u6ce8\u673a\u5236attention mechanism\u6765\u5e2e\u52a9\u5b83\u5728\u8f93\u5165\u7684\u67d0\u4e9b\u90e8\u5206\u751f\u6210\u8f93\u51fa\u65f6\"\u4fdd\u6301\u4e13\u6ce8\"\u3002\u5bf9\u4e8e\u6211\u4eec\u7684\u6a21\u578b\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86 Luong et al\u7b49\u4eba\u7684\u201c\u5168\u5c40\u5173\u6ce8<code>Global attention</code>\u201d\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u89e3\u7801\u6a21\u578b\u4e2d\u7684\u5b50\u6a21\u5757\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_4","title":"\u6570\u636e\u5904\u7406","text":"<p>\u5c3d\u7ba1\u6211\u4eec\u7684\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u5904\u7406\u6807\u8bb0\u5e8f\u5217\uff0c\u4f46\u5728\u73b0\u5b9e\u4e2d\uff0c\u5b83\u4eec\u4e0e\u6240\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e00\u6837\u5904\u7406\u6570\u5b57\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5728\u8bad\u7ec3\u4e4b\u524d\u5efa\u7acb\u7684\u6a21\u578b\u8bcd\u6c47\u8868\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u90fd\u6620\u5c04\u5230\u4e00\u4e2a\u6574\u6570\u7d22\u5f15\u3002\u6211\u4eec\u4f7f\u7528<code>Voc</code>\u5bf9\u8c61\u6765\u5305\u542b\u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\uff0c\u4ee5\u53ca\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\u603b\u6570\u3002\u6211\u4eec\u5c06\u5728\u8fd0\u884c\u6a21\u578b\u4e4b\u524d\u52a0\u8f7d\u5bf9\u8c61\u3002</p> <p>\u6b64\u5916\uff0c\u4e3a\u4e86\u80fd\u591f\u8fdb\u884c\u8bc4\u4f30\uff0c\u6211\u4eec\u5fc5\u987b\u63d0\u4f9b\u4e00\u4e2a\u5904\u7406\u5b57\u7b26\u4e32\u8f93\u5165\u7684\u5de5\u5177\u3002<code>normalizeString</code>\u51fd\u6570\u5c06\u5b57\u7b26\u4e32\u4e2d\u7684\u6240\u6709\u5b57\u7b26\u8f6c\u6362\u4e3a\u5c0f\u5199\uff0c\u5e76\u5220\u9664\u6240\u6709\u975e\u5b57\u6bcd\u5b57\u7b26\u3002<code>indexesFromSentence</code>\u51fd\u6570\u63a5\u53d7\u4e00\u4e2a\u5355\u8bcd\u7684\u53e5\u5b50\u5e76\u8fd4\u56de\u76f8\u5e94\u7684\u5355\u8bcd\u7d22\u5f15\u5e8f\u5217\u3002</p> <pre><code>class Voc:\n    def __init__(self, name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count SOS, EOS, PAD\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words += 1\n        else:\n            self.word2count[word] += 1\n\n    # Remove words below a certain count threshold\n    def trim(self, min_count):\n        if self.trimmed:\n            return\n        self.trimmed = True\n        keep_words = []\n        for k, v in self.word2count.items():\n            if v &gt;= min_count:\n                keep_words.append(k)\n\n        print('keep_words {} / {} = {:.4f}'.format(\n            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n        ))\n        # Reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count default tokens\n        for word in keep_words:\n            self.addWord(word)\n\n# Lowercase and remove non-letter characters\ndef normalizeString(s):\n    s = s.lower()\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\n# Takes string sentence, returns sentence of word indexes\ndef indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_5","title":"\u7f16\u7801\u5668\u5b9a\u4e49","text":"<p>\u6211\u4eec\u901a\u8fc7<code>torch.nn.GRU</code>\u6a21\u5757\u5b9e\u73b0\u7f16\u7801\u5668\u7684RNN\u3002\u672c\u6a21\u5757\u63a5\u53d7\u4e00\u6279\u8bed\u53e5(\u5d4c\u5165\u5355\u8bcd\u7684\u5411\u91cf)\u7684\u8f93\u5165\uff0c\u5b83\u5728\u5185\u90e8\u904d\u5386\u8fd9\u4e9b\u53e5\u5b50\uff0c\u6bcf\u6b21\u4e00\u4e2a\u6807\u8bb0\uff0c\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u3002\u6211\u4eec\u5c06\u8fd9\u4e2a\u6a21\u5757\u521d\u59cb\u5316\u4e3a\u53cc\u5411\u7684\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u6709\u4e24\u4e2a\u72ec\u7acb\u7684GRUs:\u4e00\u4e2a\u6309\u65f6\u95f4\u987a\u5e8f\u904d\u5386\u5e8f\u5217\uff0c\u53e6\u4e00\u4e2a\u6309\u76f8\u53cd\u987a\u5e8f\u904d\u5386\u5e8f\u5217\u3002\u6211\u4eec\u6700\u7ec8\u8fd4\u56de\u8fd9\u4e24\u4e2aGRUs\u8f93\u51fa\u7684\u548c\u3002\u7531\u4e8e\u6211\u4eec\u7684\u6a21\u578b\u662f\u4f7f\u7528\u6279\u5904\u7406\u8fdb\u884c\u8bad\u7ec3\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u7684<code>EncoderRNN</code>\u6a21\u578b\u7684<code>forward</code>\u51fd\u6570\u9700\u8981\u4e00\u4e2a\u586b\u5145\u7684\u8f93\u5165\u6279\u5904\u7406\u3002\u4e3a\u4e86\u6279\u91cf\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u7684\u53e5\u5b50\uff0c\u6211\u4eec\u901a\u8fc7<code>MAX_LENGTH</code>\u4ee4\u724c\u5141\u8bb8\u4e00\u4e2a\u53e5\u5b50\u4e2d\u652f\u6301\u7684\u6700\u5927\u957f\u5ea6\uff0c\u5e76\u4e14\u6279\u5904\u7406\u4e2d\u6240\u6709\u5c0f\u4e8e<code>MAX_LENGTH</code>\u4ee4\u724c\u7684\u53e5\u5b50\u90fd\u4f7f\u7528\u6211\u4eec\u4e13\u7528\u7684<code>PAD_token</code>\u4ee4\u724c\u586b\u5145\u5728\u6700\u540e\u3002\u8981\u4f7f\u7528\u5e26\u6709PyTorch RNN\u6a21\u5757\u7684\u6279\u91cf\u586b\u5145\uff0c\u6211\u4eec\u5fc5\u987b\u628a\u8f6c\u53d1<code>forward</code>\u5bc6\u4ee4\u5728\u8c03\u7528<code>torch.nn.utils.rnn.pack_padded_sequence</code>\u548c<code>torch.nn.utils.rnn.pad_packed_sequence</code>\u6570\u636e\u8f6c\u6362\u65f6\u8fdb\u884c\u6253\u5305\u3002\u6ce8\u610f\uff0c<code>forward</code>\u51fd\u6570\u8fd8\u63a5\u53d7\u4e00\u4e2a<code>input_length</code>\u5217\u8868\uff0c\u5176\u4e2d\u5305\u542b\u6279\u5904\u7406\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\u3002\u8be5\u8f93\u5165\u5728\u586b\u5145\u65f6\u901a\u8fc7<code>torch.nn.utils.rnn.pack_padded_sequence</code>\u4f7f\u7528\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_6","title":"\u6df7\u5408\u524d\u7aef\u7b14\u8bb0:","text":"<p>\u7531\u4e8e\u7f16\u7801\u5668\u7684\u8f6c\u53d1\u51fd\u6570<code>forward</code>\u4e0d\u5305\u542b\u4efb\u4f55\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u63a7\u5236\u6d41\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u4f7f\u7528\u8ddf\u8e2a<code>tracing</code>\u5c06\u5176\u8f6c\u6362\u4e3a\u811a\u672c\u6a21\u5f0f<code>script mode</code>\u3002\u5728\u8ddf\u8e2a\u6a21\u5757\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u4fdd\u6301\u6a21\u5757\u5b9a\u4e49\u4e0d\u53d8\u3002\u5728\u8fd0\u884c\u8bc4\u4f30\u4e4b\u524d\uff0c\u6211\u4eec\u5c06\u5728\u672c\u6587\u672b\u5c3e\u521d\u59cb\u5316\u6240\u6709\u6a21\u578b\u3002</p> <pre><code>class EncoderRNN(nn.Module):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n        # because our input size is a word embedding with number of features == hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Convert word indexes to embeddings\n        embedded = self.embedding(input_seq)\n        # Pack padded batch of sequences for RNN module\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        # Forward pass through GRU\n        outputs, hidden = self.gru(packed, hidden)\n        # Unpack padding\n        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n        # Sum bidirectional GRU outputs\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        # Return output and final hidden state\n        return outputs, hidden\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_7","title":"\u89e3\u7801\u4e13\u6ce8\u6a21\u5757\u5b9a\u4e49","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u5b9a\u4e49\u6211\u4eec\u7684\u6ce8\u610f\u529b\u6a21\u5757(<code>Attn</code>)\u3002\u8bf7\u6ce8\u610f\uff0c\u6b64\u6a21\u5757\u5c06\u7528\u4f5c\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u7684\u5b50\u6a21\u5757\u3002Luong\u7b49\u4eba\u8003\u8651\u4e86\u5404\u79cd\u201c\u5206\u6570\u51fd\u6570\u201d<code>score functions</code>\uff0c\u5b83\u4eec\u53d6\u5f53\u524d\u89e3\u7801\u5668RNN\u8f93\u51fa\u548c\u6574\u4e2a\u7f16\u7801\u5668\u8f93\u51fa\uff0c\u5e76\u8fd4\u56de\u5173\u6ce8\u70b9\u201c\u80fd\u503c\u201d<code>engergies</code>\u3002\u8fd9\u4e2a\u5173\u6ce8\u80fd\u503c\u5f20\u91cf<code>attension energies tensor</code>\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u7684\u5927\u5c0f\u76f8\u540c\uff0c\u4e24\u8005\u6700\u7ec8\u76f8\u4e58\uff0c\u5f97\u5230\u4e00\u4e2a\u52a0\u6743\u5f20\u91cf\uff0c\u5176\u6700\u5927\u503c\u8868\u793a\u5728\u7279\u5b9a\u65f6\u95f4\u6b65\u957f\u89e3\u7801\u7684\u67e5\u8be2\u8bed\u53e5\u6700\u91cd\u8981\u7684\u90e8\u5206\u3002</p> <pre><code># Luong attention layer\nclass Attn(torch.nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n        self.method = method\n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, \"is not an appropriate attention method.\")\n        self.hidden_size = hidden_size\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n\n    def dot_score(self, hidden, encoder_output):\n        return torch.sum(hidden * encoder_output, dim=2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n\n    def forward(self, hidden, encoder_outputs):\n        # Calculate the attention weights (energies) based on the given method\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden, encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden, encoder_outputs)\n\n        # Transpose max_length and batch_size dimensions\n        attn_energies = attn_energies.t()\n\n        # Return the softmax normalized probability scores (with added dimension)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_8","title":"\u89e3\u7801\u5668\u5b9a\u4e49","text":"<p>\u7c7b\u4f3c\u4e8e<code>EncoderRNN</code>\uff0c\u6211\u4eec\u4f7f\u7528<code>torch.nn.GRU</code>\u6a21\u5757\u4f5c\u4e3a\u6211\u4eec\u7684\u89e3\u7801\u5668RNN\u3002\u7136\u800c\uff0c\u8fd9\u4e00\u6b21\u6211\u4eec\u4f7f\u7528\u5355\u5411GRU\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e0e\u7f16\u7801\u5668\u4e0d\u540c\uff0c\u6211\u4eec\u5c06\u5411\u89e3\u7801\u5668RNN\u6bcf\u6b21\u63d0\u4f9b\u4e00\u4e2a\u5355\u8bcd\u3002\u6211\u4eec\u9996\u5148\u5f97\u5230\u5f53\u524d\u5355\u8bcd\u7684\u5d4c\u5165\u5e76\u5e94\u7528\u629b\u51fa\u529f\u80fddropout\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u5d4c\u5165\u548c\u6700\u540e\u7684\u9690\u85cf\u72b6\u6001\u8f6c\u53d1\u7ed9GRU\uff0c\u5f97\u5230\u5f53\u524d\u7684GRU\u8f93\u51fa\u548c\u9690\u85cf\u72b6\u6001\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528Attn\u6a21\u5757\u4f5c\u4e3a\u4e00\u4e2a\u5c42\u6765\u83b7\u5f97\u4e13\u6ce8\u6743\u91cd\uff0c\u6211\u4eec\u5c06\u5176\u4e58\u4ee5\u7f16\u7801\u5668\u7684\u8f93\u51fa\u6765\u83b7\u5f97\u6211\u4eec\u7684\u53c2\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u3002\u6211\u4eec\u4f7f\u7528\u8fd9\u4e2a\u53c2\u4e0e\u7f16\u7801\u5668\u8f93\u51fa\u4f5c\u4e3a\u6587\u672c<code>context</code>\u5f20\u91cf\uff0c\u5b83\u8868\u793a\u4e00\u4e2a\u52a0\u6743\u548c\uff0c\u8868\u793a\u7f16\u7801\u5668\u8f93\u51fa\u7684\u54ea\u4e9b\u90e8\u5206\u9700\u8981\u6ce8\u610f\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u5c42<code>linear layer</code>\u548c<code>softmax normalization</code>\u89c4\u8303\u5316\u6765\u9009\u62e9\u8f93\u51fa\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_9","title":"\u6df7\u5408\u524d\u7aef\u7b14\u8bb0:","text":"<p>\u4e0e<code>EncoderRNN</code>\u7c7b\u4f3c\uff0c\u6b64\u6a21\u5757\u4e0d\u5305\u542b\u4efb\u4f55\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u63a7\u5236\u6d41\u3002\u56e0\u6b64\uff0c\u5728\u521d\u59cb\u5316\u8be5\u6a21\u578b\u5e76\u52a0\u8f7d\u5176\u53c2\u6570\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u518d\u6b21\u4f7f\u7528\u8ddf\u8e2a<code>tracing</code>\u5c06\u5176\u8f6c\u6362\u4e3aTorch\u811a\u672c\u3002</p> <pre><code>class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        # Note: we run this one step (word) at a time\n        # Get embedding of current input word\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # Forward through unidirectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        # Calculate attention weights from the current GRU output\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n        # Concatenate weighted context vector and GRU output using Luong eq. 5\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # Predict next word using Luong eq. 6\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        # Return output and final hidden state\n        return output, hidden\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_10","title":"\u8bc4\u4f30\u5b9a\u4e49","text":""},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#greedysearchdecoder","title":"\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u5668(GreedySearchDecoder)","text":"<p>\u5728\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528<code>GreedySearchDecoder</code>\u6a21\u5757\u6765\u7b80\u5316\u5b9e\u9645\u7684\u89e3\u7801\u8fc7\u7a0b\u3002\u8be5\u6a21\u5757\u5c06\u8bad\u7ec3\u597d\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u4f5c\u4e3a\u5c5e\u6027\uff0c\u9a71\u52a8\u8f93\u5165\u8bed\u53e5(\u8bcd\u7d22\u5f15\u5411\u91cf)\u7684\u7f16\u7801\u8fc7\u7a0b\uff0c\u5e76\u4e00\u6b21\u4e00\u4e2a\u8bcd(\u8bcd\u7d22\u5f15)\u8fed\u4ee3\u5730\u89e3\u7801\u8f93\u51fa\u54cd\u5e94\u5e8f\u5217</p> <p>\u5bf9\u8f93\u5165\u5e8f\u5217\u8fdb\u884c\u7f16\u7801\u5f88\u7b80\u5355:\u53ea\u9700\u5c06\u6574\u4e2a\u5e8f\u5217\u5f20\u91cf\u53ca\u5176\u5bf9\u5e94\u7684\u957f\u5ea6\u5411\u91cf\u8f6c\u53d1\u7ed9\u7f16\u7801\u5668\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e2a\u6a21\u5757\u4e00\u6b21\u53ea\u5904\u7406\u4e00\u4e2a\u8f93\u5165\u5e8f\u5217\uff0c\u800c\u4e0d\u662f\u6210\u6279\u7684\u5e8f\u5217\u3002\u56e0\u6b64\uff0c\u5f53\u5e38\u65701\u7528\u4e8e\u58f0\u660e\u5f20\u91cf\u5927\u5c0f\u65f6\uff0c\u5b83\u5bf9\u5e94\u4e8e\u6279\u5904\u7406\u5927\u5c0f\u4e3a1\u3002\u8981\u89e3\u7801\u7ed9\u5b9a\u7684\u89e3\u7801\u5668\u8f93\u51fa\uff0c\u6211\u4eec\u5fc5\u987b\u901a\u8fc7\u89e3\u7801\u5668\u6a21\u578b\u8fed\u4ee3\u5730\u5411\u524d\u8fd0\u884c\uff0c\u8be5\u89e3\u7801\u5668\u6a21\u578b\u8f93\u51fasoftmax\u5206\u6570\uff0c\u8be5\u5206\u6570\u5bf9\u5e94\u4e8e\u6bcf\u4e2a\u5355\u8bcd\u5728\u89e3\u7801\u5e8f\u5217\u4e2d\u662f\u6b63\u786e\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u7684\u6982\u7387\u3002\u6211\u4eec\u5c06<code>decoder_input</code>\u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5305\u542bSOS_token\u7684\u5f20\u91cf\u3002\u5728\u6bcf\u6b21\u901a\u8fc7\u89e3\u7801\u5668\u4e4b\u540e\uff0c\u6211\u4eec\u8d2a\u5a6a\u5730\u5c06<code>softmax</code>\u6982\u7387\u6700\u9ad8\u7684\u5355\u8bcd\u8ffd\u52a0\u5230<code>decoded_words</code>\u5217\u8868\u4e2d\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u8fd9\u4e2a\u5355\u8bcd\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8fed\u4ee3\u7684decoder_input<code>\u3002\u5982\u679c``decoded_words</code>\u5217\u8868\u7684\u957f\u5ea6\u8fbe\u5230<code>MAX_LENGTH</code>\uff0c\u6216\u8005\u9884\u6d4b\u7684\u5355\u8bcd\u662f<code>EOS_token</code>\uff0c\u90a3\u4e48\u89e3\u7801\u8fc7\u7a0b\u5c06\u7ec8\u6b62\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_11","title":"\u6df7\u5408\u524d\u7aef\u6ce8\u91ca:","text":"<p>\u8be5\u6a21\u5757\u7684<code>forward</code>\u65b9\u6cd5\u6d89\u53ca\u5230\u5728\u6bcf\u6b21\u89e3\u7801\u4e00\u4e2a\u5355\u8bcd\u7684\u8f93\u51fa\u5e8f\u5217\u65f6\uff0c\u904d\u5386/([0,max/_length]/)\u7684\u8303\u56f4\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5e94\u8be5\u4f7f\u7528\u811a\u672c\u5c06\u8fd9\u4e2a\u6a21\u5757\u8f6c\u6362\u4e3aTorch\u811a\u672c\u3002\u4e0e\u6211\u4eec\u53ef\u4ee5\u8ddf\u8e2a\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u4e0d\u540c\uff0c\u6211\u4eec\u5fc5\u987b\u5bf9<code>GreedySearchDecoder</code>\u6a21\u5757\u8fdb\u884c\u4e00\u4e9b\u5fc5\u8981\u7684\u66f4\u6539\uff0c\u4ee5\u4fbf\u5728\u4e0d\u51fa\u9519\u7684\u60c5\u51b5\u4e0b\u521d\u59cb\u5316\u5bf9\u8c61\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u5fc5\u987b\u786e\u4fdd\u6211\u4eec\u7684\u6a21\u5757\u9075\u5b88\u811a\u672c\u673a\u5236\u7684\u89c4\u5219\uff0c\u5e76\u4e14\u4e0d\u4f7f\u7528Torch\u811a\u672c\u5305\u542b\u7684Python\u5b50\u96c6\u4e4b\u5916\u7684\u4efb\u4f55\u8bed\u8a00\u7279\u6027\u3002</p> <p>\u4e3a\u4e86\u4e86\u89e3\u53ef\u80fd\u9700\u8981\u7684\u4e00\u4e9b\u64cd\u4f5c\uff0c\u6211\u4eec\u5c06\u56de\u987e\u804a\u5929\u673a\u5668\u4eba\u6559\u7a0b\u4e2d\u7684<code>GreedySearchDecoder</code>\u5b9e\u73b0\u4e0e\u4e0b\u9762\u5355\u5143\u4e2d\u4f7f\u7528\u7684\u5b9e\u73b0\u4e4b\u95f4\u7684\u533a\u522b\u3002\u8bf7\u6ce8\u610f\uff0c\u7528\u7ea2\u8272\u7a81\u51fa\u663e\u793a\u7684\u884c\u662f\u4ece\u539f\u59cb\u5b9e\u73b0\u4e2d\u5220\u9664\u7684\u884c\uff0c\u800c\u7528\u7eff\u8272\u7a81\u51fa\u663e\u793a\u7684\u884c\u662f\u65b0\u7684\u3002</p> <p></p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_12","title":"\u53d8\u66f4\u4e8b\u9879:","text":"<ul> <li><code>nn.Module</code> -&gt; <code>torch.jit.ScriptModule</code></li> <li>\u4e3a\u4e86\u5728\u6a21\u5757\u4e0a\u4f7f\u7528PyTorch\u7684\u811a\u672c\u5316\u673a\u5236, \u6a21\u578b\u9700\u8981\u4ece <code>torch.jit.ScriptModule</code>\u7ee7\u627f\u3002</li> <li>\u5c06 <code>decoder_n_layers</code> \u8ffd\u52a0\u5230\u7ed3\u6784\u53c2\u6570</li> <li>\u8fd9\u79cd\u53d8\u5316\u6e90\u4e8e\u8fd9\u6837\u4e00\u4e2a\u4e8b\u5b9e\uff0c\u5373\u6211\u4eec\u4f20\u9012\u7ed9\u8fd9\u4e2a\u6a21\u5757\u7684\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u5c06\u662f<code>TracedModule</code>(\u975e\u6a21\u5757)\u7684\u5b50\u6a21\u5757\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u65e0\u6cd5\u4f7f\u7528<code>decoder.n_layers</code>\u8bbf\u95ee\u89e3\u7801\u5668\u7684\u5c42\u6570\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u5bf9\u6b64\u8fdb\u884c\u8ba1\u5212\uff0c\u5e76\u5728\u6a21\u5757\u6784\u5efa\u8fc7\u7a0b\u4e2d\u4f20\u5165\u6b64\u503c\u3002</li> <li>\u5c06\u65b0\u5c5e\u6027\u4f5c\u4e3a\u5e38\u91cf\u4fdd\u5b58</li> <li>\u5728\u6700\u521d\u7684\u5b9e\u73b0\u4e2d\uff0c \u6211\u4eec\u53ef\u4ee5\u5728<code>GreedySearchDecoder</code>\u7684<code>forward</code>\u65b9\u6cd5\u4e2d\u81ea\u7531\u5730\u4f7f\u7528\u6765\u81ea\u5468\u56f4(\u5168\u5c40)\u8303\u56f4\u7684\u53d8\u91cf. \u7136\u800c\uff0c\u73b0\u5728\u6211\u4eec\u6b63\u5728\u4f7f\u7528\u811a\u672c\uff0c\u6211\u4eec\u6ca1\u6709\u8fd9\u79cd\u81ea\u7531\uff0c\u56e0\u4e3a\u811a\u672c\u5904\u7406\u7684\u8bbe\u60f34\u662f\u6211\u4eec\u4e0d\u4e00\u5b9a\u8981\u4fdd\u7559Python\u5bf9\u8c61\uff0c\u5c24\u5176\u662f\u5728\u5bfc\u51fa\u65f6\u3002 \u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5c06\u5168\u5c40\u4f5c\u7528\u57df\u4e2d\u7684\u8fd9\u4e9b\u503c\u4f5c\u4e3a\u5c5e\u6027\u5b58\u50a8\u5230\u6784\u9020\u51fd\u6570\u4e2d\u7684\u6a21\u5757\u4e2d\uff0c \u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230\u4e00\u4e2a\u540d\u4e3a<code>__constants__</code>\u7684\u7279\u6b8a\u5217\u8868\u4e2d\uff0c\u4ee5\u4fbf\u5728<code>forward</code>\u65b9\u6cd5\u4e2d\u6784\u9020\u56fe\u5f62\u65f6\u5c06\u5b83\u4eec\u7528\u4f5c\u6587\u672c\u503c\u3002\u8fd9\u79cd\u7528\u6cd5\u7684\u4e00\u4e2a\u4f8b\u5b50\u5728\u7b2c19\u884c\uff0c\u53d6\u4ee3\u4f7f\u7528 <code>device</code> \u548c <code>SOS_token</code> \u5168\u5c40\u503c\uff0c\u6211\u4eec\u4f7f\u7528\u5e38\u91cf\u5c5e\u6027 <code>self._device</code> \u548c <code>self._SOS_token</code>\u3002</li> <li>\u5c06 <code>torch.jit.script_method</code> \u88c5\u9970\u5668\u6dfb\u52a0\u5230 <code>forward</code> \u65b9\u6cd5</li> <li>\u6dfb\u52a0\u8fd9\u4e2a\u88c5\u9970\u5668\u53ef\u4ee5\u8ba9JIT\u7f16\u8bd1\u5668\u77e5\u9053\u5b83\u6240\u88c5\u9970\u7684\u51fd\u6570\u5e94\u8be5\u662f\u811a\u672c\u5316\u7684\u3002</li> <li>\u5f3a\u5236 <code>forward</code> \u65b9\u6cd5\u7684\u53c2\u6570\u7c7b\u578b</li> <li>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cTorch\u811a\u672c\u51fd\u6570\u7684\u6240\u6709\u53c2\u6570\u90fd\u5047\u5b9a\u4e3a\u5f20\u91cf\u3002\u5982\u679c\u9700\u8981\u4f20\u9012\u4e0d\u540c\u7c7b\u578b\u7684\u53c2\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528PEP 3107\u4e2d\u5f15\u5165\u7684\u51fd\u6570\u7c7b\u578b\u6ce8\u91ca\u3002 \u6b64\u5916\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528<code>MyPy-style</code>\u7c7b\u578b\u7684\u6ce8\u91ca\u58f0\u660e\u4e0d\u540c\u7c7b\u578b\u7684\u53c2\u6570(\u53c2\u89c1(see doc))\u3002</li> <li>\u53d8\u66f4<code>decoder_input</code>\u7684\u521d\u59cb\u5316</li> <li>\u5728\u539f\u6709\u5b9e\u73b0\u4e2d\uff0c\u6211\u4eec\u7528<code>torch.LongTensor([[SOS_token]])</code>\u521d\u59cb\u5316\u4e86 <code>decoder_input</code> \u7684\u5f20\u91cf\u3002 \u5f53\u811a\u672c\u7f16\u5199\u65f6,\u6211\u4eec\u4e0d\u5141\u8bb8\u50cf\u8fd9\u6837\u4ee5\u4e00\u79cd\u6587\u5b57\u65b9\u5f0f\u521d\u59cb\u5316\u5f20\u91cf\u3002 \u53d6\u800c\u4ee3\u4e4b\u7684\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u663e\u5f0f\u7684torch\u51fd\u6570\uff0c\u6bd4\u5982<code>torch.ones</code>\u6765\u521d\u59cb\u5316\u6211\u4eec\u7684\u5f20\u91cf\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u590d\u5236\u6807\u91cf <code>decoder_input</code> \u548c\u901a\u8fc7\u5c061\u4e58\u4ee5\u6211\u4eec\u5b58\u5728\u5e38\u91cf\u4e2d\u7684<code>SOS_token</code>\u7684\u503c <code>self._SOS_token</code>\u5f97\u5230\u7684\u5f20\u91cf\u3002</li> </ul> <pre><code>class GreedySearchDecoder(torch.jit.ScriptModule):\n    def __init__(self, encoder, decoder, decoder_n_layers):\n        super(GreedySearchDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self._device = device\n        self._SOS_token = SOS_token\n        self._decoder_n_layers = decoder_n_layers\n\n    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']\n\n    @torch.jit.script_method\n    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n        # Forward input through encoder model\n        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n        # Initialize decoder input with SOS_token\n        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n        # Initialize tensors to append decoded words to\n        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n        all_scores = torch.zeros([0], device=self._device)\n        # Iteratively decode one word token at a time\n        for _ in range(max_length):\n            # Forward pass through decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            # Obtain most likely word token and its softmax score\n            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n            # Record token and score\n            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n            # Prepare current token to be next decoder input (add a dimension)\n            decoder_input = torch.unsqueeze(decoder_input, 0)\n        # Return collections of word tokens and scores\n        return all_tokens, all_scores\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_13","title":"\u8f93\u5165\u8bc4\u4f30","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5b9a\u4e49\u4e00\u4e9b\u51fd\u6570\u6765\u8ba1\u7b97\u8f93\u5165\u3002\u6c42\u503c\u51fd\u6570<code>evaluate</code>\u63a5\u53d7\u4e00\u4e2a\u89c4\u8303\u5316\u5b57\u7b26\u4e32\u8bed\u53e5\uff0c\u5c06\u5176\u5904\u7406\u4e3a\u5176\u5bf9\u5e94\u7684\u5355\u8bcd\u7d22\u5f15\u5f20\u91cf(\u6279\u5904\u7406\u5927\u5c0f\u4e3a1)\uff0c\u5e76\u5c06\u8be5\u5f20\u91cf\u4f20\u9012\u7ed9\u4e00\u4e2a\u540d\u4e3a<code>searcher</code>\u7684<code>GreedySearchDecoder</code>\u5b9e\u4f8b\uff0c\u4ee5\u5904\u7406\u7f16\u7801/\u89e3\u7801\u8fc7\u7a0b\u3002\u68c0\u7d22\u5668\u8fd4\u56de\u8f93\u51fa\u7684\u5355\u8bcd\u7d22\u5f15\u5411\u91cf\u548c\u4e00\u4e2a\u5206\u6570\u5f20\u91cf\uff0c\u8be5\u5f20\u91cf\u5bf9\u5e94\u4e8e\u6bcf\u4e2a\u89e3\u7801\u7684\u5355\u8bcd\u6807\u8bb0\u7684softmax\u5206\u6570\u3002\u6700\u540e\u4e00\u6b65\u662f\u4f7f\u7528<code>voc.index2word</code>\u5c06\u6bcf\u4e2a\u5355\u8bcd\u7d22\u5f15\u8f6c\u6362\u56de\u5176\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\u3002</p> <p>\u6211\u4eec\u8fd8\u5b9a\u4e49\u4e86\u4e24\u4e2a\u51fd\u6570\u6765\u8ba1\u7b97\u8f93\u5165\u8bed\u53e5\u3002<code>evaluateInput</code>\u51fd\u6570\u63d0\u793a\u7528\u6237\u8f93\u5165\uff0c\u5e76\u8ba1\u7b97\u8f93\u5165\u3002\u5b83\u6301\u7eed\u8bf7\u6c42\u53e6\u4e00\u6b21\u8f93\u5165\uff0c\u76f4\u5230\u7528\u6237\u8f93\u5165\u201cq\u201d\u6216\u201cquit\u201d\u3002</p> <p><code>evaluateExample</code>\u51fd\u6570\u53ea\u63a5\u53d7\u4e00\u4e2a\u5b57\u7b26\u4e32\u8f93\u5165\u8bed\u53e5\u4f5c\u4e3a\u53c2\u6570\uff0c\u5bf9\u5176\u8fdb\u884c\u89c4\u8303\u5316\u3001\u8ba1\u7b97\u5e76\u8f93\u51fa\u54cd\u5e94\u3002</p> <pre><code>def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n    ### Format input sentence as a batch\n    # words -&gt; indexes\n    indexes_batch = [indexesFromSentence(voc, sentence)]\n    # Create lengths tensor\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    # Transpose dimensions of batch to match models' expectations\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n    # Use appropriate device\n    input_batch = input_batch.to(device)\n    lengths = lengths.to(device)\n    # Decode sentence with searcher\n    tokens, scores = searcher(input_batch, lengths, max_length)\n    # indexes -&gt; words\n    decoded_words = [voc.index2word[token.item()] for token in tokens]\n    return decoded_words\n\n# Evaluate inputs from user input (stdin)\ndef evaluateInput(encoder, decoder, searcher, voc):\n    input_sentence = ''\n    while(1):\n        try:\n            # Get input sentence\n            input_sentence = input('&gt; ')\n            # Check if it is quit case\n            if input_sentence == 'q' or input_sentence == 'quit': break\n            # Normalize sentence\n            input_sentence = normalizeString(input_sentence)\n            # Evaluate sentence\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n            # Format and print response sentence\n            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n            print('Bot:', ' '.join(output_words))\n\n        except KeyError:\n            print(\"Error: Encountered unknown word.\")\n\n# Normalize input sentence and call evaluate()\ndef evaluateExample(sentence, encoder, decoder, searcher, voc):\n    print(\"&gt; \" + sentence)\n    # Normalize sentence\n    input_sentence = normalizeString(sentence)\n    # Evaluate sentence\n    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n    print('Bot:', ' '.join(output_words))\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_14","title":"\u52a0\u8f7d\u9884\u8bad\u7ec3\u53c2\u6570","text":"<p>\u597d\u7684\uff0c\u662f\u65f6\u5019\u52a0\u8f7d\u6211\u4eec\u7684\u6a21\u578b\u4e86</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_15","title":"\u4f7f\u7528\u6258\u7ba1\u6a21\u578b","text":"<p>\u6258\u7ba1\u6a21\u578b\u4f7f\u7528\u6b65\u9aa4:</p> <ol> <li>\u4e0b\u8f7d\u6a21\u578b here.</li> <li>\u8bbe\u7f6e<code>loadFilename</code>\u53d8\u91cf\u4f5c\u4e3a\u4e0b\u8f7d\u7684\u68c0\u67e5\u70b9\u6587\u4ef6\u7684\u8def\u5f84</li> <li>\u5c06<code>checkpoint = torch.load(loadFilename)</code> \u884c\u53d6\u6d88\u6ce8\u91ca\uff0c\u8868\u793a\u6258\u7ba1\u6a21\u578b\u5728CPU\u4e0a\u8bad\u7ec3\u3002</li> </ol>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_16","title":"\u4f7f\u7528\u81ea\u5df1\u7684\u6a21\u578b","text":"<p>\u52a0\u8f7d\u81ea\u5df1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u8bbe\u8ba1\u6b65\u9aa4:</p> <ol> <li>\u5c06<code>loadFilename</code>\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u5e0c\u671b\u52a0\u8f7d\u7684\u68c0\u67e5\u70b9\u6587\u4ef6\u7684\u8def\u5f84\u3002\u6ce8\u610f\uff0c\u5982\u679c\u60a8\u9075\u5faa\u4ecechatbot tutorial\u4e2d\u4fdd\u5b58\u6a21\u578b\u7684\u534f\u8bae\uff0c\u8fd9\u4f1a\u6d89\u53ca\u66f4\u6539<code>model_name</code>\u3001<code>encoder_n_layers</code>\u3001<code>decoder_n_layers</code>\u3001<code>hidden_size</code>\u548c<code>checkpoint_iter</code>(\u56e0\u4e3a\u8fd9\u4e9b\u503c\u5728\u6a21\u578b\u8def\u5f84\u4e2d\u4f7f\u7528\u5230)\u3002</li> <li>\u5982\u679c\u4f60\u5728CPU\u4e0a\u8bad\u7ec3\uff0c\u786e\u4fdd\u4f60\u5728 <code>checkpoint = torch.load(loadFilename)</code> \u884c\u6253\u5f00\u4e86\u68c0\u67e5\u70b9\u3002\u5982\u679c\u4f60\u5728GPU \u4e0a\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728CPU\u8fd0\u884c\u8fd9\u7bc7\u6559\u7a0b\uff0c\u89e3\u9664<code>checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))</code> \u7684\u6ce8\u91ca\u3002</li> </ol>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_17","title":"\u6df7\u5408\u524d\u7aef\u7684\u6ce8\u91ca:","text":"<p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u50cf\u5f80\u5e38\u4e00\u6837\u521d\u59cb\u5316\u5e76\u5c06\u53c2\u6570\u52a0\u8f7d\u5230\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\u4e2d\u3002\u53e6\u5916\uff0c\u5728\u8ddf\u8e2a\u6a21\u578b\u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u8c03\u7528<code>.to(device)</code>\u6765\u8bbe\u7f6e\u6a21\u578b\u7684\u8bbe\u5907\u9009\u9879\uff0c\u8c03\u7528<code>.eval()</code>\u6765\u8bbe\u7f6e\u629b\u51fa\u5c42<code>dropout layer</code>\u4e3atest mode\u3002<code>TracedModule</code>\u5bf9\u8c61\u4e0d\u7ee7\u627f<code>to</code>\u6216<code>eval</code>\u65b9\u6cd5</p> <pre><code>save_dir = os.path.join(\"data\", \"save\")\ncorpus_name = \"cornell movie-dialogs corpus\"\n\n# Configure models\nmodel_name = 'cb_model'\nattn_model = 'dot'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 2\ndecoder_n_layers = 2\ndropout = 0.1\nbatch_size = 64\n\n# If you're loading your own model\n# Set checkpoint to load from\ncheckpoint_iter = 4000\n# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n#                             '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n#                             '{}_checkpoint.tar'.format(checkpoint_iter))\n\n# If you're loading the hosted model\nloadFilename = 'data/4000_checkpoint.tar'\n\n# Load model\n# Force CPU device options (to match tensors in this tutorial)\ncheckpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\nencoder_sd = checkpoint['en']\ndecoder_sd = checkpoint['de']\nencoder_optimizer_sd = checkpoint['en_opt']\ndecoder_optimizer_sd = checkpoint['de_opt']\nembedding_sd = checkpoint['embedding']\nvoc = Voc(corpus_name)\nvoc.__dict__ = checkpoint['voc_dict']\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.num_words, hidden_size)\nembedding.load_state_dict(embedding_sd)\n# Initialize encoder &amp; decoder models\nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n# Load trained model params\nencoder.load_state_dict(encoder_sd)\ndecoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\n# Set dropout layers to eval mode\nencoder.eval()\ndecoder.eval()\nprint('Models built and ready to go!')\n</code></pre> <p>Out:</p> <pre><code>Building encoder and decoder ...\nModels built and ready to go!\n\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#torch","title":"\u6a21\u578b\u8f6c\u6362\u4e3a Torch \u811a\u672c","text":""},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_18","title":"\u7f16\u7801\u5668","text":"<p>\u6b63\u5982\u524d\u6587\u6240\u8ff0\uff0c\u8981\u5c06\u7f16\u7801\u5668\u6a21\u578b\u8f6c\u6362\u4e3aTorch\u811a\u672c\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u8ddf\u8e2a<code>Tracing</code>\u3002\u8ddf\u8e2a\u4efb\u4f55\u9700\u8981\u901a\u8fc7\u6a21\u578b\u7684<code>forward</code>\u65b9\u6cd5\u8fd0\u884c\u4e00\u4e2a\u793a\u4f8b\u8f93\u5165\uff0c\u4ee5\u53ca\u8ddf\u8e2a\u6570\u636e\u76f8\u9047\u65f6\u7684\u56fe\u5f62\u8ba1\u7b97\u3002\u7f16\u7801\u5668\u6a21\u578b\u63a5\u6536\u4e00\u4e2a\u8f93\u5165\u5e8f\u5217\u548c\u4e00\u4e2a\u957f\u5ea6\u76f8\u5173\u7684\u5f20\u91cf\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u5e8f\u5217<code>test_seq</code>\uff0c\u914d\u7f6e\u5408\u9002\u7684\u5927\u5c0f(MAX_LENGTH,1) \u5305\u542b\u9002\u5f53\u8303\u56f4\u5185\u7684\u6570\u503c \\(\\([0,voc.num\\_words]\\)\\) \u4ee5\u53ca\u642d\u914d\u7684\u7c7b\u578b(int64)\u3002\u6211\u4eec\u8fd8\u521b\u5efa\u4e86<code>test_seq_length</code>\u6807\u91cf\uff0c\u8be5\u6807\u91cf\u5b9e\u9645\u5305\u542b\u4e0e<code>test_seq</code>\u4e2d\u5355\u8bcd\u6570\u91cf\u5bf9\u5e94\u7684\u503c\u3002\u4e0b\u4e00\u6b65\u662f\u4f7f\u7528<code>torch.jit.trace</code>\u51fd\u6570\u6765\u8ddf\u8e2a\u6a21\u578b\u3002\u6ce8\u610f\uff0c\u6211\u4eec\u4f20\u9012\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f\u8981\u8ddf\u8e2a\u7684\u6a21\u5757\uff0c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6a21\u5757<code>forward</code>\u65b9\u6cd5\u7684\u53c2\u6570\u5143\u7ec4\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_19","title":"\u89e3\u7801\u5668","text":"<p>\u6211\u4eec\u5bf9\u89e3\u7801\u5668\u7684\u8ddf\u8e2a\u8fc7\u7a0b\u4e0e\u5bf9\u7f16\u7801\u5668\u7684\u8ddf\u8e2a\u8fc7\u7a0b\u76f8\u540c\u3002\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5bf9traced_encoder\u7684\u4e00\u7ec4\u968f\u673a\u8f93\u5165\u8c03\u7528forward\uff0c\u4ee5\u83b7\u5f97\u89e3\u7801\u5668\u6240\u9700\u7684\u8f93\u51fa\u3002\u8fd9\u4e0d\u662f\u5fc5\u9700\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u4e5f\u53ef\u4ee5\u7b80\u5355\u5730\u751f\u6210\u4e00\u4e2a\u5f62\u72b6\u3001\u7c7b\u578b\u548c\u503c\u8303\u56f4\u6b63\u786e\u7684\u5f20\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u662f\u53ef\u884c\u7684\uff0c\u56e0\u4e3a\u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5bf9\u5f20\u91cf\u7684\u503c\u6ca1\u6709\u4efb\u4f55\u7ea6\u675f\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u4efb\u4f55\u64cd\u4f5c\u53ef\u80fd\u5bfc\u81f4\u8d85\u51fa\u8303\u56f4\u7684\u8f93\u5165\u51fa\u9519\u3002</p>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#greedysearchdecoder_1","title":"\u8d2a\u5a6a\u641c\u7d22\u89e3\u7801\u5668(GreedySearchDecoder)","text":"<p>\u56de\u60f3\u4e00\u4e0b\uff0c\u7531\u4e8e\u5b58\u5728\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u63a7\u5236\u6d41\uff0c\u6211\u4eec\u4e3a\u641c\u7d22\u5668\u6a21\u5757\u7f16\u5199\u4e86\u811a\u672c\u3002\u5728\u811a\u672c\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u901a\u8fc7\u6dfb\u52a0\u4fee\u9970\u7b26\u5e76\u786e\u4fdd\u5b9e\u73b0\u7b26\u5408\u811a\u672c\u89c4\u5219\u6765\u9884\u5148\u5b8c\u6210\u8f6c\u6362\u5de5\u4f5c\u3002\u6211\u4eec\u521d\u59cb\u5316\u811a\u672c\u641c\u7d22\u5668\u7684\u65b9\u5f0f\u4e0e\u521d\u59cb\u5316\u672a\u811a\u672c\u5316\u53d8\u91cf\u7684\u65b9\u5f0f\u76f8\u540c\u3002</p> <pre><code>### Convert encoder model\n# Create artificial inputs\ntest_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words)\ntest_seq_length = torch.LongTensor([test_seq.size()[0]])\n# Trace the model\ntraced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n\n### Convert decoder model\n# Create and generate artificial inputs\ntest_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\ntest_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\ntest_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n# Trace the model\ntraced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n\n### Initialize searcher module\nscripted_searcher = GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers)\n\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_20","title":"\u56fe\u5f62\u6253\u5370","text":"<p>\u73b0\u5728\u6211\u4eec\u7684\u6a21\u578b\u662fTorch\u811a\u672c\u5f62\u5f0f\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u6253\u5370\u6bcf\u4e2a\u6a21\u578b\u7684\u56fe\u5f62\uff0c\u4ee5\u786e\u4fdd\u9002\u5f53\u5730\u6355\u83b7\u8ba1\u7b97\u56fe\u5f62\u3002\u56e0\u4e3a<code>scripted_searcher</code>\u5305\u542b<code>traced_encoder</code>\u548c<code>traced_decoder</code>\uff0c\u6240\u4ee5\u8fd9\u4e9b\u56fe\u5c06\u4ee5\u5185\u8054\u65b9\u5f0f\u6253\u5370</p> <pre><code>print('scripted_searcher graph:\\n', scripted_searcher.graph)\n\n</code></pre> <p>Out:</p> <pre><code>scripted_searcher graph:\n graph(%input_seq : Tensor\n      %input_length : Tensor\n      %max_length : int\n      %3 : Tensor\n      %4 : Tensor\n      %5 : Tensor\n      %6 : Tensor\n      %7 : Tensor\n      %8 : Tensor\n      %9 : Tensor\n      %10 : Tensor\n      %11 : Tensor\n      %12 : Tensor\n      %13 : Tensor\n      %14 : Tensor\n      %15 : Tensor\n      %16 : Tensor\n      %17 : Tensor\n      %18 : Tensor\n      %19 : Tensor\n      %118 : Tensor\n      %119 : Tensor\n      %120 : Tensor\n      %121 : Tensor\n      %122 : Tensor\n      %123 : Tensor\n      %124 : Tensor\n      %125 : Tensor\n      %126 : Tensor\n      %127 : Tensor\n      %128 : Tensor\n      %129 : Tensor\n      %130 : Tensor) {\n  %58 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN\n  %53 : float = prim::Constant[value=0](), scope: EncoderRNN\n  %43 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru]\n  %42 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru]\n  %41 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru]\n  %36 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru]\n  %34 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru]\n  %25 : int = prim::Constant[value=4](), scope: EncoderRNN\n  %24 : Device = prim::Constant[value=\"cpu\"](), scope: EncoderRNN\n  %21 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]\n  %20 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding]\n  %90 : int = prim::Constant[value=0]()\n  %94 : int = prim::Constant[value=1]()\n  %input.7 : Float(10, 1, 500) = aten::embedding(%3, %input_seq, %20, %21, %21), scope: EncoderRNN/Embedding[embedding]\n  %lengths : Long(1) = aten::to(%input_length, %24, %25, %21, %21), scope: EncoderRNN\n  %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %21), scope: EncoderRNN\n  %35 : int[] = prim::ListConstruct(%25, %94, %34), scope: EncoderRNN/GRU[gru]\n  %hx : Float(4, 1, 500) = aten::zeros(%35, %36, %90, %24), scope: EncoderRNN/GRU[gru]\n  %40 : Tensor[] = prim::ListConstruct(%4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19), scope: EncoderRNN/GRU[gru]\n  %46 : Float(10, 1000), %encoder_hidden : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %40, %41, %42, %43, %21, %41), scope: EncoderRNN/GRU[gru]\n  %49 : int = aten::size(%batch_sizes, %90), scope: EncoderRNN\n  %max_seq_length : Long() = prim::NumToTensor(%49), scope: EncoderRNN\n  %51 : int = prim::Int(%max_seq_length), scope: EncoderRNN\n  %outputs : Float(10, 1, 1000), %55 : Long(1) = aten::_pad_packed_sequence(%46, %batch_sizes, %21, %53, %51), scope: EncoderRNN\n  %60 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n  %65 : Float(10, 1, 1000) = aten::slice(%60, %94, %90, %58, %94), scope: EncoderRNN\n  %70 : Float(10, 1!, 500) = aten::slice(%65, %42, %90, %34, %94), scope: EncoderRNN\n  %75 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n  %80 : Float(10, 1, 1000) = aten::slice(%75, %94, %90, %58, %94), scope: EncoderRNN\n  %85 : Float(10, 1!, 500) = aten::slice(%80, %42, %34, %58, %94), scope: EncoderRNN\n  %encoder_outputs : Float(10, 1, 500) = aten::add(%70, %85, %94), scope: EncoderRNN\n  %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden, %90, %90, %42, %94)\n  %98 : int[] = prim::ListConstruct(%94, %94)\n  %100 : Tensor = aten::ones(%98, %25, %90, %24)\n  %decoder_input.1 : Tensor = aten::mul(%100, %94)\n  %103 : int[] = prim::ListConstruct(%90)\n  %all_tokens.1 : Tensor = aten::zeros(%103, %25, %90, %24)\n  %108 : int[] = prim::ListConstruct(%90)\n  %all_scores.1 : Tensor = aten::zeros(%108, %36, %90, %24)\n  %all_scores : Tensor, %all_tokens : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length, %41, %all_scores.1, %all_tokens.1, %decoder_hidden.1, %decoder_input.1)\n    block0(%114 : int, %188 : Tensor, %184 : Tensor, %116 : Tensor, %115 : Tensor) {\n      %input.2 : Float(1, 1, 500) = aten::embedding(%118, %115, %20, %21, %21), scope: LuongAttnDecoderRNN/Embedding[embedding]\n      %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %43, %21), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout]\n      %138 : Tensor[] = prim::ListConstruct(%119, %120, %121, %122, %123, %124, %125, %126), scope: LuongAttnDecoderRNN/GRU[gru]\n      %hidden : Float(1, 1, 500), %decoder_hidden.2 : Float(2, 1, 500) = aten::gru(%input.3, %116, %138, %41, %42, %43, %21, %21, %21), scope: LuongAttnDecoderRNN/GRU[gru]\n      %147 : Float(10, 1, 500) = aten::mul(%hidden, %encoder_outputs), scope: LuongAttnDecoderRNN/Attn[attn]\n      %149 : int[] = prim::ListConstruct(%42), scope: LuongAttnDecoderRNN/Attn[attn]\n      %attn_energies : Float(10, 1) = aten::sum(%147, %149, %21), scope: LuongAttnDecoderRNN/Attn[attn]\n      %input.4 : Float(1!, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn]\n      %154 : Float(1, 10) = aten::softmax(%input.4, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n      %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%154, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n      %159 : Float(1!, 10, 500) = aten::transpose(%encoder_outputs, %90, %94), scope: LuongAttnDecoderRNN\n      %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %159), scope: LuongAttnDecoderRNN\n      %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %90), scope: LuongAttnDecoderRNN\n      %context : Float(1, 500) = aten::squeeze(%context.1, %94), scope: LuongAttnDecoderRNN\n      %165 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN\n      %input.5 : Float(1, 1000) = aten::cat(%165, %94), scope: LuongAttnDecoderRNN\n      %168 : Float(1000!, 500!) = aten::t(%127), scope: LuongAttnDecoderRNN/Linear[concat]\n      %171 : Float(1, 500) = aten::addmm(%128, %input.5, %168, %94, %94), scope: LuongAttnDecoderRNN/Linear[concat]\n      %input.6 : Float(1, 500) = aten::tanh(%171), scope: LuongAttnDecoderRNN\n      %173 : Float(500!, 7826!) = aten::t(%129), scope: LuongAttnDecoderRNN/Linear[out]\n      %input : Float(1, 7826) = aten::addmm(%130, %input.6, %173, %94, %94), scope: LuongAttnDecoderRNN/Linear[out]\n      %decoder_output : Float(1, 7826) = aten::softmax(%input, %94), scope: LuongAttnDecoderRNN\n      %decoder_scores : Tensor, %decoder_input.2 : Tensor = aten::max(%decoder_output, %94, %21)\n      %186 : Tensor[] = prim::ListConstruct(%184, %decoder_input.2)\n      %all_tokens.2 : Tensor = aten::cat(%186, %90)\n      %190 : Tensor[] = prim::ListConstruct(%188, %decoder_scores)\n      %all_scores.2 : Tensor = aten::cat(%190, %90)\n      %decoder_input.3 : Tensor = aten::unsqueeze(%decoder_input.2, %90)\n      -&gt; (%41, %all_scores.2, %all_tokens.2, %decoder_hidden.2, %decoder_input.3)\n    }\n  %198 : (Tensor, Tensor) = prim::TupleConstruct(%all_tokens, %all_scores)\n  return (%198);\n}\n\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_21","title":"\u8fd0\u884c\u7ed3\u679c\u8bc4\u4f30","text":"<p>\u6700\u540e\uff0c\u6211\u4eec\u5c06\u4f7f\u7528Torch\u811a\u672c\u6a21\u578b\u5bf9\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002\u5982\u679c\u8f6c\u6362\u6b63\u786e\uff0c\u6a21\u578b\u7684\u884c\u4e3a\u5c06\u4e0e\u5b83\u4eec\u5728\u5373\u65f6\u6a21\u5f0f\u8868\u793a\u4e2d\u7684\u884c\u4e3a\u5b8c\u5168\u76f8\u540c\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u8ba1\u7b97\u4e00\u4e9b\u5e38\u89c1\u7684\u67e5\u8be2\u8bed\u53e5\u3002\u5982\u679c\u60a8\u60f3\u81ea\u5df1\u4e0e\u673a\u5668\u4eba\u804a\u5929\uff0c\u53d6\u6d88\u5bf9<code>evaluateInput</code>\u884c\u7684\u6ce8\u91ca\u5e76\u8ba9\u5b83\u65cb\u8f6c\u3002</p> <pre><code># Evaluate examples\nsentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"]\nfor s in sentences:\n    evaluateExample(s, traced_encoder, traced_decoder, scripted_searcher, voc)\n\n# Evaluate your input\n#evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)\n\n</code></pre> <p>Out:</p> <pre><code>&gt; hello\nBot: hello .\n&gt; what's up?\nBot: i m going to get my car .\n&gt; who are you?\nBot: i m the owner .\n&gt; where am I?\nBot: in the house .\n&gt; where are you from?\nBot: south america .\n\n</code></pre>"},{"location":"1.0/deploy_seq2seq_hybrid_frontend_tutorial/#_22","title":"\u4fdd\u5b58\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u6210\u529f\u5730\u5c06\u6a21\u578b\u8f6c\u6362\u4e3aTorch\u811a\u672c\uff0c\u63a5\u4e0b\u6765\u5c06\u5bf9\u5176\u8fdb\u884c\u5e8f\u5217\u5316\uff0c\u4ee5\u4fbf\u5728\u975epython\u90e8\u7f72\u73af\u5883\u4e2d\u4f7f\u7528\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u53ea\u9700\u4fdd\u5b58<code>scripted_searcher</code>\u6a21\u5757\uff0c\u56e0\u4e3a\u8fd9\u662f\u7528\u4e8e\u5bf9\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\u8fd0\u884c\u63a8\u7406\u7684\u9762\u5411\u7528\u6237\u7684\u63a5\u53e3\u3002\u4fdd\u5b58\u811a\u672c\u6a21\u5757\u65f6\uff0c\u4f7f\u7528<code>script_module.save(PATH)</code>\u4ee3\u66ff<code>torch.save(model, PATH)</code>\u3002</p> <pre><code>scripted_searcher.save(\"scripted_chatbot.pth\")\n\n</code></pre>"},{"location":"1.0/dist_tuto/","title":"\u4f7f\u7528PyTorch\u7f16\u5199\u5206\u5e03\u5f0f\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u8bd1\u8005\uff1afirdameng</p> <p>\u4f5c\u8005\uff1aSoumith Chintala</p> <p>\u5728\u8fd9\u4e2a\u7b80\u77ed\u7684\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bbaPyTorch\u7684\u5206\u5e03\u5f0f\u8f6f\u4ef6\u5305\u3002 \u6211\u4eec\u5c06\u770b\u5230\u5982\u4f55\u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bbe\u7f6e\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u901a\u4fe1\u7b56\u7565\uff0c\u5e76\u67e5\u770b\u5305\u7684\u5185\u90e8\u90e8\u5206\u3002</p>"},{"location":"1.0/dist_tuto/#_1","title":"\u5f00\u59cb","text":"<p>PyTorch\u4e2d\u5305\u542b\u7684\u5206\u5e03\u5f0f\u8f6f\u4ef6\u5305(\u5373torch.distributed\uff09\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u80fd\u591f\u8f7b\u677e\u5730\u8de8\u8fdb\u7a0b\u548c\u8ba1\u7b97\u673a\u96c6\u7fa4\u5e76\u884c\u5316\u4ed6\u4eec\u7684\u8ba1\u7b97\u3002 \u4e3a\u6b64\uff0c\u5b83\u5229\u7528\u6d88\u606f\u4f20\u9012\u8bed\u4e49\uff0c\u5141\u8bb8\u6bcf\u4e2a\u8fdb\u7a0b\u5c06\u6570\u636e\u4f20\u9012\u7ed9\u4efb\u4f55\u5176\u4ed6\u8fdb\u7a0b\u3002 \u4e0e\u5e76\u884c\u5904\u7406(torch.multiprocessing\uff09\u5305\u76f8\u53cd\uff0c\u8fdb\u7a0b\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u901a\u4fe1\u540e\u7aef\uff0c\u5e76\u4e14\u4e0d\u9650\u4e8e\u5728\u540c\u4e00\u53f0\u673a\u5668\u4e0a\u6267\u884c\u3002</p> <p>\u5f00\u59cb\u6211\u4eec\u9700\u8981\u80fd\u591f\u540c\u65f6\u8fd0\u884c\u591a\u4e2a\u8fdb\u7a0b\u3002 \u5982\u679c\u60a8\u6709\u6743\u8bbf\u95ee\u8ba1\u7b97\u7fa4\u96c6\uff0c\u5219\u5e94\u4f7f\u7528\u672c\u5730sysadmin\u8fdb\u884c\u68c0\u67e5\uff0c\u6216\u4f7f\u7528\u60a8\u559c\u6b22\u7684\u534f\u8c03\u5de5\u5177\u3002 (\u4f8b\u5982\uff0cpdsh\uff0cclustershell\u6216\u5176\u4ed6\uff09\u4e3a\u4e86\u672c\u6559\u7a0b\u7684\u76ee\u7684\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u5355\u4e2a\u673a\u5668\u5e76\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f\u5efa\u7acb\u591a\u4e2a\u8fdb\u7a0b\u3002</p> <pre><code>\"\"\"run.py:\"\"\"\n#!/usr/bin/env python\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    pass\n\ndef init_processes(rank, size, fn, backend='tcp'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n</code></pre> <p>\u4e0a\u9762\u7684\u811a\u672c\u4ea7\u751f\u4e86\u4e24\u4e2a\u8fdb\u7a0b\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u5c06\u8bbe\u7f6e\u5206\u5e03\u5f0f\u73af\u5883\uff0c\u521d\u59cb\u5316\u8fdb\u7a0b\u7ec4(dist.init_process_group\uff09\uff0c\u6700\u540e\u6267\u884c\u7ed9\u5b9a\u7684\u8fd0\u884c\u51fd\u6570\u3002</p> <p>\u6211\u4eec\u6765\u770b\u770binit_processes\u51fd\u6570\u3002 \u5b83\u786e\u4fdd\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u80fd\u591f\u4f7f\u7528\u76f8\u540c\u7684IP\u5730\u5740\u548c\u7aef\u53e3\u901a\u8fc7\u4e3b\u7ad9\u8fdb\u884c\u534f\u8c03\u3002 \u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u4f7f\u7528\u4e86TCP\u540e\u7aef\uff0c\u4f46\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528MPI\u6216Gloo\u3002 (\u53c2\u89c15.1\u8282\uff09\u6211\u4eec\u5c06\u5728\u672c\u6559\u7a0b\u7ed3\u675f\u65f6\u8ba8\u8bbadist.init_process_group\u4e2d\u4ea7\u751f\u7684\u7279\u6548\uff0c\u4f46\u5b83\u5b9e\u8d28\u4e0a\u5141\u8bb8\u8fdb\u7a0b\u901a\u8fc7\u5171\u4eab\u5176\u4f4d\u7f6e\u6765\u76f8\u4e92\u901a\u4fe1\u3002</p>"},{"location":"1.0/dist_tuto/#_2","title":"\u70b9\u5bf9\u70b9\u901a\u4fe1","text":"<p>\u53d1\u9001\u4e0e\u63a5\u6536</p> <p>\u5c06\u6570\u636e\u4ece\u4e00\u4e2a\u8fdb\u7a0b\u4f20\u8f93\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u79f0\u4e3a\u70b9\u5bf9\u70b9\u901a\u4fe1\u3002 \u8fd9\u4e9b\u662f\u901a\u8fc7send\u548crecv\u51fd\u6570\u6216\u5b83\u4eec\u7684\u76f4\u63a5\u5bf9\u5e94\u90e8\u5206isend\u548cirecv\u5b9e\u73b0\u7684\u3002</p> <pre><code>\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    print('Rank ', rank, ' has data ', tensor[0])\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u4e24\u4e2a\u8fdb\u7a0b\u90fd\u4ee5\u96f6\u5f20\u91cf\u5f00\u59cb\uff0c\u7136\u540e\u8fdb\u7a0b0\u9012\u589e\u5f20\u91cf\u5e76\u5c06\u5176\u53d1\u9001\u5230\u8fdb\u7a0b1\uff0c\u4ee5\u4fbf\u5b83\u4eec\u90fd\u4ee51.0\u7ed3\u675f\u3002 \u8bf7\u6ce8\u610f\uff0c\u8fdb\u7a0b1\u9700\u8981\u5206\u914d\u5185\u5b58\u4ee5\u5b58\u50a8\u5b83\u5c06\u63a5\u6536\u7684\u6570\u636e\u3002</p> <p>\u53e6\u8bf7\u6ce8\u610f\uff0csend / recv\u6b63\u5728\u963b\u585e\uff1a\u4e24\u4e2a\u8fdb\u7a0b\u90fd\u4f1a\u505c\u6b62\uff0c\u76f4\u5230\u901a\u4fe1\u5b8c\u6210\u3002 \u53e6\u4e00\u65b9\u9762\uff0cimmediates\u662f\u975e\u963b\u585e\u7684; \u811a\u672c\u7ee7\u7eed\u6267\u884c\uff0c\u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2aDistributedRequest\u5bf9\u8c61\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9wait(\uff09\u3002</p> <pre><code>\"\"\"Non-blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print('Rank 0 started sending')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print('Rank 1 started receiving')\n    req.wait()\n    print('Rank ', rank, ' has data ', tensor[0])\n</code></pre> <p>\u5f53\u4f7f\u7528immediates\u65f6\uff0c\u6211\u4eec\u5fc5\u987b\u5c0f\u5fc3\u4f7f\u7528\u53d1\u9001\u548c\u63a5\u6536\u7684\u5f20\u91cf\u3002 \u7531\u4e8e\u6211\u4eec\u4e0d\u77e5\u9053\u4f55\u65f6\u5c06\u6570\u636e\u4f20\u9012\u7ed9\u53e6\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u56e0\u6b64\u6211\u4eec\u4e0d\u5e94\u8be5\u5728req.wait(\uff09\u5b8c\u6210\u4e4b\u524d\u4fee\u6539\u53d1\u9001\u7684\u5f20\u91cf\u6216\u8bbf\u95ee\u63a5\u6536\u7684\u5f20\u91cf\u3002 \u6362\u4e00\u79cd\u8bf4\u6cd5\uff0c</p> <ul> <li>\u5728dist.isend(\uff09\u4e4b\u540e\u5199\u5165\u5f20\u91cf\u5c06\u5bfc\u81f4\u672a\u5b9a\u4e49\u7684\u884c\u4e3a\u3002</li> <li>\u5728dist.irecv(\uff09\u4e4b\u540e\u8bfb\u53d6\u5f20\u91cf\u5c06\u5bfc\u81f4\u672a\u5b9a\u4e49\u7684\u884c\u4e3a\u3002</li> </ul> <p>\u4f46\u662f\uff0c\u5728\u6267\u884creq.wait(\uff09\u4e4b\u540e\uff0c\u6211\u4eec\u4fdd\u8bc1\u53d1\u751f\u901a\u4fe1\uff0c\u5e76\u4e14\u5b58\u50a8\u5728tensor [0]\u4e2d\u7684\u503c\u4e3a1.0\u3002</p> <p>\u5f53\u6211\u4eec\u60f3\u8981\u5bf9\u6d41\u7a0b\u7684\u901a\u4fe1\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u65f6\uff0c\u70b9\u5bf9\u70b9\u901a\u4fe1\u975e\u5e38\u6709\u7528\u3002 \u5b83\u4eec\u53ef\u7528\u4e8e\u5b9e\u73b0\u5947\u5999\u7684\u7b97\u6cd5\uff0c\u4f8b\u5982\u767e\u5ea6DeepSpeech\u6216Facebook\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684\u7b97\u6cd5\u3002(\u53c2\u89c14.1\u8282\uff09</p>"},{"location":"1.0/dist_tuto/#_3","title":"\u96c6\u4f53\u901a\u4fe1","text":"<p>Scatter</p> <p></p> <p>Gather</p> <p></p> <p>Reduce</p> <p></p> <p>All-Reduce</p> <p></p> <p>Broadcast</p> <p></p> <p>All_gather</p> <p>\u4e0e\u70b9\u5bf9\u70b9\u901a\u4fe1\u76f8\u53cd\uff0c\u5728\u96c6\u4f53\u4e2d\u5141\u8bb8\u901a\u4fe1\u6a21\u5f0f\u8de8\u8d8a\u7ec4\u4e2d\u6240\u6709\u8fdb\u7a0b\u3002 \u7ec4\u662f\u6211\u4eec\u6240\u6709\u8fdb\u7a0b\u7684\u5b50\u96c6\u3002 \u8981\u521b\u5efa\u7ec4\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u961f\u5217\u5217\u8868\u4f20\u9012\u7ed9dist.new_group(\u7ec4\uff09\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u96c6\u5408\u4f53\u5728\u6240\u6709\u8fdb\u7a0b(\u4e5f\u79f0\u4e3aworld\uff09\u4e0a\u6267\u884c\u3002 \u4f8b\u5982\uff0c\u4e3a\u4e86\u83b7\u5f97\u6240\u6709\u8fc7\u7a0b\u4e2d\u6240\u6709\u5f20\u91cf\u7684\u603b\u548c\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528dist.all_reduce(tensor\uff0cop\uff0cgroup\uff09\u96c6\u5408\u3002</p> <pre><code>\"\"\" All-Reduce example.\"\"\"\ndef run(rank, size):\n    \"\"\" Simple point-to-point communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n</code></pre> <p>\u7531\u4e8e\u6211\u4eec\u60f3\u8981\u7ec4\u4e2d\u6240\u6709\u5f20\u91cf\u7684\u603b\u548c\uff0c\u6211\u4eec\u4f7f\u7528dist.reduce_op.SUM\u4f5c\u4e3areduce\u8fd0\u7b97\u7b26\u3002 \u4e00\u822c\u800c\u8a00\uff0c\u4efb\u4f55\u53ef\u4ea4\u6362\u7684\u6570\u5b66\u8fd0\u7b97\u90fd\u53ef\u4ee5\u7528\u4f5c\u8fd0\u7b97\u7b26\u3002 \u5f00\u7bb1\u5373\u7528\uff0cPyTorch\u5e26\u67094\u4e2a\u8fd9\u6837\u7684\u8fd0\u7b97\u7b26\uff0c\u6240\u6709\u8fd0\u7b97\u7b26\u90fd\u5728\u5143\u7d20\u7ea7\u522b\u4e0a\u8fd0\u884c\uff1a</p> <ul> <li><code>dist.reduce_op.SUM</code>,</li> <li><code>dist.reduce_op.PRODUCT</code>,</li> <li><code>dist.reduce_op.MAX</code>,</li> <li><code>dist.reduce_op.MIN</code>.</li> </ul> <p>\u9664\u4e86dist.all_reduce(tensor\uff0cop\uff0cgroup\uff09\u4e4b\u5916\uff0cPyTorch\u76ee\u524d\u5171\u67096\u4e2a\u96c6\u4f53\u3002</p> <ul> <li><code>dist.broadcast(tensor, src, group)</code>: Copies <code>tensor</code> from <code>src</code> to all other processes.</li> <li><code>dist.reduce(tensor, dst, op, group)</code>: Applies <code>op</code> to all <code>tensor</code> and stores the result in <code>dst</code>.</li> <li><code>dist.all_reduce(tensor, op, group)</code>: Same as reduce, but the result is stored in all processes.</li> <li><code>dist.scatter(tensor, src, scatter_list, group)</code>: Copies the \\(\\(i^{\\text{th}}\\)\\) tensor <code>scatter_list[i]</code> to the \\(\\(i^{\\text{th}}\\)\\) process.</li> <li><code>dist.gather(tensor, dst, gather_list, group)</code>: Copies <code>tensor</code> from all processes in <code>dst</code>.</li> <li><code>dist.all_gather(tensor_list, tensor, group)</code>: Copies <code>tensor</code> from all processes to <code>tensor_list</code>, on all processes.</li> <li><code>dist.barrier(group)</code>: block all processes in <code>group</code> until each one has entered this function.</li> </ul>"},{"location":"1.0/dist_tuto/#_4","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u6ce8\u610f\uff1a\u60a8\u53ef\u4ee5\u5728\u6b64GitHub\u5b58\u50a8\u5e93\u4e2d\u627e\u5230\u6b64\u90e8\u5206\u7684 \u793a\u4f8b\u811a\u672c</p> <p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u4e86\u89e3\u4e86\u5206\u5e03\u5f0f\u6a21\u5757\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e9b\u6709\u7528\u7684\u4e1c\u897f\u3002 \u6211\u4eec\u7684\u76ee\u6807\u662f\u590d\u5236DistributedDataParallel\u7684\u529f\u80fd\u3002 \u5f53\u7136\uff0c\u8fd9\u5c06\u662f\u4e00\u4e2a\u6559\u5b66\u793a\u4f8b\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u60a8\u5e94\u8be5\u4f7f\u7528\u4e0a\u9762\u94fe\u63a5\u7684\u5b98\u65b9\uff0c\u7ecf\u8fc7\u826f\u597d\u6d4b\u8bd5\u548c\u4f18\u5316\u7684\u7248\u672c\u3002</p> <p>\u5f88\u7b80\u5355\uff0c\u6211\u4eec\u60f3\u8981\u5b9e\u73b0\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u5206\u5e03\u5f0f\u7248\u672c\u3002 \u6211\u4eec\u7684\u811a\u672c\u5c06\u5141\u8bb8\u6240\u6709\u8fdb\u7a0b\u5728\u5176\u6279\u91cf\u6570\u636e\u4e0a\u8ba1\u7b97\u5176\u6a21\u578b\u7684\u68af\u5ea6\uff0c\u7136\u540e\u5e73\u5747\u5176\u6e10\u53d8\u3002 \u4e3a\u4e86\u5728\u66f4\u6539\u8fdb\u7a0b\u6570\u65f6\u786e\u4fdd\u7c7b\u4f3c\u7684\u6536\u655b\u7ed3\u679c\uff0c\u6211\u4eec\u9996\u5148\u5fc5\u987b\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u5206\u533a\u3002 (\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528tnt.dataset.SplitDataset\uff0c\u800c\u4e0d\u662f\u4e0b\u9762\u7684\u4ee3\u7801\u6bb5\u3002\uff09</p> <pre><code>\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n</code></pre> <p>\u901a\u8fc7\u4e0a\u9762\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u51e0\u884c\u7b80\u5355\u5730\u5bf9\u4efb\u4f55\u6570\u636e\u96c6\u8fdb\u884c\u5206\u533a\uff1a</p> <pre><code>\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz\n</code></pre> <p>\u5047\u8bbe\u6211\u4eec\u67092\u4e2a\u526f\u672c\uff0c\u90a3\u4e48\u6bcf\u4e2a\u8fdb\u7a0b\u5c06\u5177\u670960000/2 = 30000\u4e2a\u6837\u672c\u7684train_set\u3002 \u6211\u4eec\u8fd8\u5c06\u6279\u91cf\u5927\u5c0f\u9664\u4ee5\u526f\u672c\u6570\u91cf\uff0c\u4ee5\u4fdd\u6301\u603b\u6279\u91cf\u5927\u5c0f\u4e3a128\u3002</p> <p>\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u7f16\u5199\u6211\u4eec\u901a\u5e38\u7684\u524d\u5411\u540e\u5411\u4f18\u5316\u8bad\u7ec3\u4ee3\u7801\uff0c\u5e76\u6dfb\u52a0\u4e00\u4e2a\u51fd\u6570\u8c03\u7528\u6765\u5e73\u5747\u6211\u4eec\u6a21\u578b\u7684\u6e10\u53d8\u3002 (\u4ee5\u4e0b\u5185\u5bb9\u4e3b\u8981\u6765\u81ea\u5b98\u65b9\u7684PyTorch MNIST\u793a\u4f8b\u3002\uff09</p> <pre><code>\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)\n</code></pre> <p>\u5b83\u4ecd\u7136\u662f\u5b9e\u73b0average_gradients(\u6a21\u578b\uff09\u51fd\u6570\uff0c\u5b83\u53ea\u662f\u7b80\u5355\u5730\u63a5\u53d7\u4e00\u4e2a\u6a21\u578b\u5e76\u5728\u6574\u4e2a\u7a7a\u95f4\u4e2d\u5e73\u5747\u5176\u6e10\u53d8\u3002</p> <pre><code>\"\"\" Gradient averaging. \"\"\"\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n        param.grad.data /= size\n</code></pre> <p>\u6211\u4eec\u6210\u529f\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u540c\u6b65SGD\uff0c\u53ef\u4ee5\u5728\u5927\u578b\u8ba1\u7b97\u673a\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u4efb\u4f55\u6a21\u578b\u3002</p> <p>\u6ce8\u610f\uff1a\u867d\u7136\u6700\u540e\u4e00\u53e5\u5728\u6280\u672f\u4e0a\u662f\u6b63\u786e\u7684\uff0c\u4f46\u5b9e\u73b0\u540c\u6b65SGD\u7684\u751f\u4ea7\u7ea7\u5b9e\u73b0\u9700\u8981\u66f4\u591a\u6280\u5de7\u3002 \u518d\u6b21\uff0c\u4f7f\u7528\u5df2\u7ecf\u8fc7\u6d4b\u8bd5\u548c\u4f18\u5316\u7684\u5185\u5bb9\u3002</p>"},{"location":"1.0/dist_tuto/#ring-allreduce","title":"\u81ea\u5b9a\u4e49Ring-Allreduce","text":"<p>\u4f5c\u4e3a\u4e00\u4e2a\u989d\u5916\u7684\u6311\u6218\uff0c\u60f3\u8c61\u4e00\u4e0b\u6211\u4eec\u60f3\u8981\u5b9e\u73b0DeepSpeech\u7684\u9ad8\u6548\u73afallreduce\u3002 \u4f7f\u7528\u70b9\u5bf9\u70b9\u96c6\u5408\u76f8\u5f53\u5bb9\u6613\u5b9e\u73b0\u3002</p> <pre><code>\"\"\" Implementation of a ring-reduce with addition. \"\"\"\ndef allreduce(send, recv):\n    rank = dist.get_rank()\n    size = dist.get_world_size()\n    send_buff = th.zeros(send.size())\n    recv_buff = th.zeros(send.size())\n    accum = th.zeros(send.size())\n    accum[:] = send[:]\n\n    left = ((rank - 1) + size) % size\n    right = (rank + 1) % size\n\n    for i in range(size - 1):\n        if i % 2 == 0:\n            # Send send_buff\n            send_req = dist.isend(send_buff, right)\n            dist.recv(recv_buff, left)\n            accum[:] += recv[:]\n        else:\n            # Send recv_buff\n            send_req = dist.isend(recv_buff, right)\n            dist.recv(send_buff, left)\n            accum[:] += send[:]\n        send_req.wait()\n    recv[:] = accum[:]\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u811a\u672c\u4e2d\uff0callreduce(send\uff0crecv\uff09\u51fd\u6570\u7684\u7b7e\u540d\u4e0ePyTorch\u4e2d\u7684\u7b7e\u540d\u7565\u6709\u4e0d\u540c\u3002 \u5b83\u9700\u8981\u4e00\u4e2arecv\u5f20\u91cf\uff0c\u5e76\u5c06\u6240\u6709\u53d1\u9001\u5f20\u91cf\u7684\u603b\u548c\u5b58\u50a8\u5728\u5176\u4e2d\u3002 \u4f5c\u4e3a\u7ec3\u4e60\u7559\u7ed9\u8bfb\u8005\uff0c\u6211\u4eec\u7684\u7248\u672c\u548cDeepSpeech\u4e2d\u7684\u7248\u672c\u4e4b\u95f4\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u533a\u522b\uff1a\u5b83\u4eec\u7684\u5b9e\u73b0\u5c06\u68af\u5ea6\u5f20\u91cf\u5212\u5206\u4e3a\u5757\uff0c\u4ee5\u4fbf\u6700\u4f73\u5730\u5229\u7528\u901a\u4fe1\u5e26\u5bbd\u3002 (\u63d0\u793a\uff1atorch.chunk\uff09</p>"},{"location":"1.0/dist_tuto/#_5","title":"\u9ad8\u7ea7\u4e3b\u9898","text":"<p>\u6211\u4eec\u73b0\u5728\u51c6\u5907\u53d1\u73b0torch.distributed\u7684\u4e00\u4e9b\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\u3002 \u7531\u4e8e\u6709\u5f88\u591a\u5185\u5bb9\u9700\u8981\u4ecb\u7ecd\uff0c\u672c\u8282\u5206\u4e3a\u4e24\u4e2a\u5c0f\u8282\uff1a</p> <ol> <li>\u901a\u4fe1\u540e\u7aef\uff1a\u6211\u4eec\u5b66\u4e60\u5982\u4f55\u4f7f\u7528MPI\u548cGloo\u8fdb\u884cGPU-GPU\u901a\u4fe1\u3002</li> <li>\u521d\u59cb\u5316\u65b9\u6cd5\uff1a\u6211\u4eec\u4e86\u89e3\u5982\u4f55\u5728dist.init_process_group(\uff09\u4e2d\u6700\u597d\u5730\u8bbe\u7f6e\u521d\u59cb\u534f\u8c03\u9636\u6bb5\u3002</li> </ol>"},{"location":"1.0/dist_tuto/#_6","title":"\u901a\u4fe1\u540e\u7aef","text":"<p>torch.distributed\u6700\u4f18\u96c5\u7684\u65b9\u9762\u4e4b\u4e00\u662f\u5b83\u80fd\u591f\u5728\u4e0d\u540c\u7684\u540e\u7aef\u4e4b\u4e0a\u8fdb\u884c\u62bd\u8c61\u548c\u6784\u5efa\u3002 \u5982\u524d\u6240\u8ff0\uff0c\u76ee\u524d\u5728PyTorch\u4e2d\u5b9e\u73b0\u4e86\u4e09\u4e2a\u540e\u7aef\uff1aTCP\uff0cMPI\u548cGloo\u3002 \u6839\u636e\u6240\u9700\u7684\u7528\u4f8b\uff0c\u5b83\u4eec\u5404\u81ea\u5177\u6709\u4e0d\u540c\u7684\u89c4\u683c\u548c\u6743\u8861\u3002 \u53ef\u4ee5\u5728\u6b64\u5904\u627e\u5230\u652f\u6301\u529f\u80fd\u7684\u6bd4\u8f83\u8868\u3002 \u8bf7\u6ce8\u610f\uff0c\u81ea\u672c\u6559\u7a0b\u521b\u5efa\u4ee5\u6765\uff0c\u5df2\u6dfb\u52a0\u7b2c\u56db\u4e2a\u540e\u7aefNCCL\u3002 \u6709\u5173\u5176\u4f7f\u7528\u548c\u503c\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605torch.distributed docs\u7684\u6b64\u90e8\u5206\u3002</p> <p>TCP\u540e\u7aef</p> <p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u5df2\u5e7f\u6cdb\u4f7f\u7528TCP\u540e\u7aef\u3002 \u5b83\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u53d1\u5e73\u53f0\u975e\u5e38\u65b9\u4fbf\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u4fdd\u8bc1\u5728\u5927\u591a\u6570\u673a\u5668\u548c\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u8fd0\u884c\u3002 \u5b83\u8fd8\u652f\u6301CPU\u4e0a\u7684\u6240\u6709\u70b9\u5bf9\u70b9\u548c\u96c6\u5408\u529f\u80fd\u3002 \u4f46\u662f\uff0c\u4e0d\u652f\u6301GPU\uff0c\u5e76\u4e14\u5176\u901a\u4fe1\u4f8b\u7a0b\u4e0d\u50cfMPI\u90a3\u6837\u4f18\u5316\u3002</p> <p>Gloo\u540e\u7aef</p> <p>Gloo\u540e\u7aef\u4e3aCPU\u548cGPU\u63d0\u4f9b\u4e86\u96c6\u4f53\u901a\u4fe1\u7a0b\u5e8f\u7684\u4f18\u5316\u5b9e\u73b0\u3002 \u5b83\u7279\u522b\u9002\u7528\u4e8eGPU\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u6267\u884c\u901a\u4fe1\u800c\u65e0\u9700\u4f7f\u7528GPUDirect\u5c06\u6570\u636e\u4f20\u8f93\u5230CPU\u7684\u5185\u5b58\u3002 \u5b83\u8fd8\u80fd\u591f\u4f7f\u7528NCCL\u6267\u884c\u5feb\u901f\u7684\u8282\u70b9\u5185\u901a\u4fe1\uff0c\u5e76\u5b9e\u73b0\u5176\u81ea\u5df1\u7684\u8282\u70b9\u95f4\u4f8b\u7a0b\u7b97\u6cd5\u3002</p> <p>\u4ece\u7248\u672c0.2.0\u5f00\u59cb\uff0cGloo\u540e\u7aef\u81ea\u52a8\u5305\u542b\u5728PyTorch\u7684\u9884\u7f16\u8bd1\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u3002 \u6b63\u5982\u60a8\u5df2\u7ecf\u6ce8\u610f\u5230\u7684\u90a3\u6837\uff0c\u5982\u679c\u60a8\u5c06\u6a21\u578b\u653e\u5728GPU\u4e0a\uff0c\u6211\u4eec\u7684\u5206\u5e03\u5f0fSGD\u793a\u4f8b\u5c06\u4e0d\u8d77\u4f5c\u7528\u3002 \u8ba9\u6211\u4eec\u901a\u8fc7\u9996\u5148\u66ff\u6362init_processes\u4e2d\u7684backend ='gloo'\u6765\u4fee\u590d\u5b83(rank\uff0csize\uff0cfn\uff0cbackend ='tcp'\uff09\u3002 \u6b64\u65f6\uff0c\u811a\u672c\u4ecd\u5c06\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u4f46\u5728\u5e55\u540e\u4f7f\u7528Gloo\u540e\u7aef\u3002 \u4e3a\u4e86\u4f7f\u7528\u591a\u4e2aGPU\uff0c\u6211\u4eec\u8fd8\u8981\u8fdb\u884c\u4ee5\u4e0b\u4fee\u6539\uff1a</p> <ol> <li><code>init_processes(rank, size, fn, backend='tcp')</code> \\(\\(\\rightarrow\\)\\) <code>init_processes(rank, size, fn, backend='gloo')</code></li> <li>Use <code>device = torch.device(\"cuda:{}\".format(rank))</code></li> <li><code>model = Net()</code> \\(\\(\\rightarrow\\)\\) <code>model = Net().to(device)</code></li> <li>Use <code>data, target = data.to(device), target.to(device)</code></li> </ol> <p>\u901a\u8fc7\u4e0a\u8ff0\u4fee\u6539\uff0c\u6211\u4eec\u7684\u6a21\u578b\u73b0\u5728\u5728\u4e24\u4e2aGPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u8fd0\u884cnvidia-smi\u76d1\u63a7\u5b83\u4eec\u7684\u4f7f\u7528\u60c5\u51b5\u3002</p> <p>MPI\u540e\u7aef</p> <p>\u6d88\u606f\u4f20\u9012\u63a5\u53e3(MPI\uff09\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u7684\u6807\u51c6\u5316\u5de5\u5177\u3002 \u5b83\u5141\u8bb8\u8fdb\u884c\u70b9\u5bf9\u70b9\u548c\u96c6\u4f53\u901a\u4fe1\uff0c\u5e76\u4e14\u662ftorch.distributed\u7684API\u7684\u4e3b\u8981\u7075\u611f\u3002 \u5b58\u5728MPI\u7684\u82e5\u5e72\u5b9e\u73b0(\u4f8b\u5982\uff0cOpen-MPI\uff0cMVAPICH2\uff0cIntel MPI\uff09\uff0c\u6bcf\u4e2a\u5b9e\u73b0\u9488\u5bf9\u4e0d\u540c\u76ee\u7684\u800c\u4f18\u5316\u3002 \u4f7f\u7528MPI\u540e\u7aef\u7684\u4f18\u52bf\u5728\u4e8eMPI\u5728\u5927\u578b\u8ba1\u7b97\u673a\u96c6\u7fa4\u4e0a\u7684\u5e7f\u6cdb\u53ef\u7528\u6027\u548c\u9ad8\u7ea7\u4f18\u5316\u3002 \u6700\u8fd1\u7684\u4e00\u4e9b\u5b9e\u73b0\u4e5f\u80fd\u591f\u5229\u7528CUDA IPC\u548cGPU Direct\u6280\u672f\uff0c\u4ee5\u907f\u514d\u901a\u8fc7CPU\u8fdb\u884c\u5185\u5b58\u590d\u5236\u3002</p> <p>\u4e0d\u5e78\u7684\u662f\uff0cPyTorch\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0d\u80fd\u5305\u542bMPI\u5b9e\u73b0\uff0c\u6211\u4eec\u5fc5\u987b\u624b\u52a8\u91cd\u65b0\u7f16\u8bd1\u5b83\u3002 \u5e78\u8fd0\u7684\u662f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u975e\u5e38\u7b80\u5355\uff0c\u56e0\u4e3a\u5728\u7f16\u8bd1\u65f6\uff0cPyTorch\u4f1a\u81ea\u884c\u67e5\u770b\u53ef\u7528\u7684MPI\u5b9e\u73b0\u3002 \u4ee5\u4e0b\u6b65\u9aa4\u901a\u8fc7\u4ece\u6e90\u5b89\u88c5PyTorch\u6765\u5b89\u88c5MPI\u540e\u7aef\u3002</p> <ol> <li>\u521b\u5efa\u5e76\u6fc0\u6d3b\u60a8\u7684Anaconda\u73af\u5883\uff0c\u6309\u7167\u6307\u5357\u5b89\u88c5\u6240\u6709\u5148\u51b3\u6761\u4ef6\uff0c\u4f46\u4e0d\u8981\u8fd0\u884cpython setup.py install\u3002</li> <li>\u9009\u62e9\u5e76\u5b89\u88c5\u60a8\u6700\u559c\u6b22\u7684MPI\u5b9e\u73b0\u3002 \u8bf7\u6ce8\u610f\uff0c\u542f\u7528\u652f\u6301CUDA\u7684MPI\u53ef\u80fd\u9700\u8981\u4e00\u4e9b\u989d\u5916\u7684\u6b65\u9aa4\u3002 \u5728\u6211\u4eec\u7684\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u5c06\u575a\u6301\u4e0d\u652f\u6301GPU\u7684Open-MPI\uff1aconda install -c conda-forge openmpi</li> <li>\u73b0\u5728\uff0c\u8f6c\u5230\u514b\u9686\u7684PyTorch repo\u5e76\u6267\u884cpython setup.py install\u3002</li> </ol> <p>\u4e3a\u4e86\u6d4b\u8bd5\u6211\u4eec\u65b0\u5b89\u88c5\u7684\u540e\u7aef\uff0c\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u4fee\u6539\u3002</p> <ol> <li>\u4f7f\u7528init_processes(0,0\uff0crun\uff0cbackend ='mpi'\uff09\u66ff\u6362if name ==' main'\u4e0b\u7684\u5185\u5bb9\uff1a</li> <li>\u8fd0\u884cmpirun -n 4 python myscript.py\u3002</li> </ol> <p>\u8fd9\u4e9b\u66f4\u6539\u7684\u539f\u56e0\u662fMPI\u9700\u8981\u5728\u751f\u6210\u6d41\u7a0b\u4e4b\u524d\u521b\u5efa\u81ea\u5df1\u7684\u73af\u5883\u3002 MPI\u8fd8\u5c06\u751f\u6210\u81ea\u5df1\u7684\u8fdb\u7a0b\u5e76\u6267\u884c\u521d\u59cb\u5316\u65b9\u6cd5\u4e2d\u63cf\u8ff0\u7684\u63e1\u624b\uff0c\u4f7f\u5f97init_process_group\u7684rankand size\u53c2\u6570\u53d8\u5f97\u591a\u4f59\u3002 \u8fd9\u5b9e\u9645\u4e0a\u975e\u5e38\u5f3a\u5927\uff0c\u56e0\u4e3a\u60a8\u53ef\u4ee5\u5c06\u5176\u4ed6\u53c2\u6570\u4f20\u9012\u7ed9mpirun\uff0c\u4ee5\u4fbf\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u5b9a\u5236\u8ba1\u7b97\u8d44\u6e90\u3002 (\u4f8b\u5982\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u5185\u6838\u6570\u91cf\uff0c\u5c06\u673a\u5668\u5206\u914d\u7ed9\u7279\u5b9a\u7684\u7b49\u7ea7\uff0c\u4ee5\u53ca\u66f4\u591a\u5185\u5bb9\uff09\u8fd9\u6837\u505a\uff0c\u60a8\u5e94\u8be5\u83b7\u5f97\u4e0e\u5176\u4ed6\u901a\u4fe1\u540e\u7aef\u76f8\u540c\u7684\u719f\u6089\u8f93\u51fa\u3002</p>"},{"location":"1.0/dist_tuto/#_7","title":"\u521d\u59cb\u5316\u65b9\u6cd5","text":"<p>\u4e3a\u4e86\u5b8c\u6210\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u6765\u8c08\u8c08\u6211\u4eec\u8c03\u7528\u7684\u7b2c\u4e00\u4e2a\u51fd\u6570\uff1adist.init_process_group(backend\uff0cinit_method\uff09\u3002 \u7279\u522b\u662f\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u4e0d\u540c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8d1f\u8d23\u6bcf\u4e2a\u8fdb\u7a0b\u4e4b\u95f4\u7684\u521d\u59cb\u534f\u8c03\u6b65\u9aa4\u3002 \u8fd9\u4e9b\u65b9\u6cd5\u5141\u8bb8\u60a8\u5b9a\u4e49\u5982\u4f55\u5b8c\u6210\u6b64\u534f\u8c03\u3002 \u6839\u636e\u60a8\u7684\u786c\u4ef6\u8bbe\u7f6e\uff0c\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5e94\u8be5\u6bd4\u5176\u4ed6\u65b9\u6cd5\u66f4\u5408\u9002\u3002 \u9664\u4e86\u4ee5\u4e0b\u90e8\u5206\uff0c\u60a8\u8fd8\u5e94\u8be5\u67e5\u770b\u5b98\u65b9\u6587\u6863\u3002</p> <p>\u5728\u6df1\u5165\u7814\u7a76\u521d\u59cb\u5316\u65b9\u6cd5\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u4eceC / C ++\u7684\u89d2\u5ea6\u5feb\u901f\u4e86\u89e3init_process_group\u80cc\u540e\u7684\u60c5\u51b5\u3002</p> <ol> <li> <p>\u9996\u5148\uff0c\u89e3\u6790\u548c\u9a8c\u8bc1\u53c2\u6570\u3002</p> </li> <li> <p>\u540e\u7aef\u901a\u8fc7name2channel.at(\uff09\u51fd\u6570\u89e3\u6790\u3002 \u8fd4\u56deChannel\u7c7b\uff0c\u5c06\u7528\u4e8e\u6267\u884c\u6570\u636e\u4f20\u8f93\u3002</p> </li> <li> <p>GIL\u88ab\u5220\u9664\uff0c\u5e76\u8c03\u7528THDProcessGroupInit(\uff09\u3002 \u8fd9\u4f1a\u5b9e\u4f8b\u5316\u901a\u9053\u5e76\u6dfb\u52a0\u4e3b\u8282\u70b9\u7684\u5730\u5740\u3002</p> </li> <li> <p>\u7b49\u7ea70\u7684\u8fc7\u7a0b\u5c06\u6267\u884c\u4e3b\u8fc7\u7a0b\uff0c\u800c\u6240\u6709\u5176\u4ed6\u7b49\u7ea7\u5c06\u662f\u5de5\u4f5c\u8fdb\u7a0b\u3002</p> </li> <li> <p>\u4e3b\u8fdb\u7a0b</p> </li> </ol> <p>(1\uff09\u4e3a\u6240\u6709\u5de5\u4f5c\u8fdb\u7a0b\u521b\u5efa\u5957\u63a5\u5b57\u3002    (2\uff09\u7b49\u5f85\u6240\u6709\u5de5\u4f5c\u8fdb\u7a0b\u8fde\u63a5\u3002    (3\uff09\u5411\u4ed6\u4eec\u53d1\u9001\u6709\u5173\u5176\u4ed6\u8fdb\u7a0b\u4f4d\u7f6e\u7684\u4fe1\u606f\u3002</p> <ol> <li>\u6bcf\u4e2a\u5de5\u4f5c\u8fdb\u7a0b</li> </ol> <p>(1\uff09\u4e3a\u4e3b\u8fdb\u7a0b\u521b\u5efa\u4e00\u4e2a\u5957\u63a5\u5b57\u3002     (2\uff09\u53d1\u9001\u81ea\u5df1\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002     (3\uff09\u63a5\u6536\u6709\u5173\u5176\u4ed6\u5de5\u4f5c\u8fdb\u7a0b\u7684\u4fe1\u606f\u3002     (4\uff09\u6253\u5f00\u5957\u63a5\u5b57\u5e76\u4e0e\u6240\u6709\u5176\u4ed6\u5de5\u4f5c\u8fdb\u7a0b\u63e1\u624b\u3002 7. \u521d\u59cb\u5316\u5b8c\u6210\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u76f8\u4e92\u5efa\u7acb\u8fde\u63a5\u3002</p> <p>\u73af\u5883\u53d8\u91cf</p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u4e00\u76f4\u5728\u4f7f\u7528\u73af\u5883\u53d8\u91cf\u521d\u59cb\u5316\u65b9\u6cd5\u3002 \u901a\u8fc7\u5728\u6240\u6709\u8ba1\u7b97\u673a\u4e0a\u8bbe\u7f6e\u4ee5\u4e0b\u56db\u4e2a\u73af\u5883\u53d8\u91cf\uff0c\u6240\u6709\u8fdb\u7a0b\u90fd\u80fd\u591f\u6b63\u786e\u8fde\u63a5\u5230\u4e3b\u8fdb\u7a0b\uff0c\u83b7\u53d6\u6709\u5173\u5176\u4ed6\u8fdb\u7a0b\u7684\u4fe1\u606f\uff0c\u6700\u540e\u4e0e\u5b83\u4eec\u63e1\u624b\u3002</p> <ul> <li>MASTER_PORT\uff1a\u8ba1\u7b97\u673a\u4e0a\u7684\u4e00\u4e2a\u7a7a\u95f2\u7aef\u53e3\uff0c\u7528\u4e8e\u627f\u8f7d\u6392\u540d\u4e3a0\u7684\u8fdb\u7a0b\u3002</li> <li>MASTER_ADDR\uff1a\u5c06\u4ee50\u7ea7\u6258\u7ba1\u8fdb\u7a0b\u7684\u8ba1\u7b97\u673a\u7684IP\u5730\u5740\u3002</li> <li>WORLD_SIZE\uff1a\u8fdb\u7a0b\u603b\u6570\uff0c\u4ee5\u4fbfmaster\u77e5\u9053\u8981\u7b49\u5f85\u591a\u5c11worker\u3002</li> <li>RANK\uff1a\u6bcf\u4e2a\u6d41\u7a0b\u7684\u7b49\u7ea7\uff0c\u56e0\u6b64\u4ed6\u4eec\u5c06\u77e5\u9053\u5b83\u662f\u5426\u662fworker\u7684master\u3002</li> </ul> <p>\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf</p> <p>\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u8981\u6c42\u6240\u6709\u8fdb\u7a0b\u90fd\u53ef\u4ee5\u8bbf\u95ee\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u6587\u4ef6\u534f\u8c03\u5b83\u4eec\u3002 \u8fd9\u610f\u5473\u7740\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5c06\u6253\u5f00\u6587\u4ef6\uff0c\u5199\u5165\u5176\u4fe1\u606f\uff0c\u5e76\u7b49\u5230\u6bcf\u4e2a\u4eba\u90fd\u8fd9\u6837\u505a\u3002 \u5728\u6240\u6709\u5fc5\u9700\u4fe1\u606f\u5c06\u968f\u65f6\u53ef\u7528\u4e8e\u6240\u6709\u6d41\u7a0b\u4e4b\u540e\u3002 \u4e3a\u4e86\u907f\u514d\u7ade\u4e89\u6761\u4ef6\uff0c\u6587\u4ef6\u7cfb\u7edf\u5fc5\u987b\u652f\u6301\u901a\u8fc7fcntl\u9501\u5b9a\u3002 \u8bf7\u6ce8\u610f\uff0c\u60a8\u53ef\u4ee5\u624b\u52a8\u6307\u5b9a\u6392\u540d\uff0c\u4e5f\u53ef\u4ee5\u8ba9\u6d41\u7a0b\u81ea\u884c\u8ba1\u7b97\u3002 \u8981\u4e3a\u6bcf\u4e2a\u4f5c\u4e1a\u5b9a\u4e49\u4e00\u4e2a\u552f\u4e00\u7684\u7ec4\u540d\uff0c\u60a8\u53ef\u4ee5\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u6587\u4ef6\u8def\u5f84\u5e76\u5b89\u5168\u5730\u907f\u514d\u51b2\u7a81\u3002</p> <pre><code>dist.init_process_group(init_method='file:///mnt/nfs/sharedfile', world_size=4,\n                        group_name='mygroup')\n</code></pre> <p>TCP\u521d\u59cb\u5316\u548c\u591a\u64ad</p> <p>\u901a\u8fc7TCP\u521d\u59cb\u5316\u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f\u5b9e\u73b0\uff1a</p> <ul> <li>\u901a\u8fc7\u63d0\u4f9b\u5177\u6709\u7b49\u7ea70\u548c\u4e16\u754c\u5927\u5c0f\u7684\u8fdb\u7a0b\u7684IP\u5730\u5740\u3002</li> <li>\u901a\u8fc7\u63d0\u4f9b\u4efb\u4f55\u6709\u6548\u7684IP\u591a\u64ad\u5730\u5740\u548c\u4e16\u754c\u5927\u5c0f\u3002</li> </ul> <p>\u5728\u7b2c\u4e00\u79cd\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u5de5\u4f5c\u8fdb\u7a0b\u5c06\u80fd\u591f\u8fde\u63a5\u5230\u7b49\u7ea7\u4e3a0\u7684\u8fdb\u7a0b\u5e76\u6309\u7167\u4e0a\u8ff0\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c\u3002</p> <pre><code>dist.init_process_group(init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)\n</code></pre> <p>\u5728\u7b2c\u4e8c\u79cd\u60c5\u51b5\u4e0b\uff0c\u591a\u64ad\u5730\u5740\u6307\u5b9a\u53ef\u80fd\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u7684\u8282\u70b9\u7ec4\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u5141\u8bb8\u6bcf\u4e2a\u8fdb\u7a0b\u5728\u6267\u884c\u4e0a\u8ff0\u8fc7\u7a0b\u4e4b\u524d\u8fdb\u884c\u521d\u59cb\u63e1\u624b\u6765\u5904\u7406\u534f\u8c03\u3002 \u6b64\u5916\uff0cTCP\u591a\u64ad\u521d\u59cb\u5316\u8fd8\u652f\u6301group_name\u53c2\u6570(\u4e0e\u5171\u4eab\u6587\u4ef6\u65b9\u6cd5\u4e00\u6837\uff09\uff0c\u5141\u8bb8\u5728\u540c\u4e00\u7fa4\u96c6\u4e0a\u8c03\u5ea6\u591a\u4e2a\u4f5c\u4e1a\u3002</p> <pre><code>dist.init_process_group(init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',\n                        world_size=4)\n</code></pre>"},{"location":"1.0/dist_tuto/#_8","title":"\u81f4\u8c22","text":"<p>\u6211\u8981\u611f\u8c22PyTorch\u5f00\u53d1\u4eba\u5458\u5728\u4ed6\u4eec\u7684\u5b9e\u73b0\uff0c\u6587\u6863\u548c\u6d4b\u8bd5\u65b9\u9762\u505a\u5f97\u5f88\u597d\u3002 \u5f53\u4ee3\u7801\u4e0d\u6e05\u695a\u65f6\uff0c\u6211\u603b\u662f\u53ef\u4ee5\u4f9d\u9760\u6587\u6863\u6216\u6d4b\u8bd5\u6765\u627e\u5230\u7b54\u6848\u3002 \u7279\u522b\u662f\uff0c\u6211\u8981\u611f\u8c22Soumith Chintala\uff0cAdam Paszke\u548cNatalia Gimelshein\u63d0\u4f9b\u6709\u89c1\u5730\u7684\u8bc4\u8bba\u5e76\u56de\u7b54\u6709\u5173\u65e9\u671f\u8349\u7a3f\u7684\u95ee\u9898\u3002</p>"},{"location":"1.0/distributed/","title":"\u5206\u5e03\u5f0f\u901a\u4fe1\u5305 -  torch.distributed","text":"<p>\u8bd1\u8005\uff1auniveryinli</p>"},{"location":"1.0/distributed/#_1","title":"\u540e\u7aef","text":"<p><code>torch.distributed</code> \u652f\u6301\u4e09\u4e2a\u540e\u7aef\uff0c\u6bcf\u4e2a\u540e\u7aef\u5177\u6709\u4e0d\u540c\u7684\u529f\u80fd\u3002\u4e0b\u8868\u663e\u793a\u54ea\u4e9b\u529f\u80fd\u53ef\u7528\u4e8eCPU/CUDA\u5f20\u91cf\u3002\u4ec5\u5f53\u7528\u4e8e\u6784\u5efaPyTorch\u7684\u5b9e\u73b0\u652f\u6301\u65f6\uff0cMPI\u624d\u652f\u6301CUDA\u3002</p> \u540e\u7aef <code>gloo</code> <code>mpi</code> <code>nccl</code> \u8bbe\u5907 CPU GPU CPU --- --- --- --- \u53d1\u9001 \u2713 \u2718 \u2713 \u63a5\u6536 \u2713 \u2718 \u2713 \u5e7f\u64ad \u2713 \u2713 \u2713 all_reduce \u2713 \u2713 \u2713 reduce \u2713 \u2718 \u2713 all_gather \u2713 \u2718 \u2713 \u6536\u96c6 \u2713 \u2718 \u2713 \u5206\u6563 \u2713 \u2718 \u2713 \u5c4f\u969c \u2713 \u2718 \u2713"},{"location":"1.0/distributed/#pytorch","title":"PyTorch\u9644\u5e26\u7684\u540e\u7aef","text":"<p>\u76ee\u524dPyTorch\u5206\u53d1\u7248\u4ec5\u652f\u6301Linux\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cGloo\u548cNCCL\u540e\u7aef\u6784\u5efa\u5e76\u5305\u542b\u5728PyTorch\u7684\u5206\u5e03\u4e4b\u4e2d(\u4ec5\u5728\u4f7f\u7528CUDA\u6784\u5efa\u65f6\u4e3aNCCL\uff09\u3002MPI\u662f\u4e00\u4e2a\u53ef\u9009\u7684\u540e\u7aef\uff0c\u53ea\u6709\u4ece\u6e90\u4ee3\u7801\u6784\u5efaPyTorch\u65f6\u624d\u80fd\u5305\u542b\u5b83\u3002(\u4f8b\u5982\uff0c\u5728\u5b89\u88c5\u4e86MPI\u7684\u4e3b\u673a\u4e0a\u6784\u5efaPyTorch\uff09</p>"},{"location":"1.0/distributed/#_2","title":"\u54ea\u4e2a\u540e\u7aef\u4f7f\u7528\uff1f","text":"<p>\u5728\u8fc7\u53bb\uff0c\u6211\u4eec\u7ecf\u5e38\u88ab\u95ee\u5230\uff1a\u201c\u6211\u5e94\u8be5\u4f7f\u7528\u54ea\u4e2a\u540e\u7aef\uff1f\u201d\u3002</p> <ul> <li>\u7ecf\u9a8c\u6cd5\u5219<ul> <li>\u4f7f\u7528NCCL\u540e\u7aef\u8fdb\u884c\u5206\u5e03\u5f0f GPU \u8bad\u7ec3\u3002</li> <li>\u4f7f\u7528Gloo\u540e\u7aef\u8fdb\u884c\u5206\u5e03\u5f0f CPU \u8bad\u7ec3\u3002</li> </ul> </li> <li>\u5177\u6709InfiniBand\u4e92\u8fde\u7684GPU\u4e3b\u673a<ul> <li>\u4f7f\u7528NCCL\uff0c\u56e0\u4e3a\u5b83\u662f\u76ee\u524d\u552f\u4e00\u652f\u6301InfiniBand\u548cGPUDirect\u7684\u540e\u7aef\u3002</li> </ul> </li> <li>GPU\u4e3b\u673a\u4e0e\u4ee5\u592a\u7f51\u4e92\u8fde<ul> <li>\u4f7f\u7528NCCL\uff0c\u56e0\u4e3a\u5b83\u76ee\u524d\u63d0\u4f9b\u6700\u4f73\u7684\u5206\u5e03\u5f0fGPU\u8bad\u7ec3\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u591a\u8fdb\u7a0b\u5355\u8282\u70b9\u6216\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u5982\u679c\u60a8\u9047\u5230NCCL\u7684\u4efb\u4f55\u95ee\u9898\uff0c\u8bf7\u4f7f\u7528Gloo\u4f5c\u4e3a\u540e\u5907\u9009\u9879\u3002(\u8bf7\u6ce8\u610f\uff0cGloo\u76ee\u524d\u8fd0\u884c\u901f\u5ea6\u6bd4GPU\u7684NCCL\u6162\u3002\uff09</li> </ul> </li> <li>\u5177\u6709InfiniBand\u4e92\u8fde\u7684CPU\u4e3b\u673a<ul> <li>\u5982\u679c\u60a8\u7684InfiniBand\u5728IB\u4e0a\u5df2\u542f\u7528IP\uff0c\u8bf7\u4f7f\u7528Gloo\uff0c\u5426\u5219\u8bf7\u4f7f\u7528MPI\u3002\u6211\u4eec\u8ba1\u5212\u5728\u5373\u5c06\u53d1\u5e03\u7684\u7248\u672c\u4e2d\u4e3aGloo\u6dfb\u52a0InfiniBand\u652f\u6301\u3002</li> </ul> </li> <li>\u5177\u6709\u4ee5\u592a\u7f51\u4e92\u8fde\u7684CPU\u4e3b\u673a<ul> <li>\u9664\u975e\u60a8\u6709\u7279\u6b8a\u539f\u56e0\u8981\u4f7f\u7528MPI\uff0c\u5426\u5219\u8bf7\u4f7f\u7528Gloo\u3002</li> </ul> </li> </ul>"},{"location":"1.0/distributed/#_3","title":"\u5e38\u89c1\u7684\u73af\u5883\u53d8\u91cf","text":""},{"location":"1.0/distributed/#_4","title":"\u9009\u62e9\u8981\u4f7f\u7528\u7684\u7f51\u7edc\u63a5\u53e3","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cNCCL\u548cGloo\u540e\u7aef\u90fd\u4f1a\u5c1d\u8bd5\u67e5\u627e\u7528\u4e8e\u901a\u4fe1\u7684\u7f51\u7edc\u63a5\u53e3\u3002\u4f46\u662f\uff0c\u4ece\u6211\u4eec\u7684\u7ecf\u9a8c\u6765\u770b\uff0c\u5e76\u4e0d\u603b\u80fd\u4fdd\u8bc1\u8fd9\u4e00\u70b9\u3002\u56e0\u6b64\uff0c\u5982\u679c\u60a8\u5728\u540e\u7aef\u9047\u5230\u4efb\u4f55\u95ee\u9898\u800c\u65e0\u6cd5\u627e\u5230\u6b63\u786e\u7684\u7f51\u7edc\u63a5\u53e3\u3002\u60a8\u53ef\u4ee5\u5c1d\u8bd5\u8bbe\u7f6e\u4ee5\u4e0b\u73af\u5883\u53d8\u91cf(\u6bcf\u4e2a\u53d8\u91cf\u9002\u7528\u4e8e\u5176\u5404\u81ea\u7684\u540e\u7aef\uff09\uff1a</p> <ul> <li>NCCL_SOCKET_IFNAME, \u6bd4\u5982 <code>export NCCL_SOCKET_IFNAME=eth0</code></li> <li>GLOO_SOCKET_IFNAME, \u6bd4\u5982 <code>export GLOO_SOCKET_IFNAME=eth0</code></li> </ul>"},{"location":"1.0/distributed/#nccl","title":"\u5176\u4ed6NCCL\u73af\u5883\u53d8\u91cf","text":"<p>NCCL\u8fd8\u63d0\u4f9b\u4e86\u8bb8\u591a\u7528\u4e8e\u5fae\u8c03\u76ee\u7684\u7684\u73af\u5883\u53d8\u91cf</p> <p>\u5e38\u7528\u7684\u5305\u62ec\u4ee5\u4e0b\u7528\u4e8e\u8c03\u8bd5\u76ee\u7684\uff1a</p> <ul> <li><code>export NCCL_DEBUG=INFO</code></li> <li><code>export NCCL_DEBUG_SUBSYS=ALL</code></li> </ul> <p>\u6709\u5173NCCL\u73af\u5883\u53d8\u91cf\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u9605NVIDIA NCCL\u7684\u5b98\u65b9\u6587\u6863</p>"},{"location":"1.0/distributed/#_5","title":"\u57fa\u672c","text":"<p><code>torch.distributed</code>\u5305\u4e3a\u5728\u4e00\u53f0\u6216\u591a\u53f0\u673a\u5668\u4e0a\u8fd0\u884c\u7684\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e0a\u7684\u591a\u8fdb\u7a0b\u5e76\u884c\u6027\u63d0\u4f9bPyTorch\u652f\u6301\u548c\u901a\u4fe1\u539f\u8bed\u3002\u7c7b <code>torch.nn.parallel.DistributedDataParallel()</code>\u57fa\u4e8e\u6b64\u529f\u80fd\u6784\u5efa\uff0c\u4ee5\u63d0\u4f9b\u540c\u6b65\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f5c\u4e3a\u5305\u88c5\u5668\u4efb\u4f55PyTorch\u6a21\u578b\u3002\u8fd9\u4e0e Multiprocessing package - torch.multiprocessing \u548c <code>torch.nn.DataParallel()</code> \u56e0\u4e3a\u5b83\u652f\u6301\u591a\u4e2a\u8054\u7f51\u7684\u673a\u5668\uff0c\u5e76\u4e14\u7528\u6237\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u663e\u5f0f\u542f\u52a8\u4e3b\u8bad\u7ec3\u811a\u672c\u7684\u5355\u72ec\u526f\u672c\u3002</p> <p>\u5728\u5355\u673a\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\uff0c<code>torch.distributed</code> \u6216\u8005 <code>torch.nn.parallel.DistributedDataParallel()</code> \u4e0e\u5176\u4ed6\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5305\u88c5\u5668\u4ecd\u7136\u5177\u6709\u4f18\u52bf\uff0c\u5305\u542b <code>torch.nn.DataParallel()</code>:</p> <ul> <li>\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u7ef4\u62a4\u81ea\u5df1\u7684\u4f18\u5316\u5668\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u6267\u884c\u5b8c\u6574\u7684\u4f18\u5316\u6b65\u9aa4\u3002\u867d\u7136\u8fd9\u53ef\u80fd\u770b\u8d77\u6765\u662f\u591a\u4f59\u7684\uff0c\u4f46\u7531\u4e8e\u68af\u5ea6\u5df2\u7ecf\u805a\u96c6\u5728\u4e00\u8d77\u5e76\u4e14\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u5e73\u5747\uff0c\u56e0\u6b64\u5bf9\u4e8e\u6bcf\u4e2a\u8fc7\u7a0b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9700\u8981\u53c2\u6570\u5e7f\u64ad\u6b65\u9aa4\uff0c\u51cf\u5c11\u4e86\u5728\u8282\u70b9\u4e4b\u95f4\u4f20\u8f93\u5f20\u91cf\u6240\u82b1\u8d39\u7684\u65f6\u95f4\u3002</li> <li>\u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5305\u542b\u4e00\u4e2a\u72ec\u7acb\u7684Python\u89e3\u91ca\u5668\uff0c\u6d88\u9664\u4e86\u989d\u5916\u7684\u89e3\u91ca\u5668\u5f00\u9500\u548c\u6765\u81ea\u5355\u4e2aPython\u8fdb\u7a0b\u9a71\u52a8\u591a\u4e2a\u6267\u884c\u7ebf\u7a0b\uff0c\u6a21\u578b\u526f\u672c\u6216GPU\u7684\u201cGIL-thrashing\u201d\u3002\u8fd9\u5bf9\u4e8e\u5927\u91cf\u4f7f\u7528Python\u8fd0\u884c\u65f6\u7684\u6a21\u578b\u5c24\u5176\u91cd\u8981\uff0c\u5305\u62ec\u5177\u6709\u5faa\u73af\u5c42\u6216\u8bb8\u591a\u5c0f\u7ec4\u4ef6\u7684\u6a21\u578b\u3002</li> </ul>"},{"location":"1.0/distributed/#_6","title":"\u521d\u59cb\u5316","text":"<p>\u8fd9\u4e2a\u5305\u5728\u8c03\u7528\u5176\u4ed6\u7684\u65b9\u6cd5\u4e4b\u524d\uff0c\u9700\u8981\u4f7f\u7528 <code>torch.distributed.init_process_group()</code> \u51fd\u6570\u8fdb\u884c\u521d\u59cb\u5316\u3002\u8fd9\u5c06\u963b\u6b62\u6240\u6709\u8fdb\u7a0b\u52a0\u5165\u3002</p> <pre><code>torch.distributed.init_process_group(backend, init_method='env://', timeout=datetime.timedelta(seconds=1800), **kwargs)\n</code></pre> <p>\u521d\u59cb\u5316\u9ed8\u8ba4\u7684\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7ec4\uff0c\u8fd9\u4e5f\u5c06\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u7a0b\u5e8f\u5305</p> <p>\u53c2\u6570: </p> <ul> <li>backend (str or Backend) \u2013 \u540e\u7aef\u4f7f\u7528\u3002\u6839\u636e\u6784\u5efa\u65f6\u914d\u7f6e\uff0c\u6709\u6548\u503c\u5305\u62ec <code>mpi</code>\uff0c<code>gloo</code>\u548c<code>nccl</code>\u3002\u8be5\u5b57\u6bb5\u5e94\u8be5\u4ee5\u5c0f\u5199\u5b57\u7b26\u4e32\u5f62\u5f0f\u7ed9\u51fa(\u4f8b\u5982<code>\"gloo\"</code>)\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7<code>Backend</code>\u8bbf\u95ee\u5c5e\u6027(\u4f8b\u5982<code>Backend.GLOO</code>)\u3002</li> <li>init_method (str, optional) \u2013 \u6307\u5b9a\u5982\u4f55\u521d\u59cb\u5316\u8fdb\u7a0b\u7ec4\u7684URL\u3002</li> <li>world_size (int, optional) \u2013 \u53c2\u4e0e\u4f5c\u4e1a\u7684\u8fdb\u7a0b\u6570\u3002</li> <li>rank (int, optional) \u2013 \u5f53\u524d\u6d41\u7a0b\u7684\u6392\u540d\u3002</li> <li>timeout (timedelta__, optional) \u2013 \u9488\u5bf9\u8fdb\u7a0b\u7ec4\u6267\u884c\u7684\u64cd\u4f5c\u8d85\u65f6\uff0c\u9ed8\u8ba4\u503c\u7b49\u4e8e30\u5206\u949f\uff0c\u8fd9\u4ec5\u9002\u7528\u4e8e<code>gloo</code>\u540e\u7aef\u3002</li> <li>group_name (str, optional__, deprecated) \u2013 \u56e2\u961f\u540d\u5b57\u3002</li> </ul> <p>\u8981\u542f\u7528<code>backend == Backend.MPI</code>\uff0cPyTorch\u9700\u8981\u5728\u652f\u6301MPI\u7684\u7cfb\u7edf\u4e0a\u4ece\u6e90\u6784\u5efa\uff0c\u8fd9\u540c\u6837\u9002\u7528\u4e8eNCCL\u3002</p> <pre><code>class torch.distributed.Backend\n</code></pre> <p>\u7c7b\u4f3c\u679a\u4e3e\u7684\u53ef\u7528\u540e\u7aef\u7c7b\uff1aGLOO\uff0cNCCL\u548cMPI\u3002</p> <p>\u8fd9\u4e2a\u7c7b\u7684\u503c\u662f\u5c0f\u5199\u5b57\u7b26\u4e32\uff0c\u4f8b\u5982\u201cgloo\u201d\u3002\u5b83\u4eec\u53ef\u4ee5\u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee\uff0c\u4f8b\u5982<code>Backend.NCCL</code>\u3002</p> <p>\u53ef\u4ee5\u76f4\u63a5\u8c03\u7528\u6b64\u7c7b\u6765\u89e3\u6790\u5b57\u7b26\u4e32\uff0c\u4f8b\u5982\uff0c<code>Backend(backend_str\uff09</code>\u5c06\u68c0\u67e5<code>backend_str</code>\u662f\u5426\u6709\u6548\uff0c\u5982\u679c\u662f\uff0c\u5219\u8fd4\u56de\u89e3\u6790\u7684\u5c0f\u5199\u5b57\u7b26\u4e32\u3002\u5b83\u4e5f\u63a5\u53d7\u5927\u5199\u5b57\u7b26\u4e32\uff0c\u4f8b\u5982<code>`Backend(\u201cGLOO\u201d\uff09</code>return<code>\u201cgloo\u201d</code>\u3002 \u6ce8\u610f</p> <p>\u6761\u76ee<code>Backend.UNDEFINED</code>\u5b58\u5728\u4f46\u4ec5\u7528\u4f5c\u67d0\u4e9b\u5b57\u6bb5\u7684\u521d\u59cb\u503c\u3002\u7528\u6237\u65e2\u4e0d\u5e94\u76f4\u63a5\u4f7f\u7528\u4e5f\u4e0d\u5e94\u5047\u8bbe\u5b58\u5728\u3002</p> <pre><code>torch.distributed.get_backend(group=&lt;object object&gt;)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u8fdb\u7a0b\u7ec4\u7684\u540e\u7aef</p> \u53c2\u6570: group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002\u9ed8\u8ba4\u503c\u662f\u5e38\u89c4\u4e3b\u8fdb\u7a0b\u7ec4\u3002\u5982\u679c\u6307\u5b9a\u4e86\u53e6\u4e00\u4e2a\u7279\u5b9a\u7ec4\uff0c\u5219\u8c03\u7528\u8fdb\u7a0b\u5fc5\u987b\u662f<code>group</code>\u7684\u4e00\u90e8\u5206\u3002 \u8fd4\u56de: \u7ed9\u5b9a\u8fdb\u7a0b\u7ec4\u7684\u540e\u7aef\u4f5c\u4e3a\u5c0f\u5199\u5b57\u7b26\u4e32 --- --- <pre><code>torch.distributed.get_rank(group=&lt;object object&gt;)\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u8fdb\u7a0b\u7ec4\u7684\u6392\u540d</p> <p>Rank\u662f\u5206\u914d\u7ed9\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7ec4\u4e2d\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002\u5b83\u4eec\u603b\u662f\u4ece0\u5230<code>world_size</code>\u7684\u8fde\u7eed\u6574\u6570\u3002</p> \u53c2\u6570: group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4 \u8fd4\u56de: \u8fdb\u7a0b\u7ec4-1\u7684\u7b49\u7ea7\uff0c\u5982\u679c\u4e0d\u662f\u8be5\u7ec4\u7684\u4e00\u90e8\u5206 --- --- <pre><code>torch.distributed.get_world_size(group=&lt;object object&gt;)\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u8fdb\u7a0b\u7ec4\u4e2d\u7684\u8fdb\u7a0b\u6570</p> \u53c2\u6570: group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4 \u8fd4\u56de: \u8fdb\u7a0b\u7ec4-1\u7684\u4e16\u754c\u5927\u5c0f\uff0c\u5982\u679c\u4e0d\u662f\u8be5\u7ec4\u7684\u4e00\u90e8\u5206 --- --- <pre><code>torch.distributed.is_initialized()\n</code></pre> <p>\u68c0\u67e5\u662f\u5426\u5df2\u521d\u59cb\u5316\u9ed8\u8ba4\u8fdb\u7a0b\u7ec4</p> <pre><code>torch.distributed.is_mpi_available()\n</code></pre> <p>\u68c0\u67e5MPI\u662f\u5426\u53ef\u7528</p> <pre><code>torch.distributed.is_nccl_available()\n</code></pre> <p>\u68c0\u67e5NCCL\u662f\u5426\u53ef\u7528</p> <p>\u76ee\u524d\u652f\u6301\u4e09\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\uff1a</p>"},{"location":"1.0/distributed/#tcp","title":"TCP\u521d\u59cb\u5316","text":"<p>\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528TCP\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u9700\u8981\u4ece\u6240\u6709\u8fdb\u7a0b\u53ef\u4ee5\u8bbf\u95ee\u7684\u7f51\u7edc\u5730\u5740\u548c\u6240\u9700\u7684<code>world_size</code>\u3002\u7b2c\u4e00\u79cd\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u5c5e\u4e8erank 0\u8fdb\u7a0b\u7684\u5730\u5740\u3002\u6b64\u521d\u59cb\u5316\u65b9\u6cd5\u8981\u6c42\u6240\u6709\u8fdb\u7a0b\u90fd\u5177\u6709\u624b\u52a8\u6307\u5b9a\u7684\u6392\u540d\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u6700\u65b0\u7684\u5206\u5e03\u5f0f\u8f6f\u4ef6\u5305\u4e2d\u4e0d\u518d\u652f\u6301\u591a\u64ad\u5730\u5740\u3002<code>group_name</code>\u4e5f\u88ab\u5f03\u7528\u4e86\u3002</p> <pre><code>import torch.distributed as dist\n\n# \u4f7f\u7528\u5176\u4e2d\u4e00\u53f0\u673a\u5668\u7684\u5730\u5740\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n\n</code></pre>"},{"location":"1.0/distributed/#_7","title":"\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u521d\u59cb\u5316","text":"<p>\u53e6\u4e00\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u4e00\u4e2a\u6587\u4ef6\u7cfb\u7edf\uff0c\u8be5\u6587\u4ef6\u7cfb\u7edf\u4e0e\u7ec4\u4e2d\u7684\u6240\u6709\u673a\u5668\u5171\u4eab\u548c\u53ef\u89c1\uff0c\u4ee5\u53ca\u6240\u9700\u7684<code>world_size</code>\u3002URL\u5e94\u4ee5<code>file\uff1a//</code>\u5f00\u5934\uff0c\u5e76\u5305\u542b\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4e0d\u5b58\u5728\u7684\u6587\u4ef6(\u5728\u73b0\u6709\u76ee\u5f55\u4e2d\uff09\u7684\u8def\u5f84\u3002\u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u6587\u4ef6\u7cfb\u7edf\u521d\u59cb\u5316\u5c06\u81ea\u52a8\u521b\u5efa\u8be5\u6587\u4ef6\uff0c\u4f46\u4e0d\u4f1a\u5220\u9664\u8be5\u6587\u4ef6\u3002\u56e0\u6b64\uff0c\u4e0b\u4e00\u6b65\u521d\u59cb\u5316 <code>init_process_group()</code> \u5728\u76f8\u540c\u7684\u6587\u4ef6\u8def\u5f84\u53d1\u751f\u4e4b\u524d\u60a8\u6709\u8d23\u4efb\u786e\u4fdd\u6e05\u7406\u6587\u4ef6\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u5728\u6700\u65b0\u7684\u5206\u5e03\u5f0f\u8f6f\u4ef6\u5305\u4e2d\u4e0d\u518d\u652f\u6301\u81ea\u52a8\u6392\u540d\u5206\u914d\uff0c\u5e76\u4e14\u4e5f\u4e0d\u63a8\u8350\u4f7f\u7528<code>group_name</code>\u3002</p> <p>\u8b66\u544a</p> <p>\u6b64\u65b9\u6cd5\u5047\u5b9a\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\u4f7f\u7528<code>fcntl</code>\u8fdb\u884c\u9501\u5b9a - \u5927\u591a\u6570\u672c\u5730\u7cfb\u7edf\u548cNFS\u90fd\u652f\u6301\u5b83\u3002</p> <p>\u8b66\u544a</p> <p>\u6b64\u65b9\u6cd5\u5c06\u59cb\u7ec8\u521b\u5efa\u8be5\u6587\u4ef6\uff0c\u5e76\u5c3d\u529b\u5728\u7a0b\u5e8f\u7ed3\u675f\u65f6\u6e05\u7406\u5e76\u5220\u9664\u8be5\u6587\u4ef6\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6bcf\u6b21\u8fdb\u884c\u521d\u59cb\u5316\u90fd\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u5168\u65b0\u7684\u7a7a\u6587\u4ef6\uff0c\u4ee5\u4fbf\u521d\u59cb\u5316\u6210\u529f\u3002\u5982\u679c\u518d\u6b21\u4f7f\u7528\u5148\u524d\u521d\u59cb\u5316\u4f7f\u7528\u7684\u76f8\u540c\u6587\u4ef6(\u4e0d\u4f1a\u88ab\u6e05\u9664\uff09\uff0c\u5219\u8fd9\u662f\u610f\u5916\u884c\u4e3a\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f1a\u5bfc\u81f4\u6b7b\u9501\u548c\u6545\u969c\u3002\u56e0\u6b64\uff0c\u5373\u4f7f\u6b64\u65b9\u6cd5\u5c06\u5c3d\u529b\u6e05\u7406\u6587\u4ef6\uff0c\u5982\u679c\u81ea\u52a8\u5220\u9664\u4e0d\u6210\u529f\uff0c\u60a8\u6709\u8d23\u4efb\u786e\u4fdd\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u5220\u9664\u8be5\u6587\u4ef6\u4ee5\u9632\u6b62\u540c\u4e00\u6587\u4ef6\u88ab\u5220\u9664 \u4e0b\u6b21\u518d\u6b21\u4f7f\u7528\u3002\u5982\u679c\u4f60\u6253\u7b97\u5728\u76f8\u540c\u7684\u6587\u4ef6\u7cfb\u7edf\u8def\u5f84\u4e0b\u591a\u6b21\u8c03\u7528 <code>init_process_group()</code> \u7684\u65f6\u5019\uff0c\u5c31\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u4e86\u3002\u6362\u4e00\u79cd\u8bf4\u6cd5\uff0c\u5982\u679c\u90a3\u4e2a\u6587\u4ef6\u6ca1\u6709\u88ab\u79fb\u9664\u5e76\u4e14\u4f60\u518d\u6b21\u8c03\u7528 <code>init_process_group()</code>\uff0c\u90a3\u4e48\u5931\u8d25\u662f\u53ef\u60f3\u800c\u77e5\u7684\u3002\u8fd9\u91cc\u7684\u7ecf\u9a8c\u6cd5\u5219\u662f\uff0c\u6bcf\u5f53\u8c03\u7528<code>init_process_group()</code>\u7684\u65f6\u5019\uff0c\u786e\u4fdd\u6587\u4ef6\u4e0d\u5b58\u5728\u6216\u4e3a\u7a7a\u3002</p> <pre><code>import torch.distributed as dist\n\n# \u5e94\u59cb\u7ec8\u6307\u5b9a\u7b49\u7ea7\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n\n</code></pre>"},{"location":"1.0/distributed/#_8","title":"\u73af\u5883\u53d8\u91cf\u521d\u59cb\u5316","text":"<p>\u6b64\u65b9\u6cd5\u5c06\u4ece\u73af\u5883\u53d8\u91cf\u4e2d\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ece\u800c\u53ef\u4ee5\u5b8c\u5168\u81ea\u5b9a\u4e49\u4fe1\u606f\u7684\u83b7\u53d6\u65b9\u5f0f\u3002\u8981\u8bbe\u7f6e\u7684\u53d8\u91cf\u662f\uff1a</p> <ul> <li><code>MASTER_PORT</code> - \u9700\u8981; \u5fc5\u987b\u662f\u673a\u5668\u4e0a\u7684\u81ea\u7531\u7aef\u53e3\uff0c\u7b49\u7ea7\u4e3a0\u3002</li> <li><code>MASTER_ADDR</code> - \u8981\u6c42(0\u7ea7\u9664\u5916\uff09; \u7b49\u7ea70\u8282\u70b9\u7684\u5730\u5740\u3002</li> <li><code>WORLD_SIZE</code> - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u5728\u8c03\u7528init\u51fd\u6570\u65f6\u8bbe\u7f6e\u3002</li> <li><code>RANK</code> - \u9700\u8981; \u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u5728\u8c03\u7528init\u51fd\u6570\u65f6\u8bbe\u7f6e\u3002</li> </ul> <p>\u7b49\u7ea7\u4e3a0\u7684\u673a\u5668\u5c06\u7528\u4e8e\u8bbe\u7f6e\u6240\u6709\u8fde\u63a5\u3002</p> <p>\u8fd9\u662f\u9ed8\u8ba4\u65b9\u6cd5\uff0c\u610f\u5473\u7740\u4e0d\u5fc5\u6307\u5b9a<code>init_method</code>(\u6216\u8005\u53ef\u4ee5\u662f<code>env\uff1a//</code>\uff09\u3002</p>"},{"location":"1.0/distributed/#_9","title":"\u7ec4","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u96c6\u5408\u4f53\u5728\u9ed8\u8ba4\u7ec4(\u4e5f\u79f0\u4e3a\u4e16\u754c\uff09\u4e0a\u8fd0\u884c\uff0c\u5e76\u8981\u6c42\u6240\u6709\u8fdb\u7a0b\u90fd\u8fdb\u5165\u5206\u5e03\u5f0f\u51fd\u6570\u8c03\u7528\u3002\u4f46\u662f\uff0c\u4e00\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u4ee5\u4ece\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u4e2d\u53d7\u76ca\u3002\u8fd9\u662f\u5206\u5e03\u5f0f\u7fa4\u4f53\u53d1\u6325\u4f5c\u7528\u7684\u5730\u65b9\u3002<code>new_group()</code> \u51fd\u6570\u53ef\u7528\u4e8e\u521b\u5efa\u65b0\u7ec4\uff0c\u5177\u6709\u6240\u6709\u8fdb\u7a0b\u7684\u4efb\u610f\u5b50\u96c6\u3002\u5b83\u8fd4\u56de\u4e00\u4e2a\u4e0d\u900f\u660e\u7684\u7ec4\u53e5\u67c4\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u6240\u6709\u96c6\u5408\u4f53\u7684\u201cgroup\u201d\u53c2\u6570\u7ed9\u51fa(\u96c6\u5408\u4f53\u662f\u5206\u5e03\u5f0f\u51fd\u6570\uff0c\u7528\u4e8e\u5728\u67d0\u4e9b\u4f17\u6240\u5468\u77e5\u7684\u7f16\u7a0b\u6a21\u5f0f\u4e2d\u4ea4\u6362\u4fe1\u606f\uff09\u3002</p> <p>\u76ee\u524d<code>torch.distributed</code>\u4e0d\u652f\u6301\u521b\u5efa\u5177\u6709\u4e0d\u540c\u540e\u7aef\u7684\u7ec4\u3002\u6362\u4e00\u79cd\u8bf4\u6cd5\uff0c\u6bcf\u4e00\u4e2a\u6b63\u5728\u88ab\u521b\u5efa\u7684\u7ec4\u90fd\u4f1a\u7528\u76f8\u540c\u7684\u540e\u7aef\uff0c\u53ea\u8981\u4f60\u5728 <code>init_process_group()</code> \u91cc\u9762\u58f0\u660e\u6e05\u695a\u3002</p> <pre><code>torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800))\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5f0f\u7ec4</p> <p>\u6b64\u529f\u80fd\u8981\u6c42\u4e3b\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b(\u5373\u5c5e\u4e8e\u5206\u5e03\u5f0f\u4f5c\u4e1a\u7684\u6240\u6709\u8fdb\u7a0b\uff09\u90fd\u8fdb\u5165\u6b64\u529f\u80fd\uff0c\u5373\u4f7f\u5b83\u4eec\u4e0d\u662f\u8be5\u7ec4\u7684\u6210\u5458\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u5e94\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u4ee5\u76f8\u540c\u7684\u987a\u5e8f\u521b\u5efa\u7ec4\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>ranks (list[int]) \u2013 \u5c0f\u7ec4\u6210\u5458\u7684\u7b49\u7ea7\u5217\u8868\u3002</li> <li>timeout (timedelta__, optional) \u2013 \u9488\u5bf9\u8fdb\u7a0b\u7ec4\u6267\u884c\u7684\u64cd\u4f5c\u8d85\u65f6\uff0c\u9ed8\u8ba4\u503c\u7b49\u4e8e30\u5206\u949f\uff0c\u8fd9\u4ec5\u9002\u7528\u4e8e<code>gloo</code>\u540e\u7aef\u3002</li> </ul> \u8fd4\u56de: \u5206\u5e03\u5f0f\u7ec4\u7684\u53e5\u67c4\uff0c\u53ef\u4ee5\u7ed9\u4e88\u96c6\u4f53\u8c03\u7528"},{"location":"1.0/distributed/#_10","title":"\u70b9\u5bf9\u70b9\u901a\u4fe1","text":"<pre><code>torch.distributed.send(tensor, dst, group=&lt;object object&gt;, tag=0)\n</code></pre> <p>\u540c\u6b65\u53d1\u9001\u5f20\u91cf</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u51c6\u5907\u53d1\u9001\u7684\u5f20\u91cf\u3002</li> <li>dst (int) \u2013 \u76ee\u7684\u5730\u6392\u540d\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>tag (int, optional) \u2013 \u6807\u8bb0\u4ee5\u5339\u914d\u53d1\u9001\u4e0e\u8fdc\u7a0b\u63a5\u6536\u3002</li> </ul> <pre><code>torch.distributed.recv(tensor, src=None, group=&lt;object object&gt;, tag=0)\n</code></pre> <p>\u540c\u6b65\u63a5\u6536\u5f20\u91cf</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor (Tensor) \u2013 \u5f20\u91cf\u586b\u5145\u63a5\u6536\u7684\u6570\u636e\u3002</li> <li>src (int, optional) \u2013 \u6765\u6e90\u6392\u540d\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4ece\u4efb\u4f55\u6d41\u7a0b\u6536\u5230\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>tag (int, optional) \u2013 \u6807\u8bb0\u4ee5\u5339\u914d\u63a5\u6536\u4e0e\u8fdc\u7a0b\u53d1\u9001\u3002</li> </ul> \u8fd4\u56de: \u53d1\u4ef6\u4eba\u6392\u540d-1\uff0c\u5982\u679c\u4e0d\u662f\u8be5\u7ec4\u7684\u4e00\u90e8\u5206 <p><code>isend()</code> \u548c <code>irecv()</code> \u4f7f\u7528\u65f6\u8fd4\u56de\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61\u3002\u901a\u5e38\uff0c\u6b64\u5bf9\u8c61\u7684\u7c7b\u578b\u672a\u6307\u5b9a\uff0c\u56e0\u4e3a\u5b83\u4eec\u6c38\u8fdc\u4e0d\u5e94\u624b\u52a8\u521b\u5efa\uff0c\u4f46\u5b83\u4eec\u4fdd\u8bc1\u652f\u6301\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ul> <li><code>is_completed()</code> - \u5982\u679c\u64cd\u4f5c\u5df2\u5b8c\u6210\uff0c\u5219\u8fd4\u56deTrue\u3002</li> <li><code>wait()</code> - \u5c06\u963b\u6b62\u8be5\u8fc7\u7a0b\uff0c\u76f4\u5230\u64cd\u4f5c\u5b8c\u6210\uff0c<code>is_completed(\uff09</code>\u4fdd\u8bc1\u4e00\u65e6\u8fd4\u56de\u5c31\u8fd4\u56deTrue\u3002</li> </ul> <pre><code>torch.distributed.isend(tensor, dst, group=&lt;object object&gt;, tag=0)\n</code></pre> <p>\u5f02\u6b65\u53d1\u9001\u5f20\u91cf</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u51c6\u672c\u53d1\u9001\u7684\u5f20\u91cf\u3002</li> <li>dst (int) \u2013 \u76ee\u7684\u5730\u6392\u540d\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>tag (int, optional) \u2013 \u6807\u8bb0\u4ee5\u5339\u914d\u53d1\u9001\u4e0e\u8fdc\u7a0b\u63a5\u6536\u3002</li> </ul> \u8fd4\u56de: \u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61\u3002\u6ca1\u6709\uff0c\u5982\u679c\u4e0d\u662f\u8be5\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.irecv(tensor, src, group=&lt;object object&gt;, tag=0)\n</code></pre> <p>\u5f02\u6b65\u63a5\u6536\u5f20\u91cf</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u5f20\u91cf\u586b\u5145\u63a5\u6536\u7684\u6570\u636e\u3002</li> <li>src (int) \u2013 \u6765\u6e90\u6392\u540d\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>tag (int, optional) \u2013 \u6807\u8bb0\u4ee5\u5339\u914d\u63a5\u6536\u4e0e\u8fdc\u7a0b\u53d1\u9001\u3002</li> </ul> \u8fd4\u56de: \u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61\u3002\u6ca1\u6709\uff0c\u5982\u679c\u4e0d\u662f\u8be5\u7ec4\u7684\u4e00\u90e8\u5206"},{"location":"1.0/distributed/#_11","title":"\u540c\u6b65\u548c\u5f02\u6b65\u96c6\u5408\u64cd\u4f5c","text":"<p>\u6bcf\u4e2a\u96c6\u5408\u64cd\u4f5c\u51fd\u6570\u90fd\u652f\u6301\u4ee5\u4e0b\u4e24\u79cd\u64cd\u4f5c\uff1a</p> <p>\u540c\u6b65\u64cd\u4f5c - \u9ed8\u8ba4\u6a21\u5f0f\uff0c\u5f53<code>async_op</code>\u8bbe\u7f6e\u4e3aFalse\u65f6\u3002\u5f53\u51fd\u6570\u8fd4\u56de\u65f6\uff0c\u4fdd\u8bc1\u6267\u884c\u96c6\u5408\u64cd\u4f5c(\u5982\u679c\u5b83\u662fCUDA\u64cd\u4f5c\uff0c\u5219\u4e0d\u4e00\u5b9a\u5b8c\u6210\uff0c\u56e0\u4e3a\u6240\u6709CUDA\u64cd\u4f5c\u90fd\u662f\u5f02\u6b65\u7684\uff09\uff0c\u5e76\u4e14\u53ef\u4ee5\u8c03\u7528\u4efb\u4f55\u8fdb\u4e00\u6b65\u7684\u51fd\u6570\u8c03\u7528\uff0c\u8fd9\u53d6\u51b3\u4e8e\u96c6\u5408\u64cd\u4f5c\u7684\u6570\u636e\u3002\u5728\u540c\u6b65\u6a21\u5f0f\u4e0b\uff0c\u96c6\u5408\u51fd\u6570\u4e0d\u8fd4\u56de\u4efb\u4f55\u5185\u5bb9\u3002</p> <p>asynchronous operation - \u5f53<code>async_op</code>\u8bbe\u7f6e\u4e3aTrue\u65f6\u3002\u96c6\u5408\u64cd\u4f5c\u51fd\u6570\u8fd4\u56de\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61\u3002\u901a\u5e38\uff0c\u60a8\u4e0d\u9700\u8981\u624b\u52a8\u521b\u5efa\u5b83\uff0c\u5e76\u4e14\u4fdd\u8bc1\u652f\u6301\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ul> <li><code>is_completed()</code> - \u5982\u679c\u64cd\u4f5c\u5df2\u5b8c\u6210\uff0c\u5219\u8fd4\u56deTrue\u3002</li> <li><code>wait()</code> - \u5c06\u963b\u6b62\u8be5\u8fc7\u7a0b\uff0c\u76f4\u5230\u64cd\u4f5c\u5b8c\u6210\u3002</li> </ul>"},{"location":"1.0/distributed/#_12","title":"\u96c6\u4f53\u804c\u80fd","text":"<pre><code>torch.distributed.broadcast(tensor, src, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u5c06\u5f20\u91cf\u5e7f\u64ad\u5230\u6574\u4e2a\u7fa4\u4f53</p> <p><code>tensor</code>\u5fc5\u987b\u5728\u53c2\u4e0e\u96c6\u5408\u4f53\u7684\u6240\u6709\u8fdb\u7a0b\u4e2d\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u5982\u679c<code>src</code>\u662f\u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7\uff0c\u5219\u53d1\u9001\u7684\u6570\u636e\uff0c\u5426\u5219\u7528\u4e8e\u4fdd\u5b58\u63a5\u6536\u6570\u636e\u7684\u5f20\u91cf\u3002</li> <li>src (int) \u2013 \u6765\u6e90\u6392\u540d\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.all_reduce(tensor, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u51cf\u5c11\u6240\u6709\u673a\u5668\u4e0a\u7684\u5f20\u91cf\u6570\u636e\uff0c\u4ee5\u4fbf\u83b7\u5f97\u6700\u7ec8\u7ed3\u679c</p> <p>\u8c03\u7528<code>tensor</code>\u4e4b\u540e\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u5c06\u6309\u4f4d\u76f8\u540c\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u96c6\u4f53\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002\u8be5\u529f\u80fd\u5c31\u5730\u8fd0\u884c\u3002</li> <li>op (optional) \u2013 \u6765\u81ea<code>torch.distributed.ReduceOp</code>\u679a\u4e3e\u7684\u503c\u4e4b\u4e00\u3002\u6307\u5b9a\u7528\u4e8e\u9010\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.reduce(tensor, dst, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u51cf\u5c11\u6240\u6709\u673a\u5668\u7684\u5f20\u91cf\u6570\u636e</p> <p>\u53ea\u6709\u6392\u540d\u4e3a\u201cdst\u201d\u7684\u8fdb\u7a0b\u624d\u4f1a\u6536\u5230\u6700\u7ec8\u7ed3\u679c\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>tensor (Tensor) \u2013 \u96c6\u4f53\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002\u8be5\u529f\u80fd\u5c31\u5730\u8fd0\u884c\u3002</li> <li>dst (int) \u2013 \u76ee\u7684\u5730\u6392\u540d\u3002</li> <li>op (optional) \u2013 \u6765\u81ea<code>torch.distributed.ReduceOp</code>\u679a\u4e3e\u7684\u503c\u4e4b\u4e00\u3002\u6307\u5b9a\u7528\u4e8e\u9010\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.all_gather(tensor_list, tensor, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u4ece\u5217\u8868\u4e2d\u6536\u96c6\u6574\u4e2a\u7ec4\u7684\u5f20\u91cf</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor_list (list[Tensor]) \u2013 \u8f93\u51fa\u5217\u8868\u3002\u5b83\u5e94\u5305\u542b\u6b63\u786e\u5927\u5c0f\u7684\u5f20\u91cf\uff0c\u7528\u4e8e\u96c6\u5408\u7684\u8f93\u51fa\u3002</li> <li>tensor (Tensor) \u2013 \u4ece\u5f53\u524d\u8fdb\u7a0b\u5e7f\u64ad\u7684\u5f20\u91cf\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.gather(tensor, gather_list, dst, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u5728\u4e00\u4e2a\u8fc7\u7a0b\u4e2d\u6536\u96c6\u5f20\u91cf\u5217\u8868</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor (Tensor) \u2013 \u8f93\u5165\u5f20\u91cf\u3002</li> <li>gather_list (list[Tensor]) \u2013 \u7528\u4e8e\u63a5\u6536\u6570\u636e\u7684\u9002\u5f53\u5927\u5c0f\u7684\u5f20\u91cf\u5217\u8868\u3002\u4ec5\u5728\u63a5\u6536\u8fc7\u7a0b\u4e2d\u9700\u8981\u3002</li> <li>dst (int) \u2013 \u76ee\u7684\u5730\u6392\u540d\u3002\u9664\u63a5\u6536\u6570\u636e\u7684\u8fdb\u7a0b\u5916\uff0c\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u90fd\u662f\u5fc5\u9700\u7684\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.scatter(tensor, scatter_list, src, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u5c06\u5f20\u91cf\u5217\u8868\u5206\u6563\u5230\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b</p> <p>\u6bcf\u4e2a\u8fdb\u7a0b\u53ea\u63a5\u6536\u4e00\u4e2a\u5f20\u91cf\u5e76\u5c06\u5176\u6570\u636e\u5b58\u50a8\u5728<code>tensor</code>\u53c2\u6570\u4e2d\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor (Tensor) \u2013 \u8f93\u51fa\u5f20\u91cf\u3002</li> <li>scatter_list (list[Tensor]) \u2013 \u8981\u5206\u6563\u7684\u5f20\u91cf\u5217\u8868\u3002\u4ec5\u5728\u53d1\u9001\u6570\u636e\u7684\u8fc7\u7a0b\u4e2d\u9700\u8981\u3002</li> <li>src (int) \u2013 \u6765\u6e90\u6392\u540d\u3002\u9664\u53d1\u9001\u6570\u636e\u7684\u8fdb\u7a0b\u5916\uff0c\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u90fd\u662f\u5fc5\u9700\u7684\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206\uff0c\u65e0 <pre><code>torch.distributed.barrier(group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u540c\u6b65\u6240\u6709\u8fdb\u7a0b</p> <p>\u5982\u679casync_op\u4e3aFalse\uff0c\u6216\u8005\u5728wait(\uff09\u4e0a\u8c03\u7528\u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5219\u6b64\u96c6\u5408\u4f1a\u963b\u6b62\u8fdb\u7a0b\u76f4\u5230\u6574\u4e2a\u7ec4\u8fdb\u5165\u6b64\u51fd\u6570\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>class torch.distributed.ReduceOp\n</code></pre> <p>\u7c7b\u4f3c\u679a\u4e3e\u7684\u53ef\u7528\u51cf\u5c11\u64cd\u4f5c\u7c7b\uff1a<code>SUM</code>\uff0c<code>PRODUCT</code>\uff0c<code>MIN</code>\u548c<code>MAX</code>\u3002</p> <p>\u8be5\u7c7b\u7684\u503c\u53ef\u4ee5\u4f5c\u4e3a\u5c5e\u6027\u8bbf\u95ee\uff0c\u4f8b\u5982\uff0c<code>ReduceOp.SUM</code>\u3002\u5b83\u4eec\u7528\u4e8e\u6307\u5b9a\u51cf\u5c11\u96c6\u7fa4\u7684\u6218\u7565\uff0c\u4f8b\u5982 <code>reduce()</code>, <code>all_reduce_multigpu()</code>\u3002</p> <p>\u6210\u5458\uff1a</p> <p>SUM</p> <p>PRODUCT</p> <p>MIN</p> <p>MAX</p> <pre><code>class torch.distributed.reduce_op\n</code></pre> <p>\u7528\u4e8e\u8fd8\u539f\u64cd\u4f5c\u7684\u4e0d\u518d\u4f7f\u7528\u7684\u679a\u4e3e\u7c7b\uff1a<code>SUM</code>\uff0c<code>PRODUCT</code>\uff0c<code>MIN</code>\u548c<code>MAX</code>\u3002</p> <p>\u5efa\u8bae\u4f7f\u7528<code>ReduceOp</code> \u4ee3\u66ff\u3002</p>"},{"location":"1.0/distributed/#gpu","title":"\u591aGPU\u96c6\u7fa4\u529f\u80fd","text":"<p>\u5982\u679c\u6bcf\u4e2a\u8282\u70b9\u4e0a\u6709\u591a\u4e2aGPU\uff0c\u5219\u5728\u4f7f\u7528NCCL\u548cGloo\u540e\u7aef\u65f6\uff0c<code>broadcast_multigpu()</code> <code>all_reduce_multigpu()</code> <code>reduce_multigpu()</code> \u548c <code>all_gather_multigpu()</code> \u652f\u6301\u6bcf\u4e2a\u8282\u70b9\u5185\u591a\u4e2aGPU\u4e4b\u95f4\u7684\u5206\u5e03\u5f0f\u96c6\u5408\u64cd\u4f5c\u3002\u8fd9\u4e9b\u529f\u80fd\u53ef\u4ee5\u6f5c\u5728\u5730\u63d0\u9ad8\u6574\u4f53\u5206\u5e03\u5f0f\u8bad\u7ec3\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4f20\u9012\u5f20\u91cf\u5217\u8868\u8f7b\u677e\u4f7f\u7528\u3002\u4f20\u9012\u7684\u5f20\u91cf\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u5f20\u91cf\u9700\u8981\u4f4d\u4e8e\u8c03\u7528\u8be5\u51fd\u6570\u7684\u4e3b\u673a\u7684\u5355\u72ecGPU\u8bbe\u5907\u4e0a\u3002\u8bf7\u6ce8\u610f\uff0c\u5f20\u91cf\u5217\u8868\u7684\u957f\u5ea6\u5728\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u4e2d\u9700\u8981\u76f8\u540c\u3002\u53e6\u8bf7\u6ce8\u610f\uff0c\u76ee\u524d\u53ea\u6709NCCL\u540e\u7aef\u652f\u6301\u591aGPU\u96c6\u5408\u529f\u80fd\u3002</p> <p>\u4f8b\u5982\uff0c\u5982\u679c\u6211\u4eec\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u7cfb\u7edf\u67092\u4e2a\u8282\u70b9\uff0c\u6bcf\u4e2a\u8282\u70b9\u67098\u4e2aGPU\u3002\u572816\u4e2aGPU\u4e2d\u7684\u6bcf\u4e00\u4e2a\u4e0a\uff0c\u90fd\u6709\u4e00\u4e2a\u6211\u4eec\u5e0c\u671b\u51cf\u5c11\u7684\u5f20\u91cf\uff0c\u4ee5\u4e0b\u4ee3\u7801\u53ef\u4ee5\u4f5c\u4e3a\u53c2\u8003\uff1a</p> <p>\u4ee3\u7801\u5728\u8282\u70b90\u4e0a\u8fd0\u884c</p> <pre><code>import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n\n</code></pre> <p>\u4ee3\u7801\u5728\u8282\u70b91\u4e0a\u8fd0\u884c</p> <pre><code>import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n\n</code></pre> <p>\u8c03\u7528\u7ed3\u675f\u540e\uff0c\u4e24\u4e2a\u8282\u70b9\u4e0a\u7684\u6240\u670916\u4e2a\u5f20\u91cf\u90fd\u5c06\u5177\u670916\u7684\u5168\u51cf\u503c\u3002</p> <pre><code>torch.distributed.broadcast_multigpu(tensor_list, src, group=&lt;object object&gt;, async_op=False, src_tensor=0)\n</code></pre> <p>\u4f7f\u7528\u6bcf\u4e2a\u8282\u70b9\u591a\u4e2aGPU\u5f20\u91cf\u5c06\u5f20\u91cf\u5e7f\u64ad\u5230\u6574\u4e2a\u7ec4</p> <p><code>tensor</code>\u5fc5\u987b\u5728\u53c2\u4e0e\u96c6\u5408\u4f53\u7684\u6240\u6709\u8fdb\u7a0b\u7684\u6240\u6709GPU\u4e2d\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u3002\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u5f20\u91cf\u5fc5\u987b\u4f4d\u4e8e\u4e0d\u540c\u7684GPU\u4e0a\u3002</p> <p>\u76ee\u524d\u4ec5\u652f\u6301nccl\u548cgloo\u540e\u7aef\u5f20\u91cf\u5e94\u8be5\u53ea\u662fGPU\u5f20\u91cf</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor_list (List__[Tensor]) \u2013 \u53c2\u4e0e\u96c6\u7fa4\u64cd\u4f5c\u884c\u52a8\u7684\u5f20\u91cf\u3002\u5982\u679c<code>src</code>\u662f\u6392\u540d\uff0c\u90a3\u4e48<code>tensor_list`(`tensor_list [src_tensor]`\uff09\u7684`src_tensor</code>\u5143\u7d20\u5c06\u88ab\u5e7f\u64ad\u5230src\u8fdb\u7a0b\u4e2d\u7684\u6240\u6709\u5176\u4ed6\u5f20\u91cf(\u5728\u4e0d\u540c\u7684GPU\u4e0a\uff09\u4ee5\u53ca<code>tensor_list\u4e2d\u7684\u6240\u6709\u5f20\u91cf</code>\u5176\u4ed6\u975esrc\u8fdb\u7a0b\u3002\u60a8\u8fd8\u9700\u8981\u786e\u4fdd\u8c03\u7528\u6b64\u51fd\u6570\u7684\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7684<code>len(tensor_list\uff09</code>\u662f\u76f8\u540c\u7684\u3002</li> <li>src (int) \u2013 \u6e90\u6392\u884c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u88ab\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> <li>src_tensor (int, optional) \u2013 \u6e90\u5f20\u91cf\u7b49\u7ea7\u5728<code>tensor_list</code>\u5185\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u51cf\u5c11\u6240\u6709\u673a\u5668\u4e0a\u7684\u5f20\u91cf\u6570\u636e\uff0c\u4ee5\u4fbf\u83b7\u5f97\u6700\u7ec8\u7ed3\u679c\u3002\u6b64\u529f\u80fd\u53ef\u51cf\u5c11\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684\u591a\u4e2a\u5f20\u91cf\uff0c\u800c\u6bcf\u4e2a\u5f20\u91cf\u4f4d\u4e8e\u4e0d\u540c\u7684GPU\u4e0a\u3002\u56e0\u6b64\uff0c\u5f20\u91cf\u5217\u8868\u4e2d\u7684\u8f93\u5165\u5f20\u91cf\u9700\u8981\u662fGPU\u5f20\u91cf\u3002\u6b64\u5916\uff0c\u5f20\u91cf\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u5f20\u91cf\u90fd\u9700\u8981\u9a7b\u7559\u5728\u4e0d\u540c\u7684GPU\u4e0a\u3002</p> <p>\u5728\u8c03\u7528\u4e4b\u540e\uff0c<code>tensor_list</code>\u4e2d\u7684\u6240\u6709<code>tensor</code>\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u90fd\u662f\u6309\u4f4d\u76f8\u540c\u7684\u3002</p> <p>\u76ee\u524d\u4ec5\u652f\u6301nccl\u548cgloo\u540e\u7aef\uff0c\u5f20\u91cf\u5e94\u4ec5\u4e3aGPU\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>list (tensor) \u2013 \u96c6\u4f53\u7684\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u5217\u8868\u3002\u8be5\u529f\u80fd\u5c31\u5730\u8fd0\u884c\uff0c\u5e76\u8981\u6c42\u6bcf\u4e2a\u5f20\u91cf\u5728\u4e0d\u540c\u7684GPU\u4e0a\u4e3aGPU\u5f20\u91cf\u3002\u60a8\u8fd8\u9700\u8981\u786e\u4fdd\u8c03\u7528\u6b64\u51fd\u6570\u7684\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7684<code>len(tensor_list\uff09</code>\u662f\u76f8\u540c\u7684\u3002</li> <li>op (optional) \u2013 \u6765\u81ea<code>torch.distributed.ReduceOp</code>\u679a\u4e3e\u7684\u503c\u4e4b\u4e00\uff0c\u5e76\u4e14\u6307\u5b9a\u4e00\u4e2a\u9010\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206 <pre><code>torch.distributed.reduce_multigpu(tensor_list, dst, op=ReduceOp.SUM, group=&lt;object object&gt;, async_op=False, dst_tensor=0)\n</code></pre> <p>\u51cf\u5c11\u6240\u6709\u8ba1\u7b97\u673a\u4e0a\u591a\u4e2aGPU\u7684\u5f20\u91cf\u6570\u636e\u3002<code>tensor_list</code>\u4e2d\u7684\u6bcf\u4e2a\u5f20\u91cf\u5e94\u4f4d\u4e8e\u5355\u72ec\u7684GPU\u4e0a\u3002</p> <p>\u53ea\u6709\u7ea7\u522b\u4e3a'dst<code>\u7684\u8fdb\u7a0b\u4e2d\u7684'tensor_list [dst_tensor]</code>\u7684GPU\u624d\u4f1a\u6536\u5230\u6700\u7ec8\u7ed3\u679c\u3002</p> <p>\u76ee\u524d\u4ec5\u652f\u6301nccl\u540e\u7aef\u5f20\u91cf\u5e94\u8be5\u53ea\u662fGPU\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor_list (List__[Tensor]) \u2013 \u8f93\u5165\u548c\u8f93\u51fa\u96c6\u4f53\u7684GPU\u5f20\u91cf\u3002\u8be5\u529f\u80fd\u5c31\u5730\u8fd0\u884c\uff0c\u60a8\u8fd8\u9700\u8981\u786e\u4fdd\u8c03\u7528\u6b64\u51fd\u6570\u7684\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7684<code>len(tensor_list\uff09</code>\u662f\u76f8\u540c\u7684\u3002</li> <li>dst (int) \u2013 \u76ee\u7684\u5730\u6392\u540d\u3002</li> <li>op (optional) \u2013 \u6765\u81ea<code>torch.distributed.ReduceOp</code>\u679a\u4e3e\u7684\u503c\u4e4b\u4e00\u3002\u6307\u5b9a\u4e00\u4e2a\u9010\u5143\u7d20\u51cf\u5c11\u7684\u64cd\u4f5c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 The process group to work on</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c\u3002</li> <li>dst_tensor (int, optional) \u2013 \u76ee\u6807\u5f20\u91cf\u5728<code>tensor_list</code>\u4e2d\u6392\u540d\u3002</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u6ca1\u6709\uff0c\u5426\u5219 <pre><code>torch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=&lt;object object&gt;, async_op=False)\n</code></pre> <p>\u4ece\u5217\u8868\u4e2d\u6536\u96c6\u6574\u4e2a\u7ec4\u7684\u5f20\u91cf\u3002<code>tensor_list</code>\u4e2d\u7684\u6bcf\u4e2a\u5f20\u91cf\u5e94\u4f4d\u4e8e\u5355\u72ec\u7684GPU\u4e0a\u3002</p> <p>\u76ee\u524d\u4ec5\u652f\u6301nccl\u540e\u7aef\u5f20\u91cf\u5e94\u8be5\u53ea\u662fGPU\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>output_tensor_lists (List__[__List__[Tensor]__]) \u2013 \u8f93\u51fa\u5217\u8868\u3002\u5b83\u5e94\u8be5\u5728\u6bcf\u4e2aGPU\u4e0a\u5305\u542b\u6b63\u786e\u5927\u5c0f\u7684\u5f20\u91cf\uff0c\u4ee5\u7528\u4e8e\u96c6\u5408\u7684\u8f93\u51fa\u3002\u4f8b\u5982 <code>output_tensor_lists [i]</code>\u5305\u542b\u9a7b\u7559\u5728<code>input_tensor_list [i]</code>\u7684GPU\u4e0a\u7684all_gather\u7ed3\u679c\u3002\u8bf7\u6ce8\u610f\uff0c<code>output_tensor_lists [i]</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u5177\u6709<code>world_size * len(input_tensor_list\uff09</code>\u7684\u5927\u5c0f\uff0c\u56e0\u4e3a\u8be5\u51fd\u6570\u5168\u90e8\u6536\u96c6\u7ec4\u4e2d\u6bcf\u4e2aGPU\u7684\u7ed3\u679c\u3002\u8981\u89e3\u91ca<code>output_tensor_list [i]</code>\u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u8bf7\u6ce8\u610f\u7b49\u7ea7k\u7684<code>input_tensor_list [j]</code>\u5c06\u51fa\u73b0\u5728<code>output_tensor_list [i] [rank * world_size + j]\u4e2d\u3002\u8fd8\u8981\u6ce8\u610f</code>len(output_tensor_lists\uff09<code>\uff0c\u5e76\u4e14</code>output_tensor_lists<code>\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u5927\u5c0f(\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u4e00\u4e2a\u5217\u8868\uff0c\u56e0\u6b64</code>len(output_tensor_lists [i]\uff09`\uff09\u5bf9\u4e8e\u8c03\u7528\u6b64\u51fd\u6570\u7684\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u90fd\u9700\u8981\u76f8\u540c\u3002</li> <li>input_tensor_list (List__[Tensor]) \u2013 \u4ece\u5f53\u524d\u8fdb\u7a0b\u5e7f\u64ad\u7684\u5f20\u91cf(\u5728\u4e0d\u540c\u7684GPU\u4e0a\uff09\u7684\u5217\u8868\u3002\u8bf7\u6ce8\u610f\uff0c\u8c03\u7528\u6b64\u51fd\u6570\u7684\u6240\u6709\u5206\u5e03\u5f0f\u8fdb\u7a0b\u7684<code>len(input_tensor_list\uff09</code>\u5fc5\u987b\u76f8\u540c\u3002</li> <li>group (ProcessGroup__, optional) \u2013 \u8981\u5904\u7406\u7684\u8fdb\u7a0b\u7ec4\u3002</li> <li>async_op (bool, optional) \u2013 \u8fd9\u4e2a\u64cd\u4f5c\u662f\u5426\u5e94\u8be5\u662f\u5f02\u6b65\u64cd\u4f5c</li> </ul> \u8fd4\u56de: \u5f02\u6b65\u5de5\u4f5c\u53e5\u67c4\uff0c\u5982\u679casync_op\u8bbe\u7f6e\u4e3aTrue\u3002\u65e0\uff0c\u5982\u679c\u4e0d\u662fasync_op\u6216\u4e0d\u662f\u7ec4\u7684\u4e00\u90e8\u5206"},{"location":"1.0/distributed/#_13","title":"\u542f\u52a8\u5b9e\u7528\u7a0b\u5e8f","text":"<p><code>torch.distributed</code>\u5305\u8fd8\u5728<code>torch.distributed.launch</code>\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u542f\u52a8\u5b9e\u7528\u7a0b\u5e8f\u3002\u6b64\u5e2e\u52a9\u5b9e\u7528\u7a0b\u5e8f\u53ef\u7528\u4e8e\u4e3a\u6bcf\u4e2a\u8282\u70b9\u542f\u52a8\u591a\u4e2a\u8fdb\u7a0b\u4ee5\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002\u8be5\u5b9e\u7528\u7a0b\u5e8f\u8fd8\u652f\u6301python2\u548cpython3\u3002</p> <p><code>torch.distributed.launch</code>\u662f\u4e00\u4e2a\u6a21\u5757\uff0c\u5b83\u5728\u6bcf\u4e2a\u8bad\u7ec3\u8282\u70b9\u4e0a\u4ea7\u751f\u591a\u4e2a\u5206\u5e03\u5f0f\u8bad\u7ec3\u8fc7\u7a0b\u3002</p> <p>\u8be5\u5b9e\u7528\u7a0b\u5e8f\u53ef\u7528\u4e8e\u5355\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5176\u4e2d\u5c06\u751f\u6210\u6bcf\u4e2a\u8282\u70b9\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u8fdb\u7a0b\u3002\u8be5\u5b9e\u7528\u7a0b\u5e8f\u53ef\u7528\u4e8eCPU\u8bad\u7ec3\u6216GPU\u8bad\u7ec3\u3002\u5982\u679c\u8be5\u5b9e\u7528\u7a0b\u5e8f\u7528\u4e8eGPU\u8bad\u7ec3\uff0c\u5219\u6bcf\u4e2a\u5206\u5e03\u5f0f\u8fdb\u7a0b\u5c06\u5728\u5355\u4e2aGPU\u4e0a\u8fd0\u884c\u3002\u8fd9\u53ef\u4ee5\u5b9e\u73b0\u826f\u597d\u6539\u8fdb\u7684\u5355\u8282\u70b9\u8bad\u7ec3\u6027\u80fd\u3002\u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u4ea7\u751f\u591a\u4e2a\u8fdb\u7a0b\u6765\u83b7\u5f97\u826f\u597d\u6539\u8fdb\u7684\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\u6027\u80fd\u3002\u8fd9\u5bf9\u4e8e\u5177\u6709\u591a\u4e2a\u5177\u6709\u76f4\u63a5GPU\u652f\u6301\u7684Infiniband\u63a5\u53e3\u7684\u7cfb\u7edf\u5c24\u5176\u6709\u5229\uff0c\u56e0\u4e3a\u6240\u6709\u8fd9\u4e9b\u63a5\u53e3\u90fd\u53ef\u7528\u4e8e\u805a\u5408\u901a\u4fe1\u5e26\u5bbd\u3002</p> <p>\u5728\u5355\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\u6216\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u8be5\u5b9e\u7528\u7a0b\u5e8f\u5c06\u4e3a\u6bcf\u4e2a\u8282\u70b9\u542f\u52a8\u7ed9\u5b9a\u6570\u91cf\u7684\u8fdb\u7a0b(<code>--nproc_per_node</code>\uff09\u3002\u5982\u679c\u7528\u4e8eGPU\u8bad\u7ec3\uff0c\u6b64\u6570\u5b57\u9700\u8981\u5c0f\u4e8e\u6216\u7b49\u4e8e\u5f53\u524d\u7cfb\u7edf\u4e0a\u7684GPU\u6570\u91cf('nproc_per_node`\uff09\uff0c\u5e76\u4e14\u6bcf\u4e2a\u8fdb\u7a0b\u5c06\u5728\u5355\u4e2aGPU\u4e0a\u8fd0\u884c\uff0c\u4ece_GPU 0\u5230GPU(nproc_per_node - 1\uff09_\u3002</p> <p>\u5982\u4f55\u4f7f\u7528\u8fd9\u4e2a\u6a21\u5757\uff1a</p> <ol> <li>\u5355\u8282\u70b9\u591a\u8fdb\u7a0b\u5206\u5e03\u5f0f\u8bad\u7ec3</li> </ol> <pre><code>&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n arguments of your training script)\n\n</code></pre> <ol> <li>\u591a\u8282\u70b9\u591a\u8fdb\u7a0b\u5206\u5e03\u5f0f\u8bad\u7ec3:(\u4f8b\u5982\u4e24\u4e2a\u8282\u70b9\uff09</li> </ol> <p>\u8282\u70b91\uff1a(IP\uff1a192.168.1.1\uff0c\u5e76\u4e14\u6709\u4e00\u4e2a\u7a7a\u95f2\u7aef\u53e3\uff1a1234\uff09</p> <pre><code>&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n and all other arguments of your training script)\n\n</code></pre> <p>\u8282\u70b92\uff1a</p> <pre><code>&gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n and all other arguments of your training script)\n\n</code></pre> <p>1.\u67e5\u627e\u6b64\u6a21\u5757\u63d0\u4f9b\u7684\u53ef\u9009\u53c2\u6570\uff1a</p> <pre><code>&gt;&gt;&gt; python -m torch.distributed.launch --help\n\n</code></pre> <p>\u91cd\u8981\u544a\u793a\uff1a</p> <p>1. \u8fd9\u79cd\u5b9e\u7528\u548c\u591a\u8fdb\u7a0b\u5206\u5e03\u5f0f(\u5355\u8282\u70b9\u6216\u591a\u8282\u70b9\uff09GPU\u8bad\u7ec3\u76ee\u524d\u4ec5\u4f7f\u7528NCCL\u5206\u5e03\u5f0f\u540e\u7aef\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002\u56e0\u6b64\uff0cNCCL\u540e\u7aef\u662f\u7528\u4e8eGPU\u8bad\u7ec3\u7684\u63a8\u8350\u540e\u7aef\u3002</p> <p>2. \u5728\u60a8\u7684\u8bad\u7ec3\u7a0b\u5e8f\u4e2d\uff0c\u60a8\u5fc5\u987b\u89e3\u6790\u547d\u4ee4\u884c\u53c2\u6570\uff1a<code>--local_rank = LOCAL_PROCESS_RANK</code>\uff0c\u8fd9\u5c06\u7531\u6b64\u6a21\u5757\u63d0\u4f9b\u3002\u5982\u679c\u60a8\u7684\u8bad\u7ec3\u8ba1\u5212\u4f7f\u7528GPU\uff0c\u5219\u5e94\u786e\u4fdd\u60a8\u7684\u4ee3\u7801\u4ec5\u5728LOCAL_PROCESS_RANK\u7684GPU\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b8c\u6210\uff1a</p> <p>\u89e3\u6790local_rank\u53c2\u6570</p> <pre><code>&gt;&gt;&gt; import argparse\n&gt;&gt;&gt; parser = argparse.ArgumentParser()\n&gt;&gt;&gt; parser.add_argument(\"--local_rank\", type=int)\n&gt;&gt;&gt; args = parser.parse_args()\n\n</code></pre> <p>\u4f7f\u7528\u5176\u4e2d\u4e00\u4e2a\u5c06\u60a8\u7684\u8bbe\u5907\u8bbe\u7f6e\u4e3a\u672c\u5730\u6392\u540d</p> <pre><code>&gt;&gt;&gt; torch.cuda.set_device(arg.local_rank)  # before your code runs\n\n</code></pre> <p>\u6216\u8005</p> <pre><code>&gt;&gt;&gt; with torch.cuda.device(arg.local_rank):\n&gt;&gt;&gt;    # your code to run\n\n</code></pre> <p>3. \u5728\u60a8\u7684\u8bad\u7ec3\u8ba1\u5212\u4e2d\uff0c\u60a8\u5e94\u8be5\u5728\u5f00\u59cb\u65f6\u8c03\u7528\u4ee5\u4e0b\u51fd\u6570\u6765\u542f\u52a8\u5206\u5e03\u5f0f\u540e\u7aef\u3002\u60a8\u9700\u8981\u786e\u4fddinit_method\u4f7f\u7528<code>env\uff1a//</code>\uff0c\u8fd9\u662f\u8be5\u6a21\u5757\u552f\u4e00\u652f\u6301\u7684<code>init_method</code>\u3002</p> <pre><code>torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')\n\n</code></pre> <p>4. \u5728\u60a8\u7684\u8bad\u7ec3\u8ba1\u5212\u4e2d\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5e38\u89c4\u5206\u5e03\u5f0f\u529f\u80fd\u6216\u4f7f\u7528 <code>torch.nn.parallel.DistributedDataParallel()</code> \u6a21\u5757\u3002\u5982\u679c\u60a8\u7684\u8bad\u7ec3\u8ba1\u5212\u4f7f\u7528GPU\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e14\u60a8\u5e0c\u671b\u4f7f\u7528 <code>torch.nn.parallel.DistributedDataParallel()</code> \u6a21\u5757\u3002\u8fd9\u91cc\u662f\u5982\u4f55\u914d\u7f6e\u5b83\u3002</p> <pre><code>model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[arg.local_rank],\n                                                  output_device=arg.local_rank)\n\n</code></pre> <p>\u8bf7\u786e\u4fdd\u5c06<code>device_ids</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a\u60a8\u7684\u4ee3\u7801\u5c06\u5728\u5176\u4e0a\u8fd0\u884c\u7684\u552f\u4e00GPU\u8bbe\u5907ID\u3002\u8fd9\u901a\u5e38\u662f\u6d41\u7a0b\u7684\u672c\u5730\u6392\u540d\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c<code>device_ids</code>\u9700\u8981\u662f<code>[args.local_rank]</code>\uff0c<code>output_device</code>\u9700\u8981\u662f'args.local_rank`\u624d\u80fd\u4f7f\u7528\u8fd9\u4e2a\u5b9e\u7528\u7a0b\u5e8f\u3002</p> <p>\u8b66\u544a</p> <p><code>local_rank</code>\u4e0d\u662f\u5168\u5c40\u552f\u4e00\u7684\uff1a\u5b83\u53ea\u5bf9\u673a\u5668\u4e0a\u7684\u6bcf\u4e2a\u8fdb\u7a0b\u552f\u4e00\u3002\u56e0\u6b64\uff0c\u4e0d\u8981\u4f7f\u7528\u5b83\u6765\u51b3\u5b9a\u662f\u5426\u5e94\u8be5\uff0c\u4f8b\u5982\uff0c\u5199\u5165\u7f51\u7edc\u6587\u4ef6\u7cfb\u7edf\uff0c\u53c2\u8003 https://github.com/pytorch/pytorch/issues/12042 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u6ca1\u6709\u6b63\u786e\u6267\u884c\u6b64\u64cd\u4f5c\uff0c\u4e8b\u60c5\u53ef\u80fd\u4f1a\u51fa\u9519\u3002</p>"},{"location":"1.0/distributed/#spawn","title":"Spawn\u5b9e\u7528\u7a0b\u5e8f","text":"<p>\u5728  <code>torch.multiprocessing.spawn()</code> \u91cc\u9762\uff0ctorch.multiprocessing\u5305\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a<code>spawn</code>\u51fd\u6570. \u6b64\u8f85\u52a9\u51fd\u6570\u53ef\u7528\u4e8e\u751f\u6210\u591a\u4e2a\u8fdb\u7a0b\u3002\u5b83\u901a\u8fc7\u4f20\u9012\u60a8\u8981\u8fd0\u884c\u7684\u51fd\u6570\u5e76\u751f\u6210N\u4e2a\u8fdb\u7a0b\u6765\u8fd0\u884c\u5b83\u3002\u8fd9\u4e5f\u53ef\u4ee5\u7528\u4e8e\u591a\u8fdb\u7a0b\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002</p> <p>\u6709\u5173\u5982\u4f55\u4f7f\u7528\u5b83\u7684\u53c2\u8003\uff0c\u8bf7\u53c2\u9605 PyToch example - ImageNet implementation</p> <p>\u8bf7\u6ce8\u610f\uff0c\u6b64\u51fd\u6570\u9700\u8981Python 3.4\u6216\u66f4\u9ad8\u7248\u672c\u3002</p>"},{"location":"1.0/distributed_deprecated/","title":"\u5206\u5e03\u5f0f\u901a\u4fe1\u5305(\u5df2\u5f03\u7528\uff09-torch.distributed.deprecated","text":"<p>\u8b66\u544a</p> <p>torch.distributed.deprecated \u662f torch.distributed \u7684\u65e9\u671f\u7248\u672c\uff0c\u5f53\u524d\u5df2\u88ab\u5f03\u7528\uff0c\u5e76\u4e14\u5f88\u5feb\u5c06\u88ab\u5220\u9664\u3002\u8bf7\u53c2\u7167\u4f7f\u7528 torch.distributed \u7684\u6587\u6863\uff0c\u8fd9\u662fPyTorch\u6700\u65b0\u7684\u5206\u5e03\u5f0f\u901a\u4fe1\u5305\u3002</p> <p>torch.distributed\u63d0\u4f9b\u7c7b\u4f3cMPI\u7684\u63a5\u53e3\uff0c\u7528\u4e8e\u8de8\u591a\u673a\u7f51\u7edc\u4ea4\u6362\u5f20\u91cf\u6570\u636e\u3002\u5b83\u63d0\u4f9b\u4e00\u4e9b\u4e0d\u540c\u7684\u540e\u53f0\u5e76\u4e14\u652f\u6301\u4e0d\u540c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002</p> <p>\u5f53\u524d\u7684torch.distributed.deprecated\u652f\u6301\u56db\u4e2a\u540e\u53f0\uff0c\u6bcf\u4e2a\u90fd\u6709\u4e0d\u540c\u7684\u529f\u80fd\u3002\u8fd9\u4e2a\u8868\u5c55\u793a\u4e86\u5bf9\u4e8eCPU/GPU\u5f20\u91cf\u6765\u8bf4\uff0c\u54ea\u4e9b\u51fd\u6570\u662f\u53ef\u4ee5\u4f7f\u7528\u7684\u3002\u53ea\u6709\u5728\u7528\u4e8e\u6784\u5efaPyTorch\u7684\u5b9e\u73b0\u652f\u6301\u65f6\uff0cMPI\u624d\u652f\u6301cuda\u3002</p> <p></p>"},{"location":"1.0/distributed_deprecated/#_1","title":"\u57fa\u7840","text":"<p>torch.distributed.deprecated\u4e3a\u5728\u4e00\u53f0\u6216\u591a\u53f0\u673a\u5668\u4e0a\u8fd0\u884c\u7684\u591a\u4e2a\u8ba1\u7b97\u8282\u70b9\u4e0a\u7684\u591a\u8fdb\u7a0b\u5e76\u884c\u6027\u63d0\u4f9bPyTorch\u652f\u6301\u548c\u901a\u4fe1\u539f\u8bed\u3002torch.nn.parallel.deprecated.DistributedDataParallel()\u7c7b\u4ee5\u6b64\u529f\u80fd\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u540c\u6b65\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4f5c\u4e3a\u4efb\u4f55PyTorch\u6a21\u578b\u7684\u5305\u88c5\u5668\u3002\u8fd9\u4e0eMultiprocessing\u5305\u63d0\u4f9b\u7684\u90a3\u79cd\u5e76\u884c\u6027\u4e0d\u540c\uff0ctorch.multiprocessing\u548ctorch.nn.DataParallel()\u652f\u6301\u591a\u4e2a\u8054\u7f51\u7684\u8ba1\u7b97\u673a\uff0c\u5e76\u4e14\u7528\u6237\u5fc5\u987b\u4e3a\u6bcf\u4e2a\u8fdb\u7a0b\u663e\u5f0f\u5730\u542f\u52a8\u4e3b\u8981\u8bad\u7ec3\u811a\u672c\u7684\u72ec\u7acb\u526f\u672c\u3002</p> <p>\u5728\u5355\u673a\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\uff0ctorch.distributed.deprecated\u6216\u8005torch.nn.parallel.deprecated.DistributedDataParallel()\u5305\u88c5\u5668\u4ecd\u6bd4\u5176\u4ed6\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u6709\u4f18\u52bf\uff0c\u5305\u62ectorch.nn.DataParallel(): * \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u7ef4\u62a4\u81ea\u5df1\u7684\u4f18\u5316\u5668\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u6267\u884c\u5b8c\u6574\u7684\u4f18\u5316\u6b65\u9aa4\u3002\u867d\u7136\u8fd9\u53ef\u80fd\u770b\u4f3c\u591a\u4f59\uff0c\u4f46\u7531\u4e8e\u68af\u5ea6\u5df2\u7ecf\u805a\u96c6\u5728\u4e00\u8d77\u5e76\u4e14\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5e73\u5747\uff0c\u56e0\u6b64\u5bf9\u4e8e\u6bcf\u4e2a\u8fc7\u7a0b\u90fd\u662f\u76f8\u540c\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u9700\u8981\u53c2\u6570\u5e7f\u64ad\u6b65\u9aa4\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5728\u8282\u70b9\u4e4b\u95f4\u4f20\u8f93\u5f20\u91cf\u6240\u82b1\u8d39\u7684\u65f6\u95f4 * \u6bcf\u4e2a\u8fdb\u7a0b\u90fd\u5305\u542b\u4e00\u4e2a\u72ec\u7acb\u7684Python\u89e3\u91ca\u5668\uff0c\u6d88\u9664\u4e86\u989d\u5916\u7684\u89e3\u91ca\u5668\u5f00\u9500\u4ee5\u53ca\u6765\u81ea\u5355\u4e2aPython\u8fdb\u7a0b\u9a71\u52a8\u591a\u4e2a\u6267\u884c\u5355\u5143\u3001\u6a21\u578b\u526f\u672c\u6216\u8005GPUs\u7684\u201cGIL-thrashing\u201d</p>"},{"location":"1.0/distributed_deprecated/#_2","title":"\u521d\u59cb\u5316","text":"<p>\u5728\u8c03\u7528\u4efb\u4f55\u5176\u4ed6\u65b9\u6cd5\u4e4b\u524d\uff0c\u9700\u8981\u4f7f\u7528torch.distributed.deprecated.init_process_group(\uff09\u51fd\u6570\u521d\u59cb\u5316\u5305\u3002\u8fd9\u5c06\u963b\u6b62\u6240\u6709\u8fdb\u7a0b\u52a0\u5165\u3002</p> <pre><code>torch.distributed.deprecated.init_process_group(backend, init_method='env://', **kwargs)\n</code></pre> <p>\u521d\u59cb\u5316\u5206\u5e03\u5f0f\u5305</p> <p>\u53c2\u6570\uff1a * backend(str)-\u5f85\u4f7f\u7528\u540e\u53f0\u7684\u540d\u5b57\u3002\u53d6\u51b3\u4e8e\u6784\u5efa\u65f6\u914d\u7f6e\u6709\u6548\u503c\uff0c\u5305\u62ec\uff1atco,mpi,gloo\u4ee5\u53canccl\u3002 * init_method(str,optional)-\u6307\u5b9a\u5982\u4f55\u521d\u59cb\u5316\u5305\u7684URL * world_size(int,optional)-\u53c2\u4e0e\u7684\u8fdb\u7a0b\u6570\u91cf * rank(int,optional)-\u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7 * group_name(str,optional)-\u7ec4\u540d\u3002\u53ef\u4ee5\u53c2\u8003\u521d\u59cb\u5316\u65b9\u6cd5\u7684\u63cf\u8ff0\u3002</p> <p>\u8bbe\u7f6ebackend == mpi\uff0c\u9700\u8981\u5728\u652f\u6301MPI\u7684\u7cfb\u7edf\u4e0a\u7528\u6e90\u7801\u6784\u5efa\u3002\u5982\u679c\u60a8\u60f3\u4f7f\u7528\u652f\u6301CUDA\u7684Open MPI\uff0c\u8bf7\u4f7f\u7528Open MPI\u4e3b\u8981\u7248\u672c2\u53ca\u66f4\u9ad8\u7248\u672c\u3002</p> <p>\u6ce8\u610f</p> <p>\u6b64\u65b9\u6cd5\u521d\u59cb\u5316CUDA\u4e0a\u4e0b\u6587\u3002 \u56e0\u6b64\uff0c\u5982\u679c\u591a\u4e2a\u8fdb\u7a0b\u5728\u5355\u4e2a\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u4f46\u4f7f\u7528\u4e0d\u540c\u7684GPU\uff0c\u8bf7\u786e\u4fdd\u5728\u6b64\u65b9\u6cd5\u4e4b\u524d\u4f7f\u7528torch.cuda.set_device(\uff09\u4ee5\u907f\u514d\u5728\u7b2c\u4e00\u4e2a\u53ef\u89c1\u8bbe\u5907\u4e0a\u4e0d\u5fc5\u8981\u5730\u521b\u5efa\u4e0a\u4e0b\u6587\u3002</p> <pre><code>torch.distributed.deprecated.get_rank()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7\u3002Rank\u662f\u5206\u914d\u7ed9\u5206\u5e03\u5f0f\u7ec4\u4e2d\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002 \u5b83\u4eec\u603b\u662f\u8fde\u7eed\u7684\u6574\u6570\uff0c\u8303\u56f4\u4ece0\u5230world_size  -  1(\u5305\u62ec\uff09\u3002</p> <pre><code>torch.distributed.deprecated.get_world_size()\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u5f0f\u7ec4\u4e2d\u8fdb\u7a0b\u7684\u6570\u91cf\u3002</p> <p>\u5f53\u524d\u652f\u6301\u4e09\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\uff1a</p>"},{"location":"1.0/distributed_deprecated/#tcp","title":"TCP\u521d\u59cb\u5316","text":"<p>\u6709\u4e24\u79cd\u4f7f\u7528TCP\u521d\u59cb\u5316\u7684\u65b9\u6cd5\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u9700\u8981\u4ece\u6240\u6709\u8fdb\u7a0b\u53ef\u4ee5\u8bbf\u95ee\u7684\u7f51\u7edc\u5730\u5740\u548c\u6240\u9700\u7684world_size\u3002 \u7b2c\u4e00\u79cd\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u5c5e\u4e8erank 0\u8fdb\u7a0b\u7684\u5730\u5740\u3002 \u6b64\u521d\u59cb\u5316\u65b9\u6cd5\u8981\u6c42\u6240\u6709\u8fdb\u7a0b\u90fd\u5177\u6709\u624b\u52a8\u6307\u5b9a\u7684\u7b49\u7ea7\u3002</p> <p>\u6216\u8005\uff0c\u5730\u5740\u5fc5\u987b\u662f\u6709\u6548\u7684IP\u591a\u64ad\u5730\u5740\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u81ea\u52a8\u5206\u914d\u7b49\u7ea7\u3002 \u591a\u64ad\u521d\u59cb\u5316\u8fd8\u652f\u6301group_name\u53c2\u6570\uff0c\u8be5\u53c2\u6570\u5141\u8bb8\u60a8\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5730\u5740\uff0c\u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u7ec4\u540d\u79f0\u5373\u53ef\u3002</p> <pre><code>import torch.distributed.deprecated as dist\n\n#Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456', rank=args.rank, world_size=4)\n\n#or a multicast address - rank will be assigned automatically if unspecified\ndist.init_process_group(backend,init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',world_size=4)\n</code></pre>"},{"location":"1.0/distributed_deprecated/#_3","title":"\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u521d\u59cb\u5316","text":"<p>\u53e6\u4e00\u79cd\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u4ece\u7ec4\u4e2d\u7684\u6240\u6709\u673a\u5668\u5171\u4eab\u548c\u53ef\u89c1\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u4ee5\u53ca\u671f\u671b\u7684world_size\u3002 URL\u5e94\u4ee5file\uff1a//\u5f00\u5934\uff0c\u5e76\u5305\u542b\u5171\u4eab\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4e0d\u5b58\u5728\u7684\u6587\u4ef6(\u5728\u73b0\u6709\u76ee\u5f55\u4e2d\uff09\u7684\u8def\u5f84\u3002 \u6b64\u521d\u59cb\u5316\u65b9\u6cd5\u8fd8\u652f\u6301group_name\u53c2\u6570\uff0c\u8be5\u53c2\u6570\u5141\u8bb8\u60a8\u4e3a\u591a\u4e2a\u4f5c\u4e1a\u4f7f\u7528\u76f8\u540c\u7684\u5171\u4eab\u6587\u4ef6\u8def\u5f84\uff0c\u53ea\u8981\u5b83\u4eec\u4f7f\u7528\u4e0d\u540c\u7684\u7ec4\u540d\u79f0\u5373\u53ef\u3002</p> <p>\u8b66\u544a</p> <p>\u6b64\u65b9\u6cd5\u5047\u5b9a\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\u4f7f\u7528fcntl\u8fdb\u884c\u9501\u5b9a - \u5927\u591a\u6570\u672c\u5730\u7cfb\u7edf\u548cNFS\u90fd\u652f\u6301\u5b83</p> <pre><code>import torch.distributed.deprecated as dist\n\n#Rank will be assigned automatically if unspecified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                    world_size=4, group_name=args.group)\n</code></pre>"},{"location":"1.0/distributed_deprecated/#_4","title":"\u73af\u5883\u53d8\u91cf\u521d\u59cb\u5316","text":"<p>\u6b64\u65b9\u6cd5\u5c06\u4ece\u73af\u5883\u53d8\u91cf\u4e2d\u8bfb\u53d6\u914d\u7f6e\uff0c\u4ece\u800c\u53ef\u4ee5\u5b8c\u5168\u81ea\u5b9a\u4e49\u4fe1\u606f\u7684\u83b7\u53d6\u65b9\u5f0f\u3002 \u8981\u8bbe\u7f6e\u7684\u53d8\u91cf\u662f\uff1a</p> <ul> <li>MASTER_PORT-\u5fc5\u8981\uff1b\u5fc5\u987b\u662f\u673a\u5668\u4e0a\u7684\u81ea\u7531\u7aef\u53e3\u4e14\u7b49\u7ea7\u4e3a0</li> <li>MASTER_ADDR-\u5fc5\u8981(\u9664\u975e\u7b49\u7ea7\u4e3a0\uff09\uff1b\u7b49\u7ea7\u4e3a0\u7684\u8282\u70b9\u7684\u5730\u5740</li> <li>WORLD_SIZE-\u5fc5\u8981\uff1b\u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u5728\u8c03\u7528\u521d\u59cb\u5316\u51fd\u6570\u4e2d</li> <li>RANK-\u5fc5\u8981\uff1b\u53ef\u4ee5\u5728\u8fd9\u91cc\u8bbe\u7f6e\uff0c\u4e5f\u53ef\u4ee5\u5728\u8c03\u7528\u521d\u59cb\u5316\u51fd\u6570\u4e2d</li> </ul> <p>\u7b49\u7ea7\u4e3a0\u7684\u673a\u5668\u5c06\u7528\u4e8e\u8bbe\u7f6e\u6240\u6709\u8fde\u63a5\u3002</p> <p>\u8fd9\u662f\u9ed8\u8ba4\u65b9\u6cd5\uff0c\u8fd9\u610f\u5473\u7740\u4e0d\u5fc5\u6307\u5b9ainit_method(\u6216\u8005\u53ef\u4ee5\u662fenv\uff1a//\uff09\u3002</p>"},{"location":"1.0/distributed_deprecated/#_5","title":"\u7ec4","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u96c6\u5408\u4f53\u5728\u9ed8\u8ba4\u7ec4(\u4e5f\u79f0\u4e3a\u4e16\u754c\uff09\u4e0a\u8fd0\u884c\uff0c\u5e76\u8981\u6c42\u6240\u6709\u8fdb\u7a0b\u90fd\u8fdb\u5165\u5206\u5e03\u5f0f\u51fd\u6570\u8c03\u7528\u3002\u4f46\u662f\uff0c\u4e00\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u4ee5\u4ece\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u4e2d\u53d7\u76ca\u3002 \u8fd9\u662f\u5206\u5e03\u5f0f\u7fa4\u4f53\u53d1\u6325\u4f5c\u7528\u7684\u5730\u65b9\u3002new_group()\u51fd\u6570\u53ef\u4ee5\u7528\u6765\u521b\u5efa\u5177\u6709\u6240\u6709\u8fdb\u7a0b\u7684\u4efb\u610f\u5b50\u96c6\u7684\u65b0\u7ec4\u3002\u5b83\u8fd4\u56de\u4e00\u4e2a\u4e0d\u900f\u660e\u7684\u7ec4\u53e5\u67c4\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u6240\u6709\u96c6\u5408\u4f53\u7684\u7ec4\u53c2\u6570\u7ed9\u51fa(\u96c6\u5408\u4f53\u662f\u5206\u5e03\u5f0f\u51fd\u6570\uff0c\u7528\u4e8e\u5728\u67d0\u4e9b\u4f17\u6240\u5468\u77e5\u7684\u7f16\u7a0b\u6a21\u5f0f\u4e2d\u4ea4\u6362\u4fe1\u606f\uff09\u3002</p> <pre><code>torch.distributed.deprecated.new_group(ranks=None)\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5f0f\u7ec4\u3002</p> <p>\u6b64\u529f\u80fd\u8981\u6c42\u4e3b\u7ec4\u4e2d\u7684\u6240\u6709\u8fdb\u7a0b(\u5373\uff0c\u4f5c\u4e3a\u5206\u5e03\u5f0f\u4f5c\u4e1a\u4e00\u90e8\u5206\u7684\u6240\u6709\u8fdb\u7a0b\uff09\u90fd\u8fdb\u5165\u6b64\u529f\u80fd\uff0c\u5373\u4f7f\u5b83\u4eec\u4e0d\u662f\u8be5\u7ec4\u7684\u6210\u5458\u4e5f\u662f\u5982\u6b64\u3002 \u6b64\u5916\uff0c\u5e94\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u4ee5\u76f8\u540c\u7684\u987a\u5e8f\u521b\u5efa\u7ec4\u3002</p> <p></p>"},{"location":"1.0/distributed_deprecated/#_6","title":"\u70b9\u5230\u70b9\u901a\u8baf","text":"<pre><code>torch.distributed.deprecated.send(tensor, dst)\n</code></pre> <p>\u540c\u6b65\u53d1\u9001\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor)-\u63a5\u53d7\u6570\u636e\u7684\u5f20\u91cf</li> <li> <p>dst(int)-\u76ee\u7684\u7b49\u7ea7</p> <p>orch.distributed.deprecated.recv(tensor, src=None)</p> </li> </ul> <p>\u540c\u6b65\u63a5\u6536\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor)-\u63a5\u6536\u6570\u636e\u7684\u5f20\u91cf</li> <li>src(int,optional)-\u6e90\u7b49\u7ea7\uff0c\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5c06\u4f1a\u63a5\u53d7\u4efb\u610f\u8fdb\u7a0b\u7684\u6570\u636e</li> </ul> <p></p> <p>isend(\uff09\u548cirecv(\uff09\u5728\u4f7f\u7528\u65f6\u8fd4\u56de\u5206\u5e03\u5f0f\u8bf7\u6c42\u5bf9\u8c61\u3002 \u901a\u5e38\uff0c\u6b64\u5bf9\u8c61\u7684\u7c7b\u578b\u672a\u6307\u5b9a\uff0c\u56e0\u4e3a\u5b83\u4eec\u6c38\u8fdc\u4e0d\u5e94\u624b\u52a8\u521b\u5efa\uff0c\u4f46\u5b83\u4eec\u4fdd\u8bc1\u652f\u6301\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ul> <li>is_completed()-\u64cd\u4f5c\u5b8c\u6210\u8fd4\u56de\u771f</li> <li>wait()-\u5c06\u963b\u6b62\u8be5\u8fc7\u7a0b\uff0c\u76f4\u5230\u64cd\u4f5c\u5b8c\u6210\u3002 is_completed(\uff09\u4fdd\u8bc1\u4e00\u65e6\u8fd4\u56de\u5c31\u8fd4\u56deTrue\u3002</li> </ul> <p>\u5f53\u4f7f\u7528MPI\u540e\u53f0\u65f6\uff0cisend()\u548cirecv()\u652f\u6301\u975e\u63d2\u961f\u7279\u6027\uff0c\u8fd9\u6837\u53ef\u4ee5\u4fdd\u8bc1\u4fe1\u606f\u7684\u987a\u5e8f\u3002\u5173\u4e8e\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u8bbf\u95ee http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54</p> <pre><code>torch.distributed.deprecated.isend(tensor, dst)\n</code></pre> <p>\u5f02\u6b65\u53d1\u9001\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor)-\u53d1\u9001\u7684\u5f20\u91cf</li> <li>dst(int)-\u76ee\u7684\u7b49\u7ea7</li> </ul> <p></p> <pre><code>torch.distributed.deprecated.irecv(tensor, src)\n</code></pre> <p>\u5f02\u6b65\u63a5\u6536\u5f20\u91cf</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor)-\u63a5\u6536\u6570\u636e\u7684\u5f20\u91cf</li> <li>src(int)-\u6e90\u7b49\u7ea7</li> </ul> <p></p>"},{"location":"1.0/distributed_deprecated/#_7","title":"\u96c6\u4f53\u51fd\u6570","text":"<pre><code>torch.distributed.deprecated.broadcast(tensor, src, group=&lt;object object&gt;)\n</code></pre> <p>\u5c06\u5f20\u91cf\u5e7f\u64ad\u5230\u6574\u4e2a\u7ec4\u3002</p> <p>tensor\u5fc5\u987b\u5728\u53c2\u4e0e\u96c6\u5408\u4f53\u7684\u6240\u6709\u8fc7\u7a0b\u4e2d\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor)-\u5982\u679csrc\u662f\u5f53\u524d\u8fdb\u7a0b\u7684\u7b49\u7ea7\uff0c\u5219\u53d1\u9001\u6570\u636e\uff0c\u5426\u5219\u5f20\u91cf\u5219\u7528\u4e8e\u4fdd\u5b58\u63a5\u6536\u7684\u6570\u636e\u3002</li> <li>src(int)-\u6e90\u7b49\u7ea7</li> <li> <p>group(optional)-\u6574\u4f53\u7684\u7ec4</p> <p>torch.distributed.deprecated.all_reduce(tensor, op="},{"location":"1.0/distributed_deprecated/#gpu","title":"Distributed communication package (deprecated) - torch.distributed.deprecated","text":""},{"location":"1.0/distributed_deprecated/#_8","title":"Distributed communication package (deprecated) - torch.distributed.deprecated","text":""},{"location":"1.0/distributed_deprecated/#_9","title":"Distributed communication package (deprecated) - torch.distributed.deprecated","text":""},{"location":"1.0/distributions/","title":"\u6982\u7387\u5206\u5e03 - torch.distributions","text":"<p>\u8bd1\u8005\uff1ahijkzzz</p> <p><code>distributions</code> \u5305\u542b\u53ef\u53c2\u6570\u5316\u7684\u6982\u7387\u5206\u5e03\u548c\u91c7\u6837\u51fd\u6570. \u8fd9\u5141\u8bb8\u6784\u9020\u7528\u4e8e\u4f18\u5316\u7684\u968f\u673a\u8ba1\u7b97\u56fe\u548c\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\u5668.  \u8fd9\u4e2a\u5305\u4e00\u822c\u9075\u5faa TensorFlow Distributions \u5305\u7684\u8bbe\u8ba1.</p> <p>\u901a\u5e38, \u4e0d\u53ef\u80fd\u76f4\u63a5\u901a\u8fc7\u968f\u673a\u6837\u672c\u53cd\u5411\u4f20\u64ad.  \u4f46\u662f, \u6709\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\u53ef\u521b\u5efa\u53ef\u4ee5\u53cd\u5411\u4f20\u64ad\u7684\u4ee3\u7406\u51fd\u6570.  \u5373\u5f97\u5206\u51fd\u6570\u4f30\u8ba1\u5668/\u4f3c\u7136\u6bd4\u4f30\u8ba1\u5668/REINFORCE\u548cpathwise derivative\u4f30\u8ba1\u5668.  REINFORCE\u901a\u5e38\u88ab\u89c6\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u57fa\u7840, \u5e76\u4e14pathwise derivative\u4f30\u8ba1\u5668\u5e38\u89c1\u4e8e\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u4e2d\u7684\u91cd\u65b0\u53c2\u6570\u5316\u6280\u5de7. \u5f97\u5206\u51fd\u6570\u4ec5\u9700\u8981\u6837\u672c\u7684\u503c , pathwise derivative \u9700\u8981\u5bfc\u6570 . \u63a5\u4e0b\u6765\u7684\u90e8\u5206\u5c06\u5728\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u793a\u4f8b\u4e2d\u8ba8\u8bba\u8fd9\u4e24\u4e2a\u95ee\u9898.  \u6709\u5173\u8be6\u7ec6\u4fe1\u606f, \u8bf7\u53c2\u9605 Gradient Estimation Using Stochastic Computation Graphs .</p>"},{"location":"1.0/distributions/#_1","title":"\u5f97\u5206\u51fd\u6570","text":"<p>\u5f53\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u76f8\u5bf9\u4e8e\u5176\u53c2\u6570\u53ef\u5fae\u5206\u65f6, \u6211\u4eec\u53ea\u9700\u8981<code>sample()</code>\u548c<code>log_prob()</code>\u6765\u5b9e\u73b0REINFORCE:</p> <p></p> <p> \u662f\u53c2\u6570,  \u662f\u5b66\u4e60\u901f\u7387,  \u662f\u5956\u52b1 \u5e76\u4e14  \u662f\u5728\u72b6\u6001  \u4ee5\u53ca\u7ed9\u5b9a\u7b56\u7565 \u6267\u884c\u52a8\u4f5c  \u7684\u6982\u7387.</p> <p>\u5728\u5b9e\u8df5\u4e2d, \u6211\u4eec\u5c06\u4ece\u7f51\u7edc\u8f93\u51fa\u4e2d\u91c7\u6837\u4e00\u4e2a\u52a8\u4f5c, \u5c06\u8fd9\u4e2a\u52a8\u4f5c\u5e94\u7528\u4e8e\u4e00\u4e2a\u73af\u5883\u4e2d, \u7136\u540e\u4f7f\u7528<code>log_prob</code>\u6784\u9020\u4e00\u4e2a\u7b49\u6548\u7684\u635f\u5931\u51fd\u6570. \u8bf7\u6ce8\u610f, \u6211\u4eec\u4f7f\u7528\u8d1f\u6570\u662f\u56e0\u4e3a\u4f18\u5316\u5668\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d, \u800c\u4e0a\u9762\u7684\u89c4\u5219\u5047\u8bbe\u68af\u5ea6\u4e0a\u5347. \u6709\u4e86\u786e\u5b9a\u7684\u7b56\u7565, REINFORCE\u7684\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b:</p> <pre><code>probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n\n</code></pre>"},{"location":"1.0/distributions/#pathwise-derivative","title":"Pathwise derivative","text":"<p>\u5b9e\u73b0\u8fd9\u4e9b\u968f\u673a/\u7b56\u7565\u68af\u5ea6\u7684\u53e6\u4e00\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528\u6765\u81ea<code>rsample()</code>\u65b9\u6cd5\u7684\u91cd\u65b0\u53c2\u6570\u5316\u6280\u5de7, \u5176\u4e2d\u53c2\u6570\u5316\u968f\u673a\u53d8\u91cf\u53ef\u4ee5\u901a\u8fc7\u65e0\u53c2\u6570\u968f\u673a\u53d8\u91cf\u7684\u53c2\u6570\u786e\u5b9a\u6027\u51fd\u6570\u6784\u9020.  \u56e0\u6b64, \u91cd\u65b0\u53c2\u6570\u5316\u7684\u6837\u672c\u53d8\u5f97\u53ef\u5fae\u5206.  \u5b9e\u73b0Pathwise derivative\u7684\u4ee3\u7801\u5982\u4e0b:</p> <pre><code>params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n\n</code></pre>"},{"location":"1.0/distributions/#_2","title":"\u5206\u5e03","text":"<pre><code>class torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>object</code></p> <p>Distribution\u662f\u6982\u7387\u5206\u5e03\u7684\u62bd\u8c61\u57fa\u7c7b.</p> <pre><code>arg_constraints\n</code></pre> <p>\u4ece\u53c2\u6570\u540d\u79f0\u8fd4\u56de\u5b57\u5178\u5230 <code>Constraint</code> \u5bf9\u8c61(\u5e94\u8be5\u6ee1\u8db3\u8fd9\u4e2a\u5206\u5e03\u7684\u6bcf\u4e2a\u53c2\u6570\uff09.\u4e0d\u662f\u5f20\u91cf\u7684arg\u4e0d\u9700\u8981\u51fa\u73b0\u5728\u8fd9\u4e2a\u5b57\u5178\u4e2d.</p> <pre><code>batch_shape\n</code></pre> <p>\u8fd4\u56de\u6279\u91cf\u53c2\u6570\u7684\u5f62\u72b6.</p> <pre><code>cdf(value)\n</code></pre> <p>\u8fd4\u56de<code>value</code>\u5904\u7684\u7d2f\u79ef\u5bc6\u5ea6/\u8d28\u91cf\u51fd\u6570\u4f30\u8ba1.</p> <p>| \u53c2\u6570: | value (Tensor) \u2013 |</p> <pre><code>entropy()\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u7684\u71b5, \u6279\u91cf\u7684\u5f62\u72b6\u4e3a batch_shape.</p> <p>| \u8fd4\u56de\u503c: | Tensor \u5f62\u72b6\u4e3a batch_shape. |</p> <pre><code>enumerate_support(expand=True)\n</code></pre> <p>\u8fd4\u56de\u5305\u542b\u79bb\u6563\u5206\u5e03\u652f\u6301\u7684\u6240\u6709\u503c\u7684\u5f20\u91cf. \u7ed3\u679c\u5c06\u5728\u7ef4\u5ea60\u4e0a\u679a\u4e3e, \u6240\u4ee5\u7ed3\u679c\u7684\u5f62\u72b6\u5c06\u662f <code>(cardinality,) + batch_shape + event_shape</code> (\u5bf9\u4e8e\u5355\u53d8\u91cf\u5206\u5e03 <code>event_shape = ()</code>).</p> <p>\u6ce8\u610f, \u8fd9\u5728lock-step\u4e2d\u679a\u4e3e\u4e86\u6240\u6709\u6279\u5904\u7406\u5f20\u91cf<code>[[0, 0], [1, 1], \u2026]</code>. \u5f53 <code>expand=False</code>, \u679a\u4e3e\u6cbf\u7740\u7ef4\u5ea6 0\u8fdb\u884c, \u4f46\u662f\u5269\u4e0b\u7684\u6279\u5904\u7406\u7ef4\u5ea6\u662f\u5355\u7ef4\u5ea6, <code>[[0], [1], ..</code>.</p> <p>\u904d\u5386\u6574\u4e2a\u7b1b\u5361\u5c14\u79ef\u7684\u4f7f\u7528 <code>itertools.product(m.enumerate_support())</code>.</p> <p>| \u53c2\u6570: | expand (bool) \u2013 \u662f\u5426\u6269\u5c55\u5bf9\u6279\u5904\u7406dim\u7684\u652f\u6301\u4ee5\u5339\u914d\u5206\u5e03\u7684 <code>batch_shape</code>. |</p> <p>| \u8fd4\u56de\u503c: | \u5f20\u91cf\u5728\u7ef4\u4e0a0\u8fed\u4ee3. |</p> <pre><code>event_shape\n</code></pre> <p>\u8fd4\u56de\u5355\u4e2a\u6837\u672c\u7684\u5f62\u72b6 (\u975e\u6279\u91cf).</p> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5b9e\u4f8b(\u6216\u586b\u5145\u6d3e\u751f\u7c7b\u63d0\u4f9b\u7684\u73b0\u6709\u5b9e\u4f8b), \u5176\u6279\u5904\u7406\u7ef4\u5ea6\u6269\u5c55\u4e3a <code>batch_shape</code>.  \u8fd9\u4e2a\u65b9\u6cd5\u8c03\u7528 <code>expand</code> \u5728\u5206\u5e03\u7684\u53c2\u6570\u4e0a. \u56e0\u6b64, \u8fd9\u4e0d\u4f1a\u4e3a\u6269\u5c55\u7684\u5206\u5e03\u5b9e\u4f8b\u5206\u914d\u65b0\u7684\u5185\u5b58.  \u6b64\u5916, \u7b2c\u4e00\u6b21\u521b\u5efa\u5b9e\u4f8b\u65f6, \u8fd9\u4e0d\u4f1a\u5728\u4e2d\u91cd\u590d\u4efb\u4f55\u53c2\u6570\u68c0\u67e5\u6216\u53c2\u6570\u5e7f\u64ad\u5728 <code>__init__.py</code>.</p> <p>\u53c2\u6570: </p> <ul> <li>batch_shape (torch.Size) \u2013 \u6240\u9700\u7684\u6269\u5c55\u5c3a\u5bf8.</li> <li>_instance \u2013 \u7531\u9700\u8981\u91cd\u5199<code>.expand</code>\u7684\u5b50\u7c7b\u63d0\u4f9b\u7684\u65b0\u5b9e\u4f8b.</li> </ul> <p>| \u8fd4\u56de\u503c: | \u6279\u5904\u7406\u7ef4\u5ea6\u6269\u5c55\u4e3a<code>batch_size</code>\u7684\u65b0\u5206\u5e03\u5b9e\u4f8b. |</p> <pre><code>icdf(value)\n</code></pre> <p>\u8fd4\u56de\u6309<code>value</code>\u8ba1\u7b97\u7684\u53cd\u5411\u7d2f\u79ef\u5bc6\u5ea6/\u8d28\u91cf\u51fd\u6570.</p> <p>| \u53c2\u6570: | value (Tensor) \u2013 |</p> <pre><code>log_prob(value)\n</code></pre> <p>\u8fd4\u56de\u6309<code>value</code>\u8ba1\u7b97\u7684\u6982\u7387\u5bc6\u5ea6/\u8d28\u91cf\u51fd\u6570\u7684\u5bf9\u6570.</p> <p>| \u53c2\u6570: | value (Tensor) \u2013 |</p> <pre><code>mean\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u7684\u5e73\u5747\u503c.</p> <pre><code>perplexity()\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u7684\u56f0\u60d1\u5ea6, \u6279\u91cf\u7684\u5173\u4e8e batch_shape.</p> <p>| \u8fd4\u56de\u503c: | \u5f62\u72b6\u4e3a batch_shape \u7684\u5f20\u91cf. |</p> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <p>\u5982\u679c\u5206\u5e03\u7684\u53c2\u6570\u662f\u6279\u91cf\u7684, \u5219\u751f\u6210sample_shape\u5f62\u72b6\u7684\u91cd\u65b0\u53c2\u6570\u5316\u6837\u672c\u6216sample_shape\u5f62\u72b6\u7684\u6279\u91cf\u91cd\u65b0\u53c2\u6570\u5316\u6837\u672c.</p> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <p>\u5982\u679c\u5206\u5e03\u7684\u53c2\u6570\u662f\u6279\u91cf\u7684, \u5219\u751f\u6210sample_shape\u5f62\u72b6\u7684\u6837\u672c\u6216sample_shape\u5f62\u72b6\u7684\u6279\u91cf\u6837\u672c.</p> <pre><code>sample_n(n)\n</code></pre> <p>\u5982\u679c\u5206\u5e03\u53c2\u6570\u662f\u5206\u6279\u7684, \u5219\u751f\u6210n\u4e2a\u6837\u672c\u6216n\u6279\u6837\u672c.</p> <pre><code>stddev\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u7684\u6807\u51c6\u5dee.</p> <pre><code>support\n</code></pre> <p>\u8fd4\u56de<code>Constraint</code> \u5bf9\u8c61\u8868\u793a\u8be5\u5206\u5e03\u7684\u652f\u6301.</p> <pre><code>variance\n</code></pre> <p>\u8fd4\u56de\u5206\u5e03\u7684\u65b9\u5dee.</p>"},{"location":"1.0/distributions/#exponentialfamily","title":"ExponentialFamily","text":"<pre><code>class torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u6307\u6570\u65cf\u662f\u6307\u6570\u65cf\u6982\u7387\u5206\u5e03\u7684\u62bd\u8c61\u57fa\u7c7b, \u5176\u6982\u7387\u8d28\u91cf/\u5bc6\u5ea6\u51fd\u6570\u7684\u5f62\u5f0f\u5b9a\u4e49\u5982\u4e0b</p> <p></p> <p> \u8868\u793a\u81ea\u7136\u53c2\u6570,  \u8868\u793a\u5145\u5206\u7edf\u8ba1\u91cf,  \u662f\u7ed9\u5b9a\u65cf\u7684\u5bf9\u6570\u5f52\u4e00\u5316\u51fd\u6570   \u662fcarrier measure.</p> <p>\u6ce8\u610f</p> <p>\u8be5\u7c7b\u662f<code>Distribution</code>\u7c7b\u4e0e\u6307\u6570\u65cf\u5206\u5e03\u4e4b\u95f4\u7684\u4e2d\u4ecb, \u4e3b\u8981\u7528\u4e8e\u68c0\u9a8c<code>.entropy()</code>\u548c\u89e3\u6790KL\u6563\u5ea6\u65b9\u6cd5\u7684\u6b63\u786e\u6027. \u6211\u4eec\u4f7f\u7528\u8fd9\u4e2a\u7c7b\u6765\u8ba1\u7b97\u71b5\u548cKL\u6563\u5ea6\u4f7f\u7528AD\u6846\u67b6\u548cBregman\u6563\u5ea6 (\u51fa\u81ea: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).</p> <pre><code>entropy()\n</code></pre> <p>\u5229\u7528\u5bf9\u6570\u5f52\u4e00\u5316\u5668\u7684Bregman\u6563\u5ea6\u8ba1\u7b97\u71b5\u7684\u65b9\u6cd5.</p>"},{"location":"1.0/distributions/#bernoulli","title":"Bernoulli","text":"<pre><code>class torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u53c2\u6570\u5316\u7684\u4f2f\u52aa\u5229\u5206\u5e03, \u6839\u636e <code>probs</code> \u6216\u8005 <code>logits</code> (\u4f46\u4e0d\u662f\u540c\u65f6\u90fd\u6709).</p> <p>\u6837\u672c\u662f\u4e8c\u503c\u7684 (0 \u6216\u8005 1). \u53d6\u503c <code>1</code> \u4f34\u968f\u6982\u7387 <code>p</code> , \u6216\u8005 <code>0</code> \u4f34\u968f\u6982\u7387 <code>1 - p</code>.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Bernoulli(torch.tensor([0.3]))\n&gt;&gt;&gt; m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>probs (Number__, Tensor) \u2013 the probabilty of sampling <code>1</code></li> <li>logits (Number__, Tensor) \u2013 the log-odds of sampling <code>1</code></li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>enumerate_support(expand=True)\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_enumerate_support = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = Boolean()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#beta","title":"Beta","text":"<pre><code>class torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>Beta \u5206\u5e03, \u53c2\u6570\u4e3a <code>concentration1</code> \u548c <code>concentration0</code>.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n&gt;&gt;&gt; m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>concentration1 (float or Tensor) \u2013 \u5206\u5e03\u7684\u7b2c\u4e00\u4e2a\u6d53\u5ea6\u53c2\u6570(\u901a\u5e38\u79f0\u4e3aalpha\uff09</li> <li>concentration0 (float or Tensor) \u2013 \u5206\u5e03\u7684\u7b2c\u4e8c\u4e2a\u6d53\u5ea6\u53c2\u6570(\u901a\u5e38\u79f0\u4e3abeta)</li> </ul> <pre><code>arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>concentration0\n</code></pre> <pre><code>concentration1\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=())\n</code></pre> <pre><code>support = Interval(lower_bound=0.0, upper_bound=1.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#binomial","title":"Binomial","text":"<pre><code>class torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u4e00\u4e2aBinomial \u5206\u5e03, \u53c2\u6570\u4e3a <code>total_count</code> \u548c <code>probs</code> \u6216\u8005 <code>logits</code> (\u4f46\u4e0d\u662f\u540c\u65f6\u90fd\u6709\u4f7f\u7528). <code>total_count</code> \u5fc5\u987b\u548c [<code>probs</code>] \u4e4b\u95f4\u53ef\u5e7f\u64ad(#torch.distributions.binomial.Binomial.probs \"torch.distributions.binomial.Binomial.probs\")/<code>logits</code>.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n&gt;&gt;&gt; x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n&gt;&gt;&gt; m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n&gt;&gt;&gt; x = m.sample()\ntensor([[ 4.,  5.],\n [ 7.,  6.]])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>total_count (int or Tensor) \u2013 \u4f2f\u52aa\u5229\u8bd5\u9a8c\u6b21\u6570</li> <li>probs (Tensor) \u2013 \u4e8b\u4ef6\u6982\u7387</li> <li>logits (Tensor) \u2013 \u4e8b\u4ef6 log-odds</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}\n</code></pre> <pre><code>enumerate_support(expand=True)\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_enumerate_support = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#categorical","title":"Categorical","text":"<pre><code>class torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u4e00\u4e2a categorical \u5206\u5e03, \u53c2\u6570\u4e3a <code>probs</code> \u6216\u8005 <code>logits</code> (\u4f46\u4e0d\u662f\u540c\u65f6\u90fd\u6709).</p> <p>\u6ce8\u610f</p> <p>\u5b83\u7b49\u4ef7\u4e8e\u4ece <code>torch.multinomial()</code> \u7684\u91c7\u6837.</p> <p>\u6837\u672c\u662f\u6574\u6570\u6765\u81ea <code>K</code> \u662f <code>probs.size(-1)</code>.</p> <p>\u5982\u679c <code>probs</code> \u662f 1D \u7684, \u957f\u5ea6\u4e3a<code>K</code>, \u6bcf\u4e2a\u5143\u7d20\u662f\u5728\u8be5\u7d22\u5f15\u5904\u5bf9\u7c7b\u8fdb\u884c\u62bd\u6837\u7684\u76f8\u5bf9\u6982\u7387.</p> <p>\u5982\u679c <code>probs</code> \u662f 2D \u7684, \u5b83\u88ab\u89c6\u4e3a\u4e00\u7ec4\u76f8\u5bf9\u6982\u7387\u5411\u91cf.</p> <p>\u6ce8\u610f</p> <p><code>probs</code>  \u5fc5\u987b\u662f\u975e\u8d1f\u7684\u3001\u6709\u9650\u7684\u5e76\u4e14\u5177\u6709\u975e\u96f6\u548c, \u5e76\u4e14\u5b83\u5c06\u88ab\u5f52\u4e00\u5316\u4e3a\u548c\u4e3a1.</p> <p>\u8bf7\u53c2\u9605: <code>torch.multinomial()</code></p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>probs (Tensor) \u2013 event probabilities</li> <li>logits (Tensor) \u2013 event log probabilities</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Simplex()}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>enumerate_support(expand=True)\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_enumerate_support = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#cauchy","title":"Cauchy","text":"<pre><code>class torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u6837\u672c\u6765\u81ea\u67ef\u897f(\u6d1b\u4f26\u5179)\u5206\u5e03. \u5747\u503c\u4e3a0\u7684\u72ec\u7acb\u6b63\u6001\u5206\u5e03\u968f\u673a\u53d8\u91cf\u4e4b\u6bd4\u670d\u4ece\u67ef\u897f\u5206\u5e03. </p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (float or Tensor) \u2013 \u5206\u5e03\u7684\u6a21\u6001\u6216\u4e2d\u503c.</li> <li>scale (float or Tensor) \u2013 half width at half maximum.</li> </ul> <pre><code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(value)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#chi2","title":"Chi2","text":"<pre><code>class torch.distributions.chi2.Chi2(df, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.gamma.Gamma</code></p> <p>\u521b\u5efa\u7531\u5f62\u72b6\u53c2\u6570<code>df</code>\u53c2\u6570\u5316\u7684Chi2\u5206\u5e03.  \u8fd9\u5b8c\u5168\u7b49\u540c\u4e8e <code>Gamma(alpha=0.5*df, beta=0.5)</code></p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Chi2(torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n\n</code></pre> <p>| \u53c2\u6570: | df (float or Tensor) \u2013 \u5206\u5e03\u7684\u5f62\u72b6\u53c2\u6570 |</p> <pre><code>arg_constraints = {'df': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>df\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre>"},{"location":"1.0/distributions/#dirichlet","title":"Dirichlet","text":"<pre><code>class torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u4e00\u4e2a Dirichlet \u5206\u5e03, \u53c2\u6570\u4e3a<code>concentration</code>.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Dirichlet(torch.tensor([0.5, 0.5]))\n&gt;&gt;&gt; m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n\n</code></pre> <p>| \u53c2\u6570: | concentration (Tensor) \u2013  \u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570(\u901a\u5e38\u79f0\u4e3aalpha\uff09 |</p> <pre><code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=())\n</code></pre> <pre><code>support = Simplex()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#exponential","title":"Exponential","text":"<pre><code>class torch.distributions.exponential.Exponential(rate, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u7531<code>rate</code>\u53c2\u6570\u5316\u7684\u6307\u6570\u5206\u5e03.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Exponential(torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n\n</code></pre> <p>| \u53c2\u6570: | rate (float or Tensor) \u2013 rate = 1 / \u5206\u5e03\u7684scale  |</p> <pre><code>arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(value)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>stddev\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#fishersnedecor","title":"FisherSnedecor","text":"<pre><code>class torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u7531<code>df1</code>\u548c<code>df2</code>\u53c2\u6570\u5316\u7684Fisher-Snedecor\u5206\u5e03</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n&gt;&gt;&gt; m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>df1 (float or Tensor) \u2013  \u81ea\u7531\u5ea6\u53c2\u65701</li> <li>df2 (float or Tensor) \u2013 \u81ea\u7531\u5ea6\u53c2\u65702</li> </ul> <pre><code>arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#gamma","title":"Gamma","text":"<pre><code>class torch.distributions.gamma.Gamma(concentration, rate, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u7531<code>concentration</code>\u548c<code>rate</code>\u53c2\u6570\u5316\u7684\u4f3d\u9a6c\u5206\u5e03. .</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>concentration (float or Tensor) \u2013 \u5206\u5e03\u7684\u5f62\u72b6\u53c2\u6570(\u901a\u5e38\u79f0\u4e3aalpha\uff09</li> <li>rate (float or Tensor) \u2013 rate = 1 /  \u5206\u5e03scale (\u901a\u5e38\u79f0\u4e3abeta )</li> </ul> <pre><code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#geometric","title":"Geometric","text":"<pre><code>class torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u7531<code>probs</code>\u53c2\u6570\u5316\u7684\u51e0\u4f55\u5206\u5e03, \u5176\u4e2d<code>probs</code>\u662f\u4f2f\u52aa\u5229\u8bd5\u9a8c\u6210\u529f\u7684\u6982\u7387. \u5b83\u8868\u793a\u6982\u7387\u5728  \u6b21\u4f2f\u52aa\u5229\u8bd5\u9a8c\u4e2d,  \u524d  \u8bd5\u9a8c\u5931\u8d25, \u7136\u540e\u6210\u529f.</p> <p>\u6837\u672c\u662f\u975e\u8d1f\u6574\u6570 [0, ).</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Geometric(torch.tensor([0.3]))\n&gt;&gt;&gt; m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>probs (Number__, Tensor) \u2013  \u62bd\u6837<code>1</code>\u7684\u6982\u7387 . \u5fc5\u987b\u662f\u5728\u8303\u56f4 (0, 1]</li> <li>logits (Number__, Tensor) \u2013 \u62bd\u6837 <code>1</code>\u7684log-odds.</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = IntegerGreaterThan(lower_bound=0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#gumbel","title":"Gumbel","text":"<pre><code>class torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u6765\u81eaGumbel\u5206\u5e03\u7684\u6837\u672c.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n&gt;&gt;&gt; m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (float or Tensor) \u2013  \u5206\u5e03\u7684\u4f4d\u7f6e\u53c2\u6570</li> <li>scale (float or Tensor) \u2013  \u5206\u5e03\u7684scale \u53c2\u6570</li> </ul> <pre><code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>stddev\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#halfcauchy","title":"HalfCauchy","text":"<pre><code>class torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u521b\u5efa<code>scale</code>\u53c2\u6570\u5316\u7684\u534a\u6b63\u6001\u5206\u5e03:</p> <pre><code>X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n\n</code></pre> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = HalfCauchy(torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n\n</code></pre> <p>| \u53c2\u6570: | scale (float or Tensor) \u2013 \u5b8c\u5168\u67ef\u897f\u5206\u5e03\u7684scale |</p> <pre><code>arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(prob)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>scale\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#halfnormal","title":"HalfNormal","text":"<pre><code>class torch.distributions.half_normal.HalfNormal(scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u521b\u5efa\u6309<code>scale</code>\u53c2\u6570\u5316\u7684\u534a\u6b63\u6001\u5206\u5e03:</p> <pre><code>X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n\n</code></pre> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = HalfNormal(torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n\n</code></pre> <p>| \u53c2\u6570: | scale (float or Tensor) \u2013 \u5b8c\u5168\u6b63\u6001\u5206\u5e03\u7684scale |</p> <pre><code>arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(prob)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>scale\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#independent","title":"Independent","text":"<pre><code>class torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u91cd\u65b0\u89e3\u91ca\u4e00\u4e9b\u5206\u5e03\u7684\u6279\u91cf dims \u4f5c\u4e3a event dims.</p> <p>\u8fd9\u4e3b\u8981\u7528\u4e8e\u6539\u53d8<code>log_prob()</code>\u7ed3\u679c\u7684\u5f62\u72b6.\u4f8b\u5982, \u8981\u521b\u5efa\u4e0e\u591a\u5143\u6b63\u6001\u5206\u5e03\u5f62\u72b6\u76f8\u540c\u7684\u5bf9\u89d2\u6b63\u6001\u5206\u5e03(\u56e0\u6b64\u5b83\u4eec\u662f\u53ef\u4e92\u6362\u7684), \u60a8\u53ef\u4ee5\u8fd9\u6837\u505a:</p> <pre><code>&gt;&gt;&gt; loc = torch.zeros(3)\n&gt;&gt;&gt; scale = torch.ones(3)\n&gt;&gt;&gt; mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n&gt;&gt;&gt; [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n&gt;&gt;&gt; normal = Normal(loc, scale)\n&gt;&gt;&gt; [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n&gt;&gt;&gt; diagn = Independent(normal, 1)\n&gt;&gt;&gt; [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>base_distribution (torch.distributions.distribution.Distribution) \u2013 \u57fa\u7840\u5206\u5e03</li> <li>reinterpreted_batch_ndims (int) \u2013\u8981\u91cd\u89e3\u91ca\u7684\u6279\u91cfdims\u7684\u6570\u91cf</li> </ul> <pre><code>arg_constraints = {}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>enumerate_support(expand=True)\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_enumerate_support\n</code></pre> <pre><code>has_rsample\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#laplace","title":"Laplace","text":"<pre><code>class torch.distributions.laplace.Laplace(loc, scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u53c2\u6570\u5316\u7684\u62c9\u666e\u62c9\u65af\u5206\u5e03, \u53c2\u6570\u662f <code>loc</code> \u548c :attr:'scale'.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (float or Tensor) \u2013 \u5206\u5e03\u5747\u503c</li> <li>scale (float or Tensor) \u2013 \u5206\u5e03scale</li> </ul> <pre><code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(value)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>stddev\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#lognormal","title":"LogNormal","text":"<pre><code>class torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u521b\u5efa\u53c2\u6570\u5316\u7684\u5bf9\u6570\u6b63\u6001\u5206\u5e03, \u53c2\u6570\u4e3a <code>loc</code> \u548c <code>scale</code>:</p> <pre><code>X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n\n</code></pre> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (float or Tensor) \u2013  \u5206\u5e03\u5bf9\u6570\u5e73\u5747\u503c</li> <li>scale (float or Tensor) \u2013  \u5206\u5e03\u5bf9\u6570\u7684\u6807\u51c6\u5dee</li> </ul> <pre><code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>loc\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>scale\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#lowrankmultivariatenormal","title":"LowRankMultivariateNormal","text":"<pre><code>class torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u4f7f\u7528\u7531<code>cov_factor</code>\u548c<code>cov_diag</code>\u53c2\u6570\u5316\u7684\u4f4e\u79e9\u5f62\u5f0f\u7684\u534f\u65b9\u5dee\u77e9\u9635\u521b\u5efa\u591a\u5143\u6b63\u6001\u5206\u5e03:</p> <pre><code>covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n\n</code></pre> <p>Example</p> <pre><code>&gt;&gt;&gt; m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1]))\n&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (Tensor) \u2013 \u5206\u5e03\u7684\u5747\u503c, \u5f62\u72b6\u4e3a <code>batch_shape + event_shape</code></li> <li>cov_factor (Tensor) \u2013 \u534f\u65b9\u5dee\u77e9\u9635\u4f4e\u79e9\u5f62\u5f0f\u7684\u56e0\u5b50\u90e8\u5206, \u5f62\u72b6\u4e3a <code>batch_shape + event_shape + (rank,)</code></li> <li>cov_diag (Tensor) \u2013 \u534f\u65b9\u5dee\u77e9\u9635\u7684\u4f4e\u79e9\u5f62\u5f0f\u7684\u5bf9\u89d2\u90e8\u5206, \u5f62\u72b6\u4e3a <code>batch_shape + event_shape</code></li> </ul> <p>\u6ce8\u610f</p> <p>\u907f\u514d\u4e86\u534f\u65b9\u5dee\u77e9\u9635\u7684\u884c\u5217\u5f0f\u548c\u9006\u7684\u8ba1\u7b97, \u5f53 <code>cov_factor.shape[1] &lt;&lt; cov_factor.shape[0]</code> \u7531\u4e8e Woodbury matrix identity \u548c matrix determinant lemma.  \u7531\u4e8e\u8fd9\u4e9b\u516c\u5f0f, \u6211\u4eec\u53ea\u9700\u8981\u8ba1\u7b97\u5c0f\u5c3a\u5bf8\u201ccapacitance\u201d\u77e9\u9635\u7684\u884c\u5217\u5f0f\u548c\u9006:</p> <pre><code>capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n\n</code></pre> <pre><code>arg_constraints = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()}\n</code></pre> <pre><code>covariance_matrix\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>precision_matrix\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>scale_tril\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#multinomial","title":"Multinomial","text":"<pre><code>class torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u7531<code>total_count</code>\u548c<code>probs</code>\u6216<code>logits</code>(\u4f46\u4e0d\u662f\u4e24\u8005\uff09\u53c2\u6570\u5316\u7684\u591a\u9879\u5f0f\u5206\u5e03.  <code>probs</code>\u7684\u6700\u5185\u5c42\u7ef4\u5ea6\u662f\u5bf9\u7c7b\u522b\u7684\u7d22\u5f15.  \u6240\u6709\u5176\u4ed6\u7ef4\u5ea6\u7d22\u5f15\u6279\u6b21. </p> <p>\u6ce8\u610f <code>total_count</code> \u4e0d\u9700\u8981\u6307\u5b9a, \u5f53\u53ea\u6709 <code>log_prob()</code> \u88ab\u8c03\u7528</p> <p>\u6ce8\u610f</p> <p><code>probs</code> \u5fc5\u987b\u662f\u975e\u8d1f\u7684\u3001\u6709\u9650\u7684\u5e76\u4e14\u5177\u6709\u975e\u96f6\u548c, \u5e76\u4e14\u5b83\u5c06\u88ab\u5f52\u4e00\u5316\u4e3a\u548c\u4e3a1.</p> <ul> <li><code>sample()</code> \u6240\u6709\u53c2\u6570\u548c\u6837\u672c\u90fd\u9700\u8981\u4e00\u4e2a\u5171\u4eab\u7684<code>total_count</code>.</li> <li><code>log_prob()</code>  \u5141\u8bb8\u6bcf\u4e2a\u53c2\u6570\u548c\u6837\u672c\u4f7f\u7528\u4e0d\u540c\u7684<code>total_count</code>.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n&gt;&gt;&gt; x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n&gt;&gt;&gt; Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>total_count (int) \u2013 \u8bd5\u9a8c\u6b21\u6570</li> <li>probs (Tensor) \u2013 \u4e8b\u4ef6\u6982\u7387</li> <li>logits (Tensor) \u2013 \u4e8b\u4ef6\u5bf9\u6570\u6982\u7387</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Simplex()}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#multivariatenormal","title":"MultivariateNormal","text":"<pre><code>class torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u7531\u5747\u503c\u5411\u91cf\u548c\u534f\u65b9\u5dee\u77e9\u9635\u53c2\u6570\u5316\u7684\u591a\u5143\u6b63\u6001(\u4e5f\u79f0\u4e3a\u9ad8\u65af)\u5206\u5e03.</p> <p>\u591a\u5143\u6b63\u6001\u5206\u5e03\u53ef\u4ee5\u7528\u6b63\u5b9a\u534f\u65b9\u5dee\u77e9\u9635\u6765\u53c2\u6570\u5316\u6216\u8005\u4e00\u4e2a\u6b63\u5b9a\u7684\u7cbe\u5ea6\u77e9\u9635   \u6216\u8005\u662f\u4e00\u4e2a\u6b63\u5bf9\u89d2\u9879\u7684\u4e0b\u4e09\u89d2\u77e9\u9635 , \u4f8b\u5982 . \u8fd9\u4e2a\u4e09\u89d2\u77e9\u9635\u53ef\u4ee5\u901a\u8fc7\u534f\u65b9\u5dee\u7684Cholesky\u5206\u89e3\u5f97\u5230.</p> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (Tensor) \u2013 \u5206\u5e03\u7684\u5747\u503c</li> <li>covariance_matrix (Tensor) \u2013 \u6b63\u5b9a\u534f\u65b9\u5dee\u77e9\u9635</li> <li>precision_matrix (Tensor) \u2013 \u6b63\u5b9a\u7cbe\u5ea6\u77e9\u9635</li> <li>scale_tril (Tensor) \u2013 \u5177\u6709\u6b63\u503c\u5bf9\u89d2\u7ebf\u7684\u4e0b\u4e09\u89d2\u534f\u65b9\u5dee\u56e0\u5b50</li> </ul> <p>\u6ce8\u610f</p> <p>\u4ec5\u4ec5\u4e00\u4e2a <code>covariance_matrix</code> \u6216\u8005 <code>precision_matrix</code> \u6216\u8005 <code>scale_tril</code> \u53ef\u88ab\u6307\u5b9a.</p> <p>\u4f7f\u7528 <code>scale_tril</code>  \u4f1a\u66f4\u6709\u6548\u7387: \u5185\u90e8\u7684\u6240\u6709\u8ba1\u7b97\u90fd\u57fa\u4e8e <code>scale_tril</code>. \u5982\u679c <code>covariance_matrix</code> \u6216\u8005 <code>precision_matrix</code> \u5df2\u7ecf\u88ab\u4f20\u5165, \u5b83\u4ec5\u7528\u4e8e\u4f7f\u7528Cholesky\u5206\u89e3\u8ba1\u7b97\u76f8\u5e94\u7684\u4e0b\u4e09\u89d2\u77e9\u9635.</p> <pre><code>arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}\n</code></pre> <pre><code>covariance_matrix\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>precision_matrix\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>scale_tril\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#negativebinomial","title":"NegativeBinomial","text":"<pre><code>class torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u4e00\u4e2a\u8d1f\u4e8c\u9879\u5206\u5e03, \u5373\u5728\u8fbe\u5230<code>total_count</code>\u5931\u8d25\u4e4b\u524d\u6240\u9700\u7684\u72ec\u7acb\u76f8\u540c\u4f2f\u52aa\u5229\u8bd5\u9a8c\u7684\u6570\u91cf\u7684\u5206\u5e03. \u6bcf\u6b21\u4f2f\u52aa\u5229\u8bd5\u9a8c\u6210\u529f\u7684\u6982\u7387\u90fd\u662f<code>probs</code>. </p> <p>\u53c2\u6570: </p> <ul> <li>total_count (float or Tensor) \u2013  \u975e\u8d1f\u6570\u4f2f\u52aa\u5229\u8bd5\u9a8c\u505c\u6b62\u7684\u6b21\u6570, \u867d\u7136\u5206\u5e03\u4ecd\u7136\u5bf9\u5b9e\u6570\u6709\u6548</li> <li>probs (Tensor) \u2013 \u4e8b\u4ef6\u6982\u7387, \u533a\u95f4\u4e3a [0, 1)</li> <li>logits (Tensor) \u2013 \u4e8b\u4ef6\u5bf9\u6570\u51e0\u7387 - \u6210\u529f\u6982\u7387\u7684\u51e0\u7387</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = IntegerGreaterThan(lower_bound=0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#normal","title":"Normal","text":"<pre><code>class torch.distributions.normal.Normal(loc, scale, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u7531<code>loc</code>\u548c<code>scale</code>\u53c2\u6570\u5316\u7684\u6b63\u6001(\u4e5f\u79f0\u4e3a\u9ad8\u65af\uff09\u5206\u5e03</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>loc (float or Tensor) \u2013 \u5747\u503c (\u4e5f\u88ab\u79f0\u4e3a mu)</li> <li>scale (float or Tensor) \u2013 \u6807\u51c6\u5dee(\u4e5f\u88ab\u79f0\u4e3a) sigma)</li> </ul> <pre><code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(value)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>stddev\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#onehotcategorical","title":"OneHotCategorical","text":"<pre><code>class torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u521b\u5efa\u4e00\u4e2a\u7531<code>probs</code>\u6216l<code>ogits</code>\u53c2\u6570\u5316\u7684One Hot Categorical \u5206\u5e03</p> <p>\u6837\u672c\u662f\u5927\u5c0f\u4e3a <code>probs.size(-1)</code>\u70ed\u7f16\u7801\u5411\u91cf.</p> <p>\u6ce8\u610f</p> <p><code>probs</code>\u5fc5\u987b\u662f\u975e\u8d1f\u7684, \u6709\u9650\u7684\u5e76\u4e14\u5177\u6709\u975e\u96f6\u548c, \u5e76\u4e14\u5b83\u5c06\u88ab\u5f52\u4e00\u5316\u4e3a\u603b\u548c\u4e3a1. </p> <p>\u8bf7\u53c2\u89c1: <code>torch.distributions.Categorical()</code> \u5bf9\u4e8e\u6307\u5b9a <code>probs</code> \u548c <code>logits</code>.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>probs (Tensor) \u2013 event probabilities</li> <li>logits (Tensor) \u2013 event log probabilities</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Simplex()}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>enumerate_support(expand=True)\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_enumerate_support = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>param_shape\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = Simplex()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#pareto","title":"Pareto","text":"<pre><code>class torch.distributions.pareto.Pareto(scale, alpha, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u6765\u81eaPareto Type 1\u5206\u5e03\u7684\u6837\u672c.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>scale (float or Tensor) \u2013 \u5206\u5e03\u7684Scale</li> <li>alpha (float or Tensor) \u2013 \u5206\u5e03\u7684Shape</li> </ul> <pre><code>arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#poisson","title":"Poisson","text":"<pre><code>class torch.distributions.poisson.Poisson(rate, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.exp_family.ExponentialFamily</code></p> <p>\u521b\u5efa\u6309<code>rate</code>\u53c2\u6570\u5316\u7684\u6cca\u677e\u5206\u5e03</p> <p>\u6837\u672c\u662f\u975e\u8d1f\u6574\u6570, pmf\u662f</p> <p></p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Poisson(torch.tensor([4]))\n&gt;&gt;&gt; m.sample()\ntensor([ 3.])\n\n</code></pre> <p>| \u53c2\u6570: | rate (Number__, Tensor) \u2013 rate \u53c2\u6570 |</p> <pre><code>arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = IntegerGreaterThan(lower_bound=0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#relaxedbernoulli","title":"RelaxedBernoulli","text":"<pre><code>class torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u521b\u5efa\u4e00\u4e2aRelaxedBernoulli\u5206\u5e03, \u901a\u8fc7<code>temperature</code>\u53c2\u6570\u5316, \u4ee5\u53ca<code>probs</code>\u6216<code>logits</code>(\u4f46\u4e0d\u662f\u4e24\u8005\uff09.  \u8fd9\u662f\u4f2f\u52aa\u5229\u5206\u5e03\u7684\u677e\u5f1b\u7248\u672c, \u56e0\u6b64\u503c\u5728(0,1\uff09\u4e2d, \u5e76\u4e14\u5177\u6709\u53ef\u91cd\u53c2\u6570\u5316\u7684\u6837\u672c. </p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = RelaxedBernoulli(torch.tensor([2.2]),\n torch.tensor([0.1, 0.2, 0.3, 0.99]))\n&gt;&gt;&gt; m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>temperature (Tensor) \u2013 \u677e\u5f1b temperature</li> <li>probs (Number__, Tensor) \u2013\u91c7\u6837 <code>1</code> \u7684\u6982\u7387</li> <li>logits (Number__, Tensor) \u2013 \u91c7\u6837 <code>1</code> \u7684\u5bf9\u6570\u6982\u7387</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>support = Interval(lower_bound=0.0, upper_bound=1.0)\n</code></pre> <pre><code>temperature\n</code></pre>"},{"location":"1.0/distributions/#relaxedonehotcategorical","title":"RelaxedOneHotCategorical","text":"<pre><code>class torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u521b\u5efa\u4e00\u4e2a\u7531\u6e29\u5ea6\u53c2\u6570\u5316\u7684<code>RelaxedOneHotCategorical</code>\u5206\u5e03, \u4ee5\u53ca<code>probs</code>\u6216<code>logits</code>.  \u8fd9\u662f<code>OneHotCategorical</code>\u5206\u5e03\u7684\u677e\u5f1b\u7248\u672c, \u56e0\u6b64\u5b83\u7684\u6837\u672c\u662f\u5355\u4e00\u7684, \u5e76\u4e14\u53ef\u4ee5\u91cd\u53c2\u6570\u5316. </p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n torch.tensor([0.1, 0.2, 0.3, 0.4]))\n&gt;&gt;&gt; m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>temperature (Tensor) \u2013 \u677e\u5f1b temperature</li> <li>probs (Tensor) \u2013 \u4e8b\u4ef6\u6982\u7387</li> <li>logits (Tensor) \u2013\u5bf9\u6570\u4e8b\u4ef6\u6982\u7387.</li> </ul> <pre><code>arg_constraints = {'logits': Real(), 'probs': Simplex()}\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>logits\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>support = Simplex()\n</code></pre> <pre><code>temperature\n</code></pre>"},{"location":"1.0/distributions/#studentt","title":"StudentT","text":"<pre><code>class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u6839\u636e\u81ea\u7531\u5ea6<code>df</code>, \u5e73\u5747<code>loc</code>\u548c<code>scale</code>\u521b\u5efa\u5b66\u751ft\u5206\u5e03. </p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = StudentT(torch.tensor([2.0]))\n&gt;&gt;&gt; m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>df (float or Tensor) \u2013 \u81ea\u7531\u5ea6</li> <li>loc (float or Tensor) \u2013 \u5747\u503c</li> <li>scale (float or Tensor) \u2013 \u5206\u5e03\u7684scale</li> </ul> <pre><code>arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>support = Real()\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#transformeddistribution","title":"TransformedDistribution","text":"<pre><code>class torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>Distribution\u7c7b\u7684\u6269\u5c55, \u5b83\u5c06\u4e00\u7cfb\u5217\u53d8\u6362\u5e94\u7528\u4e8e\u57fa\u672c\u5206\u5e03. \u5047\u8bbef\u662f\u6240\u5e94\u7528\u53d8\u6362\u7684\u7ec4\u6210:</p> <pre><code>X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n\n</code></pre> <p>\u6ce8\u610f <code>.event_shape</code> of a <code>TransformedDistribution</code> \u662f\u5176\u57fa\u672c\u5206\u5e03\u53ca\u5176\u53d8\u6362\u7684\u6700\u5927\u5f62\u72b6, \u56e0\u4e3a\u53d8\u6362\u53ef\u4ee5\u5f15\u5165\u4e8b\u4ef6\u4e4b\u95f4\u7684\u76f8\u5173\u6027.</p> <p>\u4e00\u4e2a\u4f7f\u7528\u4f8b\u5b50 <code>TransformedDistribution</code>:</p> <pre><code># Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n\n</code></pre> <p>\u6709\u5173\u66f4\u591a\u793a\u4f8b, \u8bf7\u67e5\u770b\u6709\u5173\u5b9e\u73b0 <code>Gumbel</code>, <code>HalfCauchy</code>, <code>HalfNormal</code>, <code>LogNormal</code>, <code>Pareto</code>, <code>Weibull</code>, <code>RelaxedBernoulli</code> \u548c <code>RelaxedOneHotCategorical</code></p> <pre><code>arg_constraints = {}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <p>\u901a\u8fc7\u9006\u53d8\u6362\u548c\u8ba1\u7b97\u57fa\u5206\u5e03\u7684\u5206\u6570\u6765\u8ba1\u7b97\u7d2f\u79ef\u5206\u5e03\u51fd\u6570.</p> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample\n</code></pre> <pre><code>icdf(value)\n</code></pre> <p>\u4f7f\u7528transform(s)\u8ba1\u7b97\u9006\u7d2f\u79ef\u5206\u5e03\u51fd\u6570, \u5e76\u8ba1\u7b97\u57fa\u5206\u5e03\u7684\u5206\u6570.</p> <pre><code>log_prob(value)\n</code></pre> <p>\u901a\u8fc7\u53cd\u8f6c\u53d8\u6362\u5e76\u4f7f\u7528\u57fa\u672c\u5206\u5e03\u7684\u5206\u6570\u548c\u65e5\u5fd7abs det jacobian\u8ba1\u7b97\u5206\u6570\u6765\u5bf9\u6837\u672c\u8fdb\u884c\u8bc4\u5206</p> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <p>\u5982\u679c\u5206\u5e03\u53c2\u6570\u662f\u6279\u5904\u7406\u7684, \u5219\u751f\u6210sample_shape\u5f62\u72b6\u7684\u91cd\u65b0\u53c2\u6570\u5316\u6837\u672c\u6216sample_shape\u5f62\u72b6\u7684\u91cd\u65b0\u53c2\u6570\u5316\u6837\u672c\u6279\u6b21.  \u9996\u5148\u4ece\u57fa\u672c\u5206\u5e03\u4e2d\u91c7\u6837, \u5e76\u5bf9\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u53d8\u6362\u5e94\u7528<code>transform()</code></p> <pre><code>sample(sample_shape=torch.Size([]))\n</code></pre> <p>\u5982\u679c\u5206\u5e03\u53c2\u6570\u662f\u6279\u5904\u7406\u7684, \u5219\u751f\u6210sample_shape\u5f62\u6837\u672c\u6216sample_shape\u5f62\u6837\u672c\u6279\u5904\u7406.  \u9996\u5148\u4ece\u57fa\u672c\u5206\u5e03\u4e2d\u91c7\u6837, \u5e76\u5bf9\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u53d8\u6362\u5e94\u7528<code>transform()</code>. </p> <pre><code>support\n</code></pre>"},{"location":"1.0/distributions/#uniform","title":"Uniform","text":"<pre><code>class torch.distributions.uniform.Uniform(low, high, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.distribution.Distribution</code></p> <p>\u4ece\u534a\u5f00\u533a\u95f4<code>[low, high)</code>\u751f\u6210\u5747\u5300\u5206\u5e03\u7684\u968f\u673a\u6837\u672c</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n&gt;&gt;&gt; m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>low (float or Tensor) \u2013  \u4e0b\u9650(\u542b\uff09.</li> <li>high (float or Tensor) \u2013 \u4e0a\u9650(\u6392\u9664).</li> </ul> <pre><code>arg_constraints = {'high': Dependent(), 'low': Dependent()}\n</code></pre> <pre><code>cdf(value)\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>has_rsample = True\n</code></pre> <pre><code>icdf(value)\n</code></pre> <pre><code>log_prob(value)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>rsample(sample_shape=torch.Size([]))\n</code></pre> <pre><code>stddev\n</code></pre> <pre><code>support\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#weibull","title":"Weibull","text":"<pre><code>class torch.distributions.weibull.Weibull(scale, concentration, validate_args=None)\n</code></pre> <p>\u57fa\u7c7b: <code>torch.distributions.transformed_distribution.TransformedDistribution</code></p> <p>\u6765\u81ea\u53cc\u53c2\u6570Weibull\u5206\u5e03\u7684\u6837\u672c.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n&gt;&gt;&gt; m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>scale (float or Tensor) \u2013 Scale (lambda).</li> <li>concentration (float or Tensor) \u2013 Concentration (k/shape).</li> </ul> <pre><code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}\n</code></pre> <pre><code>entropy()\n</code></pre> <pre><code>expand(batch_shape, _instance=None)\n</code></pre> <pre><code>mean\n</code></pre> <pre><code>support = GreaterThan(lower_bound=0.0)\n</code></pre> <pre><code>variance\n</code></pre>"},{"location":"1.0/distributions/#kl-divergence","title":"<code>KL Divergence</code>","text":"<pre><code>torch.distributions.kl.kl_divergence(p, q)\n</code></pre> <p>\u8ba1\u7b97Kullback-Leibler\u6563\u5ea6  \u5bf9\u4e8e\u4e24\u4e2a\u5206\u5e03.</p> <p></p> <p>\u53c2\u6570: </p> <ul> <li>p (Distribution) \u2013 <code>Distribution</code> \u5bf9\u8c61.</li> <li>q (Distribution) \u2013 <code>Distribution</code> \u5bf9\u8c61.</li> </ul> <p>| \u8fd4\u56de\u503c: | \u6279\u91cf\u7684 KL \u6563\u5ea6, \u5f62\u72b6\u4e3a <code>batch_shape</code>. |</p> <p>| \u8fd4\u56de\u7c7b\u578b\uff1a | Tensor |</p> <p>| \u5f02\u5e38: | <code>NotImplementedError</code> \u2013 \u5982\u679c\u5206\u5e03\u7c7b\u578b\u5c1a\u672a\u901a\u8fc7\u6ce8\u518c <code>register_kl()</code>. |</p> <pre><code>torch.distributions.kl.register_kl(type_p, type_q)\n</code></pre> <p>\u88c5\u9970\u5668\u6ce8\u518c<code>kl_divergence()</code>\u7684\u6210\u5bf9\u51fd\u6570</p> <pre><code>@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n\n</code></pre> <p>Lookup\u8fd4\u56de\u7531\u5b50\u7c7b\u6392\u5e8f\u7684\u6700\u5177\u4f53(type,type)\u5339\u914d.  \u5982\u679c\u5339\u914d\u4e0d\u660e\u786e, \u5219\u4f1a\u5f15\u53d1<code>RuntimeWarning</code>.  \u4f8b\u5982, \u89e3\u51b3\u6a21\u68f1\u4e24\u53ef\u7684\u60c5\u51b5</p> <pre><code>@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n\n</code></pre> <p>\u4f60\u5e94\u8be5\u6ce8\u518c\u7b2c\u4e09\u4e2a\u6700\u5177\u4f53\u7684\u5b9e\u73b0, \u4f8b\u5982:</p> <pre><code>register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>type_p (type) \u2013 \u5b50\u7c7b <code>Distribution</code>.</li> <li>type_q (type) \u2013 \u5b50\u7c7b <code>Distribution</code>.</li> </ul>"},{"location":"1.0/distributions/#transforms","title":"<code>Transforms</code>","text":"<pre><code>class torch.distributions.transforms.Transform(cache_size=0)\n</code></pre> <p>\u6709\u53ef\u8ba1\u7b97\u7684log det jacobians\u8fdb\u884c\u53ef\u9006\u53d8\u6362\u7684\u62bd\u8c61\u7c7b.  \u5b83\u4eec\u4e3b\u8981\u7528\u4e8e <code>torch.distributions.TransformedDistribution</code>.</p> <p>\u7f13\u5b58\u5bf9\u4e8e\u5176\u53cd\u8f6c\u6602\u8d35\u6216\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u53d8\u6362\u5f88\u6709\u7528.  \u8bf7\u6ce8\u610f, \u5fc5\u987b\u6ce8\u610f\u8bb0\u5fc6\u503c, \u56e0\u4e3a\u53ef\u4ee5\u98a0\u5012\u81ea\u52a8\u8bb0\u5f55\u56fe.  \u4f8b\u5982, \u4ee5\u4e0b\u64cd\u4f5c\u6709\u6216\u6ca1\u6709\u7f13\u5b58:</p> <pre><code>y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n\n</code></pre> <p>\u4f46\u662f, \u7531\u4e8e\u4f9d\u8d56\u6027\u53cd\u8f6c, \u7f13\u5b58\u65f6\u4f1a\u51fa\u73b0\u4ee5\u4e0b\u9519\u8bef:</p> <pre><code>y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n\n</code></pre> <p>\u6d3e\u751f\u7c7b\u5e94\u8be5\u5b9e\u73b0<code>_call()</code>\u6216<code>_inverse()</code>\u4e2d\u7684\u4e00\u4e2a\u6216\u4e24\u4e2a.  \u8bbe\u7f6e<code>bijective=True</code>\u7684\u6d3e\u751f\u7c7b\u4e5f\u5e94\u8be5\u5b9e\u73b0<code>log_abs_det_jacobian()</code></p> <p>| \u53c2\u6570: | cache_size (int) \u2013 \u7f13\u5b58\u5927\u5c0f.  \u5982\u679c\u4e3a\u96f6, \u5219\u4e0d\u8fdb\u884c\u7f13\u5b58.  \u5982\u679c\u662f, \u5219\u7f13\u5b58\u6700\u65b0\u7684\u5355\u4e2a\u503c.  \u4ec5\u652f\u63010\u548c1 |</p> <p>| Variables: | </p> <ul> <li>domain (<code>Constraint</code>) \u2013  \u8868\u793a\u8be5\u53d8\u6362\u6709\u6548\u8f93\u5165\u7684\u7ea6\u675f.</li> <li>codomain (<code>Constraint</code>) \u2013 \u8868\u793a\u6b64\u8f6c\u6362\u7684\u6709\u6548\u8f93\u51fa\u7684\u7ea6\u675f, \u8fd9\u4e9b\u8f93\u51fa\u662f\u9006\u53d8\u6362\u7684\u8f93\u5165.</li> <li>bijective (bool) \u2013  \u8fd9\u4e2a\u53d8\u6362\u662f\u5426\u662f\u53cc\u5c04\u7684. \u53d8\u6362 <code>t</code> \u662f\u53cc\u5c04\u7684 \u5982\u679c <code>t.inv(t(x)) == x</code> \u5e76\u4e14 <code>t(t.inv(y)) == y</code> \u5bf9\u4e8e\u6bcf\u4e00\u4e2a <code>x</code> \u548c <code>y</code>. \u4e0d\u662f\u53cc\u5c04\u7684\u53d8\u6362\u5e94\u8be5\u81f3\u5c11\u4fdd\u6301\u8f83\u5f31\u7684\u4f2a\u9006\u5c5e\u6027 <code>t(t.inv(t(x)) == t(x)</code> and <code>t.inv(t(t.inv(y))) == t.inv(y)</code>.</li> <li>sign (int or Tensor) \u2013 \u5bf9\u4e8e\u53cc\u5c04\u5355\u53d8\u91cf\u53d8\u6362, \u5b83\u5e94\u8be5\u662f+1\u6216-1, \u8fd9\u53d6\u51b3\u4e8e\u53d8\u6362\u662f\u5355\u8c03\u9012\u589e\u8fd8\u662f\u9012\u51cf.</li> <li>event_dim (int) \u2013 \u53d8\u6362event_shape\u4e2d\u76f8\u5173\u7684\u7ef4\u6570.  \u8fd9\u5bf9\u4e8e\u9010\u70b9\u53d8\u6362\u5e94\u8be5\u662f0, \u5bf9\u4e8e\u5728\u77e2\u91cf\u4e0a\u5171\u540c\u4f5c\u7528\u7684\u53d8\u6362\u662f1, \u5bf9\u4e8e\u5728\u77e9\u9635\u4e0a\u5171\u540c\u4f5c\u7528\u7684\u53d8\u6362\u662f2, \u7b49\u7b49.</li> </ul> <pre><code>inv\n</code></pre> <p>\u8fd4\u56de\u9006<code>Transform</code>. \u6ee1\u8db3 <code>t.inv.inv is t</code>.</p> <pre><code>sign\n</code></pre> <p>\u5982\u679c\u9002\u7528, \u8fd4\u56de\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u7b26\u53f7.  \u4e00\u822c\u6765\u8bf4, \u8fd9\u53ea\u9002\u7528\u4e8e\u53cc\u5c04\u53d8\u6362.</p> <pre><code>log_abs_det_jacobian(x, y)\n</code></pre> <p>\u8ba1\u7b97 log det jacobian <code>log |dy/dx|</code> \u7ed9\u5b9a\u8f93\u5165\u548c\u8f93\u51fa.</p> <pre><code>class torch.distributions.transforms.ComposeTransform(parts)\n</code></pre> <p>\u5728\u4e00\u4e2a\u94fe\u4e2d\u7ec4\u5408\u591a\u4e2a\u8f6c\u6362. \u6b63\u5728\u7ec4\u5408\u7684\u8f6c\u6362\u8d1f\u8d23\u7f13\u5b58.</p> <p>| \u53c2\u6570: | parts (list of <code>Transform</code>) \u2013 \u5217\u8868 transforms. |</p> <pre><code>class torch.distributions.transforms.ExpTransform(cache_size=0)\n</code></pre> <p>\u8f6c\u6362\u901a\u8fc7\u6620\u5c04 .</p> <pre><code>class torch.distributions.transforms.PowerTransform(exponent, cache_size=0)\n</code></pre> <p>\u8f6c\u6362\u901a\u8fc7\u6620\u5c04 .</p> <pre><code>class torch.distributions.transforms.SigmoidTransform(cache_size=0)\n</code></pre> <p>\u8f6c\u6362\u901a\u8fc7\u6620\u5c04  and .</p> <pre><code>class torch.distributions.transforms.AbsTransform(cache_size=0)\n</code></pre> <p>\u8f6c\u6362\u901a\u8fc7\u6620\u5c04 .</p> <pre><code>class torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)\n</code></pre> <p>\u901a\u8fc7\u9010\u70b9\u4eff\u5c04\u6620\u5c04\u8fdb\u884c\u8f6c\u6362 .</p> <p>\u53c2\u6570: </p> <ul> <li>loc (Tensor or float) \u2013 Location.</li> <li>scale (Tensor or float) \u2013 Scale.</li> <li>event_dim (int) \u2013 \u53ef\u9009\u7684 <code>event_shape</code> \u5927\u5c0f. T\u5bf9\u4e8e\u5355\u53d8\u91cf\u968f\u673a\u53d8\u91cf, \u8be5\u503c\u5e94\u4e3a\u96f6, \u5bf9\u4e8e\u77e2\u91cf\u5206\u5e03, 1\u5e94\u4e3a\u96f6, \u5bf9\u4e8e\u77e9\u9635\u7684\u5206\u5e03, \u5e94\u4e3a2.</li> </ul> <pre><code>class torch.distributions.transforms.SoftmaxTransform(cache_size=0)\n</code></pre> <p>\u4ece\u65e0\u7ea6\u675f\u7a7a\u95f4\u5230\u5355\u7eaf\u5f62\u7684\u8f6c\u6362, \u901a\u8fc7  \u7136\u540e\u5f52\u4e00\u5316.</p> <p>\u8fd9\u4e0d\u662f\u53cc\u5c04\u7684, \u4e0d\u80fd\u7528\u4e8eHMC.  \u7136\u800c, \u8fd9\u4e3b\u8981\u662f\u534f\u8c03\u7684(\u9664\u4e86\u6700\u7ec8\u7684\u5f52\u4e00\u5316\uff09, \u56e0\u6b64\u9002\u5408\u4e8e\u5750\u6807\u65b9\u5f0f\u7684\u4f18\u5316\u7b97\u6cd5. </p> <pre><code>class torch.distributions.transforms.StickBreakingTransform(cache_size=0)\n</code></pre> <p>\u5c06\u65e0\u7ea6\u675f\u7a7a\u95f4\u901a\u8fc7 stick-breaking \u8fc7\u7a0b\u8f6c\u5316\u4e3a\u4e00\u4e2a\u989d\u5916\u7ef4\u5ea6\u7684\u5355\u7eaf\u5f62. </p> <p>\u8fd9\u79cd\u53d8\u6362\u662f<code>Dirichlet</code>\u5206\u5e03\u7684\u7834\u68d2\u6784\u9020\u4e2d\u7684\u8fed\u4ee3sigmoid\u53d8\u6362:\u7b2c\u4e00\u4e2a\u903b\u8f91\u901a\u8fc7sigmoid\u53d8\u6362\u6210\u7b2c\u4e00\u4e2a\u6982\u7387\u548c\u6240\u6709\u5176\u4ed6\u6982\u7387, \u7136\u540e\u8fd9\u4e2a\u8fc7\u7a0b\u91cd\u590d\u51fa\u73b0. </p> <p>\u8fd9\u662f\u53cc\u5c04\u7684, \u9002\u5408\u5728HMC\u4e2d\u4f7f\u7528; \u7136\u800c, \u5b83\u5c06\u5750\u6807\u6df7\u5408\u5728\u4e00\u8d77, \u4e0d\u592a\u9002\u5408\u4f18\u5316.</p> <pre><code>class torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)\n</code></pre> <p>\u5c06\u65e0\u7ea6\u675f\u77e9\u9635\u8f6c\u6362\u4e3a\u5177\u6709\u975e\u8d1f\u5bf9\u89d2\u9879\u7684\u4e0b\u4e09\u89d2\u77e9\u9635.</p> <p>\u8fd9\u5bf9\u4e8e\u6839\u636eCholesky\u5206\u89e3\u6765\u53c2\u6570\u5316\u6b63\u5b9a\u77e9\u9635\u662f\u6709\u7528\u7684.</p>"},{"location":"1.0/distributions/#constraints","title":"<code>Constraints</code>","text":"<p>The following constraints are implemented:</p> <ul> <li><code>constraints.boolean</code></li> <li><code>constraints.dependent</code></li> <li><code>constraints.greater_than(lower_bound)</code></li> <li><code>constraints.integer_interval(lower_bound, upper_bound)</code></li> <li><code>constraints.interval(lower_bound, upper_bound)</code></li> <li><code>constraints.lower_cholesky</code></li> <li><code>constraints.lower_triangular</code></li> <li><code>constraints.nonnegative_integer</code></li> <li><code>constraints.positive</code></li> <li><code>constraints.positive_definite</code></li> <li><code>constraints.positive_integer</code></li> <li><code>constraints.real</code></li> <li><code>constraints.real_vector</code></li> <li><code>constraints.simplex</code></li> <li><code>constraints.unit_interval</code></li> </ul> <pre><code>class torch.distributions.constraints.Constraint\n</code></pre> <p>constraints \u7684\u62bd\u8c61\u57fa\u7c7b.</p> <p>constraint\u5bf9\u8c61\u8868\u793a\u53d8\u91cf\u6709\u6548\u7684\u533a\u57df, \u4f8b\u5982,  \u5176\u4e2d\u53ef\u4ee5\u4f18\u5316\u53d8\u91cf</p> <pre><code>check(value)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5b57\u8282\u5f20\u91cf <code>sample_shape + batch_shape</code> \u6307\u793a\u503c\u4e2d\u7684\u6bcf\u4e2a\u4e8b\u4ef6\u662f\u5426\u6ee1\u8db3\u6b64\u7ea6\u675f.</p> <pre><code>torch.distributions.constraints.dependent_property\n</code></pre> <p>alias of <code>torch.distributions.constraints._DependentProperty</code></p> <pre><code>torch.distributions.constraints.integer_interval\n</code></pre> <p>alias of <code>torch.distributions.constraints._IntegerInterval</code></p> <pre><code>torch.distributions.constraints.greater_than\n</code></pre> <p>alias of <code>torch.distributions.constraints._GreaterThan</code></p> <pre><code>torch.distributions.constraints.greater_than_eq\n</code></pre> <p>alias of <code>torch.distributions.constraints._GreaterThanEq</code></p> <pre><code>torch.distributions.constraints.less_than\n</code></pre> <p>alias of <code>torch.distributions.constraints._LessThan</code></p> <pre><code>torch.distributions.constraints.interval\n</code></pre> <p>alias of <code>torch.distributions.constraints._Interval</code></p> <pre><code>torch.distributions.constraints.half_open_interval\n</code></pre> <p>alias of <code>torch.distributions.constraints._HalfOpenInterval</code></p>"},{"location":"1.0/distributions/#constraint-registry","title":"<code>Constraint Registry</code>","text":"<p>PyTorch \u63d0\u4f9b\u4e24\u4e2a\u5168\u5c40 <code>ConstraintRegistry</code> \u5bf9\u8c61 , \u94fe\u63a5 <code>Constraint</code> \u5bf9\u8c61\u5230 <code>Transform</code> \u5bf9\u8c61. \u8fd9\u4e9b\u5bf9\u8c61\u65e2\u6709\u8f93\u5165\u7ea6\u675f, \u4e5f\u6709\u8fd4\u56de\u53d8\u6362, \u4f46\u662f\u5b83\u4eec\u5bf9\u53cc\u5c04\u6027\u6709\u4e0d\u540c\u7684\u4fdd\u8bc1.</p> <ol> <li><code>biject_to(constraint)</code>  \u67e5\u627e\u4e00\u4e2a\u53cc\u5c04\u7684 <code>Transform</code> \u4ece <code>constraints.real</code> \u5230\u7ed9\u5b9a\u7684 <code>constraint</code>.  \u8fd4\u56de\u7684\u8f6c\u6362\u4fdd\u8bc1\u5177\u6709 <code>.bijective = True</code> \u5e76\u4e14\u5e94\u8be5\u5b9e\u73b0\u4e86 <code>.log_abs_det_jacobian()</code>.</li> <li><code>transform_to(constraint)</code> \u67e5\u627e\u4e00\u4e2a\u4e0d\u4e00\u5b9a\u662f\u53cc\u5c04\u7684 <code>Transform</code> \u4ece <code>constraints.real</code> \u5230\u7ed9\u5b9a\u7684 <code>constraint</code>. \u8fd4\u56de\u7684\u8f6c\u6362\u4e0d\u4fdd\u8bc1\u5b9e\u73b0 <code>.log_abs_det_jacobian()</code>.</li> </ol> <p><code>transform_to()</code>\u6ce8\u518c\u8868\u5bf9\u4e8e\u5bf9\u6982\u7387\u5206\u5e03\u7684\u7ea6\u675f\u53c2\u6570\u6267\u884c\u65e0\u7ea6\u675f\u4f18\u5316\u975e\u5e38\u6709\u7528, \u8fd9\u4e9b\u53c2\u6570\u7531\u6bcf\u4e2a\u5206\u5e03\u7684<code>.arg_constraints</code>\u6307\u793a.  \u8fd9\u4e9b\u53d8\u6362\u901a\u5e38\u4f1a\u8fc7\u5ea6\u53c2\u6570\u5316\u7a7a\u95f4\u4ee5\u907f\u514d\u65cb\u8f6c; \u56e0\u6b64, \u5b83\u4eec\u66f4\u9002\u5408\u50cfAdam\u90a3\u6837\u7684\u5750\u6807\u4f18\u5316\u7b97\u6cd5</p> <pre><code>loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n\n</code></pre> <p><code>biject_to()</code> \u6ce8\u518c\u8868\u5bf9\u4e8eHamiltonian Monte Carlo\u975e\u5e38\u6709\u7528, \u5176\u4e2d\u6765\u81ea\u5177\u6709\u7ea6\u675f. <code>.support</code>\u7684\u6982\u7387\u5206\u5e03\u7684\u6837\u672c\u5728\u65e0\u7ea6\u675f\u7a7a\u95f4\u4e2d\u4f20\u64ad, \u5e76\u4e14\u7b97\u6cd5\u901a\u5e38\u662f\u65cb\u8f6c\u4e0d\u53d8\u7684</p> <pre><code>dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e00\u4e2a <code>transform_to</code> \u548c <code>biject_to</code> \u4e0d\u540c\u7684\u4f8b\u5b50\u662f <code>constraints.simplex</code>: <code>transform_to(constraints.simplex)</code> \u8fd4\u56de\u4e00\u4e2a <code>SoftmaxTransform</code> \u7b80\u5355\u5730\u5bf9\u5176\u8f93\u5165\u8fdb\u884c\u6307\u6570\u5316\u548c\u5f52\u4e00\u5316;  \u8fd9\u662f\u4e00\u79cd\u5ec9\u4ef7\u4e14\u4e3b\u8981\u662f\u5750\u6807\u7684\u64cd\u4f5c, \u9002\u7528\u4e8e\u50cfSVI\u8fd9\u6837\u7684\u7b97\u6cd5. \u76f8\u53cd, <code>biject_to(constraints.simplex)</code> \u8fd4\u56de\u4e00\u4e2a <code>StickBreakingTransform</code> \u5c06\u5176\u8f93\u5165\u751f\u6210\u4e00\u4e2a\u8f83\u5c0f\u7ef4\u5ea6\u7684\u7a7a\u95f4; \u8fd9\u662f\u4e00\u79cd\u66f4\u6602\u8d35\u7684\u6570\u503c\u66f4\u5c11\u7684\u6570\u503c\u7a33\u5b9a\u7684\u53d8\u6362, \u4f46\u5bf9\u4e8e\u50cfHM\u200b\u200bC\u8fd9\u6837\u7684\u7b97\u6cd5\u662f\u5fc5\u9700\u7684. </p> <p><code>biject_to</code> \u548c <code>transform_to</code> \u5bf9\u8c61\u53ef\u4ee5\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u7ea6\u675f\u8fdb\u884c\u6269\u5c55, \u5e76\u4f7f\u7528<code>.register()</code>\u65b9\u6cd5\u8fdb\u884c\u8f6c\u6362, \u4f5c\u4e3a\u5355\u4f8b\u7ea6\u675f\u7684\u51fd\u6570</p> <pre><code>transform_to.register(my_constraint, my_transform)\n\n</code></pre> <p>\u6216\u4f5c\u4e3a\u53c2\u6570\u5316\u7ea6\u675f\u7684\u88c5\u9970\u5668:</p> <pre><code>@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n\n</code></pre> <p>\u60a8\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa\u65b0\u7684<code>ConstraintRegistry</code>\u521b\u5efa\u81ea\u5df1\u7684\u6ce8\u518c\u8868.</p> <pre><code>class torch.distributions.constraint_registry.ConstraintRegistry\n</code></pre> <p>\u6ce8\u518c\u8868, \u5c06\u7ea6\u675f\u94fe\u63a5\u5230\u8f6c\u6362.</p> <pre><code>register(constraint, factory=None)\n</code></pre> <p>\u5728\u6b64\u6ce8\u518c\u8868\u6ce8\u518c\u4e00\u4e2a <code>Constraint</code> \u5b50\u7c7b. \u7528\u6cd5:</p> <pre><code>@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>constraint (subclass of <code>Constraint</code>) \u2013  [<code>Constraint</code>]\u7684\u5b50\u7c7b(#torch.distributions.constraints.Constraint \"torch.distributions.constraints.Constraint\"), \u6216\u8005\u6d3e\u751f\u7c7b\u7684\u5bf9\u8c61.</li> <li>factory (callable) \u2013 \u53ef\u8c03\u7528\u5bf9\u8c61, \u8f93\u5165 constraint \u5bf9\u8c61\u8fd4\u56de <code>Transform</code> \u5bf9\u8c61.</li> </ul>"},{"location":"1.0/dlpack/","title":"torch.utils.dlpack","text":"<p>\u8bd1\u8005\uff1akunwuz</p> <pre><code>torch.utils.dlpack.from_dlpack(dlpack) \u2192 Tensor\n</code></pre> <p>\u5c06DLPack\u89e3\u7801\u6210Tensor\u5f20\u91cf\u3002</p> \u53c2\u6570: dlpack \u2013 \u4e00\u4e2a\u6709\u7740dltensor\u5f20\u91cf\u7684PyCapsule\u5bf9\u8c61 <p>\u8fd9\u4e2a\u5f20\u91cf\u4f1a\u4e0edlpack\u5bf9\u8c61\u5171\u4eab\u5b58\u50a8\u7a7a\u95f4\u3002\u6ce8\u610f\u6bcf\u4e2adlpack\u5bf9\u8c61\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21\u3002</p> <pre><code>torch.utils.dlpack.to_dlpack(tensor) \u2192 PyCapsule\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u8868\u793a\u5f20\u91cf\u7684DLPack\u3002</p> \u53c2\u6570: tensor \u2013\u4e00\u4e2a\u7528\u6765\u8f93\u51fa\u7684tensor\u5f20\u91cf <p>\u8fd9\u4e2a\u5f20\u91cf\u4f1a\u4e0edlpack\u5bf9\u8c61\u5171\u4eab\u5b58\u50a8\u7a7a\u95f4\u3002\u6ce8\u610f\u6bcf\u4e2adlpack\u5bf9\u8c61\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21\u3002</p>"},{"location":"1.0/docs_cpp_extension/","title":"torch.utils.cpp_extension","text":"<p>\u8bd1\u8005:  belonHan</p> <pre><code>torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2aC++\u7684setuptools.Extension\u3002</p> <p>\u4fbf\u6377\u5730\u521b\u5efa\u4e00\u4e2asetuptools.Extension\u5177\u6709\u6700\u5c0f(\u4f46\u901a\u5e38\u662f\u8db3\u591f\uff09\u7684\u53c2\u6570\u6765\u6784\u5efaC++\u6269\u5c55\u7684\u65b9\u6cd5\u3002</p> <p>\u6240\u6709\u53c2\u6570\u90fd\u88ab\u8f6c\u53d1\u7ed9setuptools.Extension\u6784\u9020\u51fd\u6570\u3002</p> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; from setuptools import setup\n&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CppExtension\n&gt;&gt;&gt; setup(\nname='extension',\next_modules=[\nCppExtension(\nname='extension',\nsources=['extension.cpp'],\nextra_compile_args=['-g'])),\n],\ncmdclass={\n'build_ext': BuildExtension\n})\n\n</code></pre> <pre><code>torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)\n</code></pre> <p>\u4e3aCUDA/C++\u521b\u5efa\u4e00\u4e2a<code>setuptools.Extension</code>\u3002</p> <p>\u521b\u5efa\u4e00\u4e2asetuptools.Extension\u7528\u4e8e\u6784\u5efaCUDA/C ++\u6269\u5c55\u7684\u6700\u5c11\u53c2\u6570(\u4f46\u901a\u5e38\u662f\u8db3\u591f\u7684\uff09\u7684\u4fbf\u6377\u65b9\u6cd5\u3002\u8fd9\u91cc\u5305\u62ecCUDA\u8def\u5f84\uff0c\u5e93\u8def\u5f84\u548c\u8fd0\u884c\u5e93\u3002 \u6240\u6709\u53c2\u6570\u90fd\u88ab\u8f6c\u53d1\u7ed9setuptools.Extension\u6784\u9020\u51fd\u6570\u3002</p> <p>\u6240\u6709\u53c2\u6570\u90fd\u88ab\u8f6c\u53d1\u7ed9setuptools.Extension\u6784\u9020\u51fd\u6570\u3002</p> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; from setuptools import setup\n&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n&gt;&gt;&gt; setup(\nname='cuda_extension',\next_modules=[\nCUDAExtension(\nname='cuda_extension',\nsources=['extension.cpp', 'extension_kernel.cu'],\nextra_compile_args={'cxx': ['-g'],\n'nvcc': ['-O2']})\n],\ncmdclass={\n'build_ext': BuildExtension\n})\n\n</code></pre> <pre><code>torch.utils.cpp_extension.BuildExtension(*args, **kwargs)\n</code></pre> <p>\u81ea\u5b9a\u4e49setuptools\u6784\u5efa\u6269\u5c55\u3002</p> <p><code>setuptools.build_ext</code>\u5b50\u7c7b\u8d1f\u8d23\u4f20\u9012\u6240\u9700\u7684\u6700\u5c0f\u7f16\u8bd1\u5668\u53c2\u6570(\u4f8b\u5982<code>-std=c++11</code>\uff09\u4ee5\u53ca\u6df7\u5408\u7684C ++/CUDA\u7f16\u8bd1(\u4ee5\u53ca\u4e00\u822c\u5bf9CUDA\u6587\u4ef6\u7684\u652f\u6301\uff09\u3002</p> <p>\u5f53\u4f7f\u7528<code>BuildExtension</code>\u65f6\uff0c\u5b83\u5c06\u63d0\u4f9b\u4e00\u4e2a\u7528\u4e8e<code>extra_compile_args</code>(\u4e0d\u662f\u666e\u901a\u5217\u8868\uff09\u7684\u8bcd\u5178\uff0c\u901a\u8fc7\u8bed\u8a00(<code>cxx</code>\u6216<code>cuda</code>\uff09\u6620\u5c04\u5230\u53c2\u6570\u5217\u8868\u63d0\u4f9b\u7ed9\u7f16\u8bd1\u5668\u3002\u8fd9\u6837\u53ef\u4ee5\u5728\u6df7\u5408\u7f16\u8bd1\u671f\u95f4\u4e3aC ++\u548cCUDA\u7f16\u8bd1\u5668\u63d0\u4f9b\u4e0d\u540c\u7684\u53c2\u6570\u3002</p> <pre><code>torch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True)\n</code></pre> <p>\u5373\u65f6\u52a0\u8f7d(JIT)PyTorch C ++\u6269\u5c55\u3002</p> <p>\u4e3a\u4e86\u52a0\u8f7d\u6269\u5c55\uff0c\u4f1a\u521b\u5efa\u4e00\u4e2aNinja\u6784\u5efa\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u7528\u4e8e\u5c06\u6307\u5b9a\u7684\u6e90\u7f16\u8bd1\u4e3a\u52a8\u6001\u5e93\u3002\u968f\u540e\u5c06\u8be5\u5e93\u4f5c\u4e3a\u6a21\u5757\u52a0\u8f7d\u5230\u5f53\u524dPython\u8fdb\u7a0b\u4e2d\uff0c\u5e76\u4ece\u8be5\u51fd\u6570\u8fd4\u56de\uff0c\u4ee5\u5907\u4f7f\u7528\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6784\u5efa\u6587\u4ef6\u521b\u5efa\u7684\u76ee\u5f55\u4ee5\u53ca\u7f16\u8bd1\u7ed3\u679c\u5e93\u662f<code>&amp;lt;tmp&amp;gt;/torch_extensions/&amp;lt;name&amp;gt;</code>\uff0c\u5176\u4e2d<code>&amp;lt;tmp&amp;gt;</code>\u662f\u5f53\u524d\u5e73\u53f0\u4e0a\u7684\u4e34\u65f6\u6587\u4ef6\u5939\u4ee5\u53ca<code>&amp;lt;name&amp;gt;</code>\u4e3a\u6269\u5c55\u540d\u3002\u8fd9\u4e2a\u4f4d\u7f6e\u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u88ab\u8986\u76d6\u3002\u9996\u5148\uff0c\u5982\u679c<code>TORCH_EXTENSIONS_DIR</code>\u8bbe\u7f6e\u4e86\u73af\u5883\u53d8\u91cf\uff0c\u5b83\u5c06\u66ff\u6362<code>&amp;lt;tmp&amp;gt;/torch_extensions</code>\u5e76\u5c06\u6240\u6709\u6269\u5c55\u7f16\u8bd1\u5230\u6b64\u76ee\u5f55\u7684\u5b50\u6587\u4ef6\u5939\u4e2d\u3002\u5176\u6b21\uff0c\u5982\u679c<code>build_directory</code>\u51fd\u6570\u8bbe\u7f6e\u4e86\u53c2\u6570\uff0c\u5b83\u4e5f\u5c06\u8986\u76d6\u6574\u4e2a\u8def\u5f84\uff0c\u5373,\u5e93\u5c06\u76f4\u63a5\u7f16\u8bd1\u5230\u8be5\u6587\u4ef6\u5939\u4e2d\u3002</p> <p>\u8981\u7f16\u8bd1\u6e90\u6587\u4ef6\uff0c\u4f7f\u7528\u9ed8\u8ba4\u7684\u7cfb\u7edf\u7f16\u8bd1\u5668(<code>c++</code>\uff09\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>CXX</code>\u73af\u5883\u53d8\u91cf\u6765\u8986\u76d6\u5b83\u3002\u5c06\u5176\u4ed6\u53c2\u6570\u4f20\u9012\u7ed9\u7f16\u8bd1\u8fc7\u7a0b\uff0c<code>extra_cflags</code>\u6216\u8005<code>extra_ldflags</code>\u53ef\u4ee5\u63d0\u4f9b\u3002\u4f8b\u5982\uff0c\u8981\u901a\u8fc7\u4f18\u5316\u6765\u7f16\u8bd1\u60a8\u7684\u6269\u5c55\uff0c\u4f60\u53ef\u4ee5\u4f20\u9012<code>extra_cflags=['-O3']</code>\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528 <code>extra_cflags</code>\u4f20\u9012\u8fdb\u4e00\u6b65\u5305\u542b\u76ee\u5f55\u3002</p> <p>\u63d0\u4f9b\u4e86\u6df7\u5408\u7f16\u8bd1\u7684CUDA\u652f\u6301\u3002\u53ea\u9700\u5c06CUDA\u6e90\u6587\u4ef6(.cu\u6216.cuh\uff09\u4e0e\u5176\u4ed6\u6e90\u4e00\u8d77\u4f20\u9012\u5373\u53ef\u3002\u8fd9\u4e9b\u6587\u4ef6\u5c06\u88ab\u68c0\u6d4b\uff0c\u5e76\u4e14\u4f7f\u7528nvcc\u800c\u4e0d\u662fC ++\u7f16\u8bd1\u5668\u8fdb\u884c\u7f16\u8bd1\u3002\u5305\u62ec\u5c06CUDA lib64\u76ee\u5f55\u4f5c\u4e3a\u5e93\u76ee\u5f55\u4f20\u9012\u5e76\u8fdb\u884ccudart\u94fe\u63a5\u3002\u60a8\u53ef\u4ee5\u5c06\u5176\u4ed6\u53c2\u6570\u4f20\u9012\u7ed9nvcc extra_cuda_cflags\uff0c\u5c31\u50cf\u4f7f\u7528C ++\u7684extra_cflags\u4e00\u6837\u3002\u4f7f\u7528\u4e86\u5404\u79cd\u539f\u59cb\u65b9\u6cd5\u6765\u67e5\u627eCUDA\u5b89\u88c5\u76ee\u5f55\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\u3002\u5982\u679c\u4e0d\u53ef\u4ee5\uff0c\u6700\u597d\u8bbe\u7f6eCUDA_HOME\u73af\u5883\u53d8\u91cf\u3002</p> <p>\u53c2\u6570: *   name - \u8981\u6784\u5efa\u7684\u6269\u5c55\u540d\u3002\u8fd9\u4e2a\u5fc5\u987b\u548c<code>pybind11</code>\u6a21\u5757\u7684\u540d\u5b57\u4e00\u6837\uff01 *   sources - <code>C++</code>\u6e90\u6587\u4ef6\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u8def\u5f84\u5217\u8868\u3002 *   extra_cflags - \u7f16\u8bd1\u5668\u53c2\u6570\u7684\u53ef\u9009\u5217\u8868\uff0c\u7528\u4e8e\u8f6c\u53d1\u5230\u6784\u5efa\u3002 *   extra_cuda_cflags - \u7f16\u8bd1\u5668\u6807\u8bb0\u7684\u53ef\u9009\u5217\u8868\uff0c\u5728\u6784\u5efa<code>CUDA</code>\u6e90\u65f6\u8f6c\u53d1\u7ed9<code>nvcc</code>\u3002 *   extra_ldflags - \u94fe\u63a5\u5668\u53c2\u6570\u7684\u53ef\u9009\u5217\u8868\uff0c\u7528\u4e8e\u8f6c\u53d1\u5230\u6784\u5efa\u3002 *   extra_include_paths - \u8f6c\u53d1\u5230\u6784\u5efa\u7684\u5305\u542b\u76ee\u5f55\u7684\u53ef\u9009\u5217\u8868\u3002 *   build_directory - \u53ef\u9009\u8def\u5f84\u4f5c\u4e3a\u6784\u5efa\u533a\u57df\u3002 *   verbose - \u5982\u679c\u4e3a<code>True</code>\uff0c\u6253\u5f00\u52a0\u8f7d\u6b65\u9aa4\u7684\u8be6\u7ec6\u8bb0\u5f55\u3002 *   with_cuda \u2013 \u786e\u5b9a\u6784\u5efa\u662f\u662f\u5426\u5305\u542bCUDA\u5934/\u5e93. \u9ed8\u8ba4\u503c <code>None</code>, \u81ea\u52a8\u901a\u8fc7<code>sources</code>\u76ee\u5f55\u662f\u5426\u5b58\u5728 <code>.cu</code> \u6216 <code>.cuh</code>\u6587\u4ef6\u786e\u5b9a.  <code>True</code>\u5f3a\u5236\u5305\u542b. *   is_python_module \u2013 \u9ed8\u8ba4\u503c <code>True</code>: python\u6a21\u5757\u65b9\u5f0f\u5bfc\u5165. <code>False</code>: \u666e\u901a\u52a8\u6001\u5e93\u65b9\u5f0f\u52a0\u8f7d\u5230\u7a0b\u5e8f.</p> \u8fd4\u56de: <code>is_python_module</code> == <code>True</code>, \u52a0\u8f7d<code>PyTorch</code>\u6269\u5c55\u4f5c\u4e3a<code>Python</code>\u6a21\u5757\u3002If <code>is_python_module</code> == <code>False</code> \u65e0\u8fd4\u56de (\u526f\u4f5c\u7528\u662f\u5171\u4eab\u5e93\u88ab\u52a0\u8f7d\u5230\u8fdb\u7a0b). <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; from torch.utils.cpp_extension import load\n&gt;&gt;&gt; module = load(\nname='extension',\nsources=['extension.cpp', 'extension_kernel.cu'],\nextra_cflags=['-O2'],\nverbose=True)\n\n</code></pre> <pre><code>torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True)\n</code></pre> <p>\u5728\u8fd0\u884c\u65f6\u7f16\u8bd1\u52a0\u8f7dPyTorch C++ \u6269\u5c55</p> <p>\u8fd9\u4e2a\u51fd\u6570\u5f88\u50cf<code>load()</code>\uff0c\u4f46\u662f\u5b83\u7684\u6e90\u6587\u4ef6\u662f\u5b57\u7b26\u4e32\u800c\u4e0d\u662f\u6587\u4ef6\u540d\u3002\u5728\u628a\u8fd9\u4e9b\u5b57\u7b26\u4e32\u4fdd\u5b58\u5230\u6784\u5efa\u76ee\u5f55\u540e\uff0c<code>load_inline()</code> \u7b49\u4ef7\u4e8e <code>load()</code>.</p> <p>\u4f8b\u5b50\uff1a the tests </p> <p>\u6e90\u4ee3\u7801\u53ef\u80fd\u4f1a\u7701\u7565\u975e\u5185\u8054c++\u6269\u5c55\u7684\u4e24\u4e2a\u5fc5\u8981\u90e8\u5206:\u5fc5\u8981\u7684\u5934\u6587\u4ef6,\u4ee5\u53ca(pybind11)\u7ed1\u5b9a\u4ee3\u7801\u3002\u66f4\u51c6\u786e\u5730\u8bf4\uff0c\u4f20\u9012\u7ed9<code>cpp_sources</code>\u7684\u5b57\u7b26\u4e32\u9996\u5148\u8fde\u63a5\u6210\u4e00\u4e2a\u5355\u72ec\u7684<code>.cpp</code>\u6587\u4ef6\u3002\u7136\u540e\u5728\u8fd9\u4e2a\u6587\u4ef6\u524d\u9762\u52a0\u4e0a<code>#include &amp; lt;torch/extension.h&amp;gt;</code></p> <p>\u6b64\u5916\uff0c\u5982\u679c\u63d0\u4f9b\u4e86<code>functions</code>\u7684\u53c2\u6570\uff0c\u6307\u5b9a\u7684\u51fd\u6570\u5c06\u81ea\u52a8\u751f\u6210\u7ed1\u5b9a\u3002<code>functions</code>\u53ef\u4ee5\u662f\u51fd\u6570\u540d\u5217\u8868\uff0c\u4e5f\u53ef\u4ee5\u662f{\u51fd\u6570\u540d:\u6587\u6863\u5b57\u7b26\u4e32}\u7684\u5b57\u5178\u3002\u5982\u679c\u7ed9\u5b9a\u4e86\u4e00\u4e2a\u5217\u8868\uff0c\u5219\u6bcf\u4e2a\u51fd\u6570\u7684\u540d\u79f0\u7528\u4f5c\u5176\u6587\u6863\u5b57\u7b26\u4e32\u3002</p> <p><code>cuda_sources</code>\u4e2d\u7684\u4ee3\u7801\u6309\u987a\u5e8f\u8fde\u63a5\u5230\u5355\u72ec\u7684<code>.cu</code>\u6587\u4ef6,\u8ffd\u52a0<code>torch/types.h</code>, <code>cuda.h</code> and <code>cuda_runtime.h</code>\u5934\u6587\u4ef6.<code>.cpp</code> \u548c <code>.cu</code> \u6587\u4ef6\u5206\u5f00\u7f16\u8bd1, \u6700\u7ec8\u8fde\u63a5\u5230\u4e00\u4e2a\u5e93\u4e2d. \u6ce8\u610f<code>cuda_sources</code>\u4e2d\u7684\u51fd\u6570\u672c\u8eab\u6ca1\u6709\u7ed1\u5b9a,\u4e3a\u4e86\u7ed1\u5b9aCUDA\u6838\u51fd\u6570,\u5fc5\u987b\u65b0\u5efa\u4e00\u4e2aC++\u51fd\u6570\u6765\u8c03\u7528\u5b83,\u6216\u8005\u5728<code>cpp_sources</code> \u4e2d\u58f0\u660e\u6216\u5b9a\u4e49(\u5e76\u4e14\u5728<code>functions</code>\u4e2d\u5305\u542b\u5b83).</p> <p><code>load()</code>\u67e5\u770b\u4e0b\u9762\u5ffd\u7565\u7684\u53c2\u6570.</p> <p>\u53c2\u6570:</p> <ul> <li>cpp_sources \u2013 \u5b57\u7b26\u4e32, or \u5b57\u7b26\u4e32\u5217\u8868, \u5305\u542bC++\u6e90\u4ee3\u7801</li> <li>cuda_sources \u2013 \u5b57\u7b26\u4e32, or \u5b57\u7b26\u4e32\u5217\u8868, \u5305\u542bCUDA\u6e90\u4ee3\u7801</li> <li>functions \u2013 \u51fd\u6570\u540d\u5217\u8868 \u7528\u4e8e\u751f\u6210\u51fd\u6570\u7ed1\u5b9a. \u5982\u679c\u662f\u5b57\u5178,key=\u51fd\u6570\u540d,value=\u6587\u6863\u63cf\u8ff0.</li> <li>with_cuda \u2013 \u786e\u5b9a\u662f\u5426\u6dfb\u52a0CUDA\u5934/\u5e93. \u9ed8\u8ba4\u503c <code>None</code> (default), \u53d6\u51b3\u4e8e\u53c2\u6570<code>cuda_sources</code> . <code>True</code>\u5f3a\u5236\u5305\u542bCUDA\u5934/\u5e93.</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; from torch.utils.cpp_extension import load_inline\n&gt;&gt;&gt; source = '''\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\nreturn x.sin() + y.sin();\n}\n'''\n&gt;&gt;&gt; module = load_inline(name='inline_extension',\ncpp_sources=[source],\nfunctions=['sin_add'])\n\n</code></pre> <pre><code>torch.utils.cpp_extension.include_paths(cuda=False)\n</code></pre> <p>\u83b7\u53d6\u6784\u5efa<code>C++</code>\u6216<code>CUDA</code>\u6269\u5c55\u6240\u9700\u7684\u8def\u5f84\u3002</p> <ul> <li>\u53c2\u6570\uff1a <code>cuda</code> - \u5982\u679c\u4e3aTrue\uff0c\u5219\u5305\u542b<code>CUDA</code>\u7279\u5b9a\u7684\u5305\u542b\u8def\u5f84\u3002</li> <li>\u8fd4\u56de\uff1a \u5305\u542b\u8def\u5f84\u5b57\u7b26\u4e32\u7684\u5217\u8868\u3002</li> </ul> <pre><code>torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)\n</code></pre> <p>\u9a8c\u8bc1\u7ed9\u5b9a\u7684\u7f16\u8bd1\u5668\u662f\u5426\u4e0e<code>PyTorch</code> ABI\u517c\u5bb9\u3002</p> <ul> <li>\u53c2\u6570\uff1acompiler(str) - \u8981\u68c0\u67e5\u53ef\u6267\u884c\u7684\u7f16\u8bd1\u5668\u6587\u4ef6\u540d(\u4f8b\u5982<code>g++</code>),\u5fc5\u987b\u5728<code>shell</code>\u8fdb\u7a0b\u4e2d\u53ef\u6267\u884c\u3002</li> <li>\u8fd4\u56de\uff1a\u5982\u679c\u7f16\u8bd1\u5668(\u53ef\u80fd\uff09\u4e0e<code>PyTorch</code>ABI\u4e0d\u517c\u5bb9\uff0c\u5219\u4e3a<code>False</code>\uff0c\u5426\u5219\u8fd4\u56de<code>True</code>\u3002</li> </ul> <pre><code>torch.utils.cpp_extension.verify_ninja_availability()\n</code></pre> <p>\u5982\u679c\u53ef\u4ee5\u5728ninja\u4e0a\u8fd0\u884c\u5219\u8fd4\u56de<code>True</code>\u3002</p>"},{"location":"1.0/docs_notes/","title":"\u6ce8\u89e3","text":""},{"location":"1.0/docs_package_ref/","title":"\u5305\u53c2\u8003","text":""},{"location":"1.0/docs_torchvision_ref/","title":"torchvision \u53c2\u8003","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <p><code>torchvision</code> \u5305\u6536\u5f55\u4e86\u82e5\u5e72\u91cd\u8981\u7684\u516c\u5f00\u6570\u636e\u96c6\u3001\u7f51\u7edc\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e38\u7528\u56fe\u50cf\u53d8\u6362</p> <p>\u5305\u53c2\u8003</p> <ul> <li>torchvision.datasets<ul> <li>MNIST</li> <li>Fashion-MNIST</li> <li>EMNIST</li> <li>COCO</li> <li>LSUN</li> <li>ImageFolder</li> <li>DatasetFolder</li> <li>Imagenet-12</li> <li>CIFAR</li> <li>STL10</li> <li>SVHN</li> <li>PhotoTour</li> <li>SBU</li> <li>Flickr</li> <li>VOC</li> </ul> </li> <li>torchvision.models<ul> <li>Alexnet</li> <li>VGG</li> <li>ResNet</li> <li>SqueezeNet</li> <li>DenseNet</li> <li>Inception v3</li> </ul> </li> <li>torchvision.transforms<ul> <li>Transforms on PIL Image</li> <li>Transforms on torch.*Tensor</li> <li>Conversion Transforms</li> <li>Generic Transforms</li> <li>Functional Transforms</li> </ul> </li> <li>torchvision.utils</li> </ul> <pre><code>torchvision.get_image_backend()\n</code></pre> <p>\u67e5\u770b\u8f7d\u5165\u56fe\u7247\u7684\u5305\u7684\u540d\u79f0</p> <pre><code>torchvision.set_image_backend(backend)\n</code></pre> <p>\u6307\u5b9a\u7528\u4e8e\u8f7d\u5165\u56fe\u7247\u7684\u5305</p> \u53c2\u6570: backend (string) \u2013 \u56fe\u7247\u5904\u7406\u540e\u7aef\u7684\u540d\u79f0\uff0c\u987b\u4e3a{'PIL', 'accimage'}\u4e2d\u7684\u4e00\u4e2a\u3002<code>accimage</code>\u5305\u4f7f\u7528\u4e86\u82f1\u7279\u5c14IPP\u5e93\u3002\u8fd9\u4e2a\u5e93\u901a\u5e38\u6bd4PIL\u5feb\uff0c\u4f46\u662f\u652f\u6301\u7684\u64cd\u4f5c\u6bd4PIL\u8981\u5c11\u3002"},{"location":"1.0/fgsm_tutorial/","title":"\u5bf9\u6297\u6027\u793a\u4f8b\u751f\u6210","text":"<p>\u8bd1\u8005\uff1acangyunye</p> <p>\u4f5c\u8005: Nathan Inkawhich</p> <p>\u5982\u679c\u4f60\u6b63\u5728\u9605\u8bfb\u8fd9\u7bc7\u6587\u7ae0\uff0c\u5e0c\u671b\u4f60\u80fd\u7406\u89e3\u4e00\u4e9b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u662f\u591a\u4e48\u6709\u6548\u3002\u73b0\u5728\u7684\u7814\u7a76\u6b63\u5728\u4e0d\u65ad\u63a8\u52a8ML\u6a21\u578b\u53d8\u5f97\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u548c\u66f4\u9ad8\u6548\u3002\u7136\u800c\uff0c\u5728\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6a21\u578b\u4e2d\u7ecf\u5e38\u4f1a\u5ffd\u89c6\u7684\u662f\u5b89\u5168\u6027\u548c\u5065\u58ee\u6027\u65b9\u9762\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6b3a\u9a97\u6a21\u578b\u7684\u5bf9\u624b\u65f6\u3002</p> <p>\u672c\u6559\u7a0b\u5c06\u63d0\u9ad8\u60a8\u5bf9ML\u6a21\u578b\u5b89\u5168\u6f0f\u6d1e\u7684\u8ba4\u8bc6\uff0c\u5e76\u5c06\u6df1\u5165\u63a2\u8ba8\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u8fd9\u4e00\u70ed\u95e8\u8bdd\u9898\u3002\u60a8\u53ef\u80fd\u4f1a\u60ca\u8bb6\u5730\u53d1\u73b0\uff0c\u5728\u56fe\u50cf\u4e2d\u6dfb\u52a0\u7ec6\u5fae\u7684\u5e72\u6270\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u7684\u5de8\u5927\u5dee\u5f02\u3002\u9274\u4e8e\u8fd9\u662f\u4e00\u4e2a\u6559\u7a0b\uff0c\u6211\u4eec\u5c06\u901a\u8fc7\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668\u4e0a\u7684\u793a\u4f8b\u6765\u63a2\u7d22\u8fd9\u4e2a\u4e3b\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u7b2c\u4e00\u4e2a\u4e5f\u662f\u6700\u6d41\u884c\u7684\u653b\u51fb\u65b9\u6cd5\u4e4b\u4e00\uff0c\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u653b\u51fb<code>Fast Gradient Sign Attack</code>(FGSM)\uff0c\u4ee5\u6b3a\u9a97\u4e00\u4e2aMNIST\u5206\u7c7b\u5668\u3002</p>"},{"location":"1.0/fgsm_tutorial/#_2","title":"\u5a01\u80c1\u6a21\u578b","text":"<p>\u5c31\u4e0a\u4e0b\u6587\u800c\u8a00\uff0c\u6709\u8bb8\u591a\u7c7b\u578b\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u6bcf\u4e00\u7c7b\u653b\u51fb\u90fd\u6709\u4e0d\u540c\u7684\u76ee\u6807\u548c\u5bf9\u653b\u51fb\u8005\u77e5\u8bc6\u7684\u5047\u8bbe\u3002\u7136\u800c\uff0c\u603b\u7684\u76ee\u6807\u662f\u5728\u8f93\u5165\u6570\u636e\u4e2d\u6dfb\u52a0\u6700\u5c11\u7684\u6270\u52a8\uff0c\u4ee5\u5bfc\u81f4\u6240\u9700\u7684\u9519\u8bef\u5206\u7c7b\u3002\u653b\u51fb\u8005\u7684\u77e5\u8bc6\u6709\u51e0\u79cd\u5047\u8bbe\uff0c\u5176\u4e2d\u4e24\u79cd\u662f:\u767d\u76d2\u548c\u9ed1\u76d2\u3002\u767d\u76d2\u653b\u51fb\u5047\u5b9a\u653b\u51fb\u8005\u5177\u6709\u5bf9\u6a21\u578b\u7684\u5168\u90e8\u77e5\u8bc6\u548c\u8bbf\u95ee\u6743\uff0c\u5305\u62ec\u4f53\u7cfb\u7ed3\u6784\u3001\u8f93\u5165\u3001\u8f93\u51fa\u548c\u6743\u91cd\u3002\u9ed1\u76d2\u653b\u51fb\u5047\u8bbe\u653b\u51fb\u8005\u53ea\u8bbf\u95ee\u6a21\u578b\u7684\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u5bf9\u5e95\u5c42\u67b6\u6784\u6216\u6743\u91cd\u4e00\u65e0\u6240\u77e5\u3002\u76ee\u6807\u4e5f\u6709\u51e0\u79cd\u7c7b\u578b\uff0c\u5305\u62ec\u9519\u8bef\u5206\u7c7b\u548c\u6e90/\u76ee\u6807\u9519\u8bef\u5206\u7c7b\u3002\u9519\u8bef\u5206\u7c7b\u7684\u76ee\u6807\u610f\u5473\u7740\u5bf9\u624b\u53ea\u5e0c\u671b\u8f93\u51fa\u5206\u7c7b\u662f\u9519\u8bef\u7684\uff0c\u800c\u4e0d\u5173\u5fc3\u65b0\u7684\u5206\u7c7b\u662f\u4ec0\u4e48\u3002\u6e90/\u76ee\u6807\u9519\u8bef\u5206\u7c7b\u610f\u5473\u7740\u5bf9\u624b\u60f3\u8981\u66f4\u6539\u539f\u6765\u5c5e\u4e8e\u7279\u5b9a\u6e90\u7c7b\u7684\u56fe\u50cf\uff0c\u4ee5\u4fbf\u5c06\u5176\u5206\u7c7b\u4e3a\u7279\u5b9a\u7684\u76ee\u6807\u7c7b\u3002</p> <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cFGSM\u653b\u51fb\u662f\u4e00\u79cd\u4ee5\u9519\u8bef\u5206\u7c7b\u4e3a\u76ee\u6807\u7684\u767d\u76d2\u653b\u51fb\u3002\u6709\u4e86\u8fd9\u4e9b\u80cc\u666f\u4fe1\u606f\uff0c\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u8be6\u7ec6\u8ba8\u8bba\u653b\u51fb\u3002</p>"},{"location":"1.0/fgsm_tutorial/#_3","title":"\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u653b\u51fb","text":"<p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6700\u65e9\u4e5f\u662f\u6700\u6d41\u884c\u7684\u5bf9\u6297\u6027\u653b\u51fb\u4e4b\u4e00\u88ab\u79f0\u4e3a\u5feb\u901f\u68af\u5ea6\u7b26\u53f7\u653b\u51fb(FGSM)\uff0c\u7531Goodfellow\u7b49\u4eba\u5728\u89e3\u91ca\u548c\u5229\u7528\u5bf9\u6297\u6027\u793a\u4f8b(  Explaining and Harnessing Adversarial Examples)\u65f6\u4ecb\u7ecd\u5230\u3002\u8fd9\u79cd\u653b\u51fb\u975e\u5e38\u5f3a\u5927\uff0c\u800c\u4e14\u76f4\u89c2\u3002\u5b83\u88ab\u8bbe\u8ba1\u7528\u6765\u653b\u51fb\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u4ed6\u4eec\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u68af\u5ea6<code>gradients</code>\u3002\u8fd9\u4e2a\u60f3\u6cd5\u5f88\u7b80\u5355\uff0c\u6bd4\u8d77\u6839\u636e\u540e\u5411\u4f20\u64ad\u68af\u5ea6\u6765\u8c03\u6574\u6743\u91cd\u4f7f\u635f\u5931\u6700\u5c0f\u5316\uff0c\u8fd9\u79cd\u653b\u51fb\u662f\u6839\u636e\u76f8\u540c\u7684\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u8c03\u6574\u8f93\u5165\u6570\u636e\u6765\u6700\u5927\u5316\u635f\u5931\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u653b\u51fb\u4f7f\u7528\u4e86\u8f93\u5165\u6570\u636e\u76f8\u5173\u7684\u68af\u5ea6\u635f\u5931\u65b9\u5f0f\uff0c\u901a\u8fc7\u8c03\u6574\u8f93\u5165\u6570\u636e\uff0c\u4f7f\u635f\u5931\u6700\u5927\u5316\u3002</p> <p>\u5728\u6211\u4eec\u6df1\u5165\u4ee3\u7801\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u770b\u770b\u8457\u540d\u7684FGSM  panda\u793a\u4f8b\u5e76\u63d0\u53d6\u4e00\u4e9b\u7b26\u53f7\u3002</p> <p></p> <p>\u4ece\u56fe\u50cf\u4e2d\u770b\uff0c\\(\\(\\mathbf{x}\\)\\) \u662f\u4e00\u4e2a\u6b63\u786e\u5206\u7c7b\u4e3a\u201c\u718a\u732b\u201d(panda)\u7684\u539f\u59cb\u8f93\u5165\u56fe\u50cf\uff0c \\(\\(y\\)\\) \u662f\u5bf9 \\(\\(\\mathbf{x}\\)\\) \u7684\u771f\u5b9e\u8868\u5f81\u6807\u7b7e<code>ground truth label</code>, \\(\\(\\mathbf{\\theta}\\)\\) \u8868\u793a\u6a21\u578b\u53c2\u6570\uff0c \u800c \\(\\(J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)\\) \u662f\u7528\u6765\u8bad\u7ec3\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u3002 \u8fd9\u79cd\u653b\u51fb\u5c06\u68af\u5ea6\u540e\u5411\u4f20\u64ad\u5230\u8f93\u5165\u6570\u636e\u6765\u8ba1\u7b97 \\(\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)\\)\u3002\u7136\u540e\u5c06\u8f93\u5165\u6570\u636e\u901a\u8fc7\u4e00\u5c0f\u6b65 \\(\\(\\epsilon\\)\\) \u6216 \u5982\u56fe\u4e2d\u7684\\(\\(0.007\\)\\) ) \u5728(i.e. \\(\\(sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\)\\)  \u65b9\u5411\u4e0a\u8c03\u6574\uff0c\u4f7f\u635f\u5931\u6700\u5927\u5316\u3002\u7ed3\u679c\u5c06\u5f97\u5230\u53d7\u5230\u5e72\u6270\u7684\u56fe\u50cf\uff0c \\(\\(x'\\)\\)\uff0c\u5c3d\u7ba1\u56fe\u7247\u8fd8\u662f\u201c\u718a\u732b\u201d\uff0c\u4f46\u5b83\u4e00\u676f\u76ee\u6807\u7f51\u7edc\u9519\u8bef\u5206\u7c7b\u4e3a\u201c\u957f\u81c2\u733f\u201d(gibbon)\u4e86</p> <p>\u5e0c\u671b\u770b\u5230\u73b0\u5728\u7684\u4f60\uff0c\u5df2\u7ecf\u660e\u786e\u4e86\u89e3\u4e86\u672c\u6559\u7a0b\u7684\u52a8\u673a\uff0c\u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u5f00\u59cb\u5b9e\u73b0\u5b83\u5427\u3002</p> <pre><code>from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#_4","title":"\u5b9e\u73b0","text":"<p>\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u672c\u6559\u7a0b\u7684\u8f93\u5165\u53c2\u6570\uff0c\u5b9a\u4e49\u53d7\u653b\u51fb\u7684\u6a21\u578b\uff0c\u7136\u540e\u7f16\u5199\u653b\u51fb\u4ee3\u7801\u5e76\u8fd0\u884c\u4e00\u4e9b\u6d4b\u8bd5\u3002</p>"},{"location":"1.0/fgsm_tutorial/#_5","title":"\u8f93\u5165","text":"<p>\u672c\u6559\u7a0b\u53ea\u6709\u4e09\u4e2a\u8f93\u5165\uff0c\u5b9a\u4e49\u5982\u4e0b:</p> <ul> <li>epsilons - \u8981\u7528\u4e8e\u8fd0\u884c\u7684epsilon\u503c\u7684\u5217\u8868\u3002\u5728\u5217\u8868\u4e2d\u4fdd\u75590\u662f\u5f88\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u5b83\u4ee3\u8868\u4e86\u539f\u59cb\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6a21\u578b\u6027\u80fd\u3002\u800c\u4e14\uff0c\u76f4\u89c9\u4e0a\u6211\u4eec\u8ba4\u4e3a\uff0cepsilon\u8d8a\u5927\uff0c\u6270\u52a8\u8d8a\u660e\u663e\uff0c\u4f46\u5728\u964d\u4f4e\u6a21\u578b\u7cbe\u5ea6\u65b9\u9762\u653b\u51fb\u8d8a\u6709\u6548\u3002\u56e0\u4e3a\u8fd9\u91cc\u7684\u6570\u636e\u8303\u56f4\u662f \\(\\([0,1]\\)\\)\uff0c\u6240\u4ee5\u53d6\u503c\u4e0d\u5e94\u8be5\u8d85\u8fc71\u3002</li> <li>pretrained_model - \u8868\u793a\u4f7f\u7528 pytorch/examples/mnist\u8fdb\u884c\u8bad\u7ec3\u7684\u9884\u8bad\u7ec3MNIST\u6a21\u578b\u7684\u8def\u5f84\u3002\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u5728\u8fd9\u91cc \u4e0b\u8f7d\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u3002</li> <li>use_cuda - \u5982\u679c\u9700\u8981\u548c\u53ef\u7528\uff0c\u4f7f\u7528CUDA\u7684\u5e03\u5c14\u6807\u5fd7\u3002\u6ce8\u610f\uff0c\u5e26\u6709CUDA\u7684GPU\u5bf9\u4e8e\u672c\u6559\u7a0b\u6765\u8bf4\u5e76\u4e0d\u91cd\u8981\uff0c\u56e0\u4e3aCPU\u4e0d\u4f1a\u5360\u7528\u592a\u591a\u65f6\u95f4\u3002</li> </ul> <pre><code>epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#_6","title":"\u53d7\u653b\u6a21\u578b","text":"<p>\u5982\u524d\u6240\u8ff0\uff0c\u53d7\u653b\u6a21\u578b\u4e0epytorch/examples/mnist\u4e2d\u7684MNIST\u6a21\u578b\u76f8\u540c\u3002\u60a8\u53ef\u4ee5\u8bad\u7ec3\u5e76\u4fdd\u5b58\u81ea\u5df1\u7684MNIST\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u4e0b\u8f7d\u5e76\u4f7f\u7528\u63d0\u4f9b\u7684\u6a21\u578b\u3002\u8fd9\u91cc\u7684Net\u5b9a\u4e49\u548c\u6d4b\u8bd5dataloader\u662f\u4eceMNIST\u793a\u4f8b\u4e2d\u590d\u5236\u7684\u3002\u672c\u8282\u7684\u76ee\u7684\u662f\u5b9a\u4e49\u6a21\u578b\u548c\u52a0\u8f7d\u6570\u636e\uff0c\u7136\u540e\u521d\u59cb\u5316\u6a21\u578b\u5e76\u52a0\u8f7d\u9884\u5148\u8bad\u7ec3\u7684\u6743\u91cd\u3002</p> <pre><code># LeNet Model definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# MNIST Test dataset and dataloader declaration\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n            transforms.ToTensor(),\n            ])),\n        batch_size=1, shuffle=True)\n\n# Define what device we are using\nprint(\"CUDA Available: \",torch.cuda.is_available())\ndevice = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\n# Initialize the network\nmodel = Net().to(device)\n\n# Load the pretrained model\nmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\nmodel.eval()\n\n</code></pre> <p>Out:</p> <pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\nCUDA Available:  True\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#fgsm","title":"FGSM \u653b\u51fb\u65b9\u5f0f","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2a\u901a\u8fc7\u6253\u4e71\u539f\u59cb\u8f93\u5165\u6765\u751f\u6210\u5bf9\u6297\u6027\u793a\u4f8b\u7684\u51fd\u6570\u3002 <code>fgsm_attack</code> \u51fd\u6570\u67093\u4e2a\u8f93\u5165, image \u662f\u539f\u59cb\u56fe\u50cf \\(\\(x\\)\\) \uff0c epsilon \u662f\u50cf\u7d20\u7ea7\u5e72\u6270\u91cf \\(\\(\\epsilon\\)\\)\uff0cdata_grad \u662f\u5173\u4e8e\u8f93\u5165\u56fe\u50cf \\(\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)\\) \u7684\u635f\u5931\u3002\u7136\u540e\u8be5\u51fd\u6570\u521b\u5efa\u5e72\u6270\u56fe\u50cf\u5982\u4e0b</p> \\[perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\] <p>\u6700\u540e\uff0c\u4e3a\u4e86\u4fdd\u6301\u6570\u636e\u7684\u539f\u59cb\u8303\u56f4\uff0c\u5c06\u6270\u52a8\u540e\u7684\u56fe\u50cf\u622a\u53d6\u8303\u56f4\u5728 \\(\\([0,1]\\)\\)\u3002</p> <pre><code># FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#_7","title":"\u529f\u80fd\u9a8c\u8bc1","text":"<p>\u6700\u540e\uff0c\u672c\u6559\u7a0b\u7684\u6838\u5fc3\u7ed3\u679c\u6765\u81ea\u6d4b\u8bd5<code>test</code>\u51fd\u6570\u3002\u5bf9\u8fd9\u4e2a\u6d4b\u8bd5\u51fd\u6570\u7684\u6bcf\u6b21\u8c03\u7528\u90fd\u5728MNIST\u6d4b\u8bd5\u96c6\u4e0a\u6267\u884c\u4e00\u4e2a\u5b8c\u6574\u7684\u6d4b\u8bd5\u6b65\u9aa4\uff0c\u7136\u540e\u7ed9\u51fa\u4e00\u4e2a\u6700\u7ec8\u51c6\u786e\u6027\u62a5\u544a\u3002\u4f46\u662f\uff0c\u6ce8\u610f\u8fd9\u4e2a\u51fd\u6570\u4e5f\u63a5\u53d7\u4e00\u4e2a<code>epsilon</code>\u8f93\u5165\u3002\u8fd9\u662f\u56e0\u4e3a\u6d4b\u8bd5<code>test</code>\u51fd\u6570\u62a5\u544a\u4e86\u4e00\u4e2a\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u8be5\u6a21\u578b\u6b63\u53d7\u5230\u5f3a\u5ea6\u4e3a\\(\\(\\epsilon\\)\\)\u7684\u5bf9\u624b\u7684\u653b\u51fb\u3002\u66f4\u5177\u4f53\u5730\u8bf4\uff0c\u5bf9\u4e8e\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\uff0c\u8be5\u51fd\u6570\u8ba1\u7b97\u548c\u8f93\u5165\u6570\u636e  \\(\\(data\\_grad\\)\\) \u76f8\u5173\u7684\u635f\u5931\u68af\u5ea6\uff0c\u7528<code>fgsm_attack</code> \\(\\(perturbed\\_data\\)\\) \u521b\u5efa\u4e00\u4e2a\u5e72\u6270\u56fe\u50cf\uff0c\u7136\u540e\u68c0\u67e5\u5e72\u6270\u7684\u4f8b\u5b50\u662f\u5426\u662f\u5bf9\u6297\u6027\u7684\u3002\u9664\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u5916\uff0c\u51fd\u6570\u8fd8\u9700\u8981\u4fdd\u5b58\u548c\u8fd4\u56de\u4e00\u4e9b\u6210\u529f\u6027\u7684\u793a\u4f8b\u4ee5\u4fbf\u65e5\u540e\u67e5\u770b\u3002</p> <pre><code>def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) &lt; 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex))\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) &lt; 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex))\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#_8","title":"\u542f\u52a8\u653b\u51fb","text":"<p>\u5b9e\u73b0\u7684\u6700\u540e\u4e00\u90e8\u5206\u662f\u8fd0\u884c\u653b\u51fb\u64cd\u4f5c\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5bf9\u8f93\u5165\u4e2d\u7684\u6bcf\u4e2a<code>epsilon</code>\u503c\u8fd0\u884c\u4e00\u4e2a\u5b8c\u6574\u7684\u6d4b\u8bd5\u6b65\u9aa4\u3002\u5bf9\u4e8e\u6bcf\u4e2a<code>epsilon</code>\uff0c\u6211\u4eec\u4e5f\u4fdd\u5b58\u6700\u540e\u7684\u7cbe\u5ea6\u548c\u4e00\u4e9b\u5c06\u5728\u63a5\u4e0b\u6765\u7684\u90e8\u5206\u4e2d\u7ed8\u5236\u7684\u6210\u529f\u7684\u5bf9\u6297\u6027\u4f8b\u5b50\u3002\u8bf7\u6ce8\u610f\uff0c\u968f\u7740<code>epsilon</code>\u503c\u7684\u589e\u52a0\uff0c\u6253\u5370\u51fa\u6765\u7684\u7cbe\u5ea6\u662f\u5982\u4f55\u964d\u4f4e\u7684\u3002\u53e6\u5916\uff0c\u6ce8\u610f\\(\\(\\epsilon=0\\)\\)\u7528\u4f8b\u8868\u793a\u539f\u59cb\u672a\u53d7\u653b\u51fb\u7684\u6d4b\u8bd5\u51c6\u786e\u6027\u3002</p> <pre><code>accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)\n\n</code></pre> <p>Out:</p> <pre><code>Epsilon: 0      Test Accuracy = 9810 / 10000 = 0.981\nEpsilon: 0.05   Test Accuracy = 9426 / 10000 = 0.9426\nEpsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851\nEpsilon: 0.15   Test Accuracy = 6826 / 10000 = 0.6826\nEpsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301\nEpsilon: 0.25   Test Accuracy = 2082 / 10000 = 0.2082\nEpsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869\n\n</code></pre>"},{"location":"1.0/fgsm_tutorial/#_9","title":"\u7ed3\u679c","text":""},{"location":"1.0/fgsm_tutorial/#vs-epsilon","title":"\u51c6\u786e\u6027 vs Epsilon","text":"<p>\u7b2c\u4e00\u4e2a\u7ed3\u679c\u662f\u76f8\u5bf9\u4e8e<code>epsilon</code>\u7684\u7cbe\u786e\u5ea6\u3002\u6b63\u5982\u524d\u9762\u63d0\u5230\u7684\uff0c\u968f\u7740<code>epsilon</code>\u7684\u589e\u52a0\uff0c\u6211\u4eec\u9884\u671f\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u4f1a\u964d\u4f4e\u3002\u8fd9\u662f\u56e0\u4e3a\u66f4\u5927\u7684<code>epsilon</code>\u610f\u5473\u7740\u6211\u4eec\u5728\u4f7f\u635f\u5931\u6700\u5927\u5316\u7684\u65b9\u5411\u4e0a\u8fc8\u51fa\u4e86\u66f4\u5927\u7684\u4e00\u6b65\u3002\u6ce8\u610f\uff0c\u5373\u4f7f<code>epsilon</code>\u503c\u662f\u7ebf\u6027\u95f4\u9694\u7684\uff0c\u66f2\u7ebf\u7684\u8d8b\u52bf\u5374\u4e0d\u662f\u7ebf\u6027\u7684\u3002\u6bd4\u5982\u8bf4\uff0c\u7cbe\u5ea6\u5728\\(\\(\\epsilon=0.05\\)\\) \u53ea\u6bd4\\(\\(\\epsilon=0\\)\\)\u5c0f\u7ea64%\uff0c\u4f46\u662f\u8fd9\u4e2a \\(\\(\\epsilon=0.2\\)\\)\u7cbe\u5ea6\u5374\u6bd4 \\(\\(\\epsilon=0.15\\)\\)\u5c0f\u4e8625%\u3002 \u53e6\u5916\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728 \\(\\(\\epsilon=0.25\\)\\) \u548c \\(\\(\\epsilon=0.3\\)\\)\u4e4b\u95f4\u505a\u4e8610\u6b21\u5206\u7c7b\u7684\u5206\u7c7b\u5668\uff0c\u6a21\u578b\u7684\u7cbe\u5ea6\u4f1a\u8fbe\u5230\u968f\u673a\u7cbe\u5ea6\u3002</p> <pre><code>plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n</code></pre> <p></p>"},{"location":"1.0/fgsm_tutorial/#_10","title":"\u5bf9\u6297\u6027\u7528\u4f8b\u6837\u672c","text":"<p>\u5e76\u6ca1\u6709\u4ec0\u4e48\u5c3d\u5584\u5c3d\u7f8e\u4e4b\u4e8b\uff0c\u5728\u8fd9\u91cc\uff0c\u968f\u7740<code>epsilon</code>\u7684\u589e\u52a0\uff0c\u6d4b\u8bd5\u7cbe\u5ea6\u964d\u4f4e\uff0c\u4f46\u6270\u52a8\u53d8\u5f97\u66f4\u5bb9\u6613\u5bdf\u89c9\u3002\u5b9e\u9645\u4e0a\uff0c\u653b\u51fb\u8005\u5fc5\u987b\u8003\u8651\u51c6\u786e\u6027\u4e0b\u964d\u548c\u53ef\u611f\u77e5\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5728\u6bcf\u4e2a\u503c\u4e0a\u6210\u529f\u7684\u5bf9\u6297\u6027\u4f8b\u5b50\u3002\u56fe\u4e2d\u7684\u6bcf\u4e00\u884c\u90fd\u663e\u793a\u4e0d\u540c\u7684<code>epsilon</code>\u503c\u3002\u7b2c\u4e00\u884c\u662f\\(\\(\\epsilon=0\\)\\)\u7684\u4f8b\u5b50\uff0c\u5b83\u8868\u793a\u539f\u59cb\u7684\u65e0\u6270\u52a8\u7684\u7eaf\u51c0\u56fe\u50cf\u3002\u6bcf\u4e2a\u56fe\u50cf\u7684\u6807\u9898\u663e\u793a\u201c\u539f\u59cb\u5206\u7c7b-&gt;\u5e72\u6270\u5206\u7c7b(adversarial classification\uff09\u201d\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\\(\\(\\epsilon=0.15\\)\\)\u548c\\(\\(\\epsilon=0.3\\)\\)\u5904\u5f00\u59cb\u51fa\u73b0\u660e\u663e\u7684\u6270\u52a8\u3002\u7136\u800c\uff0c\u5728\u6240\u6709\u60c5\u51b5\u4e0b\uff0c\u5c3d\u7ba1\u6dfb\u52a0\u4e86\u8e81\u52a8\u56e0\u7d20(\u5e72\u6270\uff09\uff0c\u4eba\u7c7b\u4ecd\u7136\u80fd\u591f\u8bc6\u522b\u6b63\u786e\u7684\u7c7b\u3002</p> <pre><code># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n</code></pre> <p></p>"},{"location":"1.0/fgsm_tutorial/#_11","title":"\u63a5\u4e0b\u6765\u7684\u65b9\u5411","text":"<p>\u5e0c\u671b\u672c\u6559\u7a0b\u5bf9\u60a8\u6765\u8bf4\uff0c\u80fd\u591f\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u4e3b\u9898\u7684\u89c1\u89e3\u3002\u4ece\u8fd9\u91cc\u5f00\u59cb\u6709\u5f88\u591a\u53ef\u80fd\u7684\u65b9\u5411\u3002\u8fd9\u79cd\u653b\u51fb\u4ee3\u8868\u4e86\u5bf9\u6297\u6027\u653b\u51fb\u7814\u7a76\u7684\u5f00\u59cb\uff0c\u5e76\u4e14\u81ea\u4ece\u6709\u4e86\u8bb8\u591a\u5173\u4e8e\u5982\u4f55\u653b\u51fb\u548c\u4fdd\u62a4ML\u6a21\u578b\u4e0d\u53d7\u5bf9\u624b\u653b\u51fb\u7684\u540e\u7eed\u60f3\u6cd5\u4ee5\u6765\u3002\u4e8b\u5b9e\u4e0a\uff0c\u5728NIPS 2017\u5e74\u6709\u4e00\u573a\u5bf9\u6297\u6027\u7684\u653b\u9632\u7ade\u8d5b\uff0c\u672c\u6587\u63cf\u8ff0\u4e86\u5f88\u591a\u6bd4\u8d5b\u4e2d\u4f7f\u7528\u7684\u65b9\u6cd5:\u5bf9\u6297\u6027\u7684\u653b\u9632\u53ca\u7ade\u8d5b(Adversarial Attacks and Defences Competition\uff09\u3002\u5728\u9632\u5fa1\u65b9\u9762\u7684\u5de5\u4f5c\u4e5f\u5f15\u5165\u4e86\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u66f4\u5065\u58ee<code>*robust*</code>\u7684\u60f3\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u7136\u6270\u52a8\u548c\u53cd\u5411\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f93\u5165\u3002</p> <p>\u53e6\u4e00\u4e2a\u7814\u7a76\u65b9\u5411\u662f\u4e0d\u540c\u9886\u57df\u7684\u5bf9\u6297\u6027\u653b\u51fb\u548c\u9632\u5fa1\u3002\u5bf9\u6297\u6027\u7814\u7a76\u5e76\u4e0d\u5c40\u9650\u4e8e\u56fe\u50cf\u9886\u57df\uff0c\u5c31\u6bd4\u5982\u8fd9\u79cd\u8bed\u97f3\u5230\u6587\u672c\u6a21\u578b<code>speech-to-text models</code>\u7684\u653b\u51fb\u3002\u5f53\u7136\uff0c\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u7684\u6700\u597d\u65b9\u6cd5\u662f\u591a\u52a8\u624b\u3002\u9996\u5148\uff0c\u5c1d\u8bd5\u5b9e\u73b0\u4e00\u4e2a\u4e0d\u540c\u4e8eNIPS 2017\u6bd4\u8d5b\u7684\u653b\u51fb\uff0c\u770b\u770b\u5b83\u4e0eFGSM\u6709\u4ec0\u4e48\u4e0d\u540c\uff0c\u7136\u540e\uff0c\u5c1d\u8bd5\u8bbe\u8ba1\u4fdd\u62a4\u6a21\u578b\uff0c\u4f7f\u5176\u514d\u4e8e\u81ea\u5df1\u7684\u653b\u51fb\u3002</p>"},{"location":"1.0/finetuning_torchvision_models_tutorial/","title":"Torchvision\u6a21\u578b\u5fae\u8c03","text":"<p>\u8bd1\u8005\uff1aZHHAYO</p> <p>\u4f5c\u8005: Nathan Inkawhich</p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u6df1\u5165\u63a2\u8ba8\u5982\u4f55\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6torchvision \u6a21\u578b\uff0c\u6240\u6709\u8fd9\u4e9b\u6a21\u578b\u90fd\u5df2\u7ecf\u9884\u5148\u57281000\u7c7b\u7684magenet\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5b8c\u6210\u3002\u672c\u7a0b\u5c06\u6df1\u5165\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u51e0\u4e2a\u73b0\u4ee3\u7684CNN\u67b6\u6784\uff0c\u5e76\u5c06\u4e3a\u5fae\u8c03\u4efb\u610f\u7684PyTorch\u6a21\u578b\u5efa\u7acb\u4e00\u4e2a\u76f4\u89c9\u3002 \u7531\u4e8e\u6bcf\u4e2a\u6a21\u578b\u67b6\u6784\u662f\u6709\u5dee\u5f02\u7684\uff0c\u56e0\u6b64\u6ca1\u6709\u53ef\u4ee5\u5728\u6240\u6709\u573a\u666f\u4e2d\u4f7f\u7528\u7684\u6837\u677f\u5fae\u8c03\u4ee3\u7801\u3002 \u7136\u800c\uff0c\u7814\u7a76\u4eba\u5458\u5fc5\u987b\u67e5\u770b\u73b0\u6709\u67b6\u6784\u5e76\u5bf9\u6bcf\u4e2a\u6a21\u578b\u8fdb\u884c\u81ea\u5b9a\u4e49\u8c03\u6574\u3002</p> <p>\u5728\u672c\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u5c06\u6267\u884c\u4e24\u79cd\u7c7b\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff1a\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u3002 \u5728\u5fae\u8c03\u4e2d\uff0c\u6211\u4eec\u4ece\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\uff0c\u7136\u540e\u4e3a\u6211\u4eec\u7684\u65b0\u4efb\u52a1\u66f4\u65b0\u6240\u6709\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u8d28\u4e0a\u5c31\u662f\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u3002 \u5728\u7279\u5f81\u63d0\u53d6\u4e2d\uff0c\u6211\u4eec\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u5f00\u59cb\uff0c\u53ea\u66f4\u65b0\u4ea7\u751f\u9884\u6d4b\u7684\u6700\u540e\u4e00\u5c42\u7684\u6743\u91cd\u3002\u5b83\u88ab\u79f0\u4e3a\u7279\u5f81\u63d0\u53d6\u662f\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7684CNN\u4f5c\u4e3a\u56fa\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u4e14\u4ec5\u6539\u53d8\u8f93\u51fa\u5c42\u3002 \u6709\u5173\u8fc1\u79fb\u5b66\u4e60\u7684\u66f4\u591a\u6280\u672f\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605here\u548chere.</p> <p>\u901a\u5e38\uff0c\u8fd9\u4e24\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u90fd\u9075\u5faa\u4ee5\u4e0b\u51e0\u4e2a\u6b65\u9aa4\uff1a</p> <ul> <li>\u521d\u59cb\u5316\u9884\u8bad\u7ec3\u6a21\u578b</li> <li>\u91cd\u7ec4\u6700\u540e\u4e00\u5c42\uff0c\u4f7f\u5176\u5177\u6709\u4e0e\u65b0\u6570\u636e\u96c6\u7c7b\u522b\u6570\u76f8\u540c\u7684\u8f93\u51fa\u6570</li> <li>\u4e3a\u4f18\u5316\u7b97\u6cd5\u5b9a\u4e49\u6211\u4eec\u60f3\u8981\u5728\u8bad\u7ec3\u671f\u95f4\u66f4\u65b0\u7684\u53c2\u6570</li> <li>\u8fd0\u884c\u8bad\u7ec3\u6b65\u9aa4</li> </ul> <pre><code>from __future__ import print_function\nfrom __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\nprint(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>PyTorch Version:  1.0.0.dev20190117\nTorchvision Version:  0.2.1\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_1","title":"\u8f93\u5165","text":"<p>\u4ee5\u4e0b\u4e3a\u8fd0\u884c\u65f6\u9700\u8981\u66f4\u6539\u7684\u6240\u6709\u53c2\u6570\u3002 \u6211\u4eec\u5c06\u4f7f\u7528\u7684\u6570\u636e\u96c6hymenoptera_data\u53ef\u5728\u6b64\u5904\u4e0b\u8f7d\u3002 \u8be5\u6570\u636e\u96c6\u5305\u542b\u4e24\u7c7b\uff1a\u871c\u8702\u548c\u8682\u8681\uff0c\u5176\u7ed3\u6784\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 ImageFolder \u6570\u636e\u96c6\uff0c\u4e0d\u9700\u8981\u7f16\u5199\u6211\u4eec\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002\u4e0b\u8f7d\u6570\u636e\u5e76\u8bbe\u7f6e <code>data_dir</code> \u4e3a\u6570\u636e\u96c6\u7684\u6839\u76ee\u5f55\u3002<code>model_name</code>\u662f\u60a8\u8981\u4f7f\u7528\u7684\u6a21\u578b\u540d\u79f0\uff0c\u5fc5\u987b\u4ece\u6b64\u5217\u8868\u4e2d\u9009\u62e9\uff1a</p> <pre><code>[resnet, alexnet, vgg, squeezenet, densenet, inception]\n\n</code></pre> <p>\u5176\u4ed6\u8f93\u5165\u5982\u4e0b\uff1a<code>num_classes</code>\u4e3a\u6570\u636e\u96c6\u7684\u7c7b\u522b\u6570\uff0c<code>batch_size</code>\u662f\u8bad\u7ec3\u7684batch\u5927\u5c0f\uff0c\u53ef\u4ee5\u6839\u636e\u60a8\u673a\u5668\u7684\u8ba1\u7b97\u80fd\u529b\u8fdb\u884c\u8c03\u6574\uff0c<code>num_epochsis</code>\u662f\u6211\u4eec\u60f3\u8981\u8fd0\u884c\u7684\u8bad\u7ec3epoch\u6570\uff0c<code>feature_extractis</code> \u662f\u5b9a\u4e49\u6211\u4eec\u9009\u62e9\u5fae\u8c03\u8fd8\u662f\u7279\u5f81\u63d0\u53d6\u7684\u5e03\u5c14\u503c\u3002 \u5982\u679c<code>feature_extract = False</code>\uff0c\u5c06\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u66f4\u65b0\u6240\u6709\u6a21\u578b\u53c2\u6570\u3002 \u5982\u679c<code>feature_extract = True</code>\uff0c\u5219\u4ec5\u66f4\u65b0\u6700\u540e\u4e00\u5c42\u7684\u53c2\u6570\uff0c\u5176\u4ed6\u53c2\u6570\u4fdd\u6301\u4e0d\u53d8\u3002</p> <pre><code># Top level data directory. Here we assume the format of the directory conforms\n#   to the ImageFolder structure\ndata_dir = \"./data/hymenoptera_data\"\n\n# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\nmodel_name = \"squeezenet\"\n\n# Number of classes in the dataset\nnum_classes = 2\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 8\n\n# Number of epochs to train for\nnum_epochs = 15\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_2","title":"\u8f85\u52a9\u51fd\u6570","text":"<p>\u5728\u7f16\u5199\u8c03\u6574\u6a21\u578b\u7684\u4ee3\u7801\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u5b9a\u4e49\u4e00\u4e9b\u8f85\u52a9\u51fd\u6570\u3002</p>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_3","title":"\u6a21\u578b\u8bad\u7ec3\u548c\u9a8c\u8bc1\u4ee3\u7801","text":"<p><code>train_model</code>\u51fd\u6570\u5904\u7406\u7ed9\u5b9a\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002 \u4f5c\u4e3a\u8f93\u5165\uff0c\u5b83\u9700\u8981PyTorch\u6a21\u578b\uff0c\u6570\u636e\u52a0\u8f7d\u5668\u5b57\u5178\uff0c\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u5668\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1epoch\u6570\uff0c\u4ee5\u53ca\u5f53\u6a21\u578b\u662f\u521d\u59cb\u6a21\u578b\u65f6\u7684\u5e03\u5c14\u6807\u5fd7\u3002 is_inception \u6807\u5fd7\u7528\u4e8e\u5bb9\u7eb3 Inception v3 \u6a21\u578b\uff0c\u56e0\u4e3a\u8be5\u4f53\u7cfb\u7ed3\u6784\u4f7f\u7528\u8f85\u52a9\u8f93\u51fa\uff0c\u5e76\u4e14\u6574\u4f53\u6a21\u578b\u635f\u5931\u6d89\u53ca\u8f85\u52a9\u8f93\u51fa\u548c\u6700\u7ec8\u8f93\u51fa\uff0c\u5982\u6b64\u5904\u6240\u8ff0\u3002 \u8fd9\u4e2a\u51fd\u6570\u8bad\u7ec3\u6307\u5b9a\u6570\u91cf\u7684epoch,\u5e76\u4e14\u5728\u6bcf\u4e2aepoch\u4e4b\u540e\u8fd0\u884c\u5b8c\u6574\u7684\u9a8c\u8bc1\u6b65\u9aa4\u3002 \u5b83\u8fd8\u8ddf\u8e2a\u6700\u4f73\u6027\u80fd\u7684\u6a21\u578b(\u4ece\u9a8c\u8bc1\u51c6\u786e\u7387\u65b9\u9762\uff09\uff0c\u5e76\u5728\u8bad\u7ec3\u7ed3\u675f\u65f6\u8fd4\u56de\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\u3002 \u5728\u6bcf\u4e2aepoch\u4e4b\u540e\uff0c\u6253\u5370\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6b63\u786e\u7387\u3002</p> <pre><code>def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    #   mode we calculate the loss by summing the final output and the auxiliary output\n                    #   but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#requires_grad","title":"\u8bbe\u7f6e\u6a21\u578b\u53c2\u6570\u7684.requires_grad\u5c5e\u6027","text":"<p>\u5f53\u6211\u4eec\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u65f6\uff0c\u6b64\u8f85\u52a9\u51fd\u6570\u5c06\u6a21\u578b\u4e2d\u53c2\u6570\u7684 <code>.requires_grad</code> \u5c5e\u6027\u8bbe\u7f6e\u4e3aFalse\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f53\u6211\u4eec\u52a0\u8f7d\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u6240\u6709\u53c2\u6570\u90fd\u662f <code>.requires_grad = True</code>\uff0c\u5982\u679c\u6211\u4eec\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u5c31\u6ca1\u95ee\u9898\u3002 \u4f46\u662f\uff0c\u5982\u679c\u6211\u4eec\u8981\u8fd0\u884c\u7279\u5f81\u63d0\u53d6\u5e76\u4e14\u53ea\u60f3\u4e3a\u65b0\u521d\u59cb\u5316\u7684\u5c42\u8ba1\u7b97\u68af\u5ea6\uff0c\u90a3\u4e48\u6211\u4eec\u5e0c\u671b\u6240\u6709\u5176\u4ed6\u53c2\u6570\u4e0d\u9700\u8981\u68af\u5ea6\u53d8\u5316\u3002\u8fd9\u5c06\u5728\u7a0d\u540e\u66f4\u80fd\u7406\u89e3\u3002</p> <pre><code>def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_4","title":"\u521d\u59cb\u5316\u548c\u91cd\u5851\u7f51\u7edc","text":"<p>\u73b0\u5728\u6765\u5230\u6700\u6709\u8da3\u7684\u90e8\u5206\u3002\u5728\u8fd9\u91cc\u6211\u4eec\u5bf9\u6bcf\u4e2a\u7f51\u7edc\u8fdb\u884c\u91cd\u5851\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e0d\u662f\u4e00\u4e2a\u81ea\u52a8\u8fc7\u7a0b\uff0c\u5e76\u4e14\u5bf9\u6bcf\u4e2a\u6a21\u578b\u90fd\u662f\u552f\u4e00\u7684\u3002 \u56de\u60f3\u4e00\u4e0b\uff0cCNN\u6a21\u578b\u7684\u6700\u540e\u4e00\u5c42(\u901a\u5e38\u662fFC\u5c42\uff09\u4e0e\u6570\u636e\u96c6\u4e2d\u7684\u8f93\u51fa\u7c7b\u7684\u6570\u91cf\u5177\u6709\u76f8\u540c\u7684\u8282\u70b9\u6570\u3002 \u7531\u4e8e\u6240\u6709\u6a21\u578b\u90fd\u5df2\u5728Imagenet\u4e0a\u9884\u5148\u8bad\u7ec3\uff0c\u56e0\u6b64\u5b83\u4eec\u90fd\u5177\u6709\u5927\u5c0f\u4e3a1000\u7684\u8f93\u51fa\u5c42\uff0c\u6bcf\u4e2a\u7c7b\u4e00\u4e2a\u8282\u70b9\u3002 \u8fd9\u91cc\u7684\u76ee\u6807\u662f\u5c06\u6700\u540e\u4e00\u5c42\u91cd\u5851\u4e3a\u4e0e\u4e4b\u524d\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u8f93\u5165\uff0c\u5e76\u4e14\u5177\u6709\u4e0e\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u6570\u76f8\u540c\u7684\u8f93\u51fa\u6570\u3002 \u5728\u4ee5\u4e0b\u90e8\u5206\u4e2d\uff0c\u6211\u4eec\u5c06\u8ba8\u8bba\u5982\u4f55\u66f4\u6539\u6bcf\u4e2a\u6a21\u578b\u7684\u4f53\u7cfb\u7ed3\u6784\u3002 \u4f46\u9996\u5148\uff0c\u6709\u4e00\u4e2a\u5173\u4e8e\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u4e4b\u95f4\u5dee\u5f02\u7684\u91cd\u8981\u7ec6\u8282\u3002</p> <p>\u5f53\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u65f6\uff0c\u6211\u4eec\u53ea\u60f3\u66f4\u65b0\u6700\u540e\u4e00\u5c42\u7684\u53c2\u6570\uff0c\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u53ea\u60f3\u66f4\u65b0\u6211\u4eec\u6b63\u5728\u91cd\u5851\u5c42\u7684\u53c2\u6570\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u4e0d\u9700\u8981\u6539\u53d8\u7684\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u56e0\u6b64\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u5c06\u5176\u5b83\u5c42\u7684.requires_grad\u5c5e\u6027\u8bbe\u7f6e\u4e3aFalse\u3002 \u8fd9\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6b64\u5c5e\u6027\u8bbe\u7f6e\u4e3aTrue\u3002 \u7136\u540e\uff0c\u5f53\u6211\u4eec\u521d\u59cb\u5316\u65b0\u5c42\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u65b0\u53c2\u6570<code>.requires_grad = True</code>\uff0c\u56e0\u6b64\u53ea\u66f4\u65b0\u65b0\u5c42\u7684\u53c2\u6570\u3002 \u5f53\u6211\u4eec\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6240\u6709.required_grad\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u503cTrue\u3002</p> <p>\u6700\u540e\uff0c\u8bf7\u6ce8\u610finception_v3\u7684\u8f93\u5165\u5927\u5c0f\u4e3a(299,299\uff09\uff0c\u800c\u6240\u6709\u5176\u4ed6\u6a21\u578b\u90fd\u8f93\u5165\u4e3a(224,224\uff09\u3002</p>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#resnet","title":"Resnet","text":"<p>\u8bba\u6587Deep Residual Learning for Image Recognition\u4ecb\u7ecd\u4e86Resnet\u6a21\u578b\u3002\u6709\u51e0\u79cd\u4e0d\u540c\u5c3a\u5bf8\u7684\u53d8\u4f53\uff0c\u5305\u62ecResnet18\uff0cResnet34\uff0cResnet50\uff0cResnet101\u548cResnet152\uff0c\u6240\u6709\u8fd9\u4e9b\u6a21\u578b\u90fd\u53ef\u4ee5\u4ecetorchvision\u6a21\u578b\u4e2d\u83b7\u5f97\u3002\u56e0\u4e3a\u6211\u4eec\u7684\u6570\u636e\u96c6\u5f88\u5c0f\uff0c\u53ea\u6709\u4e24\u4e2a\u7c7b\uff0c\u6240\u4ee5\u6211\u4eec\u4f7f\u7528Resnet18\u3002 \u5f53\u6211\u4eec\u6253\u5370\u8fd9\u4e2a\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u770b\u5230\u6700\u540e\u4e00\u5c42\u662f\u5168\u8fde\u63a5\u5c42\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>(fc): Linear(in_features=512, out_features=1000, bias=True)\n\n</code></pre> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u5c06<code>model.fc</code>\u91cd\u65b0\u521d\u59cb\u5316\u4e3a\u5177\u6709512\u4e2a\u8f93\u5165\u7279\u5f81\u548c2\u4e2a\u8f93\u51fa\u7279\u5f81\u7684\u7ebf\u6027\u5c42\uff1a</p> <pre><code>model.fc = nn.Linear(512, num_classes)\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#alexnet","title":"Alexnet","text":"<p>Alexnet\u5728\u8bba\u6587ImageNet Classification with Deep Convolutional Neural Networks\u4e2d\u88ab\u4ecb\u7ecd\uff0c\u662fImageNet\u6570\u636e\u96c6\u4e0a\u7b2c\u4e00\u4e2a\u975e\u5e38\u6210\u529f\u7684CNN\u3002\u5f53\u6211\u4eec\u6253\u5370\u6a21\u578b\u67b6\u6784\u65f6\uff0c\u6211\u4eec\u770b\u5230\u6a21\u578b\u8f93\u51fa\u4e3a\u5206\u7c7b\u5668\u7684\u7b2c6\u5c42</p> <pre><code>(classifier): Sequential(\n    ...\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n )\n\n</code></pre> <p>\u8981\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u5c06\u6b64\u56fe\u5c42\u91cd\u65b0\u521d\u59cb\u5316\u4e3a</p> <pre><code>model.classifier[6] = nn.Linear(4096,num_classes)\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#vgg","title":"VGG","text":"<p>VGG\u5728\u8bba\u6587Very Deep Convolutional Networks for Large-Scale Image Recognition\u4e2d\u88ab\u5f15\u5165\u3002Torchvision\u63d0\u4f9b\u4e868\u79cd\u4e0d\u540c\u957f\u5ea6\u7684VGG\u7248\u672c\uff0c\u5176\u4e2d\u4e00\u4e9b\u7248\u672c\u5177\u6709\u6279\u6807\u51c6\u5316\u5c42\u3002\u8fd9\u91cc\u6211\u4eec\u4f7f\u7528VGG-11\u8fdb\u884c\u6279\u6807\u51c6\u5316\u3002\u8f93\u51fa\u5c42\u4e0eAlexnet\u7c7b\u4f3c\uff0c\u5373</p> <pre><code>(classifier): Sequential(\n    ...\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n )\n\n</code></pre> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u6765\u4fee\u6539\u8f93\u51fa\u5c42</p> <pre><code>model.classifier[6] = nn.Linear(4096,num_classes)\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#squeezenet","title":"Squeezenet","text":"<p>\u8bba\u6587SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\u63cf\u8ff0\u4e86Squeeznet\u67b6\u6784\uff0c\u4f7f\u7528\u4e86\u4e0e\u6b64\u5904\u663e\u793a\u7684\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\u4e0d\u540c\u7684\u8f93\u51fa\u7ed3\u6784\u3002Torchvision\u7684Squeezenet\u6709\u4e24\u4e2a\u7248\u672c\uff0c\u6211\u4eec\u4f7f\u75281.0\u7248\u672c\u3002\u8f93\u51fa\u6765\u81ea1x1\u5377\u79ef\u5c42\uff0c\u5b83\u662f\u5206\u7c7b\u5668\u7684\u7b2c\u4e00\u5c42\uff1a</p> <pre><code>(classifier): Sequential(\n    (0): Dropout(p=0.5)\n    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n    (2): ReLU(inplace)\n    (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n )\n\n</code></pre> <p>\u4e3a\u4e86\u4fee\u6539\u7f51\u7edc\uff0c\u6211\u4eec\u91cd\u65b0\u521d\u59cb\u5316Conv2d\u5c42\uff0c\u4f7f\u8f93\u51fa\u7279\u5f81\u56fe\u6df1\u5ea6\u4e3a2</p> <pre><code>model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#densenet","title":"Densenet","text":"<p>\u8bba\u6587Densely Connected Convolutional Networks\u5f15\u5165\u4e86Densenet\u6a21\u578b\u3002 Torchvision\u6709\u56db\u79cdDensenet\u53d8\u578b\uff0c\u4f46\u5728\u8fd9\u91cc\u6211\u4eec\u53ea\u4f7f\u7528Densenet-121\u3002 \u8f93\u51fa\u5c42\u662f\u4e00\u4e2a\u5177\u67091024\u4e2a\u8f93\u5165\u7279\u5f81\u7684\u7ebf\u6027\u5c42\uff1a</p> <pre><code>(classifier): Linear(in_features=1024, out_features=1000, bias=True)\n\n</code></pre> <p>\u4e3a\u4e86\u91cd\u5851\u8fd9\u4e2a\u7f51\u7edc\uff0c\u6211\u4eec\u5c06\u5206\u7c7b\u5668\u7684\u7ebf\u6027\u5c42\u91cd\u65b0\u521d\u59cb\u5316\u4e3a</p> <pre><code>model.classifier = nn.Linear(1024, num_classes)\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#inception-v3","title":"Inception v3","text":"<p>\u6700\u540e\uff0cInception v3\u9996\u5148\u5728\u8bba\u6587 Rethinking the Inception Architecture for Computer Vision\u4e2d\u63cf\u8ff0\u3002\u8be5\u7f51\u7edc\u7684\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5b83\u5728\u8bad\u7ec3\u65f6\u6709\u4e24\u4e2a\u8f93\u51fa\u5c42\u3002\u7b2c\u4e8c\u4e2a\u8f93\u51fa\u79f0\u4e3a\u8f85\u52a9\u8f93\u51fa\uff0c\u5305\u542b\u5728\u7f51\u7edc\u7684AuxLogits\u90e8\u5206\u4e2d\u3002\u4e3b\u8f93\u51fa\u662f\u7f51\u7edc\u672b\u7aef\u7684\u7ebf\u6027\u5c42\u3002\u6ce8\u610f\uff0c\u6d4b\u8bd5\u65f6\u6211\u4eec\u53ea\u8003\u8651\u4e3b\u8f93\u51fa\u3002 \u52a0\u8f7d\u6a21\u578b\u7684\u8f85\u52a9\u8f93\u51fa\u548c\u4e3b\u8f93\u51fa\u6253\u5370\u4e3a\uff1a</p> <pre><code>(AuxLogits): InceptionAux(\n    ...\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n )\n ...\n(fc): Linear(in_features=2048, out_features=1000, bias=True)\n\n</code></pre> <p>\u8981\u5fae\u8c03\u8fd9\u4e2a\u6a21\u578b\uff0c\u6211\u4eec\u5fc5\u987b\u91cd\u5851\u8fd9\u4e24\u4e2a\u5c42\u3002 \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b8c\u6210</p> <pre><code>model.AuxLogits.fc = nn.Linear(768, num_classes)\nmodel.fc = nn.Linear(2048, num_classes)\n\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c\u8bb8\u591a\u6a21\u578b\u5177\u6709\u76f8\u4f3c\u7684\u8f93\u51fa\u7ed3\u6784\uff0c\u4f46\u6bcf\u4e2a\u6a21\u578b\u7684\u5904\u7406\u65b9\u5f0f\u7565\u6709\u4e0d\u540c\u3002 \u53e6\u5916\uff0c\u8bf7\u67e5\u770b\u91cd\u5851\u7f51\u7edc\u7684\u6a21\u578b\u4f53\u7cfb\u7ed3\u6784\uff0c\u5e76\u786e\u4fdd\u8f93\u51fa\u7279\u5f81\u6570\u4e0e\u6570\u636e\u96c6\u4e2d\u7684\u7c7b\u522b\u6570\u76f8\u540c\u3002</p> <pre><code>def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"alexnet\":\n        \"\"\" Alexnet\n \"\"\"\n        model_ft = models.alexnet(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n\n    elif model_name == \"squeezenet\":\n        \"\"\" Squeezenet\n \"\"\"\n        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n        model_ft.num_classes = num_classes\n        input_size = 224\n\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n\n    elif model_name == \"inception\":\n        \"\"\" Inception v3\n Be careful, expects (299,299) sized images and has auxiliary output\n \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size\n\n# Initialize the model for this run\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Print the model we just instantiated\nprint(model_ft)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>SqueezeNet(\n  (features): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n    (1): ReLU(inplace)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (3): Fire(\n      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (4): Fire(\n      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (5): Fire(\n      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (7): Fire(\n      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (8): Fire(\n      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (9): Fire(\n      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (10): Fire(\n      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n    (12): Fire(\n      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n      (squeeze_activation): ReLU(inplace)\n      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (expand1x1_activation): ReLU(inplace)\n      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (expand3x3_activation): ReLU(inplace)\n    )\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5)\n    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n    (2): ReLU(inplace)\n    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n)\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#load-data","title":"Load Data","text":"<p>\u73b0\u5728\u6211\u4eec\u77e5\u9053\u8f93\u5165\u5c3a\u5bf8\u5927\u5c0f\u5fc5\u987b\u662f\u4ec0\u4e48\uff0c\u6211\u4eec\u53ef\u4ee5\u521d\u59cb\u5316\u6570\u636e\u8f6c\u6362\uff0c\u56fe\u50cf\u6570\u636e\u96c6\u548c\u6570\u636e\u52a0\u8f7d\u5668\u3002\u8bf7\u6ce8\u610f\uff0c\u6a21\u578b\u662f\u4f7f\u7528\u786c\u7f16\u7801\u6807\u51c6\u5316\u503c\u8fdb\u884c\u9884\u5148\u8bad\u7ec3\u7684\uff0c\u5982here\u6240\u8ff0\u3002</p> <pre><code># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nprint(\"Initializing Datasets and Dataloaders...\")\n\n# Create training and validation datasets\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n# Create training and validation dataloaders\ndataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Initializing Datasets and Dataloaders...\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_5","title":"\u521b\u5efa\u4f18\u5316\u5668","text":"<p>\u73b0\u5728\u6a21\u578b\u7ed3\u6784\u662f\u6b63\u786e\u7684\uff0c\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u7684\u6700\u540e\u4e00\u6b65\u662f\u521b\u5efa\u4e00\u4e2a\u53ea\u66f4\u65b0\u6240\u9700\u53c2\u6570\u7684\u4f18\u5316\u5668\u3002 \u56de\u60f3\u4e00\u4e0b\uff0c\u5728\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u540e\uff0c\u4f46\u5728\u91cd\u5851\u4e4b\u524d\uff0c\u5982\u679c<code>feature_extract = True</code>\uff0c\u6211\u4eec\u624b\u52a8\u5c06\u6240\u6709\u53c2\u6570\u7684<code>.requires_grad</code>\u5c5e\u6027\u8bbe\u7f6e\u4e3aFalse\u3002\u7136\u540e\u91cd\u65b0\u521d\u59cb\u5316\u9ed8\u8ba4\u4e3a<code>.requires_grad = True</code>\u7684\u7f51\u7edc\u5c42\u53c2\u6570\u3002\u6240\u4ee5\u73b0\u5728\u6211\u4eec\u77e5\u9053\u5e94\u8be5\u4f18\u5316\u6240\u6709\u5177\u6709 <code>.requires_grad = True</code>\u7684\u53c2\u6570\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5217\u51fa\u8fd9\u4e9b\u53c2\u6570\u5e76\u5c06\u6b64\u5217\u8868\u8f93\u5165\u5230SGD\u7b97\u6cd5\u6784\u9020\u5668\u3002</p> <p>\u8981\u9a8c\u8bc1\u8fd9\u4e00\u70b9\uff0c\u53ef\u4ee5\u67e5\u770b\u8981\u5b66\u4e60\u7684\u53c2\u6570\u3002\u5fae\u8c03\u65f6\uff0c\u6b64\u5217\u8868\u5e94\u8be5\u5f88\u957f\u5e76\u5305\u542b\u6240\u6709\u6a21\u578b\u53c2\u6570\u3002\u4f46\u662f\uff0c\u5f53\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u65f6\uff0c\u6b64\u5217\u8868\u5e94\u8be5\u5f88\u77ed\u5e76\u4e14\u4ec5\u5305\u62ec\u91cd\u5851\u5c42\u7684\u6743\u91cd\u548c\u504f\u5dee\u3002</p> <pre><code># Send the model to GPU\nmodel_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Params to learn:\n         classifier.1.weight\n         classifier.1.bias\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_6","title":"\u8fd0\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1","text":"<p>\u6700\u540e\u4e00\u6b65\u662f\u4e3a\u6a21\u578b\u8bbe\u7f6e\u635f\u5931\uff0c\u7136\u540e\u5bf9\u8bbe\u5b9a\u7684epoch\u6570\u8fd0\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u51fd\u6570\u3002\u8bf7\u6ce8\u610f\uff0c\u53d6\u51b3\u4e8eepoch\u7684\u6570\u91cf\uff0c\u6b64\u6b65\u9aa4\u5728CPU\u4e0a\u53ef\u80fd\u9700\u8981\u6267\u884c\u4e00\u6bb5\u65f6\u95f4\u3002 \u6b64\u5916\uff0c\u9ed8\u8ba4\u7684\u5b66\u4e60\u7387\u5bf9\u6240\u6709\u6a21\u578b\u90fd\u4e0d\u662f\u6700\u4f73\u7684\uff0c\u56e0\u6b64\u4e3a\u4e86\u83b7\u5f97\u6700\u5927\u7cbe\u5ea6\uff0c\u6709\u5fc5\u8981\u5206\u522b\u8c03\u6574\u6bcf\u4e2a\u6a21\u578b\u3002</p> <pre><code># Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Epoch 0/14\n----------\ntrain Loss: 0.5981 Acc: 0.7131\nval Loss: 0.3849 Acc: 0.8889\n\nEpoch 1/14\n----------\ntrain Loss: 0.3282 Acc: 0.8402\nval Loss: 0.3023 Acc: 0.9085\n\nEpoch 2/14\n----------\ntrain Loss: 0.2248 Acc: 0.9139\nval Loss: 0.3363 Acc: 0.8758\n\nEpoch 3/14\n----------\ntrain Loss: 0.1924 Acc: 0.9057\nval Loss: 0.2833 Acc: 0.9150\n\nEpoch 4/14\n----------\ntrain Loss: 0.1359 Acc: 0.9344\nval Loss: 0.3221 Acc: 0.9150\n\nEpoch 5/14\n----------\ntrain Loss: 0.1583 Acc: 0.9426\nval Loss: 0.3069 Acc: 0.9412\n\nEpoch 6/14\n----------\ntrain Loss: 0.1918 Acc: 0.9344\nval Loss: 0.3139 Acc: 0.9150\n\nEpoch 7/14\n----------\ntrain Loss: 0.1950 Acc: 0.9262\nval Loss: 0.2431 Acc: 0.9281\n\nEpoch 8/14\n----------\ntrain Loss: 0.1534 Acc: 0.9344\nval Loss: 0.2680 Acc: 0.9281\n\nEpoch 9/14\n----------\ntrain Loss: 0.1796 Acc: 0.9262\nval Loss: 0.2573 Acc: 0.9346\n\nEpoch 10/14\n----------\ntrain Loss: 0.1181 Acc: 0.9549\nval Loss: 0.2987 Acc: 0.9216\n\nEpoch 11/14\n----------\ntrain Loss: 0.1401 Acc: 0.9262\nval Loss: 0.2977 Acc: 0.9281\n\nEpoch 12/14\n----------\ntrain Loss: 0.1463 Acc: 0.9221\nval Loss: 0.3645 Acc: 0.8889\n\nEpoch 13/14\n----------\ntrain Loss: 0.1388 Acc: 0.9508\nval Loss: 0.3617 Acc: 0.9281\n\nEpoch 14/14\n----------\ntrain Loss: 0.1799 Acc: 0.9180\nval Loss: 0.3371 Acc: 0.9216\n\nTraining complete in 0m 19s\nBest val Acc: 0.941176\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_7","title":"\u4e0e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u6bd4\u8f83","text":"<p>\u53ea\u662f\u4e3a\u4e86\u597d\u73a9\uff0c\u770b\u770b\u5982\u679c\u6211\u4eec\u4e0d\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u6a21\u578b\u5c06\u5982\u4f55\u5b66\u4e60\u3002\u5fae\u8c03\u4e0e\u7279\u5f81\u63d0\u53d6\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\uff0c\u4f46\u4e00\u822c\u800c\u8a00\uff0c\u4e24\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u603b\u4f53\u51c6\u786e\u6027\u65b9\u9762\u4ea7\u751f\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002</p> <pre><code># Initialize the non-pretrained version of the model used for this run\nscratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\nscratch_model = scratch_model.to(device)\nscratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\nscratch_criterion = nn.CrossEntropyLoss()\n_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n\n# Plot the training curves of validation accuracy vs. number\n#  of training epochs for the transfer learning method and\n#  the model trained from scratch\nohist = []\nshist = []\n\nohist = [h.cpu().numpy() for h in hist]\nshist = [h.cpu().numpy() for h in scratch_hist]\n\nplt.title(\"Validation Accuracy vs. Number of Training Epochs\")\nplt.xlabel(\"Training Epochs\")\nplt.ylabel(\"Validation Accuracy\")\nplt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\nplt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\nplt.ylim((0,1.))\nplt.xticks(np.arange(1, num_epochs+1, 1.0))\nplt.legend()\nplt.show()\n\n</code></pre> <p></p> <p>\u8f93\u51fa:</p> <pre><code>Epoch 0/14\n----------\ntrain Loss: 0.7935 Acc: 0.5246\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 1/14\n----------\ntrain Loss: 0.6931 Acc: 0.5082\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 2/14\n----------\ntrain Loss: 0.6931 Acc: 0.5041\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 3/14\n----------\ntrain Loss: 0.6931 Acc: 0.5123\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 4/14\n----------\ntrain Loss: 0.6931 Acc: 0.5123\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 5/14\n----------\ntrain Loss: 0.6931 Acc: 0.4918\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 6/14\n----------\ntrain Loss: 0.6931 Acc: 0.5000\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 7/14\n----------\ntrain Loss: 0.6931 Acc: 0.5041\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 8/14\n----------\ntrain Loss: 0.6931 Acc: 0.5164\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 9/14\n----------\ntrain Loss: 0.6931 Acc: 0.5000\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 10/14\n----------\ntrain Loss: 0.6931 Acc: 0.5041\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 11/14\n----------\ntrain Loss: 0.6931 Acc: 0.5041\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 12/14\n----------\ntrain Loss: 0.6931 Acc: 0.4918\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 13/14\n----------\ntrain Loss: 0.6931 Acc: 0.5041\nval Loss: 0.6931 Acc: 0.4575\n\nEpoch 14/14\n----------\ntrain Loss: 0.6931 Acc: 0.5000\nval Loss: 0.6931 Acc: 0.4575\n\nTraining complete in 0m 29s\nBest val Acc: 0.457516\n\n</code></pre>"},{"location":"1.0/finetuning_torchvision_models_tutorial/#_8","title":"\u6700\u540e\u7684\u60f3\u6cd5\u53ca\u4e0b\u4e00\u6b65","text":"<p>\u5c1d\u8bd5\u8fd0\u884c\u5176\u4ed6\u6a21\u578b\uff0c\u770b\u770b\u53ef\u4ee5\u5f97\u5230\u591a\u597d\u7684\u6b63\u786e\u7387\u3002\u53e6\u5916\uff0c\u8bf7\u6ce8\u610f\u7279\u5f81\u63d0\u53d6\u82b1\u8d39\u7684\u65f6\u95f4\u8f83\u5c11\uff0c\u56e0\u4e3a\u5728\u540e\u5411\u4f20\u64ad\u4e2d\u6211\u4eec\u4e0d\u9700\u8981\u8ba1\u7b97\u5927\u90e8\u5206\u7684\u68af\u5ea6\u3002\u8fd8\u6709\u5f88\u591a\u5730\u65b9\u53ef\u4ee5\u5c1d\u8bd5\u3002 \u4f60\u53ef\u4ee5\uff1a * \u5728\u66f4\u96be\u7684\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u6b64\u4ee3\u7801\uff0c\u67e5\u770b\u8fc1\u79fb\u5b66\u4e60\u7684\u66f4\u591a\u597d\u5904\u3002 * \u5728\u65b0\u7684\u9886\u57df(\u6bd4\u5982NLP\uff0c\u97f3\u9891\u7b49\uff09\u4e2d\uff0c\u4f7f\u7528\u6b64\u5904\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u66f4\u65b0\u4e0d\u540c\u7684\u6a21\u578b\u3002 * \u4e00\u65e6\u60a8\u5bf9\u4e00\u4e2a\u6a21\u578b\u611f\u5230\u6ee1\u610f\uff0c * \u53ef\u4ee5\u5c06\u5176\u5bfc\u51fa\u4e3aONNX\u6a21\u578b\uff0c\u6216\u4f7f\u7528\u6df7\u5408\u524d\u7aef\u8ddf\u8e2a\u5b83\u4ee5\u83b7\u5f97\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u4f18\u5316\u7684\u673a\u4f1a\u3002</p>"},{"location":"1.0/hub/","title":"torch.hub","text":"<p>\u8bd1\u8005\uff1akunwuz</p> <pre><code>torch.hub.load(github, model, force_reload=False, *args, **kwargs)\n</code></pre> <p>\u4ecegithub\u4e0a\u52a0\u8f7d\u4e00\u4e2a\u5e26\u6709\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u6a21\u578b\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>github \u2013 \u5fc5\u9700\uff0c\u4e00\u4e2a\u5b57\u7b26\u4e32\u5bf9\u8c61\uff0c\u683c\u5f0f\u4e3a\u201crepo_owner/repo_name[:tag_name]\u201d\uff0c\u53ef\u9009 tag/branch\u3002\u5982\u679c\u672a\u505a\u6307\u5b9a\uff0c\u9ed8\u8ba4\u7684 branch \u662f <code>master</code> \u3002\u6bd4\u65b9\u8bf4: 'pytorch/vision[:hub]'</li> <li>model \u2013 \u5fc5\u987b\uff0c\u4e00\u4e2a\u5b57\u7b26\u4e32\u5bf9\u8c61\uff0c\u540d\u5b57\u5728hubconf.py\u4e2d\u5b9a\u4e49\u3002</li> <li>force_reload \u2013 \u53ef\u9009\uff0c \u662f\u5426\u4e22\u5f03\u73b0\u6709\u7f13\u5b58\u5e76\u5f3a\u5236\u91cd\u65b0\u4e0b\u8f7d\u3002\u9ed8\u8ba4\u662f\uff1a<code>False</code>\u3002</li> <li>*args \u2013 \u53ef\u9009\uff0c \u53ef\u8c03\u7528\u7684<code>model</code>\u7684\u76f8\u5173args\u53c2\u6570\u3002</li> <li>**kwargs \u2013 \u53ef\u9009\uff0c \u53ef\u8c03\u7528\u7684<code>model</code>\u7684\u76f8\u5173kwargs\u53c2\u6570\u3002</li> </ul> \u8fd4\u56de: \u4e00\u4e2a\u6709\u76f8\u5173\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u5355\u4e00\u6a21\u578b\u3002 <pre><code>torch.hub.set_dir(d)\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u5c06<code>hub_dir</code>\u8bbe\u7f6e\u4e3a\u672c\u5730\u76ee\u5f55\u6765\u4fdd\u5b58\u4e2d\u95f4\u6a21\u578b\u548c\u68c0\u67e5\u70b9\u6587\u4ef6\u3002</p> <p>\u5982\u679c\u672a\u8bbe\u7f6e\u6b64\u53c2\u6570,\u73af\u5883\u53d8\u91cfTORCH_HUB_DIR \u4f1a\u88ab\u9996\u5148\u641c\u5bfb\uff0c~/.torch/hub \u5c06\u88ab\u521b\u5efa\u5e76\u7528\u4f5c\u540e\u5907\u3002</p>"},{"location":"1.0/jit/","title":"Torch \u811a\u672c","text":"<p>\u8bd1\u8005\uff1akeyianpai</p> <ul> <li>\u521b\u5efa Torch \u811a\u672c\u4ee3\u7801</li> <li>\u5c06\u8ffd\u8e2a\u548c\u811a\u672c\u5316\u7ed3\u5408\u8d77\u6765</li> <li>Torch \u811a\u672c\u8bed\u8a00\u53c2\u8003<ul> <li>\u7c7b\u578b</li> <li>\u8868\u8fbe\u5f0f</li> <li>\u8bed\u53e5</li> <li>\u53d8\u91cf\u89e3\u6790</li> <li>python\u503c\u7684\u4f7f\u7528</li> <li>\u8c03\u8bd5</li> <li>\u5185\u7f6e\u51fd\u6570</li> </ul> </li> </ul> <p>Torch\u811a\u672c\u662f\u4e00\u79cd\u4ecePyTorch\u4ee3\u7801\u521b\u5efa\u53ef\u5e8f\u5217\u5316\u548c\u53ef\u4f18\u5316\u6a21\u578b\u7684\u65b9\u6cd5\u3002\u7528Torch\u811a\u672c\u7f16\u5199\u7684\u4ee3\u7801\u53ef\u4ee5\u4ecePython\u8fdb\u7a0b\u4e2d\u4fdd\u5b58\uff0c\u5e76\u5728\u6ca1\u6709Python\u4f9d\u8d56\u7684\u8fdb\u7a0b\u4e2d\u52a0\u8f7d\u3002</p> <p>\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e9b\u5de5\u5177\u5e2e\u52a9\u6211\u4eec\u5c06\u6a21\u578b\u4ece\u7eafPython\u7a0b\u5e8f\u9010\u6b65\u8f6c\u6362\u4e3a\u53ef\u4ee5\u72ec\u7acb\u4e8ePython\u8fd0\u884c\u7684Torch\u811a\u672c\u7a0b\u5e8f\u3002Torch\u811a\u672c\u7a0b\u5e8f\u53ef\u4ee5\u5728\u5176\u4ed6\u8bed\u8a00\u7684\u7a0b\u5e8f\u4e2d\u8fd0\u884c(\u4f8b\u5982\uff0c\u5728\u72ec\u7acb\u7684C ++\u7a0b\u5e8f\u4e2d\uff09\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u719f\u6089\u7684\u5de5\u5177\u5728PyTorch\u4e2d\u8bad\u7ec3\u6a21\u578b\uff0c\u800c\u5c06\u6a21\u578b\u5bfc\u51fa\u5230\u51fa\u4e8e\u6027\u80fd\u548c\u591a\u7ebf\u7a0b\u539f\u56e0\u4e0d\u80fd\u5c06\u6a21\u578b\u4f5c\u4e3aPython\u7a0b\u5e8f\u8fd0\u884c\u7684\u751f\u4ea7\u73af\u5883\u4e2d\u53bb\u3002</p> <pre><code>class torch.jit.ScriptModule(optimize=True)\n</code></pre> <p>ScriptModule\u4e0e\u5176\u5185\u90e8\u7684Torch\u811a\u672c\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u521b\u5efa\uff1a</p> <p>\u8ffd\u8e2a\uff1a</p> <p>\u4f7f\u7528<code>torch.jit.trace</code>\u3002torch.jit.trace\u4ee5\u73b0\u6709\u6a21\u5757\u6216python\u51fd\u6570\u548c\u6837\u4f8b\u8f93\u5165\u4f5c\u4e3a\u53c2\u6570\uff0c\u5b83\u4f1a\u8fd0\u884c\u8be5python\u51fd\u6570\uff0c\u8bb0\u5f55\u51fd\u6570\u5728\u6240\u6709\u5f20\u91cf\u4e0a\u6267\u884c\u7684\u64cd\u4f5c\uff0c\u5e76\u5c06\u8bb0\u5f55\u8f6c\u6362\u4e3aTorch\u811a\u672c\u65b9\u6cd5\u4ee5\u4f5c\u4e3aScriptModule\u7684forward\u65b9\u6cd5\u3002\u521b\u5efa\u7684\u6a21\u5757\u5305\u542b\u539f\u59cb\u6a21\u5757\u7684\u6240\u6709\u53c2\u6570\u3002</p> <p>\u4f8b\uff1a</p> <p>```py import torch def foo(x, y):     return 2*x + y traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))</p> <p>```</p> <p>\u6ce8\u610f</p> <p>\u8ffd\u8e2a\u4e00\u4e2a \u51fd\u6570 \u5c06\u751f\u6210\u4e00\u4e2a<code>ScriptModule</code>\uff0c\u8be5ScriptModule\u4e2d\u5305\u542b\u4e00\u4e2a\u5b9e\u73b0\u88ab\u8ffd\u8e2a\u51fd\u6570\u7684<code>forward</code>\u65b9\u6cd5\uff0c\u4f46\u4e0d\u5305\u542b\u4efb\u4f55\u53c2\u6570\u3002</p> <p>\u4f8b\uff1a</p> <p>```py import torch import torchvision traced_net = torch.jit.trace(torchvision.models.resnet18(),                              torch.rand(1, 3, 224, 224))</p> <p>```</p> <p>\u6ce8\u610f</p> <p>\u8ffd\u8e2a\u4ec5\u8bb0\u5f55\u5728\u7ed9\u5b9a\u5f20\u91cf\u4e0a\u8fd0\u884c\u7ed9\u5b9a\u51fd\u6570\u65f6\u6267\u884c\u7684\u64cd\u4f5c\u3002\u56e0\u6b64\uff0c\u8fd4\u56de\u7684<code>ScriptModule</code>\u5728\u4efb\u4f55\u8f93\u5165\u4e0a\u5c06\u8fd0\u884c\u76f8\u540c\u7684\u8ffd\u8e2a\u56fe\u3002\u5f53\u4f60\u7684\u6a21\u5757\u9700\u8981\u6839\u636e\u8f93\u5165\u548c/\u6216\u6a21\u5757\u72b6\u6001\u8fd0\u884c\u4e0d\u540c\u7684\u64cd\u4f5c\u96c6\u65f6\uff0c\u8fd9\u4f1a\u4ea7\u751f\u4e00\u4e9b\u91cd\u8981\u7684\u5f71\u54cd\u3002\u4f8b\u5982\uff0c &gt;* \u8ffd\u8e2a\u4e0d\u4f1a\u8bb0\u5f55if\u8bed\u53e5\u6216\u5faa\u73af\u4e4b\u7c7b\u7684\u63a7\u5236\u6d41\u3002\u5f53\u8fd9\u4e2a\u63a7\u5236\u6d41\u5728\u4f60\u7684\u6a21\u5757\u4e2d\u4fdd\u6301\u4e0d\u53d8\u65f6\uff0c\u8fd9\u5f88\u597d\uff0c\u5b83\u901a\u5e38\u53ea\u662f\u5185\u8054\u914d\u7f6e\u51b3\u7b56\u3002\u4f46\u6709\u65f6\u63a7\u5236\u6d41\u5b9e\u9645\u4e0a\u662f\u6a21\u578b\u672c\u8eab\u7684\u4e00\u90e8\u5206\u3002\u4f8b\u5982\uff0c\u5e8f\u5217\u5230\u5e8f\u5217\u8f6c\u6362\u4e2d\u7684beam\u641c\u7d22\u662f\u5bf9(\u53ef\u53d8\uff09\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u7684\u5faa\u73af\u3002</p> <p>&gt;*\u5728\u8fd4\u56de\u7684<code>ScriptModule</code>\u4e2d\uff0c\u5728<code>training</code>\u548c<code>eval</code>\u6a21\u5f0f\u4e2d\u5177\u6709\u4e0d\u540c\u884c\u4e3a\u7684\u64cd\u4f5c\u5c06\u59cb\u7ec8\u8868\u73b0\u4e3a\u5904\u4e8e\u8ffd\u8e2a\u671f\u95f4\u7684\u6a21\u5f0f\u3002</p> <p>\u5728\u4e0a\u8ff0\u60c5\u51b5\u4e0b\uff0c\u811a\u672c\u5316\u662f\u4e00\u4e2a\u6bd4\u8ffd\u8e2a\u66f4\u597d\u7684\u9009\u62e9\u3002</p> <p>\u811a\u672c\u5316</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528Python\u8bed\u6cd5\u76f4\u63a5\u7f16\u5199Torch\u811a\u672c\u4ee3\u7801\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528<code>torch.jit.script</code>\u6ce8\u91ca(\u5bf9\u4e8e\u51fd\u6570\uff09\u6216<code>torch.jit.script_method</code>\u6ce8\u91ca(\u5bf9\u4e8eScriptModule\u5b50\u7c7b\u7684\u65b9\u6cd5\uff09\u6765\u7f16\u5199Torch\u811a\u672c\u4ee3\u7801\u3002\u901a\u8fc7\u6ce8\u91ca\uff0c\u88ab\u6ce8\u91ca\u51fd\u6570\u7684\u4e3b\u4f53\u5c06\u76f4\u63a5\u8f6c\u6362\u4e3aTorch\u811a\u672c\u3002 Torch\u811a\u672c\u672c\u8eab\u53ea\u662fPython\u8bed\u8a00\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u56e0\u6b64\u4e0d\u662fpython\u4e2d\u7684\u6240\u6709\u7279\u6027\u90fd\u53ef\u4ee5\u4f7f\u7528\uff0c\u4f46\u6211\u4eec\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u529f\u80fd\u6765\u8ba1\u7b97\u5f20\u91cf\u5e76\u6267\u884c\u4e0e\u63a7\u5236\u76f8\u5173\u7684\u64cd\u4f5c\u3002</p> <p>\u4f8b:</p> <p>```py import torch @torch.jit.script def foo(x, y):     if x.max() &gt; y.max():         r = x     else:         r = y     return r</p> <p>```</p> <p>\u6ce8\u610f</p> <p>\u811a\u672c \u51fd\u6570 \u6ce8\u91ca\u5c06\u6784\u9020\u5e26\u6709\u4e00\u4e2a<code>forward</code>\u65b9\u6cd5\u7684ScriptModule\uff0c\u8be5forward\u65b9\u6cd5\u5b9e\u73b0\u88ab\u6ce8\u91ca\u51fd\u6570\uff0c\u5e76\u4e14\u4e0d\u5305\u542b\u4efb\u4f55\u53c2\u6570\u3002</p> <p>\u4f8b\uff1a</p> <p>```py import torch class MyModule(torch.jit.ScriptModule):     def init(self, N, M):         super(MyModule, self).init()         self.weight = torch.nn.Parameter(torch.rand(N, M))</p> <pre><code>@torch.jit.script_method\ndef forward(self, input):\n    return self.weight.mv(input)\n</code></pre> <p>```</p> <p>\u4f8b\uff1a</p> <p>```py import torch import torch.nn as nn import torch.nn.functional as F from torch.jit import ScriptModule, script_method, trace</p> <p>class MyScriptModule(ScriptModule):     def init(self):         super(MyScriptModule, self).init()         # \u901a\u8fc7\u8ffd\u8e2a\u4ea7\u751fScriptModule\u7684 conv1\u548cconv2         self.conv1 = trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))         self.conv2 = trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))</p> <pre><code>@script_method\ndef forward(self, input):\n  input = F.relu(self.conv1(input))\n  input = F.relu(self.conv2(input))\n  return input\n</code></pre> <p>```</p> <pre><code>save(filename)\n</code></pre> <p>\u4fdd\u5b58\u79bb\u7ebf\u7248\u672c\u7684\u6a21\u5757\uff0c\u4ee5\u4fbf\u5c06\u6765\u5728\u5176\u4ed6\u7684\u8fdb\u7a0b\u4e2d\u4f7f\u7528\u3002\u4fdd\u5b58\u7684\u6a21\u5757\u4f1a\u5e8f\u5217\u5316\u5f53\u524d\u6a21\u5757\u7684\u6240\u6709\u65b9\u6cd5\u548c\u53c2\u6570\u3002\u4fdd\u5b58\u7684\u6a21\u5757\u53ef\u4ee5\u4f7f\u7528<code>torch :: jit :: load(filename\uff09</code>\u52a0\u8f7d\u5230C ++ API\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528<code>torch.jit.load(filename\uff09</code>\u52a0\u8f7d\u5230Python API\u4e2d\u3002</p> <p>\u4e3a\u4e86\u80fd\u591f\u4fdd\u5b58\u6a21\u5757\uff0c\u5f53\u524d\u6a21\u5757\u4e0d\u80fd\u8c03\u7528\u539f\u751fpython\u51fd\u6570\u3002\u4e5f\u5c31\u662f\u8bf4\u8981\u4fdd\u5b58\u6a21\u5757\u7684\u6240\u6709\u5b50\u6a21\u5757\u4e5f\u5fc5\u987b\u662fScriptModules\u7684\u5b50\u7c7b\u3002</p> <p>\u5371\u9669</p> <p>\u6240\u6709\u6a21\u5757\uff0c\u4e0d\u8bba\u5176\u8bbe\u5907\uff0c\u5728\u52a0\u8f7d\u8fc7\u7a0b\u4e2d\u59cb\u7ec8\u90fd\u4f1a\u52a0\u8f7d\u5230CPU\u4e2d\u3002\u8fd9\u4e0e<code>torch.load()</code>\u7684\u8bed\u4e49\u4e0d\u540c\uff0c\u5c06\u6765\u53ef\u80fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002</p> <pre><code>torch.jit.load(f, map_location=None)\n</code></pre> <p>\u4f7f\u7528<code>load</code>\u52a0\u8f7d\u4e4b\u524d\u7528<code>save</code>\u4fdd\u5b58\u7684<code>ScriptModule</code>\u3002</p> <p>\u6240\u6709\u5148\u524d\u4fdd\u5b58\u7684\u6a21\u5757\uff0c\u4e0d\u8bba\u5176\u8bbe\u5907\uff0c\u9996\u5148\u52a0\u8f7d\u5230CPU\u4e0a\uff0c\u7136\u540e\u79fb\u52a8\u5230\u4e4b\u524d\u4fdd\u5b58\u5b83\u4eec\u7684\u8bbe\u5907\u4e0a\u3002\u5982\u679c\u6b64\u64cd\u4f5c\u5931\u8d25(\u4f8b\u5982\uff0c\u8fd0\u884c\u65f6\u7cfb\u7edf\u6ca1\u6709\u67d0\u4e9b\u8bbe\u5907\uff09\uff0c\u5219\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002\u6b64\u65f6\u53ef\u4ee5\u4f7f\u7528<code>map_location</code>\u53c2\u6570\u5c06\u5b58\u50a8\u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u7ec4\u8bbe\u5907\u3002\u4e0e<code>torch.load()</code>\u76f8\u6bd4\uff0c\u6b64\u51fd\u6570\u4e2d\u7684<code>map_location</code>\u88ab\u7b80\u5316\u4e3a\u53ea\u63a5\u53d7\u5b57\u7b26\u4e32(\u4f8b\u5982'cpu'\uff0c'cuda\uff1a0'\uff09\u6216torch.device(\u4f8b\u5982\uff0ctorch.device('cpu'\uff09\uff09</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>f \u2013 \u6587\u4ef6\u7c7b\u5bf9\u8c61(\u5fc5\u987b\u5b9e\u73b0read\uff0creadline\uff0ctell\u548cseek\uff09\uff0c\u6216\u4e3a\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32</li> <li>map_location \u2013 \u53ef\u4ee5\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32(\u4f8b\u5982\uff0c'cpu'\uff0c'cuda\uff1a0'\uff09\uff0c\u4e00\u4e2a\u8bbe\u5907(\u4f8b\u5982\uff0ctorch.device('cpu'\uff09\uff09</li> </ul> \u8fd4\u56de\u503c: <code>ScriptModule</code> \u5bf9\u8c61. <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; torch.jit.load('scriptmodule.pt')\n# \u4eceio.BytesIO\u5bf9\u8c61\u52a0\u8f7dScriptModule\n&gt;&gt;&gt; with open('scriptmodule.pt', 'rb') as f:\n buffer = io.BytesIO(f.read())\n# \u5c06\u6240\u6709\u5f20\u91cf\u52a0\u8f7d\u5230\u539f\u6765\u7684\u8bbe\u5907\u4e0a\n&gt;&gt;&gt; torch.jit.load(buffer)\n# \u7528\u8bbe\u5907\u5c06\u6240\u6709\u5f20\u91cf\u52a0\u8f7d\u5230CPU\u4e0a\n&gt;&gt;&gt; torch.jit.load(buffer, map_location=torch.device('cpu'))\n# \u7528\u5b57\u7b26\u4e32\u5c06\u6240\u6709\u5f20\u91cf\u52a0\u8f7d\u5230CPU\u4e0a\n&gt;&gt;&gt; torch.jit.load(buffer, map_location='cpu')\n\n</code></pre> <pre><code>torch.jit.trace(func, example_inputs, optimize=True, check_trace=True, check_inputs=None, check_tolerance=1e-05, _force_outplace=False)\n</code></pre> <p>\u8ffd\u8e2a\u4e00\u4e2a\u51fd\u6570\u5e76\u8fd4\u56de\u4e00\u4e2a\u4f7f\u7528\u5373\u65f6\u7f16\u8bd1\u4f18\u5316\u8fc7\u7684\u53ef\u6267\u884c\u8ffd\u8e2a\u3002</p> <p>\u8b66\u544a</p> <p>\u8ffd\u8e2a\u4ec5\u6b63\u786e\u8bb0\u5f55\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u7684\u51fd\u6570\u548c\u6a21\u5757(\u4f8b\u5982\uff0c\u5bf9\u5f20\u91cf\u4e2d\u7684\u6570\u636e\u8fdb\u884c\u6761\u4ef6\u5224\u65ad\uff09\uff0c\u5e76\u4e14\u6ca1\u6709\u4efb\u4f55\u672a\u8ffd\u8e2a\u7684\u5916\u90e8\u4f9d\u8d56\u6027(\u4f8b\u5982\uff0c\u6267\u884c\u8f93\u5165/\u8f93\u51fa\u6216\u8bbf\u95ee\u5168\u5c40\u53d8\u91cf\uff09\u3002\u5982\u679c\u4f60\u8ffd\u8e2a\u6b64\u7c7b\u6a21\u578b\uff0c\u5219\u53ef\u80fd\u4f1a\u5728\u968f\u540e\u7684\u6a21\u578b\u8c03\u7528\u4e2d\u9759\u9ed8\u83b7\u53d6\u4e0d\u6b63\u786e\u7684\u7ed3\u679c\u3002\u5f53\u6267\u884c\u53ef\u80fd\u751f\u6210\u9519\u8bef\u8ffd\u8e2a\u7684\u5185\u5bb9\u65f6\uff0c\u8ffd\u8e2a\u5668\u5c06\u5c1d\u8bd5\u53d1\u51fa\u8b66\u544a\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>func (callable or torch.nn.Module) \u2013 \u5c06\u4f7f\u7528example_inputs\u4f5c\u4e3a\u8f93\u5165\u8fd0\u884c\u7684python\u51fd\u6570\u6216torch.nn.Module\u3002\u53c2\u6570\u548c\u8fd4\u56de\u503c\u5fc5\u987b\u662fTensor\u6216(\u5d4c\u5957\u7684\uff09\u5305\u542b\u5f20\u91cf\u7684\u5143\u7ec4\u3002</li> <li>example_inputs (tuple) \u2013 \u5728\u8ffd\u8e2a\u65f6\u5c06\u4f20\u9012\u7ed9\u51fd\u6570\u7684\u793a\u4f8b\u8f93\u5165\u5143\u7ec4\u3002\u5047\u8bbe\u88ab\u8ffd\u8e2a\u64cd\u4f5c\u652f\u6301\u8fd9\u4e9b\u7c7b\u578b\u548c\u5f62\u72b6\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u7684\u8ffd\u8e2a\u53ef\u4ee5\u5728\u4e0d\u540c\u7c7b\u578b\u548c\u5f62\u72b6\u7684\u8f93\u5165\u4e0b\u8fd0\u884c\u3002 example_inputs\u4e5f\u53ef\u4ee5\u662f\u5355\u4e2aTensor\uff0c\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u4f1a\u81ea\u52a8\u5305\u88c5\u5230\u5143\u7ec4\u4e2d\u3002</li> </ul> \u5173\u952e\u5b57\u53c2\u6570\uff1a <ul> <li>optimize (bool, optional) \u2013 \u662f\u5426\u5e94\u7528\u4f18\u5316\u3002\u9ed8\u8ba4\u503c\uff1a<code>True</code>\u3002</li> <li>check_trace (bool, optional) \u2013 \u68c0\u67e5\u88ab\u8ffd\u8e2a\u4ee3\u7801\u5728\u76f8\u540c\u8f93\u5165\u4e0b\u8f93\u51fa\u662f\u5426\u76f8\u540c\u3002\u9ed8\u8ba4\u503c\uff1a<code>True</code>\u3002\u4f60\u53ef\u4ee5\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7981\u7528\u6b64\u529f\u80fd\u3002\u4f8b\u5982\uff0c\u4f60\u7684\u7f51\u7edc\u5305\u542b\u975e\u786e\u5b9a\u6027\u64cd\u4f5c\uff0c\u6216\u8005\u4f60\u786e\u5b9a\u7f51\u7edc\u6b63\u786e\u3002</li> <li>check_inputs (list of tuples__, optional) \u2013 \u5e94\u8be5\u7528\u4e8e\u6839\u636e\u9884\u671f\u68c0\u67e5\u8ffd\u8e2a\u7684\u8f93\u5165\u53c2\u6570\u5143\u7ec4\u5217\u8868\u3002\u6bcf\u4e2a\u5143\u7ec4\u76f8\u5f53\u4e8e\u4e00\u4e2a\u5c06\u5728<code>args</code>\u4e2d\u6307\u5b9a\u7684\u8f93\u5165\u53c2\u6570\u96c6\u5408\u3002\u4e3a\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\uff0c\u8bf7\u4f20\u9012\u4e00\u7ec4\u68c0\u67e5\u8f93\u5165\u8868\u793a\u4f60\u671f\u671b\u7f51\u7edc\u63a5\u53d7\u7684\u5f62\u72b6\u548c\u8f93\u5165\u7c7b\u578b\u8303\u56f4\u3002\u5982\u679c\u672a\u6307\u5b9a\uff0c\u5219\u7528\u539f\u6765\u7684<code>args</code>\u68c0\u67e5\u3002</li> <li>check_tolerance (float, optional) \u2013 \u5728\u68c0\u67e5\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u6d6e\u70b9\u6bd4\u8f83\u5bb9\u5dee\u3002\u7528\u4e8e\u653e\u677e\u68c0\u67e5\u4e25\u683c\u6027\u3002 </li> </ul> \u8fd4\u56de\u503c\uff1a \u542b\u6709<code>forward()</code>\u65b9\u6cd5\u7684<code>ScriptModule</code>\u5bf9\u8c61\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u88ab\u8ffd\u8e2a\u4ee3\u7801\u3002\u5f53func\u662f<code>torch.nn.Module</code>\u65f6\uff0c\u8fd4\u56de\u7684<code>ScriptModule</code>\u5177\u6709\u4e0e\u539f\u59cb\u6a21\u5757\u76f8\u540c\u7684\u5b50\u6a21\u5757\u548c\u53c2\u6570\u96c6\u3002 <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; def f(x):\n...     return x * 2\n&gt;&gt;&gt; traced_f = torch.jit.trace(f, torch.rand(1))\n\n</code></pre> <p>\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u8ffd\u8e2a\u6216\u811a\u672c\u662f\u8f6c\u6362\u6a21\u578b\u7684\u66f4\u7b80\u5355\u65b9\u6cd5\u3002\u6211\u4eec\u5141\u8bb8\u4f60\u5c06\u8ffd\u8e2a\u548c\u811a\u672c\u7ec4\u5408\u4f7f\u7528\u4ee5\u6ee1\u8db3\u6a21\u578b\u7279\u5b9a\u90e8\u5206\u7684\u7279\u5b9a\u8981\u6c42\u3002</p> <p>\u811a\u672c\u51fd\u6570\u53ef\u4ee5\u8c03\u7528\u88ab\u8ffd\u8e2a\u51fd\u6570\u3002\u5f53\u4f60\u9700\u8981\u4f7f\u7528\u63a7\u5236\u6d41\u63a7\u5236\u7b80\u5355\u7684\u524d\u9988\u6a21\u578b\u65f6\uff0c\u8fd9\u5c24\u5176\u6709\u7528\u3002\u4f8b\u5982\uff0c\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u7684beam\u641c\u7d22\u901a\u5e38\u5c06\u4ee5\u811a\u672c\u7f16\u5199\uff0c\u4f46\u53ef\u4ee5\u8c03\u7528\u4f7f\u7528\u8ffd\u8e2a\u751f\u6210\u7684\u7f16\u7801\u5668\u6a21\u5757\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>import torch\n\ndef foo(x, y):\n    return 2 * x + y\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n\n</code></pre> <p>\u88ab\u8ffd\u8e2a\u51fd\u6570\u4e5f\u53ef\u4ee5\u8c03\u7528\u811a\u672c\u51fd\u6570\u3002\u5f53\u6a21\u578b\u5927\u4f53\u662f\u4e00\u4e2a\u524d\u9988\u7f51\u7edc\uff0c\u53ea\u6709\u6a21\u578b\u7684\u4e00\u5c0f\u90e8\u5206\u9700\u8981\u4e00\u4e9b\u63a7\u5236\u6d41\u65f6\uff0c\u8fd9\u4e5f\u5f88\u6709\u7528\u3002\u7531\u8ffd\u8e2a\u51fd\u6570\u8c03\u7528\u7684\u811a\u672c\u51fd\u6570\u5185\u90e8\u7684\u63a7\u5236\u6d41\u4f1a\u88ab\u6b63\u786e\u5730\u4fdd\u7559\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>import torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() &gt; y.max():\n        r = x\n    else:\n        r = y\n    return r\n\ndef bar(x, y, z):\n    return foo(x, y) + z\n\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3))\n\n</code></pre> <p>\u7ec4\u5408\u4e5f\u9002\u7528\u4e8e\u6a21\u5757\uff0c\u4f8b\u5982\u53ef\u4ee5\u4ece\u811a\u672c\u6a21\u5757\u7684\u65b9\u6cd5\u8c03\u7528\u8ffd\u8e2a\u6765\u751f\u6210\u5b50\u6a21\u5757\uff1a</p> <p>\u4f8b\uff1a</p> <pre><code>import torch\nimport torchvision\n\nclass MyScriptModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super(MyScriptModule, self).__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    @torch.jit.script_method\n    def forward(self, input):\n        return self.resnet(input - self.means)\n\n</code></pre> <p>Torch\u811a\u672c\u662fPython\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u53ef\u4ee5\u76f4\u63a5\u7f16\u5199(\u4f7f\u7528@script\u6ce8\u91ca\uff09\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8ffd\u8e2a\u4ecePython\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u3002\u4f7f\u7528\u8ffd\u8e2a\u65f6\uff0c\u4ee3\u7801\u4f1a\u81ea\u52a8\u8f6c\u6362\u4e3aPython\u7684\u8fd9\u4e2a\u5b50\u96c6\uff0c\u65b9\u6cd5\u662f\u4ec5\u8bb0\u5f55\u548c\u6267\u884c\u5f20\u91cf\u4e0a\u7684\u5b9e\u9645\u8fd0\u7b97\u7b26\uff0c\u5e76\u4e22\u5f03\u5176\u4ed6Python\u4ee3\u7801\u3002</p> <p>\u4f7f\u7528@script\u6ce8\u91ca\u76f4\u63a5\u7f16\u5199Torch\u811a\u672c\u65f6\uff0c\u7a0b\u5e8f\u5458\u5fc5\u987b\u53ea\u4f7f\u7528Torch\u811a\u672c\u652f\u6301\u7684Python\u5b50\u96c6\u3002\u672c\u8282\u4ee5\u8bed\u8a00\u53c2\u8003\u7684\u5f62\u5f0f\u4ecb\u7ecdTorch\u811a\u672c\u652f\u6301\u7684\u529f\u80fd\u3002\u672c\u53c2\u8003\u4e2d\u672a\u63d0\u53ca\u7684Python\u7684\u5176\u4ed6\u529f\u80fd\u90fd\u4e0d\u662fTorch\u811a\u672c\u7684\u4e00\u90e8\u5206\u3002</p> <p>\u4f5c\u4e3aPython\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u4efb\u4f55\u6709\u6548\u7684Torch\u811a\u672c\u51fd\u6570\u4e5f\u662f\u4e00\u4e2a\u6709\u6548\u7684Python\u51fd\u6570\u3002\u56e0\u6b64\u4f60\u53ef\u4ee5\u5220\u9664@script\u6ce8\u91ca\u540e\u4f7f\u7528\u6807\u51c6Python\u5de5\u5177(\u5982pdb\uff09\u8c03\u8bd5\u51fd\u6570\u3002\u53cd\u4e4b\u5219\u4e0d\u7136\uff1a\u6709\u8bb8\u591a\u6709\u6548\u7684python\u7a0b\u5e8f\u4e0d\u662f\u6709\u6548\u7684Torch\u811a\u672c\u7a0b\u5e8f\u3002Torch\u811a\u672c\u4e13\u6ce8\u4e8e\u5728Torch\u4e2d\u8868\u793a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6240\u9700\u7684Python\u7279\u6027\u3002</p> <pre><code>PYTORCH_JIT=1\n</code></pre> <p>\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>PYTORCH_JIT = 0</code>\u5c06\u7981\u7528\u6240\u6709\u811a\u672c\u548c\u8ffd\u8e2a\u6ce8\u91ca\u3002\u5982\u679c\u5728ScriptModule\u4e2d\u9047\u5230\u96be\u4ee5\u8c03\u8bd5\u7684\u9519\u8bef\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u6b64\u6807\u5fd7\u5f3a\u5236\u4f7f\u7528\u539f\u751fPython\u8fd0\u884c\u6240\u6709\u5185\u5bb9\u3002\u6b64\u65f6\u53ef\u4f7f\u7528<code>pdb</code>\u4e4b\u7c7b\u7684\u5de5\u5177\u8c03\u8bd5\u4ee3\u7801\u3002</p> <p>Torch\u811a\u672c\u4e0e\u5b8c\u6574Python\u8bed\u8a00\u4e4b\u95f4\u7684\u6700\u5927\u533a\u522b\u5728\u4e8eTorch\u811a\u672c\u4ec5\u652f\u6301\u8868\u8fbe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6240\u9700\u7684\u4e00\u4e9b\u7c7b\u578b\u3002\u7279\u522b\u5730\uff0cTorch\u811a\u672c\u652f\u6301\uff1a</p> <pre><code>Tensor\n</code></pre> <p>\u5177\u6709\u4efb\u4f55dtype\uff0c\u7ef4\u5ea6\u6216backend\u7684PyTorch\u5f20\u91cf\u3002</p> <pre><code>Tuple[T0, T1, ...]\n</code></pre> <p>\u5305\u542b\u5b50\u7c7b\u578b<code>T0</code>\uff0c<code>T1</code>\u7b49\u7684\u5143\u7ec4(\u4f8b\u5982<code>Tuple [Tensor\uff0cTensor]</code>\uff09\u3002</p> <pre><code>int\n</code></pre> <p>\u6807\u91cf\u6574\u6570</p> <pre><code>float\n</code></pre> <p>\u6807\u91cf\u6d6e\u70b9\u6570</p> <pre><code>List[T]\n</code></pre> <p>\u6240\u6709\u6210\u5458\u90fd\u662fT\u7c7b\u578b\u7684\u5217\u8868<code>T</code></p> <p>\u4e0ePython\u4e0d\u540c\uff0cTorch\u811a\u672c\u51fd\u6570\u4e2d\u7684\u6bcf\u4e2a\u53d8\u91cf\u90fd\u5fc5\u987b\u5177\u6709\u4e00\u4e2a\u9759\u6001\u7c7b\u578b\u3002\u8fd9\u6837\u4ee5\u4fbf\u4e8e\u4f18\u5316Torch\u811a\u672c\u529f\u80fd\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>@torch.jit.script\ndef an_error(x):\n    if x:\n        r = torch.rand(1)\n    else:\n        r = 4\n    return r # \u7c7b\u578b\u4e0d\u5339\u914d\uff1a\u5728\u6761\u4ef6\u4e3a\u771f\u65f6r\u4e3aTensor\u7c7b\u578b\n             # \u800c\u4e3a\u5047\u65f6\u5374\u662fint\u7c7b\u578b\n\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cTorch\u811a\u672c\u51fd\u6570\u7684\u6240\u6709\u53c2\u6570\u90fd\u4e3aTensor\u7c7b\u578b\uff0c\u56e0\u4e3a\u8fd9\u662f\u6a21\u5757\u4e2d\u6700\u5e38\u7528\u7684\u7c7b\u578b\u3002\u8981\u5c06Torch\u811a\u672c\u51fd\u6570\u7684\u53c2\u6570\u6307\u5b9a\u4e3a\u53e6\u4e00\u79cd\u7c7b\u578b\uff0c\u53ef\u4ee5\u901a\u8fc7MyPy\u98ce\u683c\u7684\u6ce8\u91ca\u4f7f\u7528\u4e0a\u9762\u5217\u51fa\u7684\u7c7b\u578b\uff1a</p> <p>\u4f8b\uff1a</p> <pre><code>@torch.jit.script\ndef foo(x, tup):\n    # type: (int, Tuple[Tensor, Tensor]) -&gt; Tensor\n    t0, t1 = tup\n    return t0 + t1 + x\n\nprint(foo(3, (torch.rand(3), torch.rand(3))))\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528Python 3\u7c7b\u578b\u6ce8\u91ca\u6765\u6ce8\u91ca\u7c7b\u578b\u3002\u5728\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u6ce8\u91ca\u7684\u6ce8\u91ca\u6765\u786e\u4fdd\u5bf9Python 2\u7684\u517c\u5bb9\u6027\u3002</p> <p>Torch\u811a\u672c\u652f\u6301\u4ee5\u4e0bPython\u8868\u8fbe\u5f0f</p> <pre><code>\u5b57\u9762\u5e38\u91cf\n</code></pre> <p><code>True</code>, <code>False</code>, <code>None</code>, <code>'string literals'</code>, <code>\"string literals\"</code>,  \u5b57\u9762\u503c<code>3</code>(\u89e3\u91ca\u4e3aint\uff09<code>3.4</code>(\u89e3\u91ca\u4e3afloat\uff09</p> <pre><code>\u53d8\u91cf\n</code></pre> <p><code>a</code></p> <p>\u6ce8\u610f</p> <p>\u8bf7\u53c2\u9605\u53d8\u91cf\u89e3\u6790\uff0c\u4e86\u89e3\u53d8\u91cf\u7684\u89e3\u6790\u65b9\u5f0f\u3002</p> <pre><code>\u5143\u7ec4\u6784\u9020\n</code></pre> <p><code>(3, 4)</code>, <code>(3,)</code></p> <pre><code>\u5217\u8868\u6784\u9020\n</code></pre> <p><code>[3, 4]</code>, <code>[]</code>, <code>[torch.rand(3), torch.rand(4)]</code></p> <p>\u6ce8\u610f</p> <p>\u7a7a\u5217\u8868\u5177\u6709\u7c7b\u578b<code>List[Tensor]</code> \u3002\u5176\u4ed6\u5217\u8868\u5b57\u9762\u5e38\u91cf\u7684\u7c7b\u578b\u7531\u6210\u5458\u7684\u7c7b\u578b\u63a8\u51fa\u3002</p> <pre><code>\u7b97\u672f\u8fd0\u7b97\u7b26\n</code></pre> <p><code>a + b</code> <code>a - b</code> <code>a * b</code> <code>a / b</code> <code>a ^ b</code> <code>a @ b</code></p> <pre><code>\u6bd4\u8f83\u8fd0\u7b97\u7b26\n</code></pre> <p><code>a == b</code> <code>a != b</code> <code>a &lt; b</code> <code>a &gt; b</code> <code>a &lt;= b</code> <code>a &gt;= b</code></p> <pre><code>\u903b\u8f91\u8fd0\u7b97\u7b26\n</code></pre> <p><code>a and b</code> <code>a or b</code> <code>not b</code></p> <pre><code>\u7d22\u5f15\n</code></pre> <p><code>t[0]</code> <code>t[-1]</code> <code>t[0:2]</code> <code>t[1:]</code> <code>t[:1]</code> <code>t[:]</code> <code>t[0, 1]</code> <code>t[0, 1:2]</code> <code>t[0, :1]</code> <code>t[-1, 1:, 0]</code> <code>t[1:, -1, 0]</code> <code>t[i:j, i]</code></p> <p>\u6ce8\u610f</p> <p>Torch\u811a\u672c\u76ee\u524d\u4e0d\u652f\u6301\u539f\u5730\u4fee\u6539\u5f20\u91cf\uff0c\u56e0\u6b64\u5bf9\u5f20\u91cf\u7d22\u5f15\u53ea\u80fd\u51fa\u73b0\u5728\u8868\u8fbe\u5f0f\u7684\u53f3\u4fa7\u3002</p> <pre><code>\u51fd\u6570\u8c03\u7528\n</code></pre> <p>\u8c03\u7528\u5185\u7f6e\u51fd\u6570\uff1a<code>torch.rand(3, dtype=torch.int)</code></p> <p>\u8c03\u7528\u5176\u4ed6\u811a\u672c\u51fd\u6570\uff1a</p> <pre><code>import torch\n\n@torch.jit.script\ndef foo(x):\n  return x + 1\n\n@torch.jit.script\ndef bar(x):\n  return foo(x)\n\n</code></pre> <pre><code>\u65b9\u6cd5\u8c03\u7528\n</code></pre> <p>\u8c03\u7528\u5185\u7f6e\u7c7b\u578b\u7684\u65b9\u6cd5\uff0c\u5982tensor: <code>x.mm(y)</code></p> <p>\u5728ScriptModule\u4e2d\u5b9a\u4e49Script\u65b9\u6cd5\u65f6\uff0c\u4f7f\u7528<code>@script_method</code>\u6279\u6ce8\u3002Script\u65b9\u6cd5\u53ef\u4ee5\u8c03\u7528\u6a21\u5757\u5185\u5176\u4ed6\u65b9\u6cd5\u6216\u5b50\u6a21\u5757\u7684\u65b9\u6cd5\u3002</p> <p>\u76f4\u63a5\u8c03\u7528\u5b50\u6a21\u5757(\u4f8b\u5982<code>self.resnet(input\uff09</code>\uff09\u7b49\u540c\u4e8e\u8c03\u7528\u5176<code>forward</code>\u65b9\u6cd5(\u4f8b\u5982<code>self.resnet.forward(input\uff09</code>\uff09</p> <pre><code>import torch\n\nclass MyScriptModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super(MyScriptModule, self).__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    @torch.jit.script_method\n    def helper(self, input):\n      return self.resnet(input - self.means)\n\n    @torch.jit.script_method\n    def forward(self, input):\n        return self.helper(input)\n\n</code></pre> <pre><code>If \u8868\u8fbe\u5f0f\n</code></pre> <p><code>x if x &gt; y else y</code></p> <pre><code>\u7c7b\u578b\u8f6c\u6362\n</code></pre> <p><code>float(ten)</code>, <code>int(3.5)</code>, <code>bool(ten)</code></p> <pre><code>\u8bbf\u95ee\u6a21\u5757\u53c2\u6570\n</code></pre> <p><code>self.my_parameter</code> <code>self.my_submodule.my_parameter</code></p> <p>Torch\u811a\u672c\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\u7684\u8bed\u53e5\uff1a</p> <p>\u7b80\u5355\u8d4b\u503c</p> <p>```py a = b a += b # short-hand for a = a + b, does not operate in-place on a a -= b</p> <p>```</p> <p>\u6a21\u5f0f\u5339\u914d\u8d4b\u503c</p> <p>```py a, b = tuple_or_list a, b, *c = a_tuple</p> <p>```</p> <p>Print \u8bed\u53e5</p> <p><code>print(\"the result of an add:\", a + b)</code></p> <p>If \u8bed\u53e5</p> <p>```py if a &lt; 4:     r = -a elif a &lt; 3:     r = a + a else:     r = 3 * a</p> <p>```</p> <p>While \u5faa\u73af</p> <p>```py a = 0 while a &lt; 4:     print(a)     a += 1</p> <p>```</p> <p>\u5e26 <code>range</code> \u7684for\u5faa\u73af</p> <p>```py x = 0 for i in range(10):     x *= i</p> <p>```</p> <p>\u6ce8\u610f</p> <p>\u811a\u672c\u76ee\u524d\u4e0d\u652f\u6301\u5bf9\u4e00\u822c\u53ef\u8fed\u4ee3\u5bf9\u8c61(\u5982\u5217\u8868\u6216\u5f20\u91cf\uff09\u8fdb\u884c\u8fed\u4ee3\uff0c\u4e5f\u4e0d\u652f\u6301range\u8d77\u59cb\u4e0e\u589e\u91cf\u53c2\u6570\uff0c\u8fd9\u4e9b\u5c06\u5728\u672a\u6765\u7248\u672c\u4e2d\u6dfb\u52a0\u3002</p> <p>\u5bf9\u5143\u7ec4\u7684for\u5faa\u73af\uff1a</p> <p>```py tup = (3, torch.rand(4)) for x in tup:     print(x)</p> <p>```</p> <p>\u6ce8\u610f</p> <p>\u5bf9\u4e8e\u5143\u7ec4\u5faa\u73af\u5c06\u5c55\u5f00\u5faa\u73af\uff0c\u4e3a\u5143\u7ec4\u7684\u6bcf\u4e2a\u6210\u5458\u751f\u6210\u4e00\u4e2a\u5faa\u73af\u4f53\u3002\u5faa\u73af\u4f53\u5185\u5fc5\u987b\u786e\u4fdd\u6bcf\u4e2a\u6210\u5458\u7c7b\u578b\u6b63\u786e\u3002</p> <p>\u5bf9\u5e38\u91cf <code>torch.nn.ModuleList</code> \u7684for\u5faa\u73af</p> <p>```py class SubModule(torch.jit.ScriptModule):     def init(self):         super(Sub, self).init()         self.weight = nn.Parameter(torch.randn(2))</p> <pre><code>@torch.jit.script_method\ndef forward(self, input):\n    return self.weight + input\n</code></pre> <p>class MyModule(torch.jit.ScriptModule):     constants = ['mods']</p> <pre><code>def __init__(self):\n    super(MyModule, self).__init__()\n    self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])\n\n@torch.jit.script_method\ndef forward(self, v):\n    for module in self.mods:\n        v = m(v)\n    return v\n</code></pre> <p>```</p> <p>\u6ce8\u610f</p> <p>\u8981\u5728<code>@script_method</code>\u4e2d\u4f7f\u7528\u6a21\u5757\u5217\u8868\uff0c\u5fc5\u987b\u901a\u8fc7\u5c06\u5c5e\u6027\u7684\u540d\u79f0\u6dfb\u52a0\u5230\u7c7b\u578b\u7684<code>__constants__</code>\u5217\u8868\u6765\u5c06\u5176\u6807\u8bb0\u4e3a\u5e38\u91cf\u3002ModuleList\u4e0a\u7684for\u5faa\u73af\u5728\u7f16\u8bd1\u65f6\u4f7f\u7528\u5e38\u91cf\u6a21\u5757\u5217\u8868\u7684\u6bcf\u4e2a\u6210\u5458\u5c55\u5f00\u5faa\u73af\u4f53\u3002</p> <pre><code>Return \u8bed\u53e5\n</code></pre> <p><code>return a, b</code></p> <p>\u6ce8\u610f</p> <p>return\u8bed\u53e5\u5fc5\u987b\u4f5c\u4e3a\u51fd\u6570\u7684\u6700\u540e\u4e00\u4e2a\u6210\u5458\uff0c\u800c\u4e0d\u80fd\u51fa\u73b0\u5728\u51fd\u6570\u7684\u5176\u4ed6\u4f4d\u7f6e\u3002\u6b64\u9650\u5236\u5c06\u5728\u4ee5\u540e\u5220\u9664\u3002</p> <p>Torch\u811a\u672c\u652f\u6301Python\u53d8\u91cf\u89e3\u6790(\u5373\u4f5c\u7528\u57df\uff09\u89c4\u5219\u7684\u5b50\u96c6\u3002\u5c40\u90e8\u53d8\u91cf\u7684\u884c\u4e3a\u4e0ePython\u4e2d\u7684\u76f8\u540c\uff0c\u4f46\u53d8\u91cf\u5fc5\u987b\u5728\u51fd\u6570\u7684\u6240\u6709\u8def\u5f84\u4e2d\u5177\u6709\u76f8\u540c\u7c7b\u578b\u3002\u5982\u679c\u53d8\u91cf\u5728if\u8bed\u53e5\u7684\u4e0d\u540c\u4fa7\u5177\u6709\u4e0d\u540c\u7684\u7c7b\u578b\uff0c\u5219\u5728if\u8bed\u53e5\u7ed3\u675f\u540e\u4f7f\u7528\u5b83\u4f1a\u62b1\u9519\u3002</p> <p>\u7c7b\u4f3c\u5730\uff0c\u5982\u679c\u4ec5\u5728\u51fd\u6570\u7684\u67d0\u4e9b\u6267\u884c\u8def\u5f84\u4e0a\u5b9a\u4e49\u53d8\u91cf\u4e5f\u4f1a\u51fa\u9519\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>@torch.jit.script\ndef foo(x):\n    if x &lt; 0:\n        y = 4\n    print(y) # \u9519\u8bef: y \u503c\u672a\u5b9a\u4e49\n\n</code></pre> <p>\u5b9a\u4e49\u51fd\u6570\u7684\u975e\u5c40\u90e8\u53d8\u91cf\u5728\u7f16\u8bd1\u65f6\u89e3\u6790\u4e3aPython\u503c\u3002\u7136\u540e\uff0c\u7528Python\u503c\u7684\u4f7f\u7528\u4e2d\u7684\u89c4\u5219\u5c06\u8fd9\u4e9b\u503c\u8f6c\u6362\u4e3aTorch\u811a\u672c\u503c\u3002</p> <p>\u4e3a\u4e86\u4f7f\u7f16\u5199Torch\u811a\u672c\u66f4\u65b9\u4fbf\uff0c\u6211\u4eec\u5141\u8bb8\u811a\u672c\u4ee3\u7801\u5f15\u7528\u5468\u56f4\u7684Python\u503c\u3002\u4f8b\u5982\uff0c\u5f53\u6211\u4eec\u5f15\u7528<code>torch</code>\u65f6\uff0cTorch\u811a\u672c\u7f16\u8bd1\u5668\u5b9e\u9645\u4e0a\u5728\u58f0\u660e\u51fd\u6570\u65f6\u5c06\u5176\u89e3\u6790\u4e3aPython\u7684<code>torch</code>\u6a21\u5757\u3002\u8fd9\u4e9bPython\u503c\u4e0d\u662fTorch\u811a\u672c\u7684\u4e00\u90e8\u5206\uff0c\u5b83\u4eec\u5728\u7f16\u8bd1\u65f6\u88ab\u8f6c\u6362\u6210Torch\u811a\u672c\u652f\u6301\u7684\u539f\u59cb\u7c7b\u578b\u3002\u672c\u8282\u4ecb\u7ecd\u5728Torch\u811a\u672c\u4e2d\u8bbf\u95eePython\u503c\u65f6\u4f7f\u7528\u7684\u89c4\u5219\u3002\u5b83\u4eec\u4f9d\u8d56\u4e8e\u5f15\u7528\u7684python\u503c\u7684\u52a8\u6001\u7c7b\u578b\u3002</p> <pre><code>\u51fd\u6570\n</code></pre> <p>Torch\u811a\u672c\u53ef\u4ee5\u8c03\u7528python\u51fd\u6570\u3002\u6b64\u529f\u80fd\u5728\u5c06\u6a21\u578b\u9010\u6b65\u8f6c\u6362\u4e3a\u811a\u672c\u65f6\u975e\u5e38\u6709\u7528\u3002\u53ef\u4ee5\u5c06\u6a21\u578b\u4e2d\u7684\u51fd\u6570\u9010\u4e2a\u8f6c\u6210\u811a\u672c\uff0c\u4fdd\u7559\u5bf9\u5176\u4f59Python\u51fd\u6570\u7684\u8c03\u7528\u3002\u8fd9\u6837\uff0c\u5728\u9010\u6b65\u8f6c\u6362\u7684\u8fc7\u7a0b\u4e2d\u4f60\u53ef\u4ee5\u968f\u65f6\u68c0\u67e5\u6a21\u578b\u7684\u6b63\u786e\u6027\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>def foo(x):\n  print(\"I am called with {}\".format(x))\n  import pdb; pdb.set_trace()\n  return x\n\n@torch.jit.script\ndef bar(x)\n  return foo(x + 1)\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4e0d\u80fd\u5728\u5305\u542bPython\u51fd\u6570\u8c03\u7528\u7684ScriptModule\u4e0a\u8c03\u7528<code>save</code>\u3002\u8be5\u529f\u80fd\u4ec5\u7528\u4e8e\u8c03\u8bd5\uff0c\u5e94\u5728\u4fdd\u5b58\u4e4b\u524d\u5220\u9664\u8c03\u7528\u6216\u5c06\u5176\u8f6c\u6362\u4e3a\u811a\u672c\u51fd\u6570\u3002</p> <pre><code>Python\u6a21\u5757\u7684\u5c5e\u6027\u67e5\u627e\n</code></pre> <p>Torch\u811a\u672c\u53ef\u4ee5\u5728\u6a21\u5757\u4e0a\u67e5\u627e\u5c5e\u6027\u3002\u50cf<code>torch.add</code>\u8fd9\u6837\u7684\u5185\u7f6e\u51fd\u6570\u5c31\u4ee5\u8fd9\u79cd\u65b9\u5f0f\u8bbf\u95ee\u3002\u8fd9\u5141\u8bb8Torch\u811a\u672c\u8c03\u7528\u5176\u4ed6\u6a21\u5757\u4e2d\u5b9a\u4e49\u7684\u51fd\u6570\u3002</p> <pre><code>Python \u4e2d\u5b9a\u4e49\u7684\u5e38\u91cf\n</code></pre> <p>Torch\u811a\u672c\u8fd8\u63d0\u4f9b\u4e86\u4f7f\u7528Python\u5e38\u91cf\u7684\u65b9\u6cd5\u3002\u8fd9\u53ef\u7528\u4e8e\u5c06\u8d85\u53c2\u6570\u786c\u7f16\u7801\u5230\u51fd\u6570\u4e2d\uff0c\u6216\u7528\u4e8e\u5b9a\u4e49\u901a\u7528\u5e38\u91cf\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u6307\u5b9aPython\u503c\u4e3a\u5e38\u91cf\u3002</p> <ol> <li> <p>\u67e5\u627e\u7684\u503c\u4e3a\u6a21\u5757\u7684\u5c5e\u6027,\u4f8b\u5982\uff1a<code>math.pi</code></p> </li> <li> <p>\u53ef\u4ee5\u5c06ScriptModule\u7684\u5c5e\u6027\u6807\u8bb0\u4e3a\u5e38\u91cf\uff0c\u65b9\u6cd5\u662f\u5c06\u5176\u5217\u4e3a\u7c7b\u7684<code>__constants__</code>\u5c5e\u6027\u6210\u5458\uff1a</p> <p>\u4f8b\uff1a</p> <p>```py class Foo(torch.jit.ScriptModule):     constants = ['a']</p> <pre><code>def __init__(self):\n    super(Foo, self).__init__(False)\n    self.a = 1 + 4\n</code></pre> <p>@torch.jit.ScriptModule    def forward(self, input):        return self.a + input</p> <p>```</p> </li> </ol> <p>\u652f\u6301\u7684Python\u5e38\u91cf\u503c\u6709</p> <ul> <li><code>int</code></li> <li><code>bool</code></li> <li><code>torch.device</code></li> <li><code>torch.layout</code></li> <li><code>torch.dtype</code></li> <li>\u5305\u542b\u652f\u6301\u7c7b\u578b\u7684\u5143\u7ec4</li> <li><code>torch.nn.ModuleList</code> \uff0c\u53ef\u4ee5\u5c06\u5176\u7528\u5728Torch \u811a\u672cfor\u5faa\u73af\u4e2d</li> </ul> <pre><code>\u7981\u7528JIT\u4ee5\u65b9\u4fbf\u8c03\u8bd5\n</code></pre> <p>\u53ef\u4ee5\u901a\u8fc7\u5c06<code>PYTORCH_JIT</code>\u73af\u5883\u53d8\u91cf\u503c\u8bbe\u7f6e\u4e3a<code>0</code>\u7981\u7528\u6240\u6709<code>JIT</code>\u6a21\u5f0f(\u8ffd\u8e2a\u548c\u811a\u672c\u5316\uff09\u4ee5\u4fbf\u5728\u539f\u59cbPython\u4e2d\u8c03\u8bd5\u7a0b\u5e8f\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u793a\u4f8b\u811a\u672c\uff1a</p> <pre><code>@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\n\ntraced_fn(torch.rand(3, 4))\n\n</code></pre> <p>\u4e3a\u4e86\u4f7f\u7528PDB\u8c03\u8bd5\u6b64\u811a\u672c\u3002\u6211\u4eec\u53ef\u4ee5\u5168\u5c40\u7981\u7528JIT\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5c06@script\u51fd\u6570\u4f5c\u4e3a\u666e\u901a\u7684python\u51fd\u6570\u8c03\u7528\u800c\u4e0d\u4f1a\u7f16\u8bd1\u5b83\u3002\u5982\u679c\u4e0a\u9762\u7684\u811a\u672c\u540d\u4e3a<code>disable_jit_example.py</code>\uff0c\u6211\u4eec\u8fd9\u6837\u8c03\u7528\u5b83\uff1a</p> <pre><code>$ PYTORCH_JIT=0 python disable_jit_example.py\n\n</code></pre> <p>\u8fd9\u6837,\u6211\u4eec\u5c31\u80fd\u591f\u4f5c\u4e3a\u666e\u901a\u7684Python\u51fd\u6570\u6b65\u5165@script\u51fd\u6570\u3002</p> <pre><code>\u89e3\u91ca\u56fe\n</code></pre> <p>TorchScript\u4f7f\u7528\u9759\u6001\u5355\u4e00\u6307\u6d3e(SSA\uff09\u4e2d\u95f4\u8868\u793a(IR\uff09\u6765\u8868\u793a\u8ba1\u7b97\u3002\u8fd9\u79cd\u683c\u5f0f\u7684\u6307\u4ee4\u5305\u62ecATen(PyTorch\u7684C ++\u540e\u7aef\uff09\u8fd0\u7b97\u7b26\u548c\u5176\u4ed6\u539f\u59cb\u8fd0\u7b97\u7b26\uff0c\u5305\u62ec\u5faa\u73af\u548c\u6761\u4ef6\u7684\u63a7\u5236\u6d41\u8fd0\u7b97\u7b26\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff1a</p> <pre><code>@torch.jit.script\ndef foo(len):\n  # type: (int) -&gt; torch.Tensor\n  rv = torch.zeros(3, 4)\n  for i in range(len):\n    if i &lt; 10:\n        rv = rv - 1.0\n    else:\n        rv = rv + 1.0\n  return rv\n\nprint(foo.graph)\n\n</code></pre> <p>\u5177\u6709\u5355\u4e2a<code>forward</code>\u65b9\u6cd5\u7684<code>ScriptModule</code>\u5177\u6709<code>graph</code>\u5c5e\u6027\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u8be5\u56fe\u6765\u68c0\u67e5\u8868\u793a\u8ba1\u7b97\u7684IR\u3002\u5982\u679cScriptModule\u6709\u591a\u4e2a\u65b9\u6cd5\uff0c\u5219\u9700\u8981\u8bbf\u95ee\u65b9\u6cd5\u672c\u8eab\u7684<code>.graph</code>\u5c5e\u6027\u3002\u4f8b\u5982\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee<code>.bar.graph</code>\u6765\u68c0\u67e5ScriptModule\u4e0a\u540d\u4e3a<code>bar</code>\u7684\u65b9\u6cd5\u7684\u56fe\u3002</p> <p>\u4e0a\u9762\u7684\u793a\u4f8b\u811a\u672c\u751f\u6210\u56fe\u5f62\uff1a</p> <pre><code>graph(%len : int) {\n  %13 : float = prim::Constant[value=1]()\n  %10 : int = prim::Constant[value=10]()\n  %2 : int = prim::Constant[value=4]()\n  %1 : int = prim::Constant[value=3]()\n  %3 : int[] = prim::ListConstruct(%1, %2)\n  %4 : int = prim::Constant[value=6]()\n  %5 : int = prim::Constant[value=0]()\n  %6 : int[] = prim::Constant[value=[0, -1]]()\n  %rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6)\n  %8 : int = prim::Constant[value=1]()\n  %rv : Dynamic = prim::Loop(%len, %8, %rv.1)\n    block0(%i : int, %12 : Dynamic) {\n      %11 : int = aten::lt(%i, %10)\n      %rv.4 : Dynamic = prim::If(%11)\n        block0() {\n          %14 : int = prim::Constant[value=1]()\n          %rv.2 : Dynamic = aten::sub(%12, %13, %14)\n          -&gt; (%rv.2)\n        }\n        block1() {\n          %16 : int = prim::Constant[value=1]()\n          %rv.3 : Dynamic = aten::add(%12, %13, %16)\n          -&gt; (%rv.3)\n        }\n      %19 : int = prim::Constant[value=1]()\n      -&gt; (%19, %rv.4)\n    }\n  return (%rv);\n}\n\n</code></pre> <p>\u4ee5\u6307\u4ee4<code>\uff05rv.1\uff1aDynamic = aten :: zeros(\uff053\uff0c\uff054\uff0c\uff055\uff0c\uff056\uff09</code>\u4e3a\u4f8b\u3002<code>\uff05rv.1\uff1aDynamic</code>\u5c06\u8f93\u51fa\u5206\u914d\u7ed9\u540d\u4e3a<code>rv.1</code>\u7684(\u552f\u4e00\uff09\u503c\uff0c\u8be5\u503c\u662f\u52a8\u6001\u7c7b\u578b\uff0c\u5373\u6211\u4eec\u4e0d\u77e5\u9053\u5b83\u7684\u5177\u4f53\u5f62\u72b6\u3002<code>aten :: zeros</code>\u662f\u8fd0\u7b97\u7b26(\u76f8\u5f53\u4e8e<code>torch.zeros</code>\uff09\uff0c\u5b83\u7684\u8f93\u5165\u5217\u8868<code>(\uff053\uff0c\uff054\uff0c\uff055\uff0c\uff056\uff09</code>\u6307\u5b9a\u8303\u56f4\u4e2d\u7684\u54ea\u4e9b\u503c\u5e94\u4f5c\u4e3a\u8f93\u5165\u4f20\u9012\u3002\u5185\u7f6e\u51fd\u6570(\u5982<code>aten :: zeros</code>\uff09\u7684\u6a21\u5f0f\u53ef\u4ee5\u5728\u5185\u7f6e\u51fd\u6570\u4e2d\u627e\u5230\u3002</p> <p>\u6ce8\u610f\uff0c\u8fd0\u7b97\u7b26\u4e5f\u53ef\u4ee5\u6709\u5173\u8054\u7684<code>block</code>\uff0c\u5982<code>prim :: Loop</code>\u548c<code>prim :: If</code>\u8fd0\u7b97\u7b26\u3002\u5728\u56fe\u5f62\u6253\u5370\u8f93\u51fa\u4e2d\uff0c\u8fd9\u4e9b\u8fd0\u7b97\u7b26\u88ab\u683c\u5f0f\u5316\u4ee5\u53cd\u6620\u4e0e\u5176\u7b49\u4ef7\u7684\u6e90\u4ee3\u7801\u5f62\u5f0f\uff0c\u4ee5\u4fbf\u4e8e\u8c03\u8bd5\u3002</p> <p>\u53ef\u4ee5\u68c0\u67e5\u56fe\u4ee5\u786e\u8ba4<code>ScriptModule</code>\u63cf\u8ff0\u7684\u8ba1\u7b97\u662f\u6b63\u786e\u7684\uff0c\u65b9\u6cd5\u5982\u4e0b\u6240\u8ff0\u3002</p> <pre><code>\u8ffd\u8e2a\u7684\u8fb9\u7f18\u60c5\u51b5\n</code></pre> <p>\u5728\u4e00\u4e9b\u8fb9\u7f18\u60c5\u51b5\u4e0b\u4e00\u4e9bPython\u51fd\u6570/\u6a21\u5757\u7684\u8ffd\u8e2a\u4e0d\u80fd\u4ee3\u8868\u5e95\u5c42\u4ee3\u7801\u3002\u8fd9\u4e9b\u60c5\u51b5\u53ef\u4ee5\u5305\u62ec\uff1a</p> <ul> <li>\u8ffd\u8e2a\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\u63a7\u5236\u6d41(\u4f8b\u5982\u5f20\u91cf\u5f62\u72b6\uff09</li> <li>\u8ffd\u8e2a\u5f20\u91cf\u89c6\u56fe\u7684\u5c31\u5730\u64cd\u4f5c(\u4f8b\u5982\uff0c\u5728\u5206\u914d\u7684\u5de6\u4fa7\u8fdb\u884c\u7d22\u5f15\uff09</li> </ul> <p>\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e9b\u60c5\u51b5\u5728\u5c06\u6765\u7248\u672c\u4e2d\u53ef\u80fd\u662f\u53ef\u8ffd\u8e2a\u7684\u3002</p> <pre><code>\u81ea\u52a8\u8ffd\u8e2a\u68c0\u67e5\n</code></pre> <p>\u901a\u8fc7\u5728<code>torch.jit.trace()</code>API\u4e0a\u4f7f\u7528<code>check_inputs</code>\uff0c\u662f\u81ea\u52a8\u6355\u83b7\u8ffd\u8e2a\u4e2d\u9519\u8bef\u7684\u4e00\u79cd\u65b9\u6cd5\u3002 <code>check_inputs</code>\u662f\u7528\u4e8e\u91cd\u65b0\u8ffd\u8e2a\u8ba1\u7b97\u5e76\u9a8c\u8bc1\u7ed3\u679c\u7684\u8f93\u5165\u5143\u7ec4\u5217\u8868\u3002\u4f8b\u5982\uff1a</p> <pre><code>def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n\n</code></pre> <p>\u4e0a\u9762\u4ee3\u7801\u4f1a\u4e3a\u6211\u4eec\u63d0\u4f9b\u4ee5\u4e0b\u8bca\u65ad\u4fe1\u606f\uff1a</p> <pre><code>ERROR: Graphs differed across invocations!\nGraph diff:\n    graph(%0 : Dynamic) {\n          %1 : int = prim::Constant[value=0]()\n          %2 : int = prim::Constant[value=0]()\n          %3 : Dynamic = aten::select(%0, %1, %2)\n          %4 : int = prim::Constant[value=0]()\n          %5 : int = prim::Constant[value=0]()\n          %6 : Dynamic = aten::select(%0, %4, %5)\n          %7 : Dynamic = aten::mul(%3, %6)\n          %8 : int = prim::Constant[value=0]()\n          %9 : int = prim::Constant[value=1]()\n          %10 : Dynamic = aten::select(%0, %8, %9)\n          %11 : Dynamic = aten::mul(%7, %10)\n          %12 : int = prim::Constant[value=0]()\n          %13 : int = prim::Constant[value=2]()\n          %14 : Dynamic = aten::select(%0, %12, %13)\n          %15 : Dynamic = aten::mul(%11, %14)\n      +   %16 : int = prim::Constant[value=0]()\n      +   %17 : int = prim::Constant[value=3]()\n      +   %18 : Dynamic = aten::select(%0, %16, %17)\n      +   %19 : Dynamic = aten::mul(%15, %18)\n      -   return (%15);\n      ?             ^\n      +   return (%19);\n      ?             ^\n    }\n\n</code></pre> <p>\u6b64\u6d88\u606f\u8868\u660e\uff0c\u6211\u4eec\u7b2c\u4e00\u6b21\u8ffd\u8e2a\u51fd\u6570\u548c\u4f7f\u7528<code>check_inputs</code>\u8ffd\u8e2a\u51fd\u6570\u65f6\u7684\u8ba1\u7b97\u5b58\u5728\u5dee\u5f02\u3002\u4e8b\u5b9e\u4e0a\uff0c<code>loop_in_traced_fn</code>\u4f53\u5185\u7684\u5faa\u73af\u53d6\u51b3\u4e8e\u8f93\u5165x\u7684\u5f62\u72b6\uff0c\u56e0\u6b64\u5f53\u6211\u4eec\u8f93\u5165\u4e0d\u540c\u5f62\u72b6\u7684<code>x</code>\u65f6\uff0c\u8f68\u8ff9\u4f1a\u6709\u6240\u4e0d\u540c\u3002</p> <p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4f7f\u7528\u811a\u672c\u6355\u83b7\u6b64\u7c7b\u6570\u636e\u76f8\u5173\u63a7\u5236\u6d41\uff1a</p> <pre><code>def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n\n</code></pre> <p>\u4e0a\u9762\u4ee3\u7801\u4f1a\u4e3a\u6211\u4eec\u63d0\u4f9b\u4ee5\u4e0b\u4fe1\u606f\uff1a</p> <pre><code>graph(%x : Dynamic) {\n  %1 : int = prim::Constant[value=0]()\n  %2 : int = prim::Constant[value=0]()\n  %result.1 : Dynamic = aten::select(%x, %2, %1)\n  %4 : int = aten::size(%x, %1)\n  %5 : int = prim::Constant[value=1]()\n  %result : Dynamic = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Dynamic) {\n      %9 : int = prim::Constant[value=0]()\n      %10 : Dynamic = aten::select(%x, %9, %i)\n      %result.2 : Dynamic = aten::mul(%7, %10)\n      %12 : int = prim::Constant[value=1]()\n      -&gt; (%12, %result.2)\n    }\n  return (%result);\n}\n\n</code></pre> <pre><code>\u8ffd\u8e2a\u5668\u8b66\u544a\n</code></pre> <p>\u8ffd\u8e2a\u5668\u4f1a\u5728\u8ffd\u8e2a\u8ba1\u7b97\u4e2d\u5bf9\u6709\u95ee\u9898\u7684\u6a21\u5f0f\u751f\u6210\u8b66\u544a\u3002\u4f8b\u5982\uff0c\u8ffd\u8e2a\u5305\u542b\u5728Tensor\u7684\u5207\u7247(\u89c6\u56fe\uff09\u4e0a\u5c31\u5730\u8d4b\u503c\u64cd\u4f5c\u7684\u51fd\u6570\uff1a</p> <pre><code>def fill_row_zero(x):\n    x[0] = torch.rand(*x.shape[1:2])\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n\n</code></pre> <p>\u8fd9\u4f1a\u51fa\u73b0\u5982\u4e0b\u8b66\u544a\u548c\u4e00\u4e2a\u7b80\u5355\u8fd4\u56de\u8f93\u5165\u7684\u56fe\uff1a</p> <pre><code>fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n  x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1\\. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n  traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n  return (%0);\n}\n\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528<code>torch.cat</code>\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\u907f\u514d\u5c31\u5730\u66f4\u65b0\u95ee\u9898\uff1a</p> <pre><code>def fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n\n</code></pre> <p>Torch\u811a\u672c\u652f\u6301\u90e8\u5206PyTorch\u5185\u7f6e\u5f20\u91cf\u548c\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u3002 Tensor\u4e0a\u7684\u5927\u591a\u6570\u65b9\u6cd5\u4ee5\u53ca<code>torch</code>\u547d\u540d\u7a7a\u95f4\u4e2d\u7684\u51fd\u6570\u90fd\u53ef\u7528\u3002 <code>torch.nn.functional</code>\u4e2d\u7684\u8bb8\u591a\u51fd\u6570\u4e5f\u53ef\u7528\u3002</p> <p>\u6211\u4eec\u76ee\u524d\u4e0d\u63d0\u4f9b\u50cf <code>Linear</code> \u6216 <code>Conv</code> \u6a21\u5757\u4e4b\u7c7b\u5185\u7f6eScriptModule,\u6b64\u529f\u80fd\u5c06\u5728\u672a\u6765\u5f00\u53d1\u3002\u76ee\u524d\u6211\u4eec\u5efa\u8bae\u4f7f\u7528<code>torch.jit.trace</code>\u5c06\u6807\u51c6\u7684<code>torch.nn</code>\u6a21\u5757\u8f6c\u6362\u4e3aScriptModule\u3002</p>"},{"location":"1.0/model_zoo/","title":"torch.utils.model_zoo","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <pre><code>torch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True)\n</code></pre> <p>\u7531\u7ed9\u5b9aURL\u52a0\u8f7dTorch\u5e8f\u5217\u5316\u5bf9\u8c61\u3002</p> <p>\u5982\u679c\u8be5\u5bf9\u8c61\u5df2\u7ecf\u5b58\u5728\u4e8e<code>model_dir</code>\u4e2d\uff0c\u5c06\u88ab\u53cd\u5e8f\u5217\u5316\u5e76\u8fd4\u56de\u3002URL\u7684\u6587\u4ef6\u540d\u90e8\u5206\u5e94\u8be5\u9075\u5faa\u7ea6\u5b9a<code>filename-&lt;sha256&gt;.ext</code>\uff0c\u5176\u4e2d<code>&lt;sha256&gt;</code>\u662f\u6587\u4ef6\u5185\u5bb9\u7684SHA256\u54c8\u5e0c\u7684\u524d\u516b\u4f4d\u6216\u66f4\u591a\u4f4d\u6570\u3002(\u54c8\u5e0c\u7528\u4e8e\u786e\u4fdd\u552f\u4e00\u7684\u540d\u79f0\u5e76\u9a8c\u8bc1\u6587\u4ef6\u7684\u5185\u5bb9\uff09</p> <p><code>model_dir</code>\u9ed8\u8ba4\u4e3a<code>$TORCH_HOME/models</code>\uff0c\u5176\u4e2d<code>$TORCH_HOME</code>\u9ed8\u8ba4\u662f<code>~/.torch</code>\u3002\u5982\u679c\u4e0d\u9700\u8981\u9ed8\u8ba4\u76ee\u5f55\uff0c\u53ef\u4ee5\u901a\u8fc7\u73af\u5883\u53d8\u91cf<code>$TORCH_MODEL_ZOO</code>\u6307\u5b9a\u5176\u5b83\u7684\u76ee\u5f55\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>url(string\uff09\u2013 \u8981\u4e0b\u8f7d\u7684\u5bf9\u8c61\u7684URL\u94fe\u63a5</li> <li>model_dir(string , \u53ef\u9009\uff09\u2013 \u4fdd\u5b58\u4e0b\u8f7d\u5bf9\u8c61\u7684\u76ee\u5f55</li> <li>map_location(\u53ef\u9009\uff09\u2013 \u51fd\u6570\u6216\u5b57\u5178\uff0c\u6307\u5b9a\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e(\u89c1torch.load\uff09</li> <li>progress(bool, \u53ef\u9009\uff09\u2013 \u662f\u5426\u5411\u6807\u51c6\u8f93\u51fa\u5c55\u793a\u8fdb\u5ea6\u6761</li> </ul> <p>\u793a\u4f8b</p> <pre><code>&gt;&gt;&gt; state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n</code></pre>"},{"location":"1.0/multiprocessing/","title":"\u591a\u8fdb\u7a0b\u5305 - torch.multiprocessing","text":"<p>\u8bd1\u8005\uff1ahijkzzz</p> <p>torch.multiprocessing \u662f\u4e00\u4e2a\u672c\u5730 <code>multiprocessing</code> \u6a21\u5757\u7684\u5305\u88c5. \u5b83\u6ce8\u518c\u4e86\u81ea\u5b9a\u4e49\u7684reducers, \u5e76\u4f7f\u7528\u5171\u4eab\u5185\u5b58\u4e3a\u4e0d\u540c\u7684\u8fdb\u7a0b\u5728\u540c\u4e00\u4efd\u6570\u636e\u4e0a\u63d0\u4f9b\u5171\u4eab\u7684\u89c6\u56fe. \u4e00\u65e6 tensor/storage \u88ab\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58 (\u89c1 <code>share_memory_()</code>), \u5c06\u5176\u53d1\u9001\u5230\u4efb\u4f55\u8fdb\u7a0b\u4e0d\u4f1a\u9020\u6210\u62f7\u8d1d\u5f00\u9500.</p> <p>\u6b64 API 100% \u517c\u5bb9\u539f\u751f\u6a21\u5757 - \u6240\u4ee5\u8db3\u4ee5\u5c06 <code>import multiprocessing</code> \u6539\u6210 <code>import torch.multiprocessing</code> \u4f7f\u5f97\u6240\u6709\u7684 tensors \u901a\u8fc7\u961f\u5217\u53d1\u9001\u6216\u8005\u4f7f\u7528\u5176\u5b83\u5171\u4eab\u673a\u5236, \u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58.</p> <p>\u56e0\u4e3a APIs \u7684\u76f8\u4f3c\u6027, \u6211\u4eec\u6ca1\u6709\u4e3a\u6b64\u5305\u63d0\u4f9b\u8db3\u591f\u7684\u6587\u6863, \u6240\u4ee5\u63a8\u8350\u53c2\u8003\u975e\u5e38\u4f18\u79c0\u7684\u539f\u751f\u8fdb\u7a0b\u6a21\u5757\u6587\u6863.</p> <p>\u8b66\u544a</p> <p>\u5982\u679c\u4e3b\u8fdb\u7a0b\u610f\u5916\u9000\u51fa (\u6bd4\u5982 \u56e0\u4e3a\u4e00\u4e2a\u4fe1\u53f7\u7684\u5230\u6765), Python's <code>multiprocessing</code> \u6709\u65f6\u5019\u4f1a\u65e0\u6cd5\u8bf7\u7406\u5b83\u7684\u5b50\u8fdb\u7a0b. \u8fd9\u662f\u4e00\u4e2a\u4f17\u6240\u5468\u77e5\u7684\u8b66\u544a, \u56e0\u6b64\uff0c\u5982\u679c\u4f60\u5728\u4e2d\u65ad\u89e3\u91ca\u5668\u540e\u53d1\u73b0\u4efb\u4f55\u8d44\u6e90\u6cc4\u6f0f\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u4f60\u521a\u521a\u53d1\u751f\u4e86\u8fd9\u79cd\u60c5\u51b5.</p>"},{"location":"1.0/multiprocessing/#_1","title":"\u7b56\u7565\u7ba1\u7406","text":"<pre><code>torch.multiprocessing.get_all_sharing_strategies()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u7cfb\u7edf\u652f\u6301\u7684\u5171\u4eab\u7b56\u7565\u7684\u96c6\u5408.</p> <pre><code>torch.multiprocessing.get_sharing_strategy()\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u7684 CPU tensors \u5171\u4eab\u7b56\u7565.</p> <pre><code>torch.multiprocessing.set_sharing_strategy(new_strategy)\n</code></pre> <p>\u8bbe\u7f6e\u4e00\u4e2a\u65b0\u7684 CPU tensors \u5171\u4eab\u7b56\u7565.</p> \u53c2\u6570: new_strategy (str) \u2013 \u9009\u5b9a\u7b56\u7565\u7684\u540d\u5b57. \u5fc5\u987b\u662f <code>get_all_sharing_strategies()</code> \u7684\u8fd4\u56de\u503c\u4e2d\u7684\u4e00\u4e2a."},{"location":"1.0/multiprocessing/#cuda-tensors","title":"\u5171\u4eab CUDA tensors","text":"<p>\u5728\u8fdb\u7a0b\u95f4\u5171\u4eab CUDA tensors \u4ec5\u4ec5\u5728 Python 3 \u4e2d\u88ab\u652f\u6301, \u4f7f\u7528 <code>spawn</code> \u6216\u8005 <code>forkserver</code> \u542f\u52a8\u65b9\u6cd5. <code>multiprocessing</code> \u5728 Python 2 \u4e2d\u53ea\u80fd\u4f7f\u7528 <code>fork</code> \u521b\u5efa\u65b0\u8fdb\u7a0b, \u7136\u800c CUDA \u8fd0\u884c\u65f6\u4e0d\u652f\u6301\u5b83.</p> <p>\u8b66\u544a</p> <p>CUDA API\u8981\u6c42\u5bfc\u51fa\u5230\u5176\u4ed6\u8fdb\u7a0b\u7684\u5206\u914d\u53ea\u8981\u88ab\u5176\u4ed6\u8fdb\u7a0b\u4f7f\u7528\u5c31\u4fdd\u6301\u6709\u6548. \u60a8\u5e94\u8be5\u5c0f\u5fc3\uff0c\u5e76\u786e\u4fdd\u5171\u4eab\u7684CUDA tensor\u5728\u5fc5\u8981\u65f6\u4e0d\u4f1a\u8d85\u51fa\u8303\u56f4. \u5171\u4eab\u6a21\u578b\u53c2\u6570\u4e0d\u5e94\u8be5\u662f\u4e00\u4e2a\u95ee\u9898\uff0c\u4f46\u662f\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e\u5e94\u8be5\u5c0f\u5fc3\u3002\u6ce8\u610f\uff0c\u6b64\u9650\u5236\u4e0d\u9002\u7528\u4e8e\u5171\u4eabCPU\u5185\u5b58.</p>"},{"location":"1.0/multiprocessing/#_2","title":"\u5171\u4eab\u7b56\u7565","text":"<p>\u672c\u8282\u7b80\u8981\u6982\u8ff0\u4e0d\u540c\u7684\u5171\u4eab\u7b56\u7565\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002\u6ce8\u610f\uff0c\u5b83\u53ea\u9002\u7528\u4e8eCPU tensor\u2014\u2014CUDA tensor\u603b\u662f\u4f7f\u7528CUDA API\uff0c\u56e0\u4e3a\u8fd9\u662f\u5b83\u4eec\u53ef\u4ee5\u5171\u4eab\u7684\u552f\u4e00\u65b9\u5f0f\u3002</p>"},{"location":"1.0/multiprocessing/#-file_descriptor","title":"\u6587\u4ef6\u63cf\u8ff0\u7b26 - <code>file_descriptor</code>","text":"<p>\u6ce8\u610f</p> <p>\u8fd9\u662f\u9ed8\u8ba4\u7b56\u7565(macOS\u548cOS X\u56e0\u4e3a\u4e0d\u652f\u6301\u9664\u5916)</p> <p>\u8be5\u7b56\u7565\u5c06\u4f7f\u7528\u6587\u4ef6\u63cf\u8ff0\u7b26\u4f5c\u4e3a\u5171\u4eab\u5185\u5b58\u53e5\u67c4\u3002\u6bcf\u5f53\u4e00\u4e2a\u5b58\u50a8\u88ab\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u65f6\uff0c\u4ece<code>shm open</code>\u83b7\u5f97\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u5c31\u4f1a\u88ab\u5bf9\u8c61\u7f13\u5b58\uff0c\u5f53\u5b83\u88ab\u53d1\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u65f6\uff0c\u6587\u4ef6\u63cf\u8ff0\u7b26\u5c31\u4f1a\u88ab\u4f20\u8f93(\u4f8b\u5982\u901a\u8fc7UNIX\u5957\u63a5\u5b57)\u5230\u5b83\u3002\u63a5\u6536\u8005\u8fd8\u5c06\u7f13\u5b58\u6587\u4ef6\u63cf\u8ff0\u7b26\u5e76<code>mmap</code>\u5b83\uff0c\u4ee5\u83b7\u5f97\u5b58\u50a8\u6570\u636e\u4e0a\u7684\u5171\u4eab\u89c6\u56fe\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c\u5982\u679c\u5171\u4eab\u4e86\u5f88\u591atensor\uff0c\u90a3\u4e48\u8fd9\u79cd\u7b56\u7565\u5c06\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6253\u5f00\u5927\u91cf\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u3002\u5982\u679c\u60a8\u7684\u7cfb\u7edf\u5bf9\u6253\u5f00\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u7684\u6570\u91cf\u9650\u5236\u5f88\u4f4e\uff0c\u5e76\u4e14\u60a8\u4e0d\u80fd\u63d0\u9ad8\u5b83\u4eec\u7684\u6570\u91cf\uff0c\u90a3\u4e48\u60a8\u5e94\u8be5\u4f7f\u7528<code>file_system</code>\u7b56\u7565\u3002</p>"},{"location":"1.0/multiprocessing/#-file_system","title":"\u6587\u4ef6\u7cfb\u7edf - <code>file_system</code>","text":"<p>\u8be5\u7b56\u7565\u5c06\u4f7f\u7528\u6307\u5b9a\u7ed9<code>shm open</code>\u7684\u6587\u4ef6\u540d\u6765\u6807\u8bc6\u5171\u4eab\u5185\u5b58\u533a\u57df\u3002\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u4e0d\u9700\u8981\u5b9e\u73b0\u7f13\u5b58\u4ece\u4e2d\u83b7\u5f97\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\uff0c\u4f46\u540c\u65f6\u5bb9\u6613\u5bfc\u81f4\u5171\u4eab\u5185\u5b58\u6cc4\u6f0f\u3002\u6587\u4ef6\u4e0d\u80fd\u5728\u521b\u5efa\u4e4b\u540e\u7acb\u5373\u5220\u9664\uff0c\u56e0\u4e3a\u5176\u4ed6\u8fdb\u7a0b\u9700\u8981\u8bbf\u95ee\u5b83\u6765\u6253\u5f00\u5b83\u4eec\u7684\u89c6\u56fe\u3002\u5982\u679c\u8fdb\u7a0b\u81f4\u547d\u5730\u5d29\u6e83\u6216\u88ab\u6740\u6b7b\uff0c\u5e76\u4e14\u4e0d\u8c03\u7528\u5b58\u50a8\u6790\u6784\u51fd\u6570\uff0c\u90a3\u4e48\u6587\u4ef6\u5c06\u4fdd\u7559\u5728\u7cfb\u7edf\u4e2d\u3002\u8fd9\u662f\u975e\u5e38\u4e25\u91cd\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f1a\u4e00\u76f4\u4f7f\u7528\u5185\u5b58\uff0c\u76f4\u5230\u7cfb\u7edf\u91cd\u65b0\u542f\u52a8\uff0c\u6216\u8005\u91cd\u65b0\u624b\u52a8\u91ca\u653e\u3002</p> <p>\u4e3a\u4e86\u89e3\u51b3\u5171\u4eab\u5185\u5b58\u6587\u4ef6\u6cc4\u6f0f\u7684\u95ee\u9898\uff0c<code>torch.multiprocessing</code>\u5c06\u751f\u6210\u4e00\u4e2a\u540d\u4e3a<code>torch_shm_manager</code>\u7684\u5b88\u62a4\u8fdb\u7a0b\uff0c\u5b83\u5c06\u81ea\u5df1\u4e0e\u5f53\u524d\u8fdb\u7a0b\u7ec4\u9694\u79bb\uff0c\u5e76\u8ddf\u8e2a\u6240\u6709\u5171\u4eab\u5185\u5b58\u5206\u914d\u3002\u8fde\u63a5\u5230\u5b83\u7684\u6240\u6709\u8fdb\u7a0b\u9000\u51fa\u540e\uff0c\u5b83\u5c06\u7b49\u5f85\u4e00\u6bb5\u65f6\u95f4\u4ee5\u786e\u4fdd\u6ca1\u6709\u65b0\u7684\u8fde\u63a5\uff0c\u5e76\u5c06\u904d\u5386\u7ec4\u5206\u914d\u7684\u6240\u6709\u5171\u4eab\u5185\u5b58\u6587\u4ef6\u3002\u5982\u679c\u5b83\u53d1\u73b0\u5176\u4e2d\u4efb\u4f55\u4e00\u4e2a\u4ecd\u7136\u5b58\u5728\uff0c\u5c31\u4f1a\u89e3\u9664\u5b83\u4eec\u7684\u5206\u914d\u3002\u6211\u4eec\u5bf9\u8fd9\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u5b83\u5bf9\u5404\u79cd\u6545\u969c\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002 \u4e0d\u8fc7\uff0c\u5982\u679c\u60a8\u7684\u7cfb\u7edf\u6709\u8db3\u591f\u9ad8\u7684\u9650\u5236\uff0c\u5e76\u4e14<code>file_descriptor</code>\u662f\u53d7\u652f\u6301\u7684\u7b56\u7565\uff0c\u6211\u4eec\u4e0d\u5efa\u8bae\u5207\u6362\u5230\u8fd9\u4e2a\u7b56\u7565\u3002</p>"},{"location":"1.0/multiprocessing/#spawning","title":"Spawning \u5b50\u7ebf\u7a0b","text":"<p>\u6ce8\u610f</p> <p>\u4ec5\u652f\u6301 Python &gt;= 3.4.</p> <p>\u4f9d\u8d56\u4e8e <code>spawn</code> \u542f\u52a8\u65b9\u6cd5(\u5728 Python \u7684 <code>multiprocessing</code> \u5305\u4e2d)\u3002</p> <p>\u901a\u8fc7\u521b\u5efa<code>\u8fdb\u7a0b</code>\u5b9e\u4f8b\u5e76\u8c03\u7528join\u6765\u7b49\u5f85\u5b83\u4eec\u5b8c\u6210\uff0c\u53ef\u4ee5\u751f\u6210\u5927\u91cf\u5b50\u8fdb\u7a0b\u6765\u6267\u884c\u67d0\u4e9b\u529f\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u5904\u7406\u5355\u4e2a\u5b50\u8fdb\u7a0b\u65f6\u5de5\u4f5c\u5f97\u5f88\u597d\uff0c\u4f46\u5728\u5904\u7406\u591a\u4e2a\u8fdb\u7a0b\u65f6\u53ef\u80fd\u4f1a\u51fa\u73b0\u95ee\u9898\u3002</p> <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u987a\u5e8f\u8fde\u63a5\u8fdb\u7a0b\u610f\u5473\u7740\u5b83\u4eec\u5c06\u987a\u5e8f\u7ec8\u6b62\u3002\u5982\u679c\u6ca1\u6709\uff0c\u5e76\u4e14\u7b2c\u4e00\u4e2a\u8fdb\u7a0b\u6ca1\u6709\u7ec8\u6b62\uff0c\u90a3\u4e48\u8fdb\u7a0b\u7ec8\u6b62\u5c06\u4e0d\u88ab\u6ce8\u610f\u3002 \u6b64\u5916\uff0c\u6ca1\u6709\u7528\u4e8e\u9519\u8bef\u4f20\u64ad\u7684\u672c\u5730\u5de5\u5177.</p> <p>\u4e0b\u9762\u7684<code>spawn</code>\u51fd\u6570\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u8d1f\u8d23\u9519\u8bef\u4f20\u64ad\u3001\u65e0\u5e8f\u7ec8\u6b62\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u5176\u4e2d\u4e00\u4e2a\u9519\u8bef\u65f6\u4e3b\u52a8\u7ec8\u6b62\u8fdb\u7a0b.</p> <pre><code>torch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False)\n</code></pre> <p>Spawns <code>nprocs</code> \u8fdb\u7a0b\u8fd0\u884c <code>fn</code> \u4f7f\u7528\u53c2\u6570 <code>args</code>.</p> <p>\u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u8fdb\u7a0b\u4ee5\u975e\u96f6\u9000\u51fa\u72b6\u6001\u9000\u51fa\uff0c\u5219\u4f1a\u6740\u6b7b\u5176\u4f59\u8fdb\u7a0b\uff0c\u5e76\u5f15\u53d1\u5f02\u5e38\uff0c\u5bfc\u81f4\u7ec8\u6b62\u3002\u5728\u5b50\u8fdb\u7a0b\u4e2d\u6355\u83b7\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u8f6c\u53d1\u8be5\u5f02\u5e38\uff0c\u5e76\u5c06\u5176\u8ddf\u8e2a\u5305\u542b\u5728\u7236\u8fdb\u7a0b\u4e2d\u5f15\u53d1\u7684\u5f02\u5e38\u4e2d\u3002</p> <p>\u53c2\u6570: </p> <ul> <li> <p>fn (function) \u2013</p> <p>\u51fd\u6570\u88ab\u79f0\u4e3a\u6d3e\u751f\u8fdb\u7a0b\u7684\u5165\u53e3\u70b9\u3002\u5fc5\u987b\u5728\u6a21\u5757\u7684\u9876\u5c42\u5b9a\u4e49\u6b64\u51fd\u6570\uff0c\u4ee5\u4fbf\u5bf9\u5176\u8fdb\u884cpickle\u548c\u6d3e\u751f\u3002\u8fd9\u662f\u591a\u8fdb\u7a0b\u5f3a\u52a0\u7684\u8981\u6c42\u3002</p> <p>\u8be5\u51fd\u6570\u79f0\u4e3a<code>fn(i\uff0c *args)</code>\uff0c\u5176\u4e2d<code>i</code>\u662f\u8fdb\u7a0b\u7d22\u5f15\uff0c<code>args</code>\u662f\u4f20\u9012\u7684\u53c2\u6570\u5143\u7ec4\u3002</p> </li> <li> <p>args (tuple) \u2013 \u4f20\u9012\u7ed9 <code>fn</code> \u7684\u53c2\u6570.</p> </li> <li>nprocs (int) \u2013 \u6d3e\u751f\u7684\u8fdb\u7a0b\u6570.</li> <li>join (bool) \u2013 \u6267\u884c\u4e00\u4e2a\u963b\u585e\u7684join\u5bf9\u4e8e\u6240\u6709\u8fdb\u7a0b.</li> <li>daemon (bool) \u2013 \u6d3e\u751f\u8fdb\u7a0b\u5b88\u62a4\u8fdb\u7a0b\u6807\u5fd7\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u5c06\u521b\u5efa\u5b88\u62a4\u8fdb\u7a0b.</li> </ul> \u8fd4\u56de\u503c: None \u5982\u679c <code>join</code> \u662f <code>True</code>, <code>SpawnContext</code> \u5982\u679c <code>join</code> \u662f <code>False</code> <pre><code>class torch.multiprocessing.SpawnContext\n</code></pre> <p>\u7531 <code>spawn()</code> \u8fd4\u56de, \u5f53 <code>join=False</code>.</p> <pre><code>join(timeout=None)\n</code></pre> <p>\u5c1d\u8bd5\u8fde\u63a5\u6b64\u6d3e\u751f\u4e0a\u4e0b\u6587\u4e2d\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u8fdb\u7a0b\u3002\u5982\u679c\u5176\u4e2d\u4e00\u4e2a\u8fdb\u7a0b\u4ee5\u975e\u96f6\u9000\u51fa\u72b6\u6001\u9000\u51fa\uff0c\u5219\u6b64\u51fd\u6570\u5c06\u6740\u6b7b\u5176\u4f59\u8fdb\u7a0b\uff0c\u5e76\u5f15\u53d1\u5f02\u5e38\uff0c\u5bfc\u81f4\u7b2c\u4e00\u4e2a\u8fdb\u7a0b\u9000\u51fa\u3002</p> <p>\u8fd4\u56de <code>True</code>\u5982\u679c\u6240\u6709\u8fdb\u7a0b\u6b63\u5e38\u9000\u51fa, <code>False</code> \u5982\u679c\u6709\u66f4\u591a\u7684\u8fdb\u7a0b\u9700\u8981 join.</p> Parameters: timeout (float) \u2013 \u653e\u5f03\u7b49\u5f85\u7684\u6700\u957f\u65f6\u95f4."},{"location":"1.0/neural_style_tutorial/","title":"\u4f7f\u7528PyTorch\u8fdb\u884c\u56fe\u50cf\u98ce\u683c\u8f6c\u6362","text":"<p>\u8bd1\u8005\uff1abdqfork</p> <p>\u4f5c\u8005: Alexis Jacq</p>"},{"location":"1.0/neural_style_tutorial/#_1","title":"\u7b80\u4ecb","text":"<p>\u672c\u6559\u7a0b\u4e3b\u8981\u8bb2\u89e3\u5982\u4f55\u5b9e\u73b0\u7531Leon A. Gatys\uff0cAlexander S. Ecker\u548cMatthias Bethge\u63d0\u51fa\u7684 Neural-Style \u7b97\u6cd5\u3002Neural-Style\u6216\u8005\u53ebNeural-Transfer\uff0c\u53ef\u4ee5\u8ba9\u4f60\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u98ce\u683c\u5c06\u6307\u5b9a\u7684\u56fe\u7247\u8fdb\u884c\u91cd\u6784\u3002\u8fd9\u4e2a\u7b97\u6cd5\u4f7f\u7528\u4e09\u5f20\u56fe\u7247\uff0c\u4e00\u5f20\u8f93\u5165\u56fe\u7247\uff0c\u4e00\u5f20\u5185\u5bb9\u56fe\u7247\u548c\u4e00\u5f20\u98ce\u683c\u56fe\u7247\uff0c\u5e76\u5c06\u8f93\u5165\u7684\u56fe\u7247\u53d8\u5f97\u4e0e\u5185\u5bb9\u56fe\u7247\u76f8\u4f3c\uff0c\u4e14\u62e5\u6709\u98ce\u683c\u56fe\u7247\u7684\u4f18\u7f8e\u98ce\u683c\u3002</p> <p></p>"},{"location":"1.0/neural_style_tutorial/#_2","title":"\u57fa\u672c\u539f\u7406","text":"<p>\u539f\u7406\u5f88\u7b80\u5355\uff1a\u6211\u4eec\u5b9a\u4e49\u4e24\u4e2a\u95f4\u8ddd\uff0c\u4e00\u4e2a\u7528\u4e8e\u5185\u5bb9<code>D_C</code>\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u98ce\u683c<code>D_S</code>\u3002<code>D_C</code>\u6d4b\u91cf\u4e24\u5f20\u56fe\u7247\u5185\u5bb9\u7684\u4e0d\u540c\uff0c\u800c<code>D_S</code>\u7528\u6765\u6d4b\u91cf\u4e24\u5f20\u56fe\u7247\u98ce\u683c\u7684\u4e0d\u540c\u3002\u7136\u540e\uff0c\u6211\u4eec\u8f93\u5165\u7b2c\u4e09\u5f20\u56fe\u7247\uff0c\u5e76\u6539\u53d8\u8fd9\u5f20\u56fe\u7247\uff0c\u4f7f\u5176\u4e0e\u5185\u5bb9\u56fe\u7247\u7684\u5185\u5bb9\u95f4\u8ddd\u548c\u98ce\u683c\u56fe\u7247\u7684\u98ce\u683c\u95f4\u8ddd\u6700\u5c0f\u5316\u3002\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u5bfc\u5165\u5fc5\u8981\u7684\u5305\uff0c\u5f00\u59cb\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u3002</p>"},{"location":"1.0/neural_style_tutorial/#_3","title":"\u5bfc\u5305\u5e76\u9009\u62e9\u8bbe\u5907","text":"<p>\u4e0b\u9762\u662f\u4e00\u5f20\u5b9e\u73b0\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u6240\u9700\u5305\u7684\u6e05\u5355\u3002</p> <ul> <li><code>torch</code>, <code>torch.nn</code>, <code>numpy</code> (\u4f7f\u7528PyTorch\u8fdb\u884c\u98ce\u683c\u8f6c\u6362\u5fc5\u4e0d\u53ef\u5c11\u7684\u5305)</li> <li><code>torch.optim</code> (\u9ad8\u6548\u7684\u68af\u5ea6\u4e0b\u964d)</li> <li><code>PIL</code>, <code>PIL.Image</code>, <code>matplotlib.pyplot</code> (\u52a0\u8f7d\u548c\u5c55\u793a\u56fe\u7247)</li> <li><code>torchvision.transforms</code> (\u5c06PIL\u56fe\u7247\u8f6c\u6362\u6210\u5f20\u91cf)</li> <li><code>torchvision.models</code> (\u8bad\u7ec3\u6216\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b)</li> <li><code>copy</code> (\u5bf9\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u62f7\u8d1d\uff1b\u7cfb\u7edf\u5305)</li> </ul> <pre><code>from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy\n\n</code></pre> <p>\u4e0b\u4e00\u6b65\uff0c\u6211\u4eec\u9009\u62e9\u7528\u54ea\u4e00\u4e2a\u8bbe\u5907\u6765\u8fd0\u884c\u795e\u7ecf\u7f51\u7edc\uff0c\u5bfc\u5165\u5185\u5bb9\u548c\u98ce\u683c\u56fe\u7247\u3002\u5728\u5927\u91cf\u56fe\u7247\u4e0a\u8fd0\u884c\u56fe\u50cf\u98ce\u683c\u7b97\u6cd5\u9700\u8981\u5f88\u957f\u65f6\u95f4\uff0c\u5728GPU\u4e0a\u8fd0\u884c\u53ef\u4ee5\u52a0\u901f\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528<code>torch.cuda.is_available()</code>\u6765\u5224\u65ad\u662f\u5426\u6709\u53ef\u7528\u7684GPU\u3002\u4e0b\u4e00\u6b65\uff0c\u6211\u4eec\u5728\u6574\u4e2a\u6559\u7a0b\u4e2d\u4f7f\u7528 <code>torch.device</code> \u3002 <code>.to(device)</code> \u65b9\u6cd5\u4e5f\u88ab\u7528\u6765\u5c06\u5f20\u91cf\u6216\u8005\u6a21\u578b\u79fb\u52a8\u5230\u6307\u5b9a\u8bbe\u5907\u3002</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"1.0/neural_style_tutorial/#_4","title":"\u52a0\u8f7d\u56fe\u7247","text":"<p>\u73b0\u5728\u6211\u4eec\u5c06\u5bfc\u5165\u98ce\u683c\u548c\u5185\u5bb9\u56fe\u7247\u3002\u539f\u59cb\u7684PIL\u56fe\u7247\u7684\u503c\u4ecb\u4e8e0\u5230255\u4e4b\u95f4\uff0c\u4f46\u662f\u5f53\u8f6c\u6362\u6210torch\u5f20\u91cf\u65f6\uff0c\u5b83\u4eec\u7684\u503c\u88ab\u8f6c\u6362\u62100\u52301\u4e4b\u95f4\u3002\u56fe\u7247\u4e5f\u9700\u8981\u88ab\u91cd\u8bbe\u6210\u76f8\u540c\u7684\u7ef4\u5ea6\u3002\u4e00\u4e2a\u91cd\u8981\u7684\u7ec6\u8282\u662f\uff0c\u6ce8\u610ftorch\u5e93\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u6765\u8bad\u7ec3\u7684\u5f20\u91cf\u7684\u503c\u4e3a0\u52301\u4e4b\u95f4\u3002\u5982\u679c\u4f60\u5c1d\u8bd5\u5c060\u5230255\u7684\u5f20\u91cf\u56fe\u7247\u52a0\u8f7d\u5230\u795e\u7ecf\u7f51\u7edc\uff0c\u7136\u540e\u6fc0\u6d3b\u7684\u7279\u5f81\u6620\u5c04\u5c06\u4e0d\u80fd\u4fa6\u6d4b\u5230\u76ee\u6807\u5185\u5bb9\u548c\u98ce\u683c\u3002\u7136\u800c\uff0cCaffe\u5e93\u4e2d\u7684\u9884\u8bad\u7ec3\u7f51\u7edc\u7528\u6765\u8bad\u7ec3\u7684\u5f20\u91cf\u503c\u4e3a0\u5230255\u4e4b\u95f4\u7684\u56fe\u7247\u3002</p> <p>\u6ce8\u610f</p> <p>\u8fd9\u662f\u4e00\u4e2a\u4e0b\u8f7d\u672c\u6559\u7a0b\u9700\u8981\u7528\u5230\u7684\u56fe\u7247\u7684\u94fe\u63a5\uff1a picasso.jpg \u548c dancing.jpg\u3002\u4e0b\u8f7d\u8fd9\u4e24\u5f20\u56fe\u7247\u5e76\u4e14\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230\u4f60\u5f53\u524d\u5de5\u4f5c\u76ee\u5f55\u4e2d\u7684 <code>images</code> \u6587\u4ef6\u5939\u3002</p> <pre><code># desired size of the output image\nimsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),  # scale imported image\n    transforms.ToTensor()])  # transform it into a torch tensor\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # fake batch dimension required to fit network's input dimensions\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\nstyle_img = image_loader(\"./data/images/neural-style/picasso.jpg\")\ncontent_img = image_loader(\"./data/images/neural-style/dancing.jpg\")\n\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\"\n\n</code></pre> <p>\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5c06\u56fe\u7247\u8f6c\u6362\u6210PIL\u683c\u5f0f\u6765\u5c55\u793a\uff0c\u5e76\u4f7f\u7528<code>plt.imshow</code>\u5c55\u793a\u5b83\u7684\u62f7\u8d1d\u3002\u6211\u4eec\u5c06\u5c1d\u8bd5\u5c55\u793a\u5185\u5bb9\u548c\u98ce\u683c\u56fe\u7247\u6765\u786e\u4fdd\u5b83\u4eec\u88ab\u6b63\u786e\u7684\u5bfc\u5165\u3002</p> <pre><code>unloader = transforms.ToPILImage()  # reconvert into PIL image\n\nplt.ion()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n    image = image.squeeze(0)      # remove the fake batch dimension\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # pause a bit so that plots are updated\n\nplt.figure()\nimshow(style_img, title='Style Image')\n\nplt.figure()\nimshow(content_img, title='Content Image')\n\n</code></pre>"},{"location":"1.0/neural_style_tutorial/#_5","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"1.0/neural_style_tutorial/#_6","title":"\u5185\u5bb9\u635f\u5931","text":"<p>\u5185\u5bb9\u635f\u5931\u662f\u4e00\u4e2a\u8868\u793a\u4e00\u5c42\u5185\u5bb9\u95f4\u8ddd\u7684\u52a0\u6743\u7248\u672c\u3002\u8fd9\u4e2a\u65b9\u6cd5\u4f7f\u7528\u7f51\u7edc\u4e2d\u7684L\u5c42\u7684\u7279\u5f81\u6620\u5c04<code>F_XL</code>\uff0c\u8be5\u7f51\u7edc\u5904\u7406\u8f93\u5165X\u5e76\u8fd4\u56de\u5728\u56fe\u7247X\u548c\u5185\u5bb9\u56fe\u7247C\u4e4b\u95f4\u7684\u52a0\u6743\u5185\u5bb9\u95f4\u8ddd<code>W_CL*D_C^L(X,C)</code>\u3002\u8be5\u65b9\u6cd5\u5fc5\u987b\u77e5\u9053\u5185\u5bb9\u56fe\u7247(<code>F_CL</code>\uff09\u7684\u7279\u5f81\u6620\u5c04\u6765\u8ba1\u7b97\u5185\u5bb9\u95f4\u8ddd\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u4ee5<code>F_CL</code>\u4f5c\u4e3a\u6784\u9020\u53c2\u6570\u8f93\u5165\u7684torch\u6a21\u578b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u3002\u95f4\u8ddd<code>||F_XL-F_CL||^2</code>\u662f\u4e24\u4e2a\u7279\u5f81\u6620\u5c04\u96c6\u5408\u4e4b\u95f4\u7684\u5e73\u5747\u65b9\u5dee\uff0c\u53ef\u4ee5\u4f7f\u7528<code>nn.MSELoss</code>\u6765\u8ba1\u7b97\u3002</p> <p>\u6211\u4eec\u5c06\u76f4\u63a5\u6dfb\u52a0\u8fd9\u4e2a\u5185\u5bb9\u635f\u5931\u6a21\u578b\u5230\u88ab\u7528\u6765\u8ba1\u7b97\u5185\u5bb9\u95f4\u8ddd\u7684\u5377\u79ef\u5c42\u4e4b\u540e\u3002\u8fd9\u6837\u6bcf\u4e00\u6b21\u8f93\u5165\u56fe\u7247\u5230\u7f51\u7edc\u4e2d\u65f6\uff0c\u5185\u5bb9\u635f\u5931\u90fd\u4f1a\u5728\u76ee\u6807\u5c42\u88ab\u8ba1\u7b97\u3002\u800c\u4e14\u56e0\u4e3a\u81ea\u52a8\u6c42\u5bfc\u7684\u7f18\u6545\uff0c\u6240\u6709\u7684\u68af\u5ea6\u90fd\u4f1a\u88ab\u8ba1\u7b97\u3002\u73b0\u5728\uff0c\u4e3a\u4e86\u4f7f\u5185\u5bb9\u635f\u5931\u5c42\u900f\u660e\u5316\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u4e00\u4e2a<code>forward</code>\u65b9\u6cd5\u6765\u8ba1\u7b97\u5185\u5bb9\u635f\u5931\uff0c\u540c\u65f6\u8fd4\u56de\u8be5\u5c42\u7684\u8f93\u5165\u3002\u8ba1\u7b97\u7684\u635f\u5931\u4f5c\u4e3a\u6a21\u578b\u7684\u53c2\u6570\u88ab\u4fdd\u5b58\u3002</p> <pre><code>class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # we 'detach' the target content from the tree used\n        # to dynamically compute the gradient: this is a stated value,\n        # not a variable. Otherwise the forward method of the criterion\n        # will throw an error.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u91cd\u8981\u7ec6\u8282\uff1a\u5c3d\u7ba1\u8fd9\u4e2a\u6a21\u578b\u7684\u540d\u79f0\u88ab\u547d\u540d\u4e3a <code>ContentLoss</code>, \u5b83\u4e0d\u662f\u4e00\u4e2a\u771f\u5b9e\u7684PyTorch\u635f\u5931\u65b9\u6cd5\u3002\u5982\u679c\u4f60\u60f3\u8981\u5b9a\u4e49\u4f60\u7684\u5185\u5bb9\u635f\u5931\u4e3aPyTorch Loss\u65b9\u6cd5\uff0c\u4f60\u5fc5\u987b\u521b\u5efa\u4e00\u4e2aPyTorch\u81ea\u52a8\u6c42\u5bfc\u65b9\u6cd5\u6765\u624b\u52a8\u7684\u5728<code>backward</code>\u65b9\u6cd5\u4e2d\u91cd\u8ba1\u7b97/\u5b9e\u73b0\u68af\u5ea6.</p>"},{"location":"1.0/neural_style_tutorial/#_7","title":"\u98ce\u683c\u635f\u5931","text":"<p>\u98ce\u683c\u635f\u5931\u6a21\u578b\u4e0e\u5185\u5bb9\u635f\u5931\u6a21\u578b\u7684\u5b9e\u73b0\u65b9\u6cd5\u7c7b\u4f3c\u3002\u5b83\u8981\u4f5c\u4e3a\u4e00\u4e2a\u7f51\u7edc\u4e2d\u7684\u900f\u660e\u5c42\uff0c\u6765\u8ba1\u7b97\u76f8\u5e94\u5c42\u7684\u98ce\u683c\u635f\u5931\u3002\u4e3a\u4e86\u8ba1\u7b97\u98ce\u683c\u635f\u5931\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97Gram\u77e9\u9635<code>G_XL</code>\u3002Gram\u77e9\u9635\u662f\u5c06\u7ed9\u5b9a\u77e9\u9635\u548c\u5b83\u7684\u8f6c\u7f6e\u77e9\u9635\u7684\u4e58\u79ef\u3002\u5728\u8fd9\u4e2a\u5e94\u7528\u4e2d\uff0c\u7ed9\u5b9a\u7684\u77e9\u9635\u662fL\u5c42\u7279\u5f81\u6620\u5c04<code>F_XL</code>\u7684\u91cd\u5851\u7248\u672c\u3002<code>F_XL</code>\u88ab\u91cd\u5851\u6210<code>F\u0302_XL</code>\uff0c\u4e00\u4e2aKxN\u7684\u77e9\u9635\uff0c\u5176\u4e2dK\u662fL\u5c42\u7279\u5f81\u6620\u5c04\u7684\u6570\u91cf\uff0cN\u662f\u4efb\u4f55\u5411\u91cf\u5316\u7279\u5f81\u6620\u5c04<code>F_XL^K</code>\u7684\u957f\u5ea6\u3002\u4f8b\u5982\uff0c\u7b2c\u4e00\u884c\u7684<code>F\u0302_XL</code>\u4e0e\u7b2c\u4e00\u4e2a\u5411\u91cf\u5316\u7684<code>F_XL^1</code>\u3002</p> <p>\u6700\u540e\uff0cGram\u77e9\u9635\u5fc5\u987b\u901a\u8fc7\u5c06\u6bcf\u4e00\u4e2a\u5143\u7d20\u9664\u4ee5\u77e9\u9635\u4e2d\u6240\u6709\u5143\u7d20\u7684\u6570\u91cf\u8fdb\u884c\u6807\u51c6\u5316\u3002\u6807\u51c6\u5316\u662f\u4e3a\u4e86\u6d88\u9664\u62e5\u6709\u5f88\u5927\u7684N\u7ef4\u5ea6<code>F\u0302_XL</code>\u5728Gram\u77e9\u9635\u4e2d\u4ea7\u751f\u7684\u5f88\u5927\u7684\u503c\u3002\u8fd9\u4e9b\u5f88\u5927\u7684\u503c\u5c06\u5728\u68af\u5ea6\u4e0b\u964d\u7684\u65f6\u5019\uff0c\u5bf9\u7b2c\u4e00\u5c42(\u5728\u6c60\u5316\u5c42\u4e4b\u524d\uff09\u4ea7\u751f\u5f88\u5927\u7684\u5f71\u54cd\u3002\u98ce\u683c\u7279\u5f81\u5f80\u5f80\u5728\u7f51\u7edc\u4e2d\u66f4\u6df1\u7684\u5c42\uff0c\u6240\u4ee5\u6807\u51c6\u5316\u6b65\u9aa4\u662f\u5f88\u91cd\u8981\u7684\u3002</p> <pre><code>def gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    # b=number of feature maps\n    # (c,d)=dimensions of a f. map (N=c*d)\n\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\n    G = torch.mm(features, features.t())  # compute the gram product\n\n    # we 'normalize' the values of the gram matrix\n    # by dividing by the number of element in each feature maps.\n    return G.div(a * b * c * d)\n\n</code></pre> <p>\u73b0\u5728\u98ce\u683c\u635f\u5931\u6a21\u578b\u770b\u8d77\u6765\u548c\u5185\u5bb9\u635f\u5931\u6a21\u578b\u5f88\u50cf\u3002\u98ce\u683c\u95f4\u8ddd\u4e5f\u7528<code>G_XL</code>\u548c<code>G_SL</code>\u4e4b\u95f4\u7684\u5747\u65b9\u5dee\u6765\u8ba1\u7b97\u3002</p> <pre><code>class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input\n\n</code></pre>"},{"location":"1.0/neural_style_tutorial/#_8","title":"\u5bfc\u5165\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u9700\u8981\u5bfc\u5165\u9884\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u5c06\u4f7f\u752819\u5c42\u7684VGG\u7f51\u7edc\uff0c\u5c31\u50cf\u8bba\u6587\u4e2d\u4f7f\u7528\u7684\u4e00\u6837\u3002</p> <p>PyTorch\u7684VGG\u6a21\u578b\u5b9e\u73b0\u88ab\u5206\u4e3a\u4e86\u4e24\u4e2a\u5b57<code>Sequential</code>\u6a21\u578b\uff1a<code>features</code>(\u5305\u542b\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42\uff09\u548c<code>classifier</code>(\u5305\u542b\u5168\u8fde\u63a5\u5c42\uff09\u3002\u6211\u4eec\u5c06\u4f7f\u7528<code>features</code>\u6a21\u578b\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u6bcf\u4e00\u5c42\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u6765\u8ba1\u7b97\u5185\u5bb9\u548c\u98ce\u683c\u635f\u5931\u3002\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u6709\u4e9b\u5c42\u4f1a\u6709\u548c\u8bc4\u4f30\u4e0d\u4e00\u6837\u7684\u884c\u4e3a\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u7528<code>.eval()</code>\u5c06\u7f51\u7edc\u8bbe\u7f6e\u6210\u8bc4\u4f30\u6a21\u5f0f\u3002</p> <pre><code>cnn = models.vgg19(pretrained=True).features.to(device).eval()\n\n</code></pre> <p>\u6b64\u5916\uff0cVGG\u7f51\u7edc\u901a\u8fc7\u4f7f\u7528mean=[0.485, 0.456, 0.406]\u548cstd=[0.229, 0.224, 0.225]\u53c2\u6570\u6765\u6807\u51c6\u5316\u56fe\u7247\u7684\u6bcf\u4e00\u4e2a\u901a\u9053\uff0c\u5e76\u5728\u56fe\u7247\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5c06\u5728\u628a\u56fe\u7247\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\uff0c\u5148\u4f7f\u7528\u8fd9\u4e9b\u53c2\u6570\u5bf9\u56fe\u7247\u8fdb\u884c\u6807\u51c6\u5316\u3002</p> <pre><code>cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# create a module to normalize input image so we can easily put it in a\n# nn.Sequential\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view the mean and std to make them [C x 1 x 1] so that they can\n        # directly work with image Tensor of shape [B x C x H x W].\n        # B is batch size. C is number of channels. H is height and W is width.\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # normalize img\n        return (img - self.mean) / self.std\n\n</code></pre> <p>\u4e00\u4e2a<code>Sequential</code>\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u987a\u5e8f\u6392\u5217\u7684\u5b50\u6a21\u578b\u5e8f\u5217\u3002\u4f8b\u5982\uff0c<code>vff19.features</code>\u5305\u542b\u4e00\u4e2a\u4ee5\u6b63\u786e\u7684\u6df1\u5ea6\u987a\u5e8f\u6392\u5217\u7684\u5e8f\u5217(Conv2d, ReLU, MaxPool2d, Conv2d, ReLU\u2026\uff09\u3002\u6211\u4eec\u9700\u8981\u5c06\u6211\u4eec\u81ea\u5df1\u7684\u5185\u5bb9\u635f\u5931\u548c\u98ce\u683c\u635f\u5931\u5c42\u5728\u611f\u77e5\u5230\u5377\u79ef\u5c42\u4e4b\u540e\u7acb\u5373\u6dfb\u52a0\u8fdb\u53bb\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684<code>Sequential</code>\u6a21\u578b\uff0c\u5e76\u6b63\u786e\u7684\u63d2\u5165\u5185\u5bb9\u635f\u5931\u548c\u98ce\u683c\u635f\u5931\u6a21\u578b\u3002</p> <pre><code># desired depth layers to compute style/content losses :\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    cnn = copy.deepcopy(cnn)\n\n    # normalization module\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # just in order to have an iterable access to or list of content/syle\n    # losses\n    content_losses = []\n    style_losses = []\n\n    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n    # to put in modules that are supposed to be activated sequentially\n    model = nn.Sequential(normalization)\n\n    i = 0  # increment every time we see a conv\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # The in-place version doesn't play very nicely with the ContentLoss\n            # and StyleLoss we insert below. So we replace with out-of-place\n            # ones here.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # add content loss:\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # add style loss:\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # now we trim off the layers after the last content and style losses\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses\n\n</code></pre> <p>\u4e0b\u4e00\u6b65\uff0c\u6211\u4eec\u9009\u62e9\u8f93\u5165\u56fe\u7247\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u5185\u5bb9\u56fe\u7247\u7684\u526f\u672c\u6216\u8005\u767d\u566a\u58f0\u3002</p> <pre><code>input_img = content_img.clone()\n# if you want to use white noise instead uncomment the below line:\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# add the original input image to the figure:\nplt.figure()\nimshow(input_img, title='Input Image')\n\n</code></pre> <p></p>"},{"location":"1.0/neural_style_tutorial/#_9","title":"\u68af\u5ea6\u4e0b\u964d","text":"<p>\u548c\u7b97\u6cd5\u7684\u4f5c\u8005Leon Gatys\u7684\u5728 \u8fd9\u91cc\u5efa\u8bae\u7684\u4e00\u6837\uff0c\u6211\u4eec\u5c06\u4f7f\u7528L-BFGS\u7b97\u6cd5\u6765\u8fdb\u884c\u6211\u4eec\u7684\u68af\u5ea6\u4e0b\u964d\u3002\u4e0e\u8bad\u7ec3\u4e00\u822c\u7f51\u7edc\u4e0d\u540c\uff0c\u6211\u4eec\u8bad\u7ec3\u8f93\u5165\u56fe\u7247\u662f\u4e3a\u4e86\u6700\u5c0f\u5316\u5185\u5bb9/\u98ce\u683c\u635f\u5931\u3002\u6211\u4eec\u8981\u521b\u5efa\u4e00\u4e2aPyTorch\u7684L-BFGS\u4f18\u5316\u5668<code>optim.LBFGS</code>\uff0c\u5e76\u4f20\u5165\u6211\u4eec\u7684\u56fe\u7247\u5230\u5176\u4e2d\uff0c\u4f5c\u4e3a\u5f20\u91cf\u53bb\u4f18\u5316\u3002</p> <pre><code>def get_input_optimizer(input_img):\n    # this line to show that input is a parameter that requires a gradient\n    optimizer = optim.LBFGS([input_img.requires_grad_()])\n    return optimizer\n\n</code></pre> <p>\u6700\u540e\uff0c\u6211\u4eec\u5fc5\u987b\u5b9a\u4e49\u4e00\u4e2a\u65b9\u6cd5\u6765\u5c55\u793a\u56fe\u50cf\u98ce\u683c\u8f6c\u6362\u3002\u5bf9\u4e8e\u6bcf\u4e00\u6b21\u7684\u7f51\u7edc\u8fed\u4ee3\uff0c\u90fd\u5c06\u66f4\u65b0\u8fc7\u7684\u8f93\u5165\u4f20\u5165\u5176\u4e2d\u5e76\u8ba1\u7b97\u635f\u5931\u3002\u6211\u4eec\u8981\u8fd0\u884c\u6bcf\u4e00\u4e2a\u635f\u5931\u6a21\u578b\u7684<code>backward</code>\u65b9\u6cd5\u6765\u8ba1\u7b97\u5b83\u4eec\u7684\u68af\u5ea6\u3002\u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a\u201c\u5173\u95ed\u201d\u65b9\u6cd5\uff0c\u5b83\u91cd\u65b0\u4f30\u8ba1\u6a21\u578b\u5e76\u4e14\u8fd4\u56de\u635f\u5931\u3002</p> <p>\u6211\u4eec\u8fd8\u6709\u6700\u540e\u4e00\u4e2a\u95ee\u9898\u8981\u89e3\u51b3\u3002\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u4f1a\u5c1d\u8bd5\u4f7f\u5f20\u91cf\u56fe\u7247\u7684\u503c\u8d85\u8fc70\u52301\u4e4b\u95f4\u6765\u4f18\u5316\u8f93\u5165\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5728\u6bcf\u6b21\u7f51\u7edc\u8fd0\u884c\u7684\u65f6\u5019\u5c06\u8f93\u5165\u7684\u503c\u77eb\u6b63\u52300\u52301\u4e4b\u95f4\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p> <pre><code>def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] &lt;= num_steps:\n\n        def closure():\n            # correct the values of updated input image\n            input_img.data.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # a last correction...\n    input_img.data.clamp_(0, 1)\n\n    return input_img\n\n</code></pre> <p>\u6700\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u8fd9\u4e2a\u7b97\u6cd5\u3002</p> <pre><code>output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nimshow(output, title='Output Image')\n\n# sphinx_gallery_thumbnail_number = 4\nplt.ioff()\nplt.show()\n\n</code></pre> <p></p> <p>\u8f93\u51fa:</p> <pre><code>Building the style transfer model..\nOptimizing..\nrun [50]:\nStyle Loss : 4.169304 Content Loss: 4.235329\n\nrun [100]:\nStyle Loss : 1.145476 Content Loss: 3.039176\n\nrun [150]:\nStyle Loss : 0.716769 Content Loss: 2.663749\n\nrun [200]:\nStyle Loss : 0.476047 Content Loss: 2.500893\n\nrun [250]:\nStyle Loss : 0.347092 Content Loss: 2.410895\n\nrun [300]:\nStyle Loss : 0.263698 Content Loss: 2.358449\n\n</code></pre>"},{"location":"1.0/nlp_advanced_tutorial/","title":"\u9ad8\u7ea7\uff1a\u5236\u5b9a\u52a8\u6001\u51b3\u7b56\u548cBi-LSTM CRF","text":"<p>\u4f5c\u8005\uff1aPyTorch</p> <p>\u8bd1\u8005\uff1aApacheCN</p> <p>\u6821\u5bf9\u8005\uff1aenningxie</p>"},{"location":"1.0/nlp_advanced_tutorial/#_1","title":"\u52a8\u6001\u4e0e\u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305","text":"<p>Pytorch\u662f\u4e00\u79cd \u52a8\u6001 \u795e\u7ecf\u7f51\u7edc\u5957\u4ef6\u3002\u53e6\u4e00\u4e2a\u52a8\u6001\u5957\u4ef6\u7684\u4f8b\u5b50\u662f Dynet (\u6211\u4e4b\u6240\u4ee5\u63d0\u5230\u8fd9\u4e00\u70b9\uff0c\u56e0\u4e3a\u4e0ePytorch\u548cDynet\u4e00\u8d77\u4f7f\u7528\u662f\u76f8\u4f3c\u7684\u3002\u5982\u679c\u4f60\u5728Dynet\u4e2d\u770b\u5230\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5b83\u53ef\u80fd\u4f1a\u5e2e\u52a9\u4f60\u5728Pytorch\u4e2d\u5b9e\u73b0\u5b83\uff09\u3002\u76f8\u53cd\u7684\u662f \u9759\u6001 \u5de5\u5177\u5305\uff0c\u5176\u4e2d\u5305\u62ecTheano\uff0cKeras\uff0cTensorFlow\u7b49\u3002\u6838\u5fc3\u533a\u522b\u5982\u4e0b\uff1a</p> <ul> <li>\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5b9a\u4e49\u4e00\u6b21\u8ba1\u7b97\u56fe\uff0c\u5bf9\u5176\u8fdb\u884c\u7f16\u8bd1\uff0c\u7136\u540e\u5c06\u5b9e\u4f8b\u6d41\u5f0f\u4f20\u8f93\u7ed9\u5b83\u3002</li> <li>\u5728\u52a8\u6001\u5de5\u5177\u5305\u4e2d\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u5b9a\u4e49\u8ba1\u7b97\u56fe\u3002\u5b83\u6c38\u8fdc\u4e0d\u4f1a\u88ab\u7f16\u8bd1\u5e76\u4e14\u662f\u5373\u65f6\u6267\u884c\u7684\u3002</li> </ul> <p>\u5728\u6ca1\u6709\u5f88\u591a\u7ecf\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u5f88\u96be\u7406\u89e3\u5176\u4e2d\u7684\u5dee\u5f02\u3002\u4e00\u4e2a\u4f8b\u5b50\u662f\u5047\u8bbe\u6211\u4eec\u60f3\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5c42\u7ec4\u6210\u89e3\u6790\u5668\u3002\u5047\u8bbe\u6211\u4eec\u7684\u6a21\u578b\u5927\u81f4\u6d89\u53ca\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ul> <li>\u6211\u4eec\u81ea\u4e0b\u800c\u4e0a\u5efa\u9020\u6811</li> <li>\u6807\u8bb0\u6839\u8282\u70b9(\u53e5\u5b50\u7684\u5355\u8bcd\uff09</li> <li>\u4ece\u90a3\u91cc\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u548c\u5355\u8bcd\u7684\u5d4c\u5165\u6765\u627e\u5230\u5f62\u6210\u7ec4\u6210\u90e8\u5206\u7684\u7ec4\u5408\u3002\u6bcf\u5f53\u4f60\u5f62\u6210\u4e00\u4e2a\u65b0\u7684\u6210\u5206\u65f6\uff0c\u4f7f\u7528\u67d0\u79cd\u6280\u672f\u6765\u5d4c\u5165\u6210\u5206\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u7f51\u7edc\u67b6\u6784\u5c06\u5b8c\u5168\u53d6\u51b3\u4e8e\u8f93\u5165\u53e5\u5b50\u3002\u5728\u201c\u7eff\u732b\u5212\u4f24\u5899\u201d\u4e00\u53e5\u4e2d\uff0c\u5728\u6a21\u578b\u4e2d\u7684\u67d0\u4e2a\u70b9\u4e0a\uff0c\u6211\u4eec\u60f3\u8981\u7ed3\u5408\u8de8\u5ea6 \\(\\((i,j,r) = (1, 3, \\text{NP})\\)\\)(\u5373\uff0cNP\u7ec4\u6210\u90e8\u5206\u8de8\u8d8a\u5355\u8bcd1\u5230\u5355\u8bcd3\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u662f\u201c\u7eff\u732b\u201d )\u3002</li> </ul> <p>\u7136\u800c\uff0c\u53e6\u4e00\u53e5\u8bdd\u53ef\u80fd\u662f\u201c\u67d0\u5904\uff0c\u5927\u80a5\u732b\u5212\u4f24\u4e86\u5899\u201d\u3002\u5728\u8fd9\u53e5\u8bdd\u4e2d\uff0c\u6211\u4eec\u5e0c\u671b\u5728\u67d0\u4e2a\u65f6\u523b\u5f62\u6210\u7ec4\u6210 \\(\\((2, 4, NP)\\)\\)\u3002\u6211\u4eec\u60f3\u8981\u5f62\u6210\u7684\u6210\u5206\u5c06\u53d6\u51b3\u4e8e\u5b9e\u4f8b\u3002\u5982\u679c\u6211\u4eec\u53ea\u7f16\u8bd1\u8ba1\u7b97\u56fe\u4e00\u6b21\uff0c\u5c31\u50cf\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\u90a3\u6837\uff0c\u7f16\u5199\u8fd9\u4e2a\u903b\u8f91\u5c06\u662f\u975e\u5e38\u56f0\u96be\u6216\u4e0d\u53ef\u80fd\u7684\u3002\u4f46\u662f\uff0c\u5728\u52a8\u6001\u5de5\u5177\u5305\u4e2d\uff0c\u4e0d\u4ec5\u67091\u4e2a\u9884\u5b9a\u4e49\u7684\u8ba1\u7b97\u56fe\u3002\u6bcf\u4e2a\u5b9e\u4f8b\u90fd\u53ef\u4ee5\u6709\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe\uff0c\u6240\u4ee5\u8fd9\u4e2a\u95ee\u9898\u5c31\u6d88\u5931\u4e86\u3002</p> <p>\u52a8\u6001\u5de5\u5177\u5305\u8fd8\u5177\u6709\u6613\u4e8e\u8c03\u8bd5\u548c\u4ee3\u7801\u66f4\u63a5\u8fd1\u5bbf\u4e3b\u8bed\u8a00\u7684\u4f18\u70b9(\u6211\u7684\u610f\u601d\u662fPytorch\u548cDynet\u770b\u8d77\u6765\u66f4\u50cf\u662f\u6bd4Keras\u6216Theano\u66f4\u5b9e\u9645\u7684Python\u4ee3\u7801\uff09\u3002</p>"},{"location":"1.0/nlp_advanced_tutorial/#bi-lstm","title":"Bi-LSTM\u6761\u4ef6\u968f\u673a\u573a\u8ba8\u8bba","text":"<p>\u5bf9\u4e8e\u672c\u8282\uff0c\u6211\u4eec\u5c06\u770b\u5230\u7528\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684Bi-LSTM\u6761\u4ef6\u968f\u673a\u573a\u7684\u5b8c\u6574\u590d\u6742\u793a\u4f8b\u3002\u4e0a\u9762\u7684LSTM\u6807\u8bb0\u7b26\u901a\u5e38\u8db3\u4ee5\u7528\u4e8e\u8bcd\u6027\u6807\u6ce8\uff0c\u4f46\u662f\u50cfCRF\u8fd9\u6837\u7684\u5e8f\u5217\u6a21\u578b\u5bf9\u4e8eNER\u4e0a\u7684\u5f3a\u5927\u6027\u80fd\u975e\u5e38\u91cd\u8981\u3002\u5047\u8bbe\u719f\u6089CRF\u3002\u867d\u7136\u8fd9\u4e2a\u540d\u5b57\u542c\u8d77\u6765\u5f88\u53ef\u6015\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u662fCRF\uff0c\u4f46\u662fLSTM\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u529f\u80fd\u3002\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u6a21\u578b\uff0c\u6bd4\u672c\u6559\u7a0b\u4e2d\u7684\u4efb\u4f55\u65e9\u671f\u6a21\u578b\u590d\u6742\u5f97\u591a\u3002\u5982\u679c\u4f60\u60f3\u8df3\u8fc7\u5b83\uff0c\u90a3\u5f88\u597d\u3002\u8981\u67e5\u770b\u60a8\u662f\u5426\u51c6\u5907\u597d\uff0c\u8bf7\u67e5\u770b\u662f\u5426\u53ef\u4ee5\uff1a</p> <ul> <li>\u5728\u6b65\u9aa4i\u4e2d\u4e3a\u6807\u8bb0k\u5199\u51fa\u7ef4\u7279\u6bd4\u53d8\u91cf\u7684\u9012\u5f52\u3002</li> <li>\u4fee\u6539\u4e0a\u8ff0\u91cd\u590d\u4ee5\u8ba1\u7b97\u8f6c\u53d1\u53d8\u91cf\u3002</li> <li>\u518d\u6b21\u4fee\u6539\u4e0a\u9762\u7684\u91cd\u590d\u8ba1\u7b97\u4ee5\u8ba1\u7b97\u65e5\u5fd7\u7a7a\u95f4\u4e2d\u7684\u8f6c\u53d1\u53d8\u91cf(\u63d0\u793a\uff1alog-sum-exp\uff09</li> </ul> <p>\u5982\u679c\u4f60\u53ef\u4ee5\u505a\u8fd9\u4e09\u4ef6\u4e8b\uff0c\u4f60\u5e94\u8be5\u80fd\u591f\u7406\u89e3\u4e0b\u9762\u7684\u4ee3\u7801\u3002\u56de\u60f3\u4e00\u4e0b\uff0cCRF\u8ba1\u7b97\u6761\u4ef6\u6982\u7387\u3002\u8bbe \\(\\(y\\)\\) \u4e3a\u6807\u7b7e\u5e8f\u5217\uff0c\\(\\(x\\)\\) \u4e3a\u5b57\u7684\u8f93\u5165\u5e8f\u5217\u3002\u7136\u540e\u6211\u4eec\u8ba1\u7b97</p> \\[P(y|x)=\\frac{\\exp{(\\text {Score}(x, y)})} {\\sum_ {y} \\exp {(\\text {Score}(x, y)})} \\] <p>\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e9b\u5bf9\u6570\u7535\u4f4d \\(\\(\\log\\psi_i(x,y)\\)\\) \u6765\u786e\u5b9a\u5f97\u5206</p> \\[\\text {Score}(x, y)= \\sum_i\\log\\psi_i(x, y)\\] <p>\u4e3a\u4e86\u4f7f\u5206\u533a\u529f\u80fd\u6613\u4e8e\u5904\u7406\uff0c\u7535\u4f4d\u5fc5\u987b\u4ec5\u67e5\u770b\u5c40\u90e8\u7279\u5f81\u3002</p> <p>\u5728Bi-LSTM CRF\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e24\u79cd\u6f5c\u529b\uff1a\u53d1\u5c04\u548c\u8fc7\u6e21\u3002\u7d22\u5f15 \\(\\(i\\)\\) \u5904\u7684\u5355\u8bcd\u7684\u53d1\u5c04\u7535\u4f4d\u6765\u81ea\u65f6\u95f4\u6b65\u957f \\(\\(i\\)\\) \u5904\u7684Bi-LSTM\u7684\u9690\u85cf\u72b6\u6001\u3002\u8f6c\u6362\u5206\u6570\u5b58\u50a8\u5728 \\(\\(|T|x|T|\\)\\) \u77e9\u9635 \\(\\(\\textbf{P}\\)\\) \u4e2d\uff0c\u5176\u4e2d \\(\\(T\\)\\) \u662f\u6807\u8bb0\u96c6\u3002\u5728\u6211\u7684\u5b9e\u73b0\u4e2d\uff0c\\(\\(\\textbf{P}_{j,k}\\)\\) \u662f\u4ece\u6807\u7b7e $$  $$ \u8f6c\u6362\u5230\u6807\u7b7e $$ j $$ \u7684\u5206\u6570\u3002\u6240\u4ee5\uff1a</p> <p>\\( \\begin{align} \\text{Score}(x, y) &amp;= \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\ &amp;= \\sum_i h_i[y_i] + \\textbf{P}{y_i, y{i-1}} \\end{align} \\)</p> <p>\u5728\u7b2c\u4e8c\u4e2a\u8868\u8fbe\u5f0f\u4e2d\uff0c\u6211\u4eec\u5c06\u6807\u8bb0\u89c6\u4e3a\u5206\u914d\u4e86\u552f\u4e00\u7684\u975e\u8d1f\u7d22\u5f15\u3002</p> <p>\u5982\u679c\u4e0a\u9762\u7684\u8ba8\u8bba\u8fc7\u4e8e\u7b80\u77ed\uff0c\u4f60\u53ef\u4ee5\u67e5\u770b\u8fd9\u4e2a\u4ece\u8fc8\u514b\u5c14\u67ef\u6797\u65af\u90a3\u91cc\u5199\u7684\u5173\u4e8eCRF\u7684\u6587\u7ae0\u3002</p>"},{"location":"1.0/nlp_advanced_tutorial/#_2","title":"\u5b9e\u65bd\u8bf4\u660e","text":"<p>\u4e0b\u9762\u7684\u793a\u4f8b\u5b9e\u73b0\u4e86\u65e5\u5fd7\u7a7a\u95f4\u4e2d\u7684\u524d\u5411\u7b97\u6cd5\u6765\u8ba1\u7b97\u5206\u533a\u51fd\u6570\uff0c\u4ee5\u53ca\u7528\u4e8e\u89e3\u7801\u7684\u7ef4\u7279\u6bd4\u7b97\u6cd5\u3002\u53cd\u5411\u4f20\u64ad\u5c06\u81ea\u52a8\u4e3a\u6211\u4eec\u8ba1\u7b97\u68af\u5ea6\u3002\u6211\u4eec\u4e0d\u9700\u8981\u624b\u5de5\u505a\u4efb\u4f55\u4e8b\u60c5\u3002</p> <p>\u5b9e\u65bd\u672a\u4f18\u5316\u3002\u5982\u679c\u60a8\u4e86\u89e3\u53d1\u751f\u4e86\u4ec0\u4e48\uff0c\u60a8\u53ef\u80fd\u4f1a\u5f88\u5feb\u53d1\u73b0\u5728\u524d\u5411\u7b97\u6cd5\u4e2d\u8fed\u4ee3\u4e0b\u4e00\u4e2a\u6807\u8bb0\u53ef\u80fd\u662f\u5728\u4e00\u4e2a\u5927\u7684\u64cd\u4f5c\u4e2d\u5b8c\u6210\u7684\u3002\u6211\u60f3\u7f16\u7801\u66f4\u5177\u53ef\u8bfb\u6027\u3002\u5982\u679c\u60a8\u60f3\u8fdb\u884c\u76f8\u5173\u66f4\u6539\uff0c\u53ef\u4ee5\u5c06\u6b64\u6807\u8bb0\u5668\u7528\u4e8e\u5b9e\u9645\u4efb\u52a1\u3002</p> <pre><code># Author: Robert Guthrie\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\n</code></pre> <p>\u5e2e\u52a9\u7a0b\u5e8f\u7684\u529f\u80fd\u662f\u4f7f\u4ee3\u7801\u66f4\u5177\u53ef\u8bfb\u6027\u3002</p> <pre><code>def argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return idx.item()\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + \\\n        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\n</code></pre> <p>\u521b\u5efa\u6a21\u578b</p> <pre><code>class BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # Matrix of transition parameters.  Entry i,j is the score of\n        # transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        # These two statements enforce the constraint that we never transfer\n        # to the start tag and we never transfer from the stop tag\n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        return (torch.randn(2, 1, self.hidden_dim // 2),\n                torch.randn(2, 1, self.hidden_dim // 2))\n\n    def _forward_alg(self, feats):\n        # Do the forward algorithm to compute the partition function\n        init_alphas = torch.full((1, self.tagset_size), -10000.)\n        # START_TAG has all of the score.\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = init_alphas\n\n        # Iterate through the sentence\n        for feat in feats:\n            alphas_t = []  # The forward tensors at this timestep\n            for next_tag in range(self.tagset_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the\n                # edge (i -&gt; next_tag) before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # Gives the score of a provided tag sequence\n        score = torch.zeros(1)\n        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.full((1, self.tagset_size), -10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = init_vvars\n        for feat in feats:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the\n                # previous step, plus the score of transitioning\n                # from tag i to next_tag.\n                # We don't include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n        # Get the emission scores from the BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq\n\n</code></pre> <p>\u8fdb\u884c\u8bad\u7ec3</p> <pre><code>START_TAG = \"&lt;START&gt;\"\nSTOP_TAG = \"&lt;STOP&gt;\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n\nmodel = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n\n# Check predictions before training\nwith torch.no_grad():\n    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n    print(model(precheck_sent))\n\n# Make sure prepare_sequence from earlier in the LSTM section is loaded\nfor epoch in range(\n        300):  # again, normally you would NOT do 300 epochs, it is toy data\n    for sentence, tags in training_data:\n        # Step 1\\. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n\n        # Step 2\\. Get our inputs ready for the network, that is,\n        # turn them into Tensors of word indices.\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n\n        # Step 3\\. Run our forward pass.\n        loss = model.neg_log_likelihood(sentence_in, targets)\n\n        # Step 4\\. Compute the loss, gradients, and update the parameters by\n        # calling optimizer.step()\n        loss.backward()\n        optimizer.step()\n\n# Check predictions after training\nwith torch.no_grad():\n    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n    print(model(precheck_sent))\n# We got it!\n\n</code></pre> <p>\u65e5\u671f\uff1a</p> <pre><code>(tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1])\n(tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n\n</code></pre>"},{"location":"1.0/nlp_advanced_tutorial/#_3","title":"\u7ec3\u4e60\uff1a\u533a\u5206\u6807\u8bb0\u7684\u65b0\u635f\u5931\u51fd\u6570","text":"<p>\u6211\u4eec\u6ca1\u6709\u5fc5\u8981\u5728\u8fdb\u884c\u89e3\u7801\u65f6\u521b\u5efa\u8ba1\u7b97\u56fe\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u4f1a\u4ece\u7ef4\u7279\u6bd4\u8def\u5f84\u5f97\u5206\u53cd\u5411\u4f20\u64ad\u3002\u56e0\u4e3a\u65e0\u8bba\u5982\u4f55\u6211\u4eec\u90fd\u6709\u5b83\uff0c\u5c1d\u8bd5\u8bad\u7ec3\u6807\u8bb0\u5668\uff0c\u5176\u4e2d\u635f\u5931\u51fd\u6570\u662f\u7ef4\u7279\u6bd4\u8def\u5f84\u5f97\u5206\u548c\u91d1\u6807\u51c6\u8def\u5f84\u5f97\u5206\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5e94\u8be5\u6e05\u695a\u7684\u662f\uff0c\u5f53\u9884\u6d4b\u7684\u6807\u7b7e\u5e8f\u5217\u662f\u6b63\u786e\u7684\u6807\u7b7e\u5e8f\u5217\u65f6\uff0c\u8be5\u529f\u80fd\u662f\u975e\u8d1f\u7684\u548c0\u3002\u8fd9\u57fa\u672c\u4e0a\u662f \u7ed3\u6784\u611f\u77e5\u5668\u3002</p> <p>\u7531\u4e8e\u5df2\u7ecf\u5b9e\u73b0\u4e86Viterbi\u548cscore_sentence\uff0c\u56e0\u6b64\u8fd9\u79cd\u4fee\u6539\u5e94\u8be5\u5f88\u77ed\u3002\u8fd9\u662f\u53d6\u51b3\u4e8e\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8ba1\u7b97\u56fe\u5f62_\u7684\u5f62\u72b6\u7684\u793a\u4f8b\u3002\u867d\u7136\u6211\u6ca1\u6709\u5c1d\u8bd5\u5728\u9759\u6001\u5de5\u5177\u5305\u4e2d\u5b9e\u73b0\u5b83\uff0c\u4f46\u6211\u60f3\u5b83\u53ef\u80fd\u4f46\u4e0d\u90a3\u4e48\u76f4\u622a\u4e86\u5f53\u3002</p> <p>\u62ff\u8d77\u4e00\u4e9b\u771f\u5b9e\u6570\u636e\u5e76\u8fdb\u884c\u6bd4\u8f83\uff01</p>"},{"location":"1.0/nlp_deep_learning_tutorial/","title":"\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60","text":"<p>\u8bd1\u8005\uff1abdqfork</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005: Robert Guthrie</p>"},{"location":"1.0/nlp_deep_learning_tutorial/#_1","title":"\u6df1\u5ea6\u5b66\u4e60\u6784\u5efa\u6a21\u5757\uff1a\u4eff\u5c04\u53d8\u6362, \u975e\u7ebf\u6027\u51fd\u6570\u4ee5\u53ca\u76ee\u6807\u51fd\u6570","text":"<p>\u6df1\u5ea6\u5b66\u4e60\u8868\u73b0\u4e3a\u4f7f\u7528\u66f4\u5de7\u5999\u7684\u65b9\u6cd5\u5c06\u7ebf\u6027\u51fd\u6570\u548c\u975e\u7ebf\u6027\u51fd\u6570\u8fdb\u884c\u7ec4\u5408\u3002\u975e\u7ebf\u6027\u51fd\u6570\u7684\u5f15\u5165\u4f7f\u5f97\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u66f4\u52a0\u5f3a\u5927\u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u8fd9\u4e9b\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5efa\u7acb\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u7406\u89e3\u6a21\u578b\u662f\u5982\u4f55\u6784\u5efa\u7684\u3002</p>"},{"location":"1.0/nlp_deep_learning_tutorial/#_2","title":"\u4eff\u5c04\u53d8\u6362","text":"<p>\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u7ec4\u4ef6\u4e4b\u4e00\u662f\u4eff\u5c04\u53d8\u6362\uff0c\u4eff\u5c04\u53d8\u6362\u662f\u4e00\u4e2a\u5173\u4e8e\u77e9\u9635A\u548c\u5411\u91cfx\uff0cb\u7684f(x)\u51fd\u6570\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <p></p> <p>\u9700\u8981\u8bad\u7ec3\u7684\u53c2\u6570\u5c31\u662f\u8be5\u516c\u5f0f\u4e2d\u7684A\u548cb\u3002</p> <p>PyTorch\u4ee5\u53ca\u5927\u591a\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6240\u505a\u7684\u4e8b\u60c5\u90fd\u4e0e\u4f20\u7edf\u7684\u7ebf\u6027\u4ee3\u6570\u6709\u4e9b\u4e0d\u540c\u3002\u5b83\u7684\u6620\u5c04\u8f93\u5165\u662f\u884c\u800c\u4e0d\u662f\u5217\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u4e0b\u9762\u4ee3\u7801\u8f93\u51fa\u7684\u7b2ci\u884c\u662f\u8f93\u5165\u7684\u7b2ci\u884c\u8fdb\u884cA\u53d8\u6362\uff0c\u5e76\u52a0\u4e0a\u504f\u79fb\u9879\u7684\u7ed3\u679c\u3002\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\uff1a</p> <pre><code># Author: Robert Guthrie\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\n</code></pre> <pre><code>lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\ndata = torch.randn(2, 5)\nprint(lin(data))  # yes\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[ 0.1755, -0.3268, -0.5069],\n        [-0.6602,  0.2260,  0.1089]], grad_fn=&lt;AddmmBackward&gt;)\n\n</code></pre>"},{"location":"1.0/nlp_deep_learning_tutorial/#_3","title":"\u975e\u7ebf\u6027\u51fd\u6570","text":"<p>\u9996\u5148\uff0c\u6ce8\u610f\u4ee5\u4e0b\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u5b83\u5c06\u89e3\u91ca\u4e3a\u4ec0\u4e48\u6211\u4eec\u9700\u8981\u975e\u7ebf\u6027\u51fd\u6570\u3002\u5047\u8bbe\u6211\u4eec\u6709\u4e24\u4e2a\u4eff\u5c04\u53d8\u6362 f(x) = Ax + b \u548c g(x) = Cx + d \u3002\u90a3\u4e48 f(g(x)) \u53c8\u662f\u4ec0\u4e48\u5462\uff1f</p> <p></p> <p>AC \u662f\u4e00\u4e2a\u77e9\u9635\uff0cAd + b\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u53ef\u4ee5\u770b\u51fa\uff0c\u4e24\u4e2a\u4eff\u5c04\u53d8\u6362\u7684\u7ec4\u5408\u8fd8\u662f\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362\u3002</p> <p>\u7531\u6b64\u53ef\u4ee5\u770b\u51fa\uff0c\u4f7f\u7528\u4ee5\u4e0a\u65b9\u6cd5\u5c06\u591a\u4e2a\u4eff\u5c04\u53d8\u6362\u7ec4\u5408\u6210\u7684\u957f\u94fe\u5f0f\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u76f8\u5bf9\u4e8e\u5355\u4e2a\u4eff\u5c04\u53d8\u6362\u5e76\u6ca1\u6709\u6027\u80fd\u4e0a\u7684\u63d0\u5347\u3002</p> <p>\u4f46\u662f\u5982\u679c\u6211\u4eec\u5728\u4e24\u4e2a\u4eff\u5c04\u53d8\u6362\u4e4b\u95f4\u5f15\u5165\u975e\u7ebf\u6027\uff0c\u90a3\u4e48\u7ed3\u679c\u5c31\u5927\u4e0d\u4e00\u6837\u4e86\uff0c\u6211\u4eec\u53ef\u4ee5\u6784\u5efa\u51fa\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u6a21\u578b\u3002</p> <p>\u6700\u5e38\u7528\u7684\u6838\u5fc3\u7684\u975e\u7ebf\u6027\u51fd\u6570\u6709\uff1atanh(x)\uff0c\u03c3(x)\uff0cReLU(x)\u3002\u4f60\u53ef\u80fd\u4f1a\u60f3\uff1a\u201c\u4e3a\u4ec0\u4e48\u662f\u8fd9\u4e9b\u51fd\u6570\uff1f\u660e\u660e\u6709\u5176\u4ed6\u66f4\u591a\u7684\u975e\u7ebf\u6027\u51fd\u6570\u3002\u201d\u8fd9\u4e9b\u51fd\u6570\u5e38\u7528\u7684\u539f\u56e0\u662f\u5b83\u4eec\u62e5\u6709\u53ef\u4ee5\u5bb9\u6613\u8ba1\u7b97\u7684\u68af\u5ea6\uff0c\u800c\u8ba1\u7b97\u68af\u5ea6\u662f\u5b66\u4e60\u7684\u672c\u8d28\u3002\u4f8b\u5982</p> <p></p> <p>\u6ce8\u610f\uff1a\u5c3d\u7ba1\u4f60\u53ef\u80fd\u5728AI\u8bfe\u7a0b\u7684\u4ecb\u7ecd\u4e2d\u5b66\u4e60\u4e86\u4e00\u4e9b\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\u4e2d\u03c3(x)\u662f\u9ed8\u8ba4\u975e\u7ebf\u6027\u7684\uff0c\u4f46\u662f\u901a\u5e38\u5728\u5b9e\u9645\u4f7f\u7528\u7684\u8fc7\u7a0b\u4e2d\u90fd\u4f1a\u907f\u5f00\u5b83\u4eec\u3002\u8fd9\u662f\u56e0\u4e3a\u5f53\u53c2\u6570\u7684\u7edd\u5bf9\u503c\u589e\u957f\u65f6\uff0c\u68af\u5ea6\u4f1a\u5f88\u5feb\u6d88\u5931\u3002\u5c0f\u68af\u5ea6\u610f\u5473\u7740\u5f88\u96be\u5b66\u4e60\u3002\u56e0\u6b64\u5927\u90e8\u5206\u4eba\u9ed8\u8ba4\u9009\u62e9tanh\u6216\u8005ReLU\u3002</p> <pre><code># In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n# Note that non-linearites typically don't have parameters like affine maps do.\n# That is, they don't have weights that are updated during training.\ndata = torch.randn(2, 2)\nprint(data)\nprint(F.relu(data))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[-0.5404, -2.2102],\n        [ 2.1130, -0.0040]])\ntensor([[0.0000, 0.0000],\n        [2.1130, 0.0000]])\n\n</code></pre>"},{"location":"1.0/nlp_deep_learning_tutorial/#softmax","title":"Softmax\u548c\u6982\u7387","text":"<p>Softmax(x)\u4e5f\u662f\u4e00\u4e2a\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u4f46\u5b83\u7684\u7279\u6b8a\u4e4b\u5904\u5728\u4e8e\uff0c\u5b83\u901a\u5e38\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u540e\u4e00\u4e2a\u64cd\u4f5c\u3002\u8fd9\u662f\u56e0\u4e3a\u5b83\u63a5\u53d7\u5b9e\u6570\u5411\u91cf\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u6982\u7387\u5206\u5e03\u3002\u5b83\u7684\u5b9a\u4e49\u5982\u4e0b\u3002\u8bbex\u4e3a\u5b9e\u6570\u5411\u91cf(\u6b63\u3001\u8d1f\uff0c\u65e0\u8bba\u4ec0\u4e48\uff0c\u6ca1\u6709\u7ea6\u675f\uff09\u3002\u7136\u540eSoftmax(x)\u7684\u7b2ci\u4e2a\u5206\u91cf\u662f\uff1a</p> <p></p> <p>\u5f88\u660e\u663e\uff0c\u8f93\u51fa\u7684\u662f\u4e00\u4e2a\u6982\u7387\u5206\u5e03\uff1a\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u975e\u8d1f\u4e14\u548c\u4e3a1\u3002</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u8ba4\u4e3a\u8fd9\u53ea\u662f\u4e00\u4e2a\u5bf9\u8f93\u5165\u7684\u5143\u7d20\u8fdb\u884c\u7684\u6c42\u5e42\u8fd0\u7b97\u7b26\uff0c\u4f7f\u6240\u6709\u7684\u5185\u5bb9\u90fd\u975e\u8d1f\uff0c\u7136\u540e\u9664\u4ee5\u89c4\u8303\u5316\u5e38\u91cf\u3002</p> <pre><code># Softmax is also in torch.nn.functional\ndata = torch.randn(5)\nprint(data)\nprint(F.softmax(data, dim=0))\nprint(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\nprint(F.log_softmax(data, dim=0))  # theres also log_softmax\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\ntensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\ntensor(1.)\ntensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n\n</code></pre>"},{"location":"1.0/nlp_deep_learning_tutorial/#_4","title":"\u76ee\u6807\u51fd\u6570","text":"<p>\u76ee\u6807\u51fd\u6570\u6b63\u662f\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u8bad\u7ec3\u6765\u6700\u5c0f\u5316\u7684\u51fd\u6570(\u56e0\u6b64\uff0c\u5b83\u5e38\u5e38\u88ab\u79f0\u4f5c\u635f\u5931\u51fd\u6570\u6216\u8005\u6210\u672c\u51fd\u6570\uff09\u3002\u8fd9\u9700\u8981\u9996\u5148\u9009\u62e9\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u5b9e\u4f8b\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd0\u884c\u5b83\u5e76\u8ba1\u7b97\u8f93\u51fa\u7684\u635f\u5931\u3002\u7136\u540e\u901a\u8fc7\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u6765\u66f4\u65b0\u6a21\u578b\u7684\u53c2\u6570\u3002\u56e0\u6b64\u76f4\u89c2\u6765\u8bb2\uff0c\u5982\u679c\u5b83\u7684\u7ed3\u679c\u662f\u9519\u8bef\u7684\uff0c\u800c\u6a21\u578b\u5b8c\u5168\u4fe1\u4efb\u4ed6\uff0c\u90a3\u4e48\u635f\u5931\u5c06\u4f1a\u5f88\u9ad8\u3002\u53cd\u4e4b\uff0c\u5f53\u6a21\u578b\u4fe1\u4efb\u8ba1\u7b97\u7ed3\u679c\u800c\u7ed3\u679c\u6b63\u786e\u65f6\uff0c\u635f\u5931\u4f1a\u5f88\u4f4e\u3002</p> <p>\u5728\u4f60\u7684\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u7684\u76ee\u7684\u662f\u4f7f\u4f60\u7684\u7f51\u7edc\u62e5\u6709\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u4ee5\u5728\u5f00\u53d1\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u53ca\u5b9e\u9645\u751f\u4ea7\u4e2d\u62e5\u6709\u5f88\u5c0f\u7684\u635f\u5931\u3002\u635f\u5931\u51fd\u6570\u7684\u4e00\u4e2a\u4f8b\u5b50\u662f\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u7ecf\u5e38\u5728\u591a\u7ea7\u5206\u7c7b\u4e2d\u51fa\u73b0\u3002\u5728\u76d1\u7763\u591a\u7ea7\u5206\u7c7b\u4e2d\uff0c\u8fd9\u610f\u5473\u7740\u8bad\u7ec3\u7f51\u7edc\u6700\u5c0f\u5316\u6b63\u786e\u8f93\u51fa\u7684\u8d1f\u5bf9\u6570\u6982\u7387(\u7b49\u6548\u7684\u4e8e\u6700\u5927\u5316\u6b63\u786e\u8f93\u51fa\u7684\u5bf9\u6570\u6982\u7387\uff09\u3002</p>"},{"location":"1.0/nlp_deep_learning_tutorial/#_5","title":"\u4f18\u5316\u548c\u8bad\u7ec3","text":"<p>\u90a3\u4e48\uff0c\u6211\u4eec\u8be5\u600e\u4e48\u8ba1\u7b97\u51fd\u6570\u5b9e\u4f8b\u7684\u635f\u5931\u51fd\u6570\u5462\uff1f\u6211\u4eec\u5e94\u8be5\u505a\u4ec0\u4e48\u5462\uff1f\u6211\u4eec\u5728\u4e4b\u524d\u4e86\u89e3\u5230TensorFlow\u4e2d\u7684Tensor\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u68af\u5ea6\u4ee5\u53ca\u8ba1\u7b97\u68af\u5ea6\u76f8\u5173\u7684\u4e1c\u897f\u3002\u7531\u4e8e\u6211\u4eec\u7684\u635f\u5931\u6b63\u662f\u4e00\u4e2aTensor\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6240\u6709\u4e0e\u68af\u5ea6\u6709\u5173\u7684\u53c2\u6570\u6765\u8ba1\u7b97\u68af\u5ea6\u3002\u7136\u540e\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u6807\u51c6\u68af\u5ea6\u66f4\u65b0\u3002\u8bbe \u03b8\u4e3a\u6211\u4eec\u7684\u53c2\u6570\uff0cL(\u03b8)\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u03b7\u4e00\u4e2a\u6b63\u7684\u5b66\u4e60\u7387\u3002\u7136\u540e\uff1a</p> <p></p> <p>\u76ee\u524d\uff0c\u6709\u5927\u91cf\u7684\u7b97\u6cd5\u548c\u79ef\u6781\u7684\u7814\u7a76\u8bd5\u56fe\u505a\u4e00\u4e9b\u9664\u4e86\u8fd9\u79cd\u666e\u901a\u7684\u68af\u5ea6\u66f4\u65b0\u4ee5\u5916\u7684\u4e8b\u60c5\u3002\u8bb8\u591a\u4eba\u5c1d\u8bd5\u53bb\u57fa\u4e8e\u8bad\u7ec3\u65f6\u53d1\u751f\u7684\u4e8b\u60c5\u6765\u6539\u53d8\u5b66\u4e60\u7387\u3002\u4f46\u662f\uff0c\u4f60\u4e0d\u9700\u8981\u62c5\u5fc3\u8fd9\u4e9b\u7279\u6b8a\u7684\u7b97\u6cd5\u5230\u5e95\u5728\u5e72\u4ec0\u4e48\uff0c\u9664\u975e\u4f60\u771f\u7684\u5f88\u611f\u5174\u8da3\u3002Torch\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u7b97\u6cd5\u5728torch.optim\u5305\u4e2d\uff0c\u4e14\u5168\u90e8\u90fd\u662f\u900f\u660e\u7684\u3002\u5728\u8bed\u6cd5\u4e0a\u4f7f\u7528\u590d\u6742\u7684\u7b97\u6cd5\u548c\u4f7f\u7528\u6700\u7b80\u5355\u7684\u68af\u5ea6\u66f4\u65b0\u4e00\u6837\u7b80\u5355\u3002\u4f46\u662f\u5c1d\u8bd5\u4e0d\u540c\u7684\u66f4\u65b0\u7b97\u6cd5\u548c\u5728\u66f4\u65b0\u7b97\u6cd5\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684\u53c2\u6570(\u4f8b\u5982\u4e0d\u540c\u7684\u521d\u59cb\u5b66\u4e60\u7387\uff09\u5bf9\u4e8e\u4f18\u5316\u4f60\u7684\u7f51\u7edc\u7684\u6027\u80fd\u5f88\u91cd\u8981\u3002\u901a\u5e38\uff0c\u4ec5\u4ec5\u5c06\u666e\u901a\u7684SGD\u66ff\u6362\u6210\u4e00\u4e2a\u4f8b\u5982Adam\u6216\u8005RMSProp\u4f18\u5316\u5668\u90fd\u53ef\u4ee5\u663e\u8457\u7684\u63d0\u5347\u6027\u80fd\u3002</p>"},{"location":"1.0/nlp_deep_learning_tutorial/#pytorch_1","title":"\u4f7f\u7528PyTorch\u521b\u5efa\u7f51\u7edc\u7ec4\u4ef6","text":"<p>\u5728\u6211\u4eec\u7ee7\u7eed\u5173\u6ce8NLP\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u5148\u4f7f\u7528PyTorch\u6784\u5efa\u4e00\u4e2a\u53ea\u7528\u4eff\u5c04\u53d8\u6362\u548c\u975e\u7ebf\u6027\u51fd\u6570\u7ec4\u6210\u7684\u7f51\u7edc\u793a\u4f8b\u3002\u6211\u4eec\u4e5f\u5c06\u4e86\u89e3\u5982\u4f55\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528PyTorch\u5185\u7f6e\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u51fd\u6570\uff0c\u4ee5\u53ca\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u6240\u6709\u7684\u7f51\u7edc\u7ec4\u4ef6\u5e94\u8be5\u7ee7\u627fnn.Module\u5e76\u8986\u76d6forward()\u65b9\u6cd5\u3002\u7ee7\u627fnn.Module\u63d0\u4f9b\u7ed9\u4e86\u4e00\u4e9b\u65b9\u6cd5\u7ed9\u4f60\u7684\u7ec4\u4ef6\u3002\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u8ddf\u8e2a\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7<code>.to(device)</code>\u65b9\u6cd5\u5728CPU\u548cGPU\u4e4b\u95f4\u4ea4\u6362\u5b83\u4eec\u3002<code>.to(device)</code>\u65b9\u6cd5\u4e2d\u7684device\u53ef\u4ee5\u662fCPU\u8bbe\u5907<code>torch.device(\"cpu\")</code>\u6216\u8005CUDA\u8bbe\u5907<code>torch.device(\"cuda:0\")</code>\u3002</p> <p>\u8ba9\u6211\u4eec\u5199\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u7684\u793a\u4f8b\uff0c\u5b83\u63a5\u53d7\u4e00\u4e9b\u7a00\u758f\u7684BOW(\u8bcd\u888b\u6a21\u5f0f)\u8868\u793a\uff0c\u7136\u540e\u8f93\u51fa\u5206\u5e03\u5728\u4e24\u4e2a\u6807\u7b7e\u4e0a\u7684\u6982\u7387\uff1a\u201cEnglish\u201d\u548c\u201cSpanish\u201d\u3002\u8fd9\u4e2a\u6a21\u578b\u53ea\u662f\u4e00\u4e2a\u903b\u8f91\u56de\u5f52\u3002</p>"},{"location":"1.0/nlp_deep_learning_tutorial/#_6","title":"\u793a\u4f8b: \u57fa\u4e8e\u903b\u8f91\u56de\u5f52\u4e0e\u8bcd\u888b\u6a21\u5f0f\u7684\u6587\u672c\u5206\u7c7b\u5668","text":"<p>\u6211\u4eec\u7684\u6a21\u578b\u5c06\u4f1a\u628aBOW\u8868\u793a\u6620\u5c04\u6210\u6807\u7b7e\u4e0a\u7684\u5bf9\u6570\u6982\u7387\u3002\u6211\u4eec\u4e3a\u8bcd\u6c47\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u6307\u5b9a\u4e00\u4e2a\u7d22\u5f15\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u6240\u6709\u7684\u8bcd\u6c47\u662f\u4e24\u4e2a\u5355\u8bcd\u201chello\u201d\u548c\"world\"\uff0c\u75280\u548c1\u8868\u793a\u3002\u53e5\u5b50\u201chello hello hello hello\u201d\u7684\u8868\u793a\u662f</p> <pre><code>[4,0]\n</code></pre> <p>\u5bf9\u4e8e\u201chello world world hello\u201d, \u5219\u8868\u793a\u6210</p> <pre><code>[2,2]\n</code></pre> <p>\u901a\u5e38\u8868\u793a\u6210</p> <pre><code>[Count(hello),Count(world)]\n</code></pre> <p>\u7528x\u6765\u8868\u793a\u8fd9\u4e2aBOW\u5411\u91cf\u3002\u7f51\u7edc\u7684\u8f93\u51fa\u662f:</p> <p></p> <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u6211\u4eec\u6570\u636e\u4f20\u5165\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362\u7136\u540e\u505a\u5bf9\u6570\u5f52\u4e00\u5316<code>logsoftmax</code>\u3002</p> <pre><code>data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n        (\"Give it to me\".split(), \"ENGLISH\"),\n        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n\ntest_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n             (\"it is lost on me\".split(), \"ENGLISH\")]\n\n# word_to_ix maps each word in the vocab to a unique integer, which will be its\n# index into the Bag of words vector\nword_to_ix = {}\nfor sent, _ in data + test_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\n\nVOCAB_SIZE = len(word_to_ix)\nNUM_LABELS = 2\n\nclass BoWClassifier(nn.Module):  # inheriting from nn.Module!\n\n    def __init__(self, num_labels, vocab_size):\n        # calls the init function of nn.Module.  Dont get confused by syntax,\n        # just always do it in an nn.Module\n        super(BoWClassifier, self).__init__()\n\n        # Define the parameters that you will need.  In this case, we need A and b,\n        # the parameters of the affine mapping.\n        # Torch defines nn.Linear(), which provides the affine map.\n        # Make sure you understand why the input dimension is vocab_size\n        # and the output is num_labels!\n        self.linear = nn.Linear(vocab_size, num_labels)\n\n        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n        # to worry about that here\n\n    def forward(self, bow_vec):\n        # Pass the input through the linear layer,\n        # then pass that through log_softmax.\n        # Many non-linearities and other functions are in torch.nn.functional\n        return F.log_softmax(self.linear(bow_vec), dim=1)\n\ndef make_bow_vector(sentence, word_to_ix):\n    vec = torch.zeros(len(word_to_ix))\n    for word in sentence:\n        vec[word_to_ix[word]] += 1\n    return vec.view(1, -1)\n\ndef make_target(label, label_to_ix):\n    return torch.LongTensor([label_to_ix[label]])\n\nmodel = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n\n# the model knows its parameters.  The first output below is A, the second is b.\n# Whenever you assign a component to a class variable in the __init__ function\n# of a module, which was done with the line\n# self.linear = nn.Linear(...)\n# Then through some Python magic from the PyTorch devs, your module\n# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\nfor param in model.parameters():\n    print(param)\n\n# To run the model, pass in a BoW vector\n# Here we don't need to train, so the code is wrapped in torch.no_grad()\nwith torch.no_grad():\n    sample = data[0]\n    bow_vector = make_bow_vector(sample[0], word_to_ix)\n    log_probs = model(bow_vector)\n    print(log_probs)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\nParameter containing:\ntensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,\n         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,\n          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n          0.0119, -0.0334],\n        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,\n          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,\n          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n          0.1344,  0.0406]], requires_grad=True)\nParameter containing:\ntensor([0.0631, 0.1465], requires_grad=True)\ntensor([[-0.5378, -0.8771]])\n\n</code></pre> <p>\u4e0a\u9762\u7684\u54ea\u4e00\u4e2a\u503c\u5bf9\u5e94\u7684\u662fENGLISH\u7684\u5bf9\u6570\u6982\u7387\uff0c\u54ea\u4e00\u4e2a\u662fSPANISH\u7684\u5bf9\u6570\u6982\u7387\uff1f\u6211\u4eec\u8fd8\u6ca1\u6709\u5b9a\u4e49\uff0c\u4f46\u662f\u5982\u679c\u6211\u5fc5\u987b\u8981\u5b9a\u4e49\u6211\u4eec\u60f3\u8981\u8bad\u7ec3\u7684\u4e1c\u897f\u3002</p> <pre><code>label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n\n</code></pre> <p>\u8ba9\u6211\u4eec\u6765\u8bad\u7ec3\u5427! \u6211\u4eec\u5c06\u5b9e\u4f8b\u4f20\u5165\u6765\u83b7\u53d6\u5bf9\u6570\u6982\u7387\uff0c\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u4e00\u4e2a\u68af\u5ea6\u6b65\u957f\u6765\u66f4\u65b0\u53c2\u6570\u3002\u5728PyTorch\u7684nn\u5305\u91cc\u63d0\u4f9b\u4e86\u635f\u5931\u51fd\u6570\u3002nn.NLLLoss()\u662f\u6211\u4eec\u60f3\u8981\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\u3002torch.optim\u4e2d\u4e5f\u5b9a\u4e49\u4e86\u4f18\u5316\u65b9\u6cd5\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u53ea\u4f7f\u7528SGD\u3002</p> <p>\u6ce8\u610f\uff0c\u56e0\u4e3aNLLLoss\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u5bf9\u6570\u6982\u7387\u7684\u5411\u91cf\u4ee5\u53ca\u76ee\u6807\u6807\u7b7e\u3002\u5b83\u4e0d\u4f1a\u4e3a\u6211\u4eec\u8ba1\u7b97\u5bf9\u6570\u6982\u7387\u3002\u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48\u6211\u4eec\u6700\u540e\u4e00\u5c42\u7f51\u7edc\u662flog_softmax\u7684\u539f\u56e0\u3002\u635f\u5931\u51fd\u6570nn.CrossEntropyLoss()\u9664\u4e86\u5bf9\u7ed3\u679c\u989d\u5916\u8ba1\u7b97\u4e86logsoftmax\u4e4b\u5916\u548cNLLLoss()\u6ca1\u4ec0\u4e48\u533a\u522b\u3002</p> <pre><code># Run on test data before we train, just to see a before-and-after\nwith torch.no_grad():\n    for instance, label in test_data:\n        bow_vec = make_bow_vector(instance, word_to_ix)\n        log_probs = model(bow_vec)\n        print(log_probs)\n\n# Print the matrix column corresponding to \"creo\"\nprint(next(model.parameters())[:, word_to_ix[\"creo\"]])\n\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Usually you want to pass over the training data several times.\n# 100 is much bigger than on a real data set, but real datasets have more than\n# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\nfor epoch in range(100):\n    for instance, label in data:\n        # Step 1\\. Remember that PyTorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n\n        # Step 2\\. Make our BOW vector and also we must wrap the target in a\n        # Tensor as an integer. For example, if the target is SPANISH, then\n        # we wrap the integer 0\\. The loss function then knows that the 0th\n        # element of the log probabilities is the log probability\n        # corresponding to SPANISH\n        bow_vec = make_bow_vector(instance, word_to_ix)\n        target = make_target(label, label_to_ix)\n\n        # Step 3\\. Run our forward pass.\n        log_probs = model(bow_vec)\n\n        # Step 4\\. Compute the loss, gradients, and update the parameters by\n        # calling optimizer.step()\n        loss = loss_function(log_probs, target)\n        loss.backward()\n        optimizer.step()\n\nwith torch.no_grad():\n    for instance, label in test_data:\n        bow_vec = make_bow_vector(instance, word_to_ix)\n        log_probs = model(bow_vec)\n        print(log_probs)\n\n# Index corresponding to Spanish goes up, English goes down!\nprint(next(model.parameters())[:, word_to_ix[\"creo\"]])\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[-0.9297, -0.5020]])\ntensor([[-0.6388, -0.7506]])\ntensor([-0.1488, -0.1313], grad_fn=&lt;SelectBackward&gt;)\ntensor([[-0.2093, -1.6669]])\ntensor([[-2.5330, -0.0828]])\ntensor([ 0.2803, -0.5605], grad_fn=&lt;SelectBackward&gt;)\n\n</code></pre> <p>\u6211\u4eec\u5f97\u5230\u4e86\u6b63\u786e\u7684\u7ed3\u679c\uff01\u4f60\u53ef\u4ee5\u770b\u5230Spanish\u7684\u5bf9\u6570\u6982\u7387\u6bd4\u7b2c\u4e00\u4e2a\u4f8b\u5b50\u4e2d\u7684\u9ad8\u7684\u591a\uff0cEnglish\u7684\u5bf9\u6570\u6982\u7387\u5728\u7b2c\u4e8c\u4e2a\u6d4b\u8bd5\u6570\u636e\u4e2d\u66f4\u9ad8\uff0c\u7ed3\u679c\u4e5f\u5e94\u8be5\u662f\u8fd9\u6837\u3002</p> <p>\u73b0\u5728\u4f60\u4e86\u89e3\u4e86\u5982\u4f55\u521b\u5efa\u4e00\u4e2aPyTorch\u7ec4\u4ef6\uff0c\u5c06\u6570\u636e\u4f20\u5165\u5e76\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0\u3002\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u53ef\u4ee5\u5f00\u59cb\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u4e0a\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e86\u3002</p>"},{"location":"1.0/nlp_pytorch_tutorial/","title":"PyTorch \u4ecb\u7ecd","text":"<p>\u8bd1\u8005\uff1aYAOKE7</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p>"},{"location":"1.0/nlp_pytorch_tutorial/#torch","title":"Torch\u5f20\u91cf\u5e93\u4ecb\u7ecd","text":"<p>\u6df1\u5ea6\u5b66\u4e60\u7684\u6240\u6709\u8ba1\u7b97\u90fd\u662f\u5728\u5f20\u91cf\u4e0a\u8fdb\u884c\u7684,\u5176\u4e2d\u5f20\u91cf\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u8d85\u8fc7\u4e8c\u7ef4\u7d22\u5f15\u7684\u77e9\u9635\u7684\u4e00\u822c\u8868\u793a\u5f62\u5f0f\u3002\u7a0d\u540e\u6211\u4eec\u5c06\u8be6\u7ec6\u8ba8\u8bba\u8fd9\u610f\u5473\u7740\u4ec0\u4e48\u3002\u9996\u5148\uff0c\u6211\u4eec\u5148\u6765\u770b\u4e00\u4e0b\u6211\u4eec\u53ef\u4ee5\u7528\u5f20\u91cf\u6765\u5e72\u4ec0\u4e48\u3002</p> <pre><code># \u4f5c\u8005: Robert Guthrie\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\n</code></pre>"},{"location":"1.0/nlp_pytorch_tutorial/#_1","title":"\u521b\u5efa\u5f20\u91cf","text":"<p>\u5f20\u91cf\u53ef\u4ee5\u5728Python list\u5f62\u5f0f\u4e0b\u901a\u8fc7torch.Tensor()\u51fd\u6570\u521b\u5efa\u3002</p> <pre><code># \u5229\u7528\u7ed9\u5b9a\u6570\u636e\u521b\u5efa\u4e00\u4e2atorch.Tensor\u5bf9\u8c61.\u8fd9\u662f\u4e00\u4e2a\u4e00\u7ef4\u5411\u91cf\nV_data = [1., 2., 3.]\nV = torch.Tensor(V_data)\nprint(V)\n\n# \u521b\u5efa\u4e00\u4e2a\u77e9\u9635\nM_data = [[1., 2., 3.], [4., 5., 6]]\nM = torch.Tensor(M_data)\nprint(M)\n\n# \u521b\u5efa2x2x2\u5f62\u5f0f\u7684\u4e09\u7ef4\u5f20\u91cf.\nT_data = [[[1., 2.], [3., 4.]],\n          [[5., 6.], [7., 8.]]]\nT = torch.Tensor(T_data)\nprint(T)\n\n</code></pre> <p>\u8f93\u51fa</p> <pre><code>tensor([1., 2., 3.])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\ntensor([[[1., 2.],\n         [3., 4.]],\n\n        [[5., 6.],\n         [7., 8.]]])\n</code></pre> <p>\u4ec0\u4e48\u662f\u4e09\u7ef4\u5f20\u91cf\uff1f\u8ba9\u6211\u4eec\u8fd9\u6837\u60f3\u8c61\u3002\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5411\u91cf,\u90a3\u4e48\u5bf9\u8fd9\u4e2a\u5411\u91cf\u7d22\u5f15\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u6807\u91cf\u3002\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u77e9\u9635\uff0c\u5bf9\u8fd9\u4e2a\u77e9\u9635\u7d22\u5f15\u90a3\u4e48\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u5411\u91cf\u3002\u5982\u679c\u4f60\u6709\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf\uff0c\u90a3\u4e48\u5bf9\u5176\u7d22\u5f15\u5c31\u4f1a\u5f97\u5230\u4e00\u4e2a\u77e9\u9635!</p> <p>\u9488\u5bf9\u672f\u8bed\u7684\u8bf4\u660e\uff1a\u5f53\u6211\u5728\u672c\u6559\u7a0b\u5185\u4f7f\u7528\u201ctensor\u201d\uff0c\u5b83\u9488\u5bf9\u7684\u662f\u6240\u6709torch.Tensor\u5bf9\u8c61\u3002\u77e9\u9635\u548c\u5411\u91cf\u662f\u7279\u6b8a\u7684torch.Tensors\uff0c\u4ed6\u4eec\u7684\u7ef4\u5ea6\u5206\u522b\u662f1\u548c2\u3002\u5f53\u6211\u8bf4\u5230\u4e09\u7ef4\u5f20\u91cf\uff0c\u6211\u4f1a\u7b80\u6d01\u7684\u4f7f\u7528\u201c3D tensor\u201d\u3002</p> <pre><code># \u7d22\u5f15V\u5f97\u5230\u4e00\u4e2a\u6807\u91cf(0\u7ef4\u5f20\u91cf\uff09\nprint(V[0])\n\n# \u4ece\u5411\u91cfV\u4e2d\u83b7\u53d6\u4e00\u4e2a\u6570\u5b57\nprint(V[0].item())\n\n# \u7d22\u5f15M\u5f97\u5230\u4e00\u4e2a\u5411\u91cf\nprint(M[0])\n\n# \u7d22\u5f15T\u5f97\u5230\u4e00\u4e2a\u77e9\u9635\nprint(T[0])\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(1.)\n1.0\ntensor([1., 2., 3.])\ntensor([[1., 2.],\n        [3., 4.]])\n</code></pre> <p>\u4f60\u4e5f\u53ef\u4ee5\u521b\u5efa\u5176\u4ed6\u6570\u636e\u7c7b\u578b\u7684tensors\u3002\u9ed8\u8ba4\u7684\u6570\u636e\u7c7b\u578b\u4e3a\u6d6e\u70b9\u578b\u3002\u53ef\u4ee5\u4f7f\u7528torch.LongTensor()\u6765\u521b\u5efa\u4e00\u4e2a\u6574\u6570\u7c7b\u578b\u7684\u5f20\u91cf\u3002\u4f60\u53ef\u4ee5\u5728\u6587\u4ef6\u4e2d\u5bfb\u627e\u66f4\u591a\u7684\u6570\u636e\u7c7b\u578b\uff0c\u4f46\u662f\u6d6e\u70b9\u578b\u548c\u957f\u6574\u5f62\u662f\u6700\u5e38\u7528\u7684\u3002</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528torch.randn()\u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\u3002\u8fd9\u4e2a\u5f20\u91cf\u62e5\u6709\u968f\u673a\u6570\u636e\u548c\u9700\u8981\u6307\u5b9a\u7684\u7ef4\u5ea6\u3002</p> <pre><code>x = torch.randn((3, 4, 5))\nprint(x)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n\n        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n\n        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n</code></pre>"},{"location":"1.0/nlp_pytorch_tutorial/#_2","title":"\u5f20\u91cf\u64cd\u4f5c","text":"<p>\u4f60\u53ef\u4ee5\u4ee5\u4f60\u60f3\u8981\u7684\u65b9\u5f0f\u64cd\u4f5c\u5f20\u91cf\u3002</p> <pre><code>x = torch.Tensor([1., 2., 3.])\ny = torch.Tensor([4., 5., 6.])\nz = x + y\nprint(z)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([5., 7., 9.])\n</code></pre> <p>\u53ef\u4ee5\u67e5\u9605 \u6587\u6863 \u83b7\u53d6\u5927\u91cf\u53ef\u7528\u64cd\u4f5c\u7684\u5b8c\u6574\u5217\u8868,\u8fd9\u4e9b\u64cd\u4f5c\u4e0d\u4ec5\u5c40\u9650\u4e8e\u6570\u5b66\u64cd\u4f5c\u8303\u56f4\u3002</p> <p>\u63a5\u4e0b\u6765\u4e00\u4e2a\u5f88\u6709\u5e2e\u52a9\u7684\u64cd\u4f5c\u5c31\u662f\u8fde\u63a5\u3002</p> <pre><code># \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u5b83\u6cbf\u7740\u7b2c\u4e00\u4e2a\u884c\u8fdb\u884c\u8fde\u63a5 (\u8fde\u63a5\u884c)\nx_1 = torch.randn(2, 5)\ny_1 = torch.randn(3, 5)\nz_1 = torch.cat([x_1, y_1])\nprint(z_1)\n\n# \u8fde\u63a5\u5217\uff1a\nx_2 = torch.randn(2, 3)\ny_2 = torch.randn(2, 5)\n# \u7b2c\u4e8c\u4e2a\u53c2\u6570\u6307\u5b9a\u4e86\u6cbf\u7740\u54ea\u6761\u8f74\u8fde\u63a5\nz_2 = torch.cat([x_2, y_2], 1)\nprint(z_2)\n\n# \u5982\u679c\u4f60\u7684tensors\u662f\u4e0d\u517c\u5bb9\u7684\uff0ctorch\u4f1a\u62a5\u9519\u3002\u53d6\u6d88\u6ce8\u91ca\u6765\u67e5\u770b\u9519\u8bef\u3002\n# torch.cat([x_1, x_2])\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\ntensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,  1.2381],\n        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668, -0.9999]])\n</code></pre>"},{"location":"1.0/nlp_pytorch_tutorial/#_3","title":"\u91cd\u6784\u5f20\u91cf","text":"<p>\u4f7f\u7528.view()\u53bb\u91cd\u6784\u5f20\u91cf\u3002\u8fd9\u662f\u4e00\u4e2a\u9ad8\u9891\u65b9\u6cd5\uff0c\u56e0\u4e3a\u8bb8\u591a\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u5143\u5bf9\u8f93\u5165\u683c\u5f0f\u6709\u660e\u786e\u7684\u8981\u6c42\u3002\u4f60\u901a\u5e38\u9700\u8981\u5148\u5c06\u6570\u636e\u91cd\u6784\u518d\u8f93\u5165\u5230\u795e\u7ecf\u5143\u4e2d\u3002</p> <pre><code>x = torch.randn(2, 3, 4)\nprint(x)\nprint(x.view(2, 12))  # \u91cd\u6784\u4e3a2\u884c12\u5217\n# \u540c\u4e0a\u3002\u5982\u679c\u7ef4\u5ea6\u4e3a-1,\u90a3\u4e48\u5b83\u7684\u5927\u5c0f\u53ef\u4ee5\u6839\u636e\u6570\u636e\u63a8\u65ad\u51fa\u6765\nprint(x.view(2, -1))\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n         [-0.6240, -0.9773,  0.8748,  0.9873],\n         [-0.0594, -2.4919,  0.2423,  0.2883]],\n\n        [[-0.1095,  0.3126,  1.5038,  0.5038],\n         [ 0.6223, -0.4481, -0.2856,  0.3880],\n         [-1.1435, -0.6512, -0.1032,  0.6937]]])\ntensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n         -0.0594, -2.4919,  0.2423,  0.2883],\n        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n         -1.1435, -0.6512, -0.1032,  0.6937]])\ntensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n         -0.0594, -2.4919,  0.2423,  0.2883],\n        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n         -1.1435, -0.6512, -0.1032,  0.6937]])\n</code></pre>"},{"location":"1.0/nlp_pytorch_tutorial/#_4","title":"\u8ba1\u7b97\u56fe\u548c\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u8ba1\u7b97\u56fe\u7684\u601d\u60f3\u5bf9\u4e8e\u6709\u6548\u7387\u7684\u6df1\u5ea6\u5b66\u4e60\u7f16\u7a0b\u662f\u5f88\u91cd\u8981\u7684\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u4f7f\u4f60\u4e0d\u5fc5\u53bb\u81ea\u5df1\u5199\u53cd\u5411\u68af\u5ea6\u4f20\u64ad\u3002\u8ba1\u7b97\u56fe\u53ea\u662f\u7b80\u5355\u5730\u8bf4\u660e\u4e86\u5982\u4f55\u5c06\u6570\u636e\u7ec4\u5408\u5728\u4e00\u8d77\u4ee5\u8f93\u51fa\u7ed3\u679c\u3002\u56e0\u4e3a\u56fe\u5b8c\u5168\u6307\u5b9a\u4e86\u64cd\u4f5c\u6240\u5305\u542b\u7684\u53c2\u6570\uff0c\u56e0\u6b64\u5b83\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u6c42\u5bfc\u3002\u8fd9\u53ef\u80fd\u542c\u8d77\u6765\u5f88\u6a21\u7cca\uff0c\u6240\u4ee5\u8ba9\u6211\u4eec\u770b\u770b\u4f7f\u7528Pytorch\u7684\u57fa\u672c\u7c7b\uff1arequires_grad\u3002</p> <p>\u9996\u5148\uff0c\u4ece\u7a0b\u5e8f\u5458\u7684\u89d2\u5ea6\u6765\u601d\u8003\u3002\u6211\u4eec\u5728\u4e0a\u9762\u521a\u521a\u521b\u5efa\u7684torch.Tensor\u5bf9\u8c61\u4e2d\u5b58\u50a8\u4e86\u4ec0\u4e48\uff1f\u663e\u7136\uff0c\u662f\u6570\u636e\u548c\u7ed3\u6784\uff0c\u4e5f\u5f88\u53ef\u80fd\u662f\u5176\u4ed6\u7684\u4e1c\u897f\u3002\u4f46\u662f\u5f53\u6211\u4eec\u5c06\u4e24\u4e2a\u5f20\u91cf\u76f8\u52a0\uff0c\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\u3002\u8fd9\u4e2a\u8f93\u51fa\u6240\u80fd\u4f53\u73b0\u51fa\u7684\u53ea\u6709\u6570\u636e\u548c\u7ed3\u6784\uff0c\u5e76\u4e0d\u80fd\u4f53\u73b0\u51fa\u662f\u7531\u4e24\u4e2a\u5f20\u91cf\u52a0\u4e4b\u548c\u5f97\u5230\u7684(\u56e0\u4e3a\u5b83\u53ef\u80fd\u662f\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684, \u4e5f\u53ef\u80fd\u662f\u5176\u4ed6\u64cd\u4f5c\u7684\u7ed3\u679c\u7b49\uff09\u3002</p> <p>\u5982\u679crequires_grad=True\uff0c\u5f20\u91cf\u5bf9\u8c61\u53ef\u4ee5\u4e00\u76f4\u8ddf\u8e2a\u5b83\u662f\u5982\u4f55\u521b\u5efa\u7684\u3002\u8ba9\u6211\u4eec\u5728\u5b9e\u9645\u4e2d\u6765\u770b\u3002</p> <pre><code># \u5f20\u91cf\u5bf9\u8c61\u5e26\u6709\u201crequires_grad\u201d\u6807\u8bb0\nx =torch.Tensor([1., 2., 3], requires_grad=True)\n\n# \u901a\u8fc7requires_grad=True\uff0c\u60a8\u4e5f\u53ef\u4ee5\u505a\u4e4b\u524d\u6240\u6709\u7684\u64cd\u4f5c\u3002\ny = torch.Tensor([4., 5., 6], requires_grad=True)\nz = x + y\nprint(z)\n\n# \u4f46\u662fz\u8fd8\u6709\u4e00\u4e9b\u989d\u5916\u7684\u4e1c\u897f.\nprint(z.grad_fn)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([5., 7., 9.], grad_fn=&lt;AddBackward0&gt;)\n&lt;AddBackward0 object at 0x7fd73f1b4c50&gt;\n</code></pre> <p>\u65e2\u7136\u53d8\u91cf\u77e5\u9053\u600e\u4e48\u521b\u5efa\u7684\u5b83\u4eec\u3002z\u77e5\u9053\u5b83\u5e76\u975e\u662f\u4ece\u6587\u4ef6\u8bfb\u53d6\u7684\uff0c\u4e5f\u4e0d\u662f\u4e58\u6cd5\u6216\u6307\u6570\u6216\u5176\u4ed6\u8fd0\u7b97\u7684\u7ed3\u679c\u3002\u5982\u679c\u4f60\u7ee7\u7eed\u8ddf\u8e2a z.grad_fn\uff0c\u4f60\u4f1a\u4ece\u4e2d\u627e\u5230x\u548cy\u7684\u75d5\u8ff9\u3002</p> <p>\u4f46\u662f\u5b83\u5982\u4f55\u5e2e\u52a9\u6211\u4eec\u8ba1\u7b97\u68af\u5ea6?</p> <pre><code># \u6211\u4eec\u6765\u5c06z\u4e2d\u6240\u6709\u9879\u4f5c\u548c\u8fd0\u7b97\ns = z.sum()\nprint(s)\nprint(s.grad_fn)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(21., grad_fn=&lt;SumBackward0&gt;)\n&lt;SumBackward0 object at 0x7fd73f1c7e48&gt;\n</code></pre> <p>\u90a3\u4e48\u8fd9\u4e2a\u8ba1\u7b97\u548c\u5bf9x\u7684\u7b2c\u4e00\u4e2a\u5206\u91cf\u7684\u5bfc\u6570\u7b49\u4e8e\u591a\u5c11? \u5728\u6570\u5b66\u4e0a,\u6211\u4eec\u6c42   </p> \\[\\frac{\\partial s}{\\partial x_0}\\] <p>s\u662f\u88ab\u4f5c\u4e3a\u5f20\u91cfz\u7684\u548c\u521b\u5efa\u7684\u3002\u5f20\u91cfz\u662fx+y\u7684\u548c</p> <p>\\(  s = \\overbrace{x_0 + y_0}^\\text{\\(z_0\\)} + \\overbrace{x_1 + y_1}^\\text{\\(z_1\\)} + \\overbrace{x_2 + y_2}^\\text{\\(z_2\\)} \\)</p> <p>\u5e76\u4e14s\u5305\u542b\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u53bb\u51b3\u5b9a\u6211\u4eec\u9700\u8981\u7684\u5bfc\u6570\u4e3a1!</p> <p>\u5f53\u7136\u5b83\u63a9\u76d6\u4e86\u5982\u4f55\u8ba1\u7b97\u5bfc\u6570\u7684\u6311\u6218\u3002\u8fd9\u662f\u56e0\u4e3as\u643a\u5e26\u4e86\u8db3\u591f\u591a\u7684\u4fe1\u606f\u6240\u4ee5\u5bfc\u6570\u53ef\u4ee5\u88ab\u8ba1\u7b97\u3002\u73b0\u5b9e\u4e2d\uff0cPytorch \u7a0b\u5e8f\u7684\u5f00\u53d1\u4eba\u5458\u7528\u7a0b\u5e8f\u6307\u4ee4sum()\u548c + \u64cd\u4f5c\u4ee5\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u5b83\u4eec\u7684\u68af\u5ea6\u5e76\u4e14\u8fd0\u884c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u3002\u6df1\u5165\u8ba8\u8bba\u6b64\u7b97\u6cd5\u8d85\u51fa\u4e86\u672c\u6559\u7a0b\u7684\u8303\u56f4.</p> <p>\u8ba9\u6211\u4eec\u7528Pytorch\u8ba1\u7b97\u68af\u5ea6\uff0c\u53d1\u73b0\u6211\u4eec\u662f\u5bf9\u7684:(\u6ce8\u610f\u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2a\u6a21\u5757\u5f88\u591a\u6b21\uff0c\u5b83\u7684\u68af\u5ea6\u4f1a\u4e0a\u5347\uff0c\u8fd9\u662f\u56e0\u4e3aPytorch\u7d2f\u79ef\u68af\u5ea6\u6e10\u53d8\u4e3a.grad\u5c5e\u6027\uff0c\u800c\u4e14\u5bf9\u4e8e\u5f88\u591a\u6a21\u578b\u5b83\u662f\u5f88\u65b9\u4fbf\u7684.)</p> <pre><code># \u5728\u4efb\u610f\u53d8\u91cf\u4e0a\u4f7f\u7528 .backward()\u5c06\u4f1a\u8fd0\u884c\u53cd\u5411,\u4ece\u5b83\u5f00\u59cb.\ns.backward()\nprint(x.grad)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([1., 1., 1.])\n</code></pre> <p>\u4f5c\u4e3a\u4e00\u4e2a\u6210\u529f\u7684\u6df1\u5ea6\u5b66\u4e60\u7a0b\u5e8f\u5458\u4e86\u89e3\u4e0b\u9762\u7684\u6a21\u5757\u5982\u4f55\u8fd0\u884c\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002</p> <pre><code>x = torch.randn((2, 2))\ny = torch.randn((2, 2))\n#\u7528\u6237\u521b\u5efa\u7684\u5f20\u91cf\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u201crequires_grad=False\u201d\nprint(x.requires_grad, y.requires_grad)\nz = x + y  \n# \u4f60\u4e0d\u80fd\u901a\u8fc7z\u53cd\u5411\u4f20\u64ad\u3002\nprint(z.grad_fn)\n\n\n# \u201c.requires_grad_( ... )\u201d\u6539\u53d8\u4e86\u201crequires_grad\u201d\u5c5e\u6027\n# \u5982\u679c\u6ca1\u6709\u6307\u5b9a\uff0c\u6807\u8bb0\u9ed8\u8ba4\u4e3aTrue\nx = x.requires_grad_()\ny = y.requires_grad_()\n#\u6b63\u5982\u6211\u4eec\u5728\u4e0a\u9762\u770b\u5230\u7684\u4e00\u6837\uff0cz\u5305\u542b\u8db3\u591f\u7684\u4fe1\u606f\u8ba1\u7b97\u68af\u5ea6\u3002\nz = x + y\nprint(z.grad_fn)\n# \u5982\u679c\u4efb\u4f55\u64cd\u4f5c\u7684\u8f93\u5165\u90e8\u5206\u5e26\u6709\u201crequires_grad=True\u201d\u90a3\u4e48\u8f93\u51fa\u5c31\u4f1a\u53d8\u4e3a\uff1a\nprint(z.requires_grad)\n\n# \u73b0\u5728z\u6709\u5173\u4e8ex,y\u7684\u5386\u53f2\u4fe1\u606f\n# \u6211\u4eec\u53ef\u4ee5\u83b7\u53d6\u5b83\u7684\u503c\uff0c\u5c06\u5176\u4ece\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\u5417\uff1f\nnew_z = z.detach()\n\n# new_z\u6709\u8db3\u591f\u7684\u4fe1\u606f\u53cd\u5411\u4f20\u64ad\u81f3x\u548cy\u5417\uff1f\n# \u7b54\u6848\u662f\u6ca1\u6709\nprint(new_z.grad_fn)\n# \u600e\u4e48\u4f1a\u8fd9\u6837\uff1f \u201cz.detach()\u201d\u51fd\u6570\u8fd4\u56de\u4e86\u4e00\u4e2a\u4e0e\u201cz\u201d\u76f8\u540c\u5b58\u50a8\u7684\u5f20\u91cf\n#\u4f46\u662f\u6ca1\u6709\u643a\u5e26\u5386\u53f2\u7684\u8ba1\u7b97\u4fe1\u606f\u3002\n# \u5b83\u5bf9\u4e8e\u81ea\u5df1\u662f\u5982\u4f55\u8ba1\u7b97\u5f97\u6765\u7684\u4e0d\u77e5\u9053\u4efb\u4f55\u4e8b\u60c5\u3002\n# \u4ece\u672c\u8d28\u4e0a\u8bb2\uff0c\u6211\u4eec\u5df2\u7ecf\u628a\u8fd9\u4e2a\u53d8\u91cf\u4ece\u8fc7\u53bb\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765\u4e86\u3002\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>False False\nNone\n&lt;AddBackward0 object at 0x7fd73f1ee588&gt;\nTrue\nNone\n</code></pre> <p>\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7<code>.requires_grad``=True by wrapping the code block in ``with torch.no_grad():</code>\u505c\u6b62\u8ddf\u8e2a\u5f20\u91cf\u7684\u5386\u53f2\u8bb0\u5f55\u4e2d\u7684\u81ea\u52a8\u6c42\u5bfc\u3002</p> <pre><code>print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n    print((x ** 2).requires_grad)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>True\nTrue\nFalse\n</code></pre>"},{"location":"1.0/nlp_sequence_models_tutorial/","title":"\u5e8f\u5217\u6a21\u578b\u548cLSTM\u7f51\u7edc(\u957f\u77ed\u8bb0\u5fc6\u7f51\u7edc)","text":"<p>\u8bd1\u8005\uff1aETCartman</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4e4b\u524d\u6211\u4eec\u5df2\u7ecf\u5b66\u8fc7\u4e86\u8bb8\u591a\u7684\u524d\u9988\u7f51\u7edc. \u6240\u8c13\u524d\u9988\u7f51\u7edc, \u5c31\u662f\u7f51\u7edc\u4e2d\u4e0d\u4f1a\u4fdd\u5b58\u72b6\u6001. \u7136\u800c\u6709\u65f6 \u8fd9\u5e76\u4e0d\u662f\u6211\u4eec\u60f3\u8981\u7684\u6548\u679c. \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP, Natural Language Processing) \u4e2d, \u5e8f\u5217\u6a21\u578b\u662f\u4e00\u4e2a\u6838\u5fc3\u7684\u6982\u5ff5. \u6240\u8c13\u5e8f\u5217\u6a21\u578b, \u5373\u8f93\u5165\u4f9d\u8d56\u4e8e\u65f6\u95f4\u4fe1\u606f\u7684\u6a21\u578b. \u4e00\u4e2a\u5178\u578b\u7684\u5e8f\u5217\u6a21\u578b\u662f\u9690\u9a6c\u5c14\u79d1\u592b\u6a21\u578b (HMM, Hidden Markov Model). \u53e6\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\u7684\u4f8b\u5b50\u662f\u6761\u4ef6\u968f\u673a\u573a (CRF, Conditional Random Field).</p> <p>\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u662f\u6307\u53ef\u4ee5\u4fdd\u5b58\u67d0\u79cd\u72b6\u6001\u7684\u795e\u7ecf\u7f51\u7edc. \u6bd4\u5982\u8bf4, \u795e\u7ecf\u7f51\u7edc\u4e2d\u4e0a\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u53ef\u4ee5\u4f5c\u4e3a\u4e0b\u4e2a \u65f6\u523b\u7684\u8f93\u5165\u7684\u4e00\u90e8\u5206, \u4ee5\u6b64\u4fe1\u606f\u5c31\u53ef\u4ee5\u901a\u8fc7\u5e8f\u5217\u5728\u7f51\u7edc\u4e2d\u4e00\u76f4\u5f80\u540e\u4f20\u9012. \u5bf9\u4e8eLSTM (Long-Short Term Memory) \u6765\u8bf4, \u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684\u9690\u72b6\u6001 \\(h_t\\), \u8be5\u9690\u72b6\u6001 \u539f\u5219\u4e0a\u53ef\u4ee5\u5305\u542b\u5e8f\u5217\u5f53\u524d\u7ed3\u70b9\u4e4b\u524d\u7684\u4efb\u4e00\u8282\u70b9\u7684\u4fe1\u606f. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u9690\u85cf\u72b6\u6001\u6765\u9884\u6d4b\u8bed\u8a00\u6a21\u578b \u4e2d\u7684\u5355\u8bcd, \u8bcd\u6027\u6807\u7b7e\u4ee5\u53ca\u5176\u4ed6\u5404\u79cd\u5404\u6837\u7684\u4e1c\u897f.</p>"},{"location":"1.0/nlp_sequence_models_tutorial/#pytorchlstm","title":"Pytorch\u4e2d\u7684LSTM","text":"<p>\u5728\u6b63\u5f0f\u5b66\u4e60\u4e4b\u524d\uff0c\u6709\u51e0\u4e2a\u70b9\u8981\u8bf4\u660e\u4e00\u4e0b\uff0cPytorch\u4e2dLSTM\u7684\u8f93\u5165\u5f62\u5f0f\u662f\u4e00\u4e2a3D\u7684Tensor\uff0c\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u90fd\u6709\u91cd\u8981\u7684\u610f\u4e49\uff0c\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5c31\u662f\u5e8f\u5217\u672c\u8eab\uff0c\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u662fmini-batch\u4e2d\u5b9e\u4f8b\u7684\u7d22\u5f15\uff0c\u7b2c\u4e09\u4e2a\u7ef4\u5ea6\u662f\u8f93\u5165\u5143\u7d20\u7684\u7d22\u5f15\uff0c\u6211\u4eec\u4e4b\u524d\u6ca1\u6709\u63a5\u89e6\u8fc7mini-batch\uff0c\u6240\u4ee5\u6211\u4eec\u5c31\u5148\u5ffd\u7565\u5b83\u5e76\u5047\u8bbe\u7b2c\u4e8c\u7ef4\u7684\u7ef4\u5ea6\u662f1\u3002</p> <p>\u5982\u679c\u8981\u7528\"The cow jumped\"\u8fd9\u4e2a\u53e5\u5b50\u6765\u8fd0\u884c\u4e00\u4e2a\u5e8f\u5217\u6a21\u578b\uff0c\u90a3\u4e48\u5c31\u5e94\u8be5\u628a\u5b83\u6574\u7406\u6210\u5982\u4e0b\u7684\u5f62\u5f0f\uff1a</p> <p>\\( \\begin{split} \\begin{bmatrix}  \\overbrace{q_\\text{The}}^\\text{row vector} \\ q_\\text{cow} \\  q_\\text{jumped}  \\end{bmatrix} \\end{split} \\)</p> <p>\u9664\u4e86\u6709\u4e00\u4e2a\u989d\u5916\u7684\u5927\u5c0f\u4e3a1\u7684\u7b2c\u4e8c\u7ef4\u5ea6.</p> <p>\u6b64\u5916, \u4f60\u8fd8\u53ef\u4ee5\u5411\u7f51\u7edc\u9010\u4e2a\u8f93\u5165\u5e8f\u5217, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u7b2c\u4e00\u4e2a\u8f74\u7684\u5927\u5c0f\u4e5f\u662f1.</p> <p>\u6765\u770b\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50.</p> <pre><code># \u4f5c\u8005: Robert Guthrie\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\n</code></pre> <pre><code>lstm = nn.LSTM(3, 3)  # \u8f93\u5165\u7ef4\u5ea6\u4e3a3\u7ef4\uff0c\u8f93\u51fa\u7ef4\u5ea6\u4e3a3\u7ef4\ninputs = [torch.randn(1, 3) for _ in range(5)]  # \u751f\u6210\u4e00\u4e2a\u957f\u5ea6\u4e3a5\u7684\u5e8f\u5217\n\n# \u521d\u59cb\u5316\u9690\u85cf\u72b6\u6001.\nhidden = (torch.randn(1, 1, 3),\n          torch.randn(1, 1, 3))\nfor i in inputs:\n    # \u5c06\u5e8f\u5217\u4e2d\u7684\u5143\u7d20\u9010\u4e2a\u8f93\u5165\u5230LSTM.\n    # \u7ecf\u8fc7\u6bcf\u6b65\u64cd\u4f5c,hidden \u7684\u503c\u5305\u542b\u4e86\u9690\u85cf\u72b6\u6001\u7684\u4fe1\u606f.\n    out, hidden = lstm(i.view(1, 1, -1), hidden)\n\n# \u53e6\u5916\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e00\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3.\n# LSTM\u7b2c\u4e00\u4e2a\u8fd4\u56de\u7684\u7b2c\u4e00\u4e2a\u503c\u662f\u6240\u6709\u65f6\u523b\u7684\u9690\u85cf\u72b6\u6001\n# \u7b2c\u4e8c\u4e2a\u8fd4\u56de\u503c\u662f\u6700\u540e\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u85cf\u72b6\u6001\n#(\u6240\u4ee5\"out\"\u7684\u6700\u540e\u4e00\u4e2a\u548c\"hidden\"\u662f\u4e00\u6837\u7684)\n# \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bbe\u8ba1:\n# \u901a\u8fc7\"out\"\u4f60\u80fd\u53d6\u5f97\u4efb\u4f55\u4e00\u4e2a\u65f6\u523b\u7684\u9690\u85cf\u72b6\u6001\uff0c\u800c\"hidden\"\u7684\u503c\u662f\u7528\u6765\u8fdb\u884c\u5e8f\u5217\u7684\u53cd\u5411\u4f20\u64ad\u8fd0\u7b97, \u5177\u4f53\u65b9\u5f0f\u5c31\u662f\u5c06\u5b83\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165\u540e\u9762\u7684 LSTM \u7f51\u7edc.\n\n# \u589e\u52a0\u989d\u5916\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6.\ninputs = torch.cat(inputs).view(len(inputs), 1, -1)\nhidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # \u6e05\u7a7a\u9690\u85cf\u72b6\u6001. \nout, hidden = lstm(inputs, hidden)\nprint(out)\nprint(hidden)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[[-0.0187,  0.1713, -0.2944]],\n\n        [[-0.3521,  0.1026, -0.2971]],\n\n        [[-0.3191,  0.0781, -0.1957]],\n\n        [[-0.1634,  0.0941, -0.1637]],\n\n        [[-0.3368,  0.0959, -0.0538]]], grad_fn=&lt;StackBackward&gt;)\n(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=&lt;StackBackward&gt;), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=&lt;StackBackward&gt;))\n\n</code></pre>"},{"location":"1.0/nlp_sequence_models_tutorial/#lstm_1","title":"\u4f8b\u5b50:\u7528LSTM\u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8","text":"<p>\u5728\u8fd9\u90e8\u5206, \u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a LSTM \u7f51\u7edc\u6765\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8. \u5728\u8fd9\u91cc\u6211\u4eec\u4e0d\u4f1a\u7528\u5230\u7ef4\u7279\u6bd4\u7b97\u6cd5, \u524d\u5411-\u540e\u5411\u7b97\u6cd5\u6216\u8005\u4efb\u4f55\u7c7b\u4f3c\u7684\u7b97\u6cd5,\u800c\u662f\u5c06\u8fd9\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u4e00\u4e2a (\u6709\u6311\u6218) \u7684\u7ec3\u4e60\u7559\u7ed9\u8bfb\u8005, \u5e0c\u671b\u8bfb\u8005\u5728\u4e86\u89e3\u4e86\u8fd9\u90e8\u5206\u7684\u5185\u5bb9\u540e\u80fd\u591f\u5b9e\u73b0\u5982\u4f55\u5c06\u7ef4\u7279\u6bd4\u7b97\u6cd5\u5e94\u7528\u5230 LSTM \u7f51\u7edc\u4e2d\u6765.</p> <p>\u8be5\u6a21\u578b\u5982\u4e0b:\u8f93\u5165\u7684\u53e5\u5b50\u662f \\(\\(w_1 ... w_M\\)\\) \u5bf9\u5e94\u7684\u8bcd\u6027\u4e3a \\(\\(y_1 ... y_M\\)\\) \uff0c\u7528 \\(\\(\\hat{y}_i\\)\\) \u8868\u793a\u5bf9\u5355\u8bcd \\(\\(w_i\\)\\) \u8bcd\u6027\u7684\u9884\u6d4b\uff0c\u6807\u7b7e\u7684\u96c6\u5408\u5b9a\u4e49\u4e3a \\(\\(T\\)\\)\u3002</p> <p>\u8fd9\u662f\u4e00\u4e2a\u7ed3\u6784\u9884\u6d4b\u6a21\u578b, \u6211\u4eec\u7684\u8f93\u51fa\u662f\u4e00\u4e2a\u5e8f\u5217\\(\\(\\hat{y}_1,...,\\hat{y}_M\\)\\), \u5176\u4e2d\\(\\(\\hat{y}_i\\in T\\)\\).</p> <p>\u5728\u8fdb\u884c\u9884\u6d4b\u65f6, \u9700\u5c06\u53e5\u5b50\u6bcf\u4e2a\u8bcd\u8f93\u5165\u5230\u4e00\u4e2a LSTM \u7f51\u7edc\u4e2d. \u5c06\u65f6\u523b\\(\\(i\\)\\)\u7684\u9690\u85cf\u72b6\u6001\u6807\u8bb0\u4e3a\\(\\(h_i\\)\\),\u540c\u6837\u5730, \u5bf9\u6bcf\u4e2a\u6807\u7b7e\u8d4b\u4e00\u4e2a\u72ec\u4e00\u65e0\u4e8c\u7684\u7d22\u5f15 (\u7c7b\u4f3c word embeddings \u90e8\u5206 word_to_ix \u7684\u8bbe\u7f6e). \u7136\u540e\u5c31\u5f97\u5230\u4e86\\(\\(\\hat{y}_i\\)\\)\u7684\u9884\u6d4b\u89c4\u5219\u3002</p> \\[\\hat{y}^i=argmaxj (logSoftmax(Ahi+b))j\\] <p>\u5373\u5148\u5bf9\u9690\u72b6\u6001\u8fdb\u884c\u4e00\u4e2a\u4eff\u5c04\u53d8\u6362, \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u5bf9\u6570 softmax, \u6700\u540e\u5f97\u5230\u7684\u9884\u6d4b\u6807\u7b7e\u5373\u4e3a\u5bf9\u6570 softmax \u4e2d\u6700\u5927\u7684\u503c\u5bf9\u5e94\u7684\u6807\u7b7e. \u6ce8\u610f, \u8fd9\u4e5f\u610f\u5473\u7740 \\(\\(A\\)\\) \u7a7a\u95f4\u7684\u7ef4\u5ea6\u662f \\(\\(|T|\\)\\).</p> <p>\u51c6\u5907\u6570\u636e:</p> <pre><code>def prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\ntraining_data = [\n    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n]\nword_to_ix = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\nprint(word_to_ix)\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n\n# \u5b9e\u9645\u4e2d\u901a\u5e38\u4f7f\u7528\u66f4\u5927\u7684\u7ef4\u5ea6\u598232\u7ef4, 64\u7ef4.\n# \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528\u5c0f\u7684\u7ef4\u5ea6, \u4e3a\u4e86\u65b9\u4fbf\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6743\u91cd\u7684\u53d8\u5316.\nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n\n</code></pre> <p>\u521b\u5efa\u6a21\u578b:</p> <pre><code>class LSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM\u4ee5word_embeddings\u4f5c\u4e3a\u8f93\u5165, \u8f93\u51fa\u7ef4\u5ea6\u4e3a hidden_dim \u7684\u9690\u85cf\u72b6\u6001\u503c\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n\n        # \u7ebf\u6027\u5c42\u5c06\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u6620\u5c04\u5230\u6807\u6ce8\u7a7a\u95f4\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # \u4e00\u5f00\u59cb\u5e76\u6ca1\u6709\u9690\u85cf\u72b6\u6001\u6240\u4ee5\u6211\u4eec\u8981\u5148\u521d\u59cb\u5316\u4e00\u4e2a\n        # \u5173\u4e8e\u7ef4\u5ea6\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u8bbe\u8ba1\u8bf7\u53c2\u8003Pytoch\u76f8\u5173\u6587\u6863\n        # \u5404\u4e2a\u7ef4\u5ea6\u7684\u542b\u4e49\u662f (num_layers, minibatch_size, hidden_dim)\n        return (torch.zeros(1, 1, self.hidden_dim),\n                torch.zeros(1, 1, self.hidden_dim))\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        lstm_out, self.hidden = self.lstm(\n            embeds.view(len(sentence), 1, -1), self.hidden)\n        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n        tag_scores = F.log_softmax(tag_space, dim=1)\n        return tag_scores\n\n</code></pre> <p>\u8bad\u7ec3\u6a21\u578b:</p> <pre><code>model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# \u67e5\u770b\u8bad\u7ec3\u524d\u7684\u5206\u6570\n# \u6ce8\u610f: \u8f93\u51fa\u7684 i,j \u5143\u7d20\u7684\u503c\u8868\u793a\u5355\u8bcd i \u7684 j \u6807\u7b7e\u7684\u5f97\u5206\n# \u8fd9\u91cc\u6211\u4eec\u4e0d\u9700\u8981\u8bad\u7ec3\u4e0d\u9700\u8981\u6c42\u5bfc\uff0c\u6240\u4ee5\u4f7f\u7528torch.no_grad()\nwith torch.no_grad():\n    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n    print(tag_scores)\n\nfor epoch in range(300):  # \u5b9e\u9645\u60c5\u51b5\u4e0b\u4f60\u4e0d\u4f1a\u8bad\u7ec3300\u4e2a\u5468\u671f, \u6b64\u4f8b\u4e2d\u6211\u4eec\u53ea\u662f\u968f\u4fbf\u8bbe\u4e86\u4e00\u4e2a\u503c\n    for sentence, tags in training_data:\n        # \u7b2c\u4e00\u6b65: \u8bf7\u8bb0\u4f4fPytorch\u4f1a\u7d2f\u52a0\u68af\u5ea6.\n        # \u6211\u4eec\u9700\u8981\u5728\u8bad\u7ec3\u6bcf\u4e2a\u5b9e\u4f8b\u524d\u6e05\u7a7a\u68af\u5ea6\n        model.zero_grad()\n\n        # \u6b64\u5916\u8fd8\u9700\u8981\u6e05\u7a7a LSTM \u7684\u9690\u72b6\u6001,\n        # \u5c06\u5176\u4ece\u4e0a\u4e2a\u5b9e\u4f8b\u7684\u5386\u53f2\u4e2d\u5206\u79bb\u51fa\u6765.\n        model.hidden = model.init_hidden()\n\n        # \u51c6\u5907\u7f51\u7edc\u8f93\u5165, \u5c06\u5176\u53d8\u4e3a\u8bcd\u7d22\u5f15\u7684 Tensor \u7c7b\u578b\u6570\u636e\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = prepare_sequence(tags, tag_to_ix)\n\n        # \u7b2c\u4e09\u6b65: \u524d\u5411\u4f20\u64ad.\n        tag_scores = model(sentence_in)\n\n        # \u7b2c\u56db\u6b65: \u8ba1\u7b97\u635f\u5931\u548c\u68af\u5ea6\u503c, \u901a\u8fc7\u8c03\u7528 optimizer.step() \u6765\u66f4\u65b0\u68af\u5ea6\n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n\n# \u67e5\u770b\u8bad\u7ec3\u540e\u7684\u5f97\u5206\nwith torch.no_grad():\n    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n\n    # \u53e5\u5b50\u662f \"the dog ate the apple\", i,j \u8868\u793a\u5bf9\u4e8e\u5355\u8bcd i, \u6807\u7b7e j \u7684\u5f97\u5206.\n    # \u6211\u4eec\u91c7\u7528\u5f97\u5206\u6700\u9ad8\u7684\u6807\u7b7e\u4f5c\u4e3a\u9884\u6d4b\u7684\u6807\u7b7e. \u4ece\u4e0b\u9762\u7684\u8f93\u51fa\u6211\u4eec\u53ef\u4ee5\u770b\u5230, \u9884\u6d4b\u5f97\n    # \u5230\u7684\u7ed3\u679c\u662f0 1 2 0 1. \u56e0\u4e3a \u7d22\u5f15\u662f\u4ece0\u5f00\u59cb\u7684, \u56e0\u6b64\u7b2c\u4e00\u4e2a\u503c0\u8868\u793a\u7b2c\u4e00\u884c\u7684\n    # \u6700\u5927\u503c, \u7b2c\u4e8c\u4e2a\u503c1\u8868\u793a\u7b2c\u4e8c\u884c\u7684\u6700\u5927\u503c, \u4ee5\u6b64\u7c7b\u63a8. \u6240\u4ee5\u6700\u540e\u7684\u7ed3\u679c\u662f DET\n    # NOUN VERB DET NOUN, \u6574\u4e2a\u5e8f\u5217\u90fd\u662f\u6b63\u786e\u7684!\n    print(tag_scores)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[-1.1389, -1.2024, -0.9693],\n        [-1.1065, -1.2200, -0.9834],\n        [-1.1286, -1.2093, -0.9726],\n        [-1.1190, -1.1960, -0.9916],\n        [-1.0137, -1.2642, -1.0366]])\ntensor([[-0.0858, -2.9355, -3.5374],\n        [-5.2313, -0.0234, -4.0314],\n        [-3.9098, -4.1279, -0.0368],\n        [-0.0187, -4.7809, -4.5960],\n        [-5.8170, -0.0183, -4.1879]])\n\n</code></pre>"},{"location":"1.0/nlp_sequence_models_tutorial/#lstm_2","title":"\u7ec3\u4e60:\u4f7f\u7528\u5b57\u7b26\u7ea7\u7279\u5f81\u6765\u589e\u5f3aLSTM\u8bcd\u6027\u6807\u6ce8\u5668","text":"<p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d, \u6bcf\u4e2a\u8bcd\u90fd\u6709\u4e00\u4e2a\u8bcd\u5d4c\u5165, \u4f5c\u4e3a\u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165. \u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u4f7f\u7528\u6bcf\u4e2a\u7684\u5355\u8bcd\u7684 \u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe\u6765\u589e\u5f3a\u8bcd\u5d4c\u5165\u3002 \u6211\u4eec\u671f\u671b\u8fd9\u4e2a\u64cd\u4f5c\u5bf9\u7ed3\u679c\u80fd\u6709\u663e\u8457\u63d0\u5347, \u56e0\u4e3a\u50cf\u8bcd\u7f00\u8fd9\u6837\u7684\u5b57\u7b26\u7ea7 \u4fe1\u606f\u5bf9\u4e8e\u8bcd\u6027\u6709\u5f88\u5927\u7684\u5f71\u54cd\u3002\u6bd4\u5982\u8bf4, \u50cf\u5305\u542b\u8bcd\u7f00 -ly \u7684\u5355\u8bcd\u57fa\u672c\u4e0a\u90fd\u662f\u88ab\u6807\u6ce8\u4e3a\u526f\u8bcd.</p> <p>\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b. \u7528\\(\\(c_w\\)\\)\u7684\u5b57\u7b26\u7ea7\u8868\u8fbe, \u540c\u4e4b\u524d\u4e00\u6837, \u6211\u4eec\u4f7f\u7528\\(\\(x_w\\)\\)\u6765\u8868\u793a\u8bcd\u5d4c\u5165. \u5e8f\u5217\u6a21\u578b\u7684\u8f93\u5165\u5c31\u53d8\u6210\u4e86\\(\\(x_w\\)\\)\u548c\\(\\(c_w\\)\\)\u7684\u62fc\u63a5. \u56e0\u6b64, \u5982\u679c  \u7684\u7ef4\u5ea6\\(\\(x_w\\)\\)\u662f5,  \u7684\u7ef4\u5ea6\\(\\(c_w\\)\\)\u662f3, \u90a3\u4e48\u6211\u4eec\u7684 LSTM \u7f51\u7edc\u7684\u8f93\u5165\u7ef4\u5ea6\u5927\u5c0f\u5c31\u662f8.</p> <p>\u4e3a\u4e86\u5f97\u5230\u5b57\u7b26\u7ea7\u522b\u7684\u8868\u8fbe, \u5c06\u5355\u8bcd\u7684\u6bcf\u4e2a\u5b57\u7b26\u8f93\u5165\u4e00\u4e2a LSTM \u7f51\u7edc, \u800c\\(\\(c_w\\)\\)\u5219\u4e3a\u8fd9\u4e2a LSTM \u7f51\u7edc\u6700\u540e\u7684\u9690\u72b6\u6001\u3002 \u4e00\u4e9b\u63d0\u793a\uff1a</p> <ul> <li>\u65b0\u6a21\u578b\u4e2d\u9700\u8981\u4e24\u4e2a LSTM, \u4e00\u4e2a\u8ddf\u4e4b\u524d\u4e00\u6837, \u7528\u6765\u8f93\u51fa\u8bcd\u6027\u6807\u6ce8\u7684\u5f97\u5206, \u53e6\u5916\u4e00\u4e2a\u65b0\u589e\u52a0\u7684\u7528\u6765 \u83b7\u53d6\u6bcf\u4e2a\u5355\u8bcd\u7684\u5b57\u7b26\u7ea7\u522b\u8868\u8fbe\u3002</li> <li>\u4e3a\u4e86\u5728\u5b57\u7b26\u7ea7\u522b\u4e0a\u8fd0\u884c\u5e8f\u5217\u6a21\u578b, \u4f60\u9700\u8981\u7528\u5d4c\u5165\u7684\u5b57\u7b26\u6765\u4f5c\u4e3a\u5b57\u7b26 LSTM \u7684\u8f93\u5165\u3002</li> </ul>"},{"location":"1.0/nlp_word_embeddings_tutorial/","title":"\u8bcd\u5d4c\u5165\uff1a\u7f16\u7801\u5f62\u5f0f\u7684\u8bcd\u6c47\u8bed\u4e49","text":"<p>\u8bd1\u8005\uff1a\u5de9\u5b50\u60e0</p> <p>\u8bcd\u5d4c\u5165\u662f\u4e00\u79cd\u7531\u771f\u5b9e\u6570\u5b57\u7ec4\u6210\u7684\u7a20\u5bc6\u5411\u91cf\uff0c\u6bcf\u4e2a\u5411\u91cf\u90fd\u4ee3\u8868\u4e86\u5355\u8bcd\u8868\u91cc\u7684\u4e00\u4e2a\u5355\u8bcd\u3002 \u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\uff0c\u603b\u4f1a\u9047\u5230\u8fd9\u6837\u7684\u60c5\u51b5\uff1a\u7279\u5f81\u5168\u662f\u5355\u8bcd\uff01\u4f46\u662f\uff0c\u5982\u4f55\u5728\u7535\u8111\u4e0a\u8868\u8ff0\u4e00\u4e2a\u5355\u8bcd\u5462\uff1f\u4f60\u5728\u7535\u8111\u4e0a\u5b58\u50a8\u7684\u5355\u8bcd\u7684<code>ascii</code>\u7801\uff0c\u4f46\u662f\u5b83\u4ec5\u4ec5\u4ee3\u8868\u5355\u8bcd\u600e\u4e48\u62fc\u5199\uff0c\u6ca1\u6709\u8bf4\u660e\u5355\u8bcd\u7684\u5185\u5728\u542b\u4e49(\u4f60\u4e5f\u8bb8\u80fd\u591f\u4ece\u8bcd\u7f00\u4e2d\u4e86\u89e3\u5b83\u7684\u8bcd\u6027\uff0c\u6216\u8005\u4ece\u5927\u5c0f\u5199\u4e2d\u5f97\u5230\u4e00\u4e9b\u5c5e\u6027\uff0c\u4f46\u4ec5\u6b64\u800c\u5df2)\u3002 \u66f4\u91cd\u8981\u7684\u662f\uff0c\u4f60\u80fd\u628a\u8fd9\u4e9b<code>ascii</code>\u7801\u5b57\u7b26\u7ec4\u5408\u6210\u4ec0\u4e48\u542b\u4e49\uff1f\u5f53\u4ee3\u8868\u8bcd\u6c47\u8868\u3001\u8f93\u5165\u6570\u636e\u662f\u7ef4\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5f80\u5f80\u60f3\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f97\u5230\u6570\u636e\u5bc6\u96c6\u7684\u7ed3\u679c\uff0c\u4f46\u662f\u7ed3\u679c\u53ea\u6709\u5f88\u5c11\u7684\u51e0\u4e2a\u7ef4\u5ea6(\u4f8b\u5982\uff0c\u9884\u6d4b\u7684\u6570\u636e\u53ea\u6709\u51e0\u4e2a\u6807\u7b7e\u65f6\uff09\u3002\u6211\u4eec\u5982\u4f55\u4ece\u5927\u7684\u6570\u636e\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u5f97\u5230\u7a0d\u5c0f\u4e00\u70b9\u7684\u7ef4\u5ea6\u7a7a\u95f4\uff1f</p> <p>\u653e\u5f03\u4f7f\u7528<code>ascii</code>\u7801\u5b57\u7b26\u7684\u5f62\u5f0f\u8868\u793a\u5355\u8bcd\uff0c\u6362\u7528<code>one-hot encoding</code>\u4f1a\u600e\u4e48\u6837\u4e86\uff1f\u597d\u5427\uff0c\u8fd9\u4e2a\u5355\u8bcd\u5c31\u80fd\u8fd9\u6837\u8868\u793a\uff1a</p> <p></p> <p>\u5176\u4e2d\uff0c1 \u8868\u793a\u7684\u72ec\u6709\u4f4d\u7f6e\uff0c\u5176\u4ed6\u4f4d\u7f6e\u5168\u662f0\u3002\u5176\u4ed6\u7684\u8bcd\u90fd\u7c7b\u4f3c\uff0c\u5728\u53e6\u5916\u4e0d\u4e00\u6837\u7684\u4f4d\u7f6e\u6709\u4e00\u4e2a1\u4ee3\u8868\u5b83\uff0c\u5176\u4ed6\u4f4d\u7f6e\u4e5f\u90fd\u662f0\u3002 \u8fd9\u79cd\u8868\u8fbe\u9664\u4e86\u5360\u7528\u5de8\u5927\u7684\u7a7a\u95f4\u5916\uff0c\u8fd8\u6709\u4e2a\u5f88\u5927\u7684\u7f3a\u9677\u3002 \u5b83\u53ea\u662f\u7b80\u5355\u7684\u628a\u8bcd\u770b\u505a\u4e00\u4e2a\u5355\u72ec\u4e2a\u4f53\uff0c\u8ba4\u4e3a\u5b83\u4eec\u4e4b\u95f4\u6beb\u65e0\u8054\u7cfb\u3002 \u6211\u4eec\u771f\u6b63\u60f3\u8981\u7684\u662f\u80fd\u591f\u8868\u8fbe\u5355\u8bcd\u4e4b\u95f4\u4e00\u4e9b\u76f8\u4f3c\u7684\u542b\u4e49\u3002\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u505a\u5462\uff1f\u6765\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\uff1a</p> <p>\u5047\u5982\u6211\u4eec\u6b63\u5728\u642d\u5efa\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u4e0b\u9762\u4e00\u4e9b\u53e5\u5b50\uff1a</p> <ul> <li>The mathematician ran to the store.</li> <li>The physicist ran to the store.</li> <li>The mathematician solved the open problem.</li> </ul> <p>\u73b0\u5728\u53c8\u5f97\u5230\u4e00\u4e2a\u6ca1\u89c1\u8fc7\u7684\u65b0\u53e5\u5b50:</p> <ul> <li>The physicist solved the open problem.</li> </ul> <p>\u6211\u4eec\u7684\u6a21\u578b\u53ef\u80fd\u5728\u8fd9\u4e2a\u53e5\u5b50\u4e0a\u8868\u73b0\u7684\u8fd8\u4e0d\u9519\uff0c\u4f46\u662f\uff0c\u5982\u679c\u5229\u7528\u4e86\u4e0b\u9762\u4e24\u4e2a\u4e8b\u5b9e\uff0c\u6a21\u578b\u4f1a\u8868\u73b0\u66f4\u4f73\uff1a</p> <ul> <li>\u6211\u4eec\u53d1\u73b0\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u53e5\u5b50\u91cc\u6709\u76f8\u540c\u7684\u4f5c\u7528\uff0c\u6240\u4ee5\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\uff0c\u4ed6\u4eec\u6709\u8bed\u4e49\u7684\u8054\u7cfb\u3002</li> <li>\u5f53\u770b\u89c1\u7269\u7406\u5b66\u5bb6\u5728\u65b0\u53e5\u5b50\u4e2d\u7684\u4f5c\u7528\u65f6\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u5b66\u5bb6\u4e5f\u6709\u8d77\u7740\u76f8\u540c\u7684\u4f5c\u7528\u3002</li> </ul> <p>\u7136\u540e\u6211\u4eec\u5c31\u63a8\u6d4b\uff0c\u7269\u7406\u5b66\u5bb6\u5728\u4e0a\u9762\u7684\u53e5\u5b50\u91cc\u4e5f\u7c7b\u4f3c\u4e8e\u6570\u5b66\u5bb6\u5417\uff1f \u8fd9\u5c31\u662f\u6211\u4eec\u6240\u6307\u7684\u76f8\u4f3c\u6027\u7406\u5ff5\uff1a \u6307\u7684\u662f\u8bed\u4e49\u76f8\u4f3c\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u62fc\u5199\u76f8\u4f3c\u3002 \u8fd9\u5c31\u662f\u4e00\u79cd\u901a\u8fc7\u8fde\u63a5\u6211\u4eec\u53d1\u73b0\u7684\u548c\u6ca1\u53d1\u73b0\u7684\u4e00\u4e9b\u5185\u5bb9\u76f8\u4f3c\u70b9\u3001\u7528\u4e8e\u89e3\u51b3\u8bed\u8a00\u6570\u636e\u7a00\u758f\u6027\u7684\u6280\u672f\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u57fa\u672c\u7684\u8bed\u8a00\u5047\u8bbe\uff1a \u90a3\u4e9b\u5728\u76f8\u4f3c\u8bed\u53e5\u4e2d\u51fa\u73b0\u7684\u5355\u8bcd\uff0c\u5728\u8bed\u4e49\u4e0a\u4e5f\u662f\u76f8\u4e92\u5173\u8054\u7684\u3002 \u8fd9\u5c31\u53eb\u505a distributional hypothesis(\u5206\u5e03\u5f0f\u5047\u8bbe\uff09\u3002</p>"},{"location":"1.0/nlp_word_embeddings_tutorial/#getting-dense-word-embeddings","title":"Getting Dense Word Embeddings(\u5bc6\u96c6\u8bcd\u5d4c\u5165\uff09","text":"<p>\u6211\u4eec\u5982\u4f55\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u5462\uff1f\u4e5f\u5c31\u662f\uff0c\u600e\u4e48\u7f16\u7801\u5355\u8bcd\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff1f \u4e5f\u8bb8\u6211\u4eec\u4f1a\u60f3\u5230\u4e00\u4e9b\u8bed\u4e49\u5c5e\u6027\u3002 \u4e3e\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u90fd\u80fd\u8dd1\uff0c \u6240\u4ee5\u4e5f\u8bb8\u53ef\u4ee5\u7ed9\u542b\u6709\u201c\u80fd\u8dd1\u201d\u8bed\u4e49\u5c5e\u6027\u7684\u5355\u8bcd\u6253\u9ad8\u5206\uff0c\u8003\u8651\u4e00\u4e0b\u5176\u4ed6\u7684\u5c5e\u6027\uff0c\u60f3\u8c61\u4e00\u4e0b\u4f60\u53ef\u80fd\u4f1a\u5728\u8fd9\u4e9b\u5c5e\u6027\u4e0a\u7ed9\u666e\u901a\u7684\u5355\u8bcd\u6253\u4ec0\u4e48\u5206\u3002</p> <p>\u5982\u679c\u6bcf\u4e2a\u5c5e\u6027\u90fd\u8868\u793a\u4e00\u4e2a\u7ef4\u5ea6\uff0c\u90a3\u6211\u4eec\u4e5f\u8bb8\u53ef\u4ee5\u7528\u4e00\u4e2a\u5411\u91cf\u8868\u793a\u4e00\u4e2a\u5355\u8bcd\uff0c\u5c31\u50cf\u8fd9\u6837\uff1a</p> <p></p> <p></p> <p>\u90a3\u4e48\uff0c\u6211\u4eec\u5c31\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u7684\u65b9\u6cd5\u5f97\u5230\u8fd9\u4e9b\u5355\u8bcd\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff1a</p> <p></p> <p>\u5c3d\u7ba1\u901a\u5e38\u60c5\u51b5\u4e0b\u9700\u8981\u8fdb\u884c\u957f\u5ea6\u5f52\u4e00\u5316\uff1a</p> <p></p> <p>\u662f\u4e24\u4e2a\u5411\u91cf\u7684\u5939\u89d2\u3002 \u8fd9\u5c31\u610f\u5473\u7740\uff0c\u5b8c\u5168\u76f8\u4f3c\u7684\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4e3a1\u3002\u5b8c\u5168\u4e0d\u76f8\u4f3c\u7684\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4e3a-1\u3002</p> <p>\u4f60\u53ef\u4ee5\u628a\u672c\u7ae0\u5f00\u5934\u4ecb\u7ecd\u7684<code>one-hot</code>\u7a00\u758f\u5411\u91cf\u770b\u505a\u662f\u6211\u4eec\u65b0\u5b9a\u4e49\u5411\u91cf\u7684\u4e00\u79cd\u7279\u6b8a\u5f62\u5f0f\uff0c\u90a3\u91cc\u7684\u5355\u8bcd\u76f8\u4f3c\u5ea6\u4e3a0\uff0c \u73b0\u5728\u6211\u4eec\u7ed9\u6bcf\u4e2a\u5355\u8bcd\u4e00\u4e9b\u72ec\u7279\u7684\u8bed\u4e49\u5c5e\u6027\u3002 \u8fd9\u4e9b\u5411\u91cf\u6570\u636e\u5bc6\u96c6\uff0c\u4e5f\u5c31\u662f\u8bf4\u5b83\u4eec\u6570\u5b57\u901a\u5e38\u90fd\u975e\u96f6\u3002</p> <p>\u4f46\u662f\u65b0\u7684\u8fd9\u4e9b\u5411\u91cf\u5b58\u5728\u4e00\u4e2a\u4e25\u91cd\u7684\u95ee\u9898\uff1a \u4f60\u53ef\u4ee5\u60f3\u5230\u6570\u5343\u79cd\u4e0d\u540c\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u5b83\u4eec\u53ef\u80fd\u90fd\u4e0e\u51b3\u5b9a\u76f8\u4f3c\u6027\u6709\u5173\uff0c\u800c\u4e14\uff0c\u5230\u5e95\u5982\u4f55\u8bbe\u7f6e\u4e0d\u540c\u5c5e\u6027\u7684\u503c\u5462\uff1f\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u5fc3\u601d\u60f3\u662f\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u7279\u5f81\u7684\u8868\u793a\uff0c\u800c\u4e0d\u662f\u7a0b\u5e8f\u5458\u53bb\u8bbe\u8ba1\u5b83\u4eec\u3002 \u6240\u4ee5\u4e3a\u4ec0\u4e48\u4e0d\u628a\u8bcd\u5d4c\u5165\u53ea\u5f53\u505a\u6a21\u578b\u53c2\u6570\uff0c\u800c\u662f\u901a\u8fc7\u8bad\u7ec3\u6765\u66f4\u65b0\u5462\uff1f \u8fd9\u5c31\u624d\u662f\u6211\u4eec\u8981\u786e\u5207\u505a\u7684\u4e8b\u3002\u6211\u4eec\u5c06\u7528\u795e\u7ecf\u7f51\u7edc\u505a\u4e00\u4e9b\u6f5c\u5728\u8bed\u4e49\u5c5e\u6027\uff0c\u4f46\u662f\u539f\u5219\u4e0a\uff0c\u5b66\u4e60\u624d\u662f\u5173\u952e\u3002 \u6ce8\u610f\uff0c\u8bcd\u5d4c\u5165\u53ef\u80fd\u65e0\u6cd5\u89e3\u91ca\u3002\u5c31\u662f\u8bf4\uff0c\u5c3d\u7ba1\u4f7f\u7528\u6211\u4eec\u4e0a\u9762\u624b\u52a8\u5236\u4f5c\u7684\u5411\u91cf\uff0c\u80fd\u591f\u53d1\u73b0\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u90fd\u559c\u6b22\u559d\u5496\u5561\u7684\u76f8\u4f3c\u6027\uff0c \u5982\u679c\u6211\u4eec\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u8bcd\u5d4c\u5165\uff0c\u90a3\u4e48\u5c31\u4f1a\u53d1\u73b0\u6570\u5b66\u5bb6\u548c\u7269\u7406\u5b66\u5bb6\u5728\u7b2c\u4e8c\u7ef4\u5ea6\u6709\u4e2a\u8f83\u5927\u7684\u503c\uff0c\u5b83\u6240\u4ee3\u8868\u7684\u542b\u4e49\u5f88\u4e0d\u6e05\u6670\u3002 \u5b83\u4eec\u5728\u4e00\u4e9b\u6f5c\u5728\u8bed\u4e49\u4e0a\u662f\u76f8\u4f3c\u7684\uff0c\u4f46\u662f\u5bf9\u6211\u4eec\u6765\u8bf4\u65e0\u6cd5\u89e3\u91ca\u3002</p> <p>\u603b\u7ed3\u4e00\u4e0b\uff0c\u8bcd\u5d4c\u5165\u662f\u5355\u8bcd\u8bed\u4e49\u7684\u8868\u793a\uff0c\u6709\u6548\u5730\u7f16\u7801\u8bed\u4e49\u4fe1\u606f\u53ef\u80fd\u4e0e\u624b\u5934\u7684\u4efb\u52a1\u6709\u5173\u3002\u4f60\u4e5f\u53ef\u4ee5\u5d4c\u5165\u5176\u4ed6\u7684\u4e1c\u897f\uff1a\u8bed\u97f3\u6807\u7b7e\uff0c\u89e3\u6790\u6811\uff0c\u5176\u4ed6\u4efb\u4f55\u4e1c\u897f\uff01\u7279\u5f81\u5d4c\u5165\u662f\u8fd9\u4e2a\u9886\u57df\u7684\u6838\u5fc3\u601d\u60f3\u3002</p>"},{"location":"1.0/nlp_word_embeddings_tutorial/#pytorch","title":"Pytorch\u4e2d\u7684\u8bcd\u5d4c\u5165","text":"<p>\u5728\u6211\u4eec\u4e3e\u4f8b\u6216\u7ec3\u4e60\u4e4b\u524d\uff0c\u8fd9\u91cc\u6709\u4e00\u4efd\u5173\u4e8e\u5982\u4f55\u5728<code>Pytorch</code>\u548c\u5e38\u89c1\u7684\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4f7f\u7528\u8bcd\u5d4c\u5165\u7684\u7b80\u8981\u4ecb\u7ecd\u3002 \u4e0e\u5236\u4f5c<code>one-hot</code>\u5411\u91cf\u65f6\u5bf9\u6bcf\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7279\u6b8a\u7684\u7d22\u5f15\u7c7b\u4f3c\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u8bcd\u5411\u91cf\u65f6\u4e5f\u9700\u8981\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u5b9a\u4e49\u4e00\u4e2a\u7d22\u5f15\u3002\u8fd9\u4e9b\u7d22\u5f15\u5c06\u662f\u67e5\u8be2\u8868\u7684\u5173\u952e\u70b9\u3002\u610f\u601d\u5c31\u662f\uff0c\u8bcd\u5d4c\u5165\u88ab\u88ab\u5b58\u50a8\u5728\u4e00\u4e2a\u7684\u5411\u91cf\u4e2d\uff0c\u5176\u4e2d\u662f\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u3002\u8bcd\u88ab\u88ab\u5206\u914d\u7684\u7d22\u5f15<code>i</code>\uff0c\u8868\u793a\u5728\u5411\u91cf\u7684\u7b2c<code>i</code>\u884c\u5b58\u50a8\u5b83\u7684\u5d4c\u5165\u3002\u5728\u6240\u6709\u7684\u4ee3\u7801\u4e2d\uff0c\u4ece\u5355\u8bcd\u5230\u7d22\u5f15\u7684\u6620\u5c04\u662f\u4e00\u4e2a\u53eb<code>word_to_ix</code>\u7684\u5b57\u5178\u3002</p> <p>\u80fd\u4f7f\u7528\u8bcd\u5d4c\u5165\u7684\u6a21\u5757\u662f<code>torch.nn.Embedding</code>\uff0c\u8fd9\u91cc\u9762\u6709\u4e24\u4e2a\u53c2\u6570\uff1a\u8bcd\u6c47\u8868\u7684\u5927\u5c0f\u548c\u8bcd\u5d4c\u5165\u7684\u7ef4\u5ea6\u3002</p> <p>\u7d22\u5f15\u8fd9\u5f20\u8868\u65f6\uff0c\u4f60\u5fc5\u987b\u4f7f\u7528<code>torch.LongTensor</code>(\u56e0\u4e3a\u7d22\u5f15\u662f\u6574\u6570\uff0c\u4e0d\u662f\u6d6e\u70b9\u6570\uff09\u3002</p> <pre><code># \u4f5c\u8005: Robert Guthrie\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\n</code></pre> <pre><code>word_to_ix = {\"hello\": 0, \"world\": 1}\nembeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\nlookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\nhello_embed = embeds(lookup_tensor)\nprint(hello_embed)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n       grad_fn=&lt;EmbeddingBackward&gt;)\n\n</code></pre>"},{"location":"1.0/nlp_word_embeddings_tutorial/#n-gram","title":"\u4f8b\u5b50\uff1a N-Gram\u8bed\u8a00\u6a21\u578b","text":"<p>\u56de\u60f3\u4e00\u4e0b\uff0c\u5728<code>n-gram</code>\u8bed\u8a00\u6a21\u578b\u4e2d,\u7ed9\u5b9a\u4e00\u4e2a\u5355\u8bcd\u5e8f\u5217\u5411\u91cf\uff0c\u6211\u4eec\u8981\u8ba1\u7b97\u7684\u662f</p> <p></p> <p>\u662f\u5355\u8bcd\u5e8f\u5217\u7684\u7b2c<code>i</code>\u4e2a\u5355\u8bcd\u3002 \u5728\u672c\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u5728\u8bad\u7ec3\u6837\u4f8b\u4e0a\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4e14\u7528\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u66f4\u65b0\u53c2\u6570\u3002</p> <pre><code>CONTEXT_SIZE = 2\nEMBEDDING_DIM = 10\n# \u6211\u4eec\u7528\u838e\u58eb\u6bd4\u4e9a\u7684\u5341\u56db\u884c\u8bd7 Sonnet 2\ntest_sentence = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\".split()\n# \u5e94\u8be5\u5bf9\u8f93\u5165\u53d8\u91cf\u8fdb\u884c\u6807\u8bb0\uff0c\u4f46\u6682\u65f6\u5ffd\u7565\u3002\n# \u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5143\u7ec4\uff0c\u6bcf\u4e2a\u5143\u7ec4\u90fd\u662f([ word_i-2, word_i-1 ], target word)\u7684\u5f62\u5f0f\u3002\ntrigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\n# \u8f93\u51fa\u524d3\u884c\uff0c\u5148\u770b\u4e0b\u662f\u4ec0\u4e48\u6837\u5b50\u3002\nprint(trigrams[:3])\n\nvocab = set(test_sentence)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n\nclass NGramLanguageModeler(nn.Module):\n\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(NGramLanguageModeler, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs).view((1, -1))\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        log_probs = F.log_softmax(out, dim=1)\n        return log_probs\n\nlosses = []\nloss_function = nn.NLLLoss()\nmodel = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(10):\n    total_loss = 0\n    for context, target in trigrams:\n\n        # \u6b65\u9aa4 1\\. \u51c6\u5907\u597d\u8fdb\u5165\u6a21\u578b\u7684\u6570\u636e (\u4f8b\u5982\u5c06\u5355\u8bcd\u8f6c\u6362\u6210\u6574\u6570\u7d22\u5f15,\u5e76\u5c06\u5176\u5c01\u88c5\u5728\u53d8\u91cf\u4e2d)\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n\n        # \u6b65\u9aa4 2\\. \u56de\u8c03torch\u7d2f\u4e58\u68af\u5ea6\n        # \u5728\u4f20\u5165\u4e00\u4e2a\u65b0\u5b9e\u4f8b\u4e4b\u524d\uff0c\u9700\u8981\u628a\u65e7\u5b9e\u4f8b\u7684\u68af\u5ea6\u7f6e\u96f6\u3002\n        model.zero_grad()\n\n        # \u6b65\u9aa4 3\\. \u7ee7\u7eed\u8fd0\u884c\u4ee3\u7801\uff0c\u5f97\u5230\u5355\u8bcd\u7684log\u6982\u7387\u503c\u3002\n        log_probs = model(context_idxs)\n\n        # \u6b65\u9aa4 4\\. \u8ba1\u7b97\u635f\u5931\u51fd\u6570(\u518d\u6b21\u6ce8\u610f\uff0cTorch\u9700\u8981\u5c06\u76ee\u6807\u5355\u8bcd\u5c01\u88c5\u5728\u53d8\u91cf\u91cc\uff09\u3002\n        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n\n        # \u6b65\u9aa4 5\\. \u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6\n        loss.backward()\n        optimizer.step()\n\n        # \u901a\u8fc7\u8c03tensor.item()\u5f97\u5230\u5355\u4e2aPython\u6570\u503c\u3002\n        total_loss += loss.item()\n    losses.append(total_loss)\nprint(losses)  # \u7528\u8bad\u7ec3\u6570\u636e\u6bcf\u6b21\u8fed\u4ee3\uff0c\u635f\u5931\u51fd\u6570\u90fd\u4f1a\u4e0b\u964d\u3002\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n[518.6343855857849, 516.0739576816559, 513.5321269035339, 511.0085496902466, 508.5003893375397, 506.0077188014984, 503.52977323532104, 501.06553316116333, 498.6121823787689, 496.16915798187256]\n\n</code></pre>"},{"location":"1.0/nlp_word_embeddings_tutorial/#_2","title":"\u7ec3\u4e60\uff1a\u8ba1\u7b97\u8fde\u7eed\u8bcd\u888b\u6a21\u578b\u7684\u8bcd\u5411\u91cf","text":"<p>\u8fde\u7eed\u8bcd\u888b\u6a21\u578b(<code>CBOW</code>\uff09\u5728<code>NLP</code>\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4f7f\u7528\u5f88\u9891\u7e41\u3002\u5b83\u662f\u4e00\u4e2a\u6a21\u578b\uff0c\u5c1d\u8bd5\u901a\u8fc7\u76ee\u6807\u8bcd\u524d\u540e\u51e0\u4e2a\u5355\u8bcd\u7684\u6587\u672c\uff0c\u6765\u9884\u6d4b\u76ee\u6807\u8bcd\u3002\u8fd9\u6709\u522b\u4e8e\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u4e3a<code>CBOW</code>\u4e0d\u662f\u5e8f\u5217\u7684\uff0c\u4e5f\u4e0d\u5fc5\u662f\u6982\u7387\u6027\u7684\u3002 <code>CBOW</code>\u5e38\u7528\u4e8e\u5feb\u901f\u5730\u8bad\u7ec3\u8bcd\u5411\u91cf\uff0c\u5f97\u5230\u7684\u5d4c\u5165\u7528\u6765\u521d\u59cb\u5316\u4e00\u4e9b\u590d\u6742\u6a21\u578b\u7684\u5d4c\u5165\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u8fd9\u88ab\u79f0\u4e3a<code>\u9884\u8bad\u7ec3\u5d4c\u5165</code>\u3002 \u5b83\u51e0\u4e4e\u603b\u80fd\u5e2e\u5fd9\u628a\u6a21\u578b\u6027\u80fd\u63d0\u5347\u51e0\u4e2a\u767e\u5206\u70b9\u3002</p> <p><code>CBOW</code>\u6a21\u578b\u5982\u4e0b\u6240\u793a\uff1a \u7ed9\u5b9a\u4e00\u4e2a\u5355\u8bcd \uff0c\u4ee3\u8868\u4e24\u8fb9\u7684\u6ed1\u7a97\u8ddd\uff0c\u5982\u548c\uff0c\u5e76\u5c06\u6240\u6709\u7684\u4e0a\u4e0b\u6587\u8bcd\u7edf\u79f0\u4e3a \uff0c<code>CBOW</code>\u8bd5\u56fe\u6700\u5c0f\u5316</p> <p></p> <p>\u5176\u4e2d\u662f\u5355\u8bcd\u7684\u5d4c\u5165\u3002</p> <p>\u5728<code>Pytorch</code>\u4e2d\uff0c\u901a\u8fc7\u586b\u5145\u4e0b\u9762\u7684\u7c7b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6a21\u578b\uff0c\u6709\u4e24\u6761\u9700\u8981\u6ce8\u610f\uff1a</p> <ul> <li>\u8003\u8651\u4e0b\u4f60\u9700\u8981\u5b9a\u4e49\u54ea\u4e9b\u53c2\u6570\u3002</li> <li>\u786e\u4fdd\u4f60\u77e5\u9053\u6bcf\u6b65\u64cd\u4f5c\u540e\u7684\u7ed3\u6784\uff0c\u5982\u679c\u60f3\u91cd\u6784\uff0c\u8bf7\u4f7f\u7528<code>.view()</code></li> </ul> <pre><code>CONTEXT_SIZE = 2  # \u5de6\u53f3\u5404\u4e24\u4e2a\u8bcd\nraw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\".split()\n\n# \u901a\u8fc7\u5bf9`raw_text`\u4f7f\u7528set()\u51fd\u6570\uff0c\u6211\u4eec\u8fdb\u884c\u53bb\u91cd\u64cd\u4f5c\nvocab = set(raw_text)\nvocab_size = len(vocab)\n\nword_to_ix = {word: i for i, word in enumerate(vocab)}\ndata = []\nfor i in range(2, len(raw_text) - 2):\n    context = [raw_text[i - 2], raw_text[i - 1],\n               raw_text[i + 1], raw_text[i + 2]]\n    target = raw_text[i]\n    data.append((context, target))\nprint(data[:5])\n\nclass CBOW(nn.Module):\n\n    def __init__(self):\n        pass\n\n    def forward(self, inputs):\n        pass\n\n# \u521b\u5efa\u6a21\u578b\u5e76\u4e14\u8bad\u7ec3\u3002\u8fd9\u91cc\u6709\u4e9b\u51fd\u6570\u5e2e\u4f60\u5728\u4f7f\u7528\u6a21\u5757\u4e4b\u524d\u5236\u4f5c\u6570\u636e\u3002\n\ndef make_context_vector(context, word_to_ix):\n    idxs = [word_to_ix[w] for w in context]\n    return torch.tensor(idxs, dtype=torch.long)\n\nmake_context_vector(data[0][0], word_to_ix)  # example\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n\n</code></pre>"},{"location":"1.0/nn/","title":"torch.nn","text":""},{"location":"1.0/nn/#parameters","title":"Parameters(\u53c2\u6570\uff09","text":"<pre><code>class torch.nn.Parameter\n</code></pre> <p>Parameters\u5bf9\u8c61\u662f\u4e00\u79cd\u4f1a\u88ab\u89c6\u4e3a\u6a21\u5757\u53c2\u6570(module parameter\uff09\u7684Tensor\u5f20\u91cf\u3002</p> <p>Parameters\u7c7b\u662f<code>Tensor</code> \u7684\u5b50\u7c7b, \u4e0d\u8fc7\u76f8\u5bf9\u4e8e\u5b83\u7684\u7236\u7c7b\uff0cParameters\u7c7b\u6709\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u7279\u6027\u5c31\u662f\u5f53\u5176\u5728 <code>Module</code>\u7c7b\u4e2d\u88ab\u4f7f\u7528\u5e76\u88ab\u5f53\u505a\u8fd9\u4e2a<code>Module</code>\u7c7b\u7684\u6a21\u5757\u5c5e\u6027\u7684\u65f6\u5019\uff0c\u90a3\u4e48\u8fd9\u4e2aParameters\u5bf9\u8c61\u4f1a\u88ab\u81ea\u52a8\u5730\u6dfb\u52a0\u5230\u8fd9\u4e2a<code>Module</code>\u7c7b\u7684\u53c2\u6570\u5217\u8868(list of parameters)\u4e4b\u4e2d\uff0c\u540c\u65f6\u4e5f\u5c31\u4f1a\u88ab\u6dfb\u52a0\u5165\u6b64<code>Module</code>\u7c7b\u7684 <code>parameters()</code>\u65b9\u6cd5\u6240\u8fd4\u56de\u7684\u53c2\u6570\u8fed\u4ee3\u5668\u4e2d\u3002\u800cParameters\u7c7b\u7684\u7236\u7c7bTensor\u7c7b\u4e5f\u53ef\u4ee5\u88ab\u7528\u4e3a\u6784\u5efa\u6a21\u5757\u7684\u5c5e\u6027\uff0c\u4f46\u4e0d\u4f1a\u88ab\u52a0\u5165\u53c2\u6570\u5217\u8868\u3002\u8fd9\u6837\u4e3b\u8981\u662f\u56e0\u4e3a\uff0c\u6709\u65f6\u53ef\u80fd\u9700\u8981\u5728\u6a21\u578b\u4e2d\u5b58\u50a8\u4e00\u4e9b\u975e\u6a21\u578b\u53c2\u6570\u7684\u4e34\u65f6\u72b6\u6001\uff0c\u6bd4\u5982RNN\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u9690\u72b6\u6001\u3002\u800c\u901a\u8fc7\u4f7f\u7528\u975e<code>Parameter</code>\u7684Tensor\u7c7b\uff0c\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u4e34\u65f6\u53d8\u91cf\u6ce8\u518c(register)\u4e3a\u6a21\u578b\u7684\u5c5e\u6027\u7684\u540c\u65f6\u4f7f\u5176\u4e0d\u88ab\u52a0\u5165\u53c2\u6570\u5217\u8868\u3002</p> <p>Parameters: </p> <ul> <li>data (Tensor) \u2013 \u53c2\u6570\u5f20\u91cf(parameter tensor).</li> <li>requires_grad (bool, optional) \u2013 \u53c2\u6570\u662f\u5426\u9700\u8981\u68af\u5ea6\uff0c \u9ed8\u8ba4\u4e3a <code>True</code>\u3002\u66f4\u591a\u7ec6\u8282\u8bf7\u770b \u5982\u4f55\u5c06\u5b50\u56fe\u8e22\u51fa\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u3002 </li> </ul>"},{"location":"1.0/nn/#containers","title":"Containers(\u5bb9\u5668\uff09","text":""},{"location":"1.0/nn/#module","title":"Module(\u6a21\u5757\uff09","text":"<pre><code>class torch.nn.Module\n</code></pre> <p>\u6a21\u5757(Module\uff09\u662f\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u57fa\u7c7b\u3002</p> <p>\u4f60\u521b\u5efa\u6a21\u578b\u7684\u65f6\u5019\u4e5f\u5e94\u8be5\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u54e6\u3002</p> <p>\u6a21\u5757(Module)\u4e2d\u8fd8\u53ef\u4ee5\u5305\u542b\u5176\u4ed6\u7684\u6a21\u5757\uff0c\u4f60\u53ef\u4ee5\u5c06\u4e00\u4e2a\u6a21\u5757\u8d4b\u503c\u6210\u4e3a\u53e6\u4e00\u4e2a\u6a21\u5757\u7684\u5c5e\u6027\uff0c\u4ece\u800c\u6210\u4e3a\u8fd9\u4e2a\u6a21\u5757\u7684\u4e00\u4e2a\u5b50\u6a21\u5757\u3002\u800c\u901a\u8fc7\u4e0d\u65ad\u7684\u8d4b\u503c\uff0c\u4f60\u53ef\u4ee5\u5c06\u4e0d\u540c\u7684\u6a21\u5757\u7ec4\u7ec7\u6210\u4e00\u4e2a\u6811\u7ed3\u6784:</p> <pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5) # \u5f53\u524d\u7684nn.Conv2d\u6a21\u5757\u5c31\u88ab\u8d4b\u503c\u6210\u4e3aModel\u6a21\u5757\u7684\u4e00\u4e2a\u5b50\u6a21\u5757\uff0c\u6210\u4e3a\u201c\u6811\u7ed3\u6784\u201d\u7684\u53f6\u5b50\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n       x = F.relu(self.conv1(x))\n       return F.relu(self.conv2(x))\n\n</code></pre> <p>\u901a\u8fc7\u8d4b\u503c\u8fd9\u79cd\u65b9\u5f0f\u6dfb\u52a0\u7684\u5b50\u6a21\u5757\u5c06\u4f1a\u88ab\u6a21\u578b\u6ce8\u518c(register)\uff0c\u800c\u540e\u5f53\u8c03\u7528\u6a21\u5757\u7684\u4e00\u4e9b\u53c2\u6570\u8f6c\u6362\u51fd\u6570(<code>to()</code>\uff09\u7684\u65f6\u5019\uff0c\u5b50\u6a21\u5757\u7684\u53c2\u6570\u4e5f\u4f1a\u4e00\u5e76\u8f6c\u6362\u3002</p> <pre><code>add_module(name, module)\n</code></pre> <p>\u5411\u5f53\u524d\u6a21\u5757\u6dfb\u52a0\u4e00\u4e2a\u5b50\u6a21\u5757\u3002 \u6b64\u5b50\u6a21\u5757\u53ef\u4ee5\u4f5c\u4e3a\u5f53\u524d\u6a21\u5757\u7684\u5c5e\u6027\u88ab\u8bbf\u95ee\u5230\uff0c\u800c\u5c5e\u6027\u540d\u5c31\u662fadd_module()\u51fd\u6570\u4e2d\u7684name\u53c2\u6570\u3002</p> <p>add_module()\u51fd\u6570\u53c2\u6570: </p> <ul> <li>name (string) \u2013 \u5b50\u6a21\u5757\u7684\u540d\u5b57. \u51fd\u6570\u8c03\u7528\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee\u5f53\u524d\u6a21\u5757\u7684\u6b64\u5b57\u6bb5\u6765\u8bbf\u95ee\u8be5\u5b50\u6a21\u5757\u3002</li> <li>parameter (Module) \u2013 \u8981\u6dfb\u52a0\u5230\u5f53\u524d\u6a21\u5757\u7684\u5b50\u6a21\u5757\u3002</li> </ul> <pre><code>apply(fn)\n</code></pre> <p>apply()\u51fd\u6570\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5c06 <code>fn</code> \u9012\u5f52\u5730\u5e94\u7528\u4e8e\u6a21\u5757\u7684\u6240\u6709\u5b50\u6a21\u5757(<code>.children()</code>\u51fd\u6570\u7684\u8fd4\u56de\u503c\uff09\u4ee5\u53ca\u6a21\u5757\u81ea\u8eab\u3002\u6b64\u51fd\u6570\u7684\u4e00\u4e2a\u7ecf\u5178\u5e94\u7528\u5c31\u662f\u521d\u59cb\u5316\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u8fd9\u4e00\u8fc7\u7a0b(\u540c\u6837\u53c2\u89c1\u4e8e torch-nn-init)\u3002</p> Parameters: fn (<code>Module</code> -&gt; None) \u2013 \u8981\u5e94\u7528\u4e8e\u6240\u6709\u5b50\u6a21\u578b\u7684\u51fd\u6570 Returns: self --- --- Return type: Module --- --- <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; def init_weights(m):\n print(m)\n if type(m) == nn.Linear:\n m.weight.data.fill_(1.0)\n print(m.weight)\n\n&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n&gt;&gt;&gt; net.apply(init_weights) # \u5c06init_weights()\u51fd\u6570\u5e94\u7528\u4e8e\u6a21\u5757\u7684\u6240\u6709\u5b50\u6a21\u5757\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n [ 1.,  1.]])\nSequential(\n (0): Linear(in_features=2, out_features=2, bias=True)\n (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n (0): Linear(in_features=2, out_features=2, bias=True)\n (1): Linear(in_features=2, out_features=2, bias=True)\n)\n\n</code></pre> <pre><code>buffers(recurse=True)\n</code></pre> <p>\u8fd4\u56de\u6a21\u5757\u7684\u7f13\u51b2\u533a\u7684\u8fed\u4ee3\u5668</p> Parameters: recurse (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ea7\u751f\u7684\u7f13\u51b2\u533a\u8fed\u4ee3\u5668\u4f1a\u904d\u5386\u6a21\u5757\u81ea\u5df1\u4e0e\u6240\u6709\u5b50\u6a21\u5757\uff0c\u5426\u5219\u53ea\u4f1a\u904d\u5386\u6a21\u5757\u7684\u76f4\u8fde\u7684\u6210\u5458\u3002 Yields: torch.Tensor \u2013 \u6a21\u578b\u7f13\u51b2\u533a --- --- <p>\u4e3e\u4f8b:</p> <pre><code>&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf.data), buf.size())\n&lt;class 'torch.FloatTensor'&gt; (20L,)\n&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)\n\n</code></pre> <pre><code>children()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5f53\u524d\u6240\u6709\u5b50\u6a21\u5757\u7684\u8fed\u4ee3\u5668 Returns an iterator over immediate children modules.</p> Yields: Module \u2013 \u5b50\u6a21\u5757 <pre><code>cpu()\n</code></pre> <p>\u5c06\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570(parameter)\u548c\u7f13\u51b2\u533a(buffer)\u90fd\u8f6c\u79fb\u5230CPU\u5185\u5b58\u4e2d\u3002</p> Returns: self Return type: Module --- --- <pre><code>cuda(device=None)\n</code></pre> <p>\u5c06\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u548c\u7f13\u51b2\u533a\u90fd\u8f6c\u79fb\u5230CUDA\u8bbe\u5907\u5185\u5b58\u4e2d\u3002</p> <p>\u56e0\u4e3acuda()\u51fd\u6570\u540c\u65f6\u4f1a\u5c06\u5904\u7406\u6a21\u5757\u4e2d\u7684\u6240\u6709\u53c2\u6570\u5e76\u7f13\u5b58\u8fd9\u4e9b\u53c2\u6570\u7684\u5bf9\u8c61\u3002\u6240\u4ee5\u5982\u679c\u60f3\u8ba9\u6a21\u5757\u5728GPU\u4e0a\u8fdb\u884c\u4f18\u5316\u64cd\u4f5c\uff0c\u4e00\u5b9a\u8981\u5728\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u8c03\u7528\u6a21\u5757\u7684cuda()\u51fd\u6570\u3002</p> Parameters: device (int, optional) \u2013 \u5982\u679c\u8bbe\u5907\u7f16\u53f7\u88ab\u6307\u5b9a\uff0c\u6240\u6709\u7684\u53c2\u6570\u90fd\u4f1a\u88ab\u62f7\u8d1d\u5230\u7f16\u53f7\u6307\u5b9a\u8bbe\u5907\u4e0a Returns: self --- --- Return type: Module --- --- <pre><code>double()\n</code></pre> <p>\u5c06\u6240\u6709\u7684\u6d6e\u70b9\u6570\u7c7b\u578b\u7684\u53c2\u6570(parameters)\u548c\u7f13\u51b2\u533a(buffers)\u8f6c\u6362\u4e3a<code>double</code>\u6570\u636e\u7c7b\u578b\u3002</p> Returns: self Return type: Module --- --- <pre><code>dump_patches = False\n</code></pre> <p>\u8fd9\u4e2a\u5b57\u6bb5\u53ef\u4ee5\u4e3a<code>load_state_dict()</code>\u63d0\u4f9b BC \u652f\u6301(BC support\u5b9e\u5728\u4e0d\u61c2\u662f\u4ec0\u4e48\u610f\u601d-.-\uff09\u3002 \u5728 <code>state_dict()</code>\u51fd\u6570\u8fd4\u56de\u7684\u72b6\u6001\u5b57\u5178(state dict\uff09\u4e2d\uff0c \u6709\u4e00\u4e2a\u540d\u4e3a<code>_metadata</code>\u7684\u5c5e\u6027\u4e2d\u5b58\u50a8\u4e86\u8fd9\u4e2astate_dict\u7684\u7248\u672c\u53f7\u3002<code>_metadata</code>\u662f\u4e00\u4e2a\u9075\u4ece\u4e86\u72b6\u6001\u5b57\u5178(state dict\uff09\u7684\u547d\u540d\u89c4\u8303\u7684\u5173\u952e\u5b57\u5b57\u5178\uff0c \u8981\u60f3\u4e86\u89e3\u8fd9\u4e2a<code>_metadata</code>\u5728\u52a0\u8f7d\u72b6\u6001(loading state dict\uff09\u7684\u65f6\u5019\u662f\u600e\u4e48\u7528\u7684\uff0c\u53ef\u4ee5\u770b\u4e00\u4e0b <code>_load_from_state_dict</code>\u90e8\u5206\u7684\u6587\u6863\u3002</p> <p>\u5982\u679c\u65b0\u7684\u53c2\u6570/\u7f13\u51b2\u533a\u88ab\u6dfb\u52a0\u4e8e/\u79fb\u9664\u81ea\u8fd9\u4e2a\u6a21\u5757\u4e4b\u4e2d\u65f6\uff0c\u8fd9\u4e2a\u7248\u672c\u53f7\u6570\u5b57\u4f1a\u968f\u4e4b\u53d1\u751f\u53d8\u5316\u3002\u540c\u65f6\u6a21\u5757\u7684<code>_load_from_state_dict</code>\u65b9\u6cd5\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\u7684\u4fe1\u606f\u5e76\u4f9d\u636e\u6b64\u72b6\u6001\u8bcd\u5178(state dict\uff09\u7684\u53d8\u5316\u505a\u51fa\u4e00\u4e9b\u9002\u5f53\u7684\u8c03\u6574\u3002</p> <pre><code>eval()\n</code></pre> <p>\u5c06\u6a21\u5757\u8f6c\u6362\u4e3a\u6d4b\u8bd5\u6a21\u5f0f\u3002</p> <p>\u8fd9\u4e2a\u51fd\u6570\u53ea\u5bf9\u7279\u5b9a\u7684\u6a21\u5757\u7c7b\u578b\u6709\u6548\uff0c\u5982 <code>Dropout</code>\u548c<code>BatchNorm</code>\u7b49\u7b49\u3002\u5982\u679c\u60f3\u4e86\u89e3\u8fd9\u4e9b\u7279\u5b9a\u6a21\u5757\u5728\u8bad\u7ec3/\u6d4b\u8bd5\u6a21\u5f0f\u4e0b\u5404\u81ea\u7684\u8fd0\u4f5c\u7ec6\u8282\uff0c\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e9b\u7279\u6b8a\u6a21\u5757\u7684\u6587\u6863\u90e8\u5206\u3002</p> <pre><code>extra_repr()\n</code></pre> <p>\u4e3a\u6a21\u5757\u8bbe\u7f6e\u989d\u5916\u7684\u5c55\u793a\u4fe1\u606f(extra representation)\u3002</p> <p>\u5982\u679c\u60f3\u8981\u6253\u5370\u5c55\u793a(print)\u4f60\u7684\u6a21\u5757\u7684\u4e00\u4e9b\u5b9a\u5236\u7684\u989d\u5916\u4fe1\u606f\uff0c\u90a3\u4f60\u5e94\u8be5\u5728\u4f60\u7684\u6a21\u5757\u4e2d\u590d\u73b0\u8fd9\u4e2a\u51fd\u6570\u3002\u5355\u884c\u548c\u591a\u884c\u7684\u5b57\u7b26\u4e32\u90fd\u53ef\u4ee5\u88ab\u63a5\u53d7\u3002</p> <pre><code>float()\n</code></pre> <p>\u5c06\u6240\u6709\u6d6e\u70b9\u6570\u7c7b\u578b\u7684\u53c2\u6570(parameters)\u548c\u7f13\u51b2\u533a(buffers)\u8f6c\u6362\u4e3a<code>float</code>\u6570\u636e\u7c7b\u578b\u3002</p> Returns: self Return type: Module --- --- <pre><code>forward(*input)\n</code></pre> <p>\u5b9a\u4e49\u4e86\u6bcf\u6b21\u6a21\u5757\u88ab\u8c03\u7528\u4e4b\u540e\u6240\u8fdb\u884c\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002</p> <p>\u5e94\u8be5\u88abModule\u7c7b\u7684\u6240\u6709\u5b50\u7c7b\u91cd\u5199\u3002</p> <p>Note</p> <p>\u5c3d\u7ba1\u6a21\u5757\u7684\u524d\u5411\u64cd\u4f5c\u90fd\u88ab\u5b9a\u4e49\u5728\u8fd9\u4e2a\u51fd\u6570\u91cc\u9762\uff0c\u4f46\u662f\u5f53\u4f60\u8981\u8fdb\u884c\u6a21\u5757\u7684\u524d\u5411\u64cd\u4f5c\u7684\u65f6\u5019\uff0c\u8fd8\u662f\u8981\u76f4\u63a5\u8c03\u7528\u6a21\u5757<code>Module</code> \u7684\u5b9e\u4f8b\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u8c03\u7528\u8fd9\u4e2aforward()\u51fd\u6570\u3002\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3a\u524d\u8005\u4f1a\u7167\u987e\u5230\u6ce8\u518c\u5728\u6b64\u6a21\u5757\u4e4b\u4e0a\u7684\u94a9\u5b50\u51fd\u6570(the registered hooks\uff09\u7684\u8fd0\u884c\uff0c\u800c\u540e\u8005\u5219\u4e0d\u4f1a\u3002</p> <pre><code>half()\n</code></pre> <p>\u5c06\u6240\u6709\u7684\u6d6e\u70b9\u6570\u7c7b\u578b\u7684\u53c2\u6570(parameters)\u548c\u7f13\u51b2\u533a(buffers)\u8f6c\u6362\u4e3a<code>half</code>\u6570\u636e\u7c7b\u578b\u3002</p> Returns: self Return type: Module --- --- <pre><code>load_state_dict(state_dict, strict=True)\n</code></pre> <p>\u5c06<code>state_dict</code>\u4e2d\u7684\u53c2\u6570(parameters\uff09\u548c\u7f13\u51b2\u533a(buffers\uff09\u62f7\u8d1d\u5230\u6a21\u5757\u548c\u5176\u5b50\u6a21\u5757\u4e4b\u4e2d\u3002\u5982\u679c<code>strict</code>\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u90a3\u4e48<code>state_dict</code>\u4e2d\u7684\u952e\u503c(keys\uff09\u5fc5\u987b\u4e0e\u6a21\u578b\u7684[<code>state_dict()</code>]\u51fd\u6570\u6240\u8fd4\u56de\u7684\u952e\u503c(keys\uff09\u4fe1\u606f\u4fdd\u6301\u5b8c\u5168\u7684\u4e00\u81f4\u3002</p> <p>load_state_dict()\u51fd\u6570\u53c2\u6570\uff1a </p> <ul> <li>state_dict (dict) \u2013 \u4e00\u4e2a\u5305\u542b\u4e86\u53c2\u6570\u548c\u6301\u4e45\u7f13\u51b2\u533a\u7684\u5b57\u5178\u3002</li> <li>strict (bool, optional) \u2013 \u662f\u5426\u4e25\u683c\u8981\u6c42 <code>state_dict</code> \u4e2d\u7684\u952e\u503c(keys\uff09\u4e0e\u6a21\u578b <code>state_dict()</code> \u51fd\u6570\u8fd4\u56de\u7684\u952e\u503c(keys\uff09\u4fe1\u606f\u4fdd\u6301\u5b8c\u5168\u4e00\u81f4\u3002 \u9ed8\u8ba4\uff1a <code>True</code></li> </ul> <pre><code>modules()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5f53\u524d\u6a21\u5757\u5185\u6240\u6709\u6a21\u5757(\u5305\u62ec\u81ea\u8eab\uff09\u7684\u8fed\u4ee3\u5668\u3002</p> Yields: Module \u2013 a module in the network <p>Note</p> <p>\u6ce8\u610f\u91cd\u590d\u7684\u6a21\u5757\u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21\u3002\u6bd4\u5728\u4e0b\u9762\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c<code>l</code>\u5c31\u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21\u3002</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n print(idx, '-&gt;', m)\n\n0 -&gt; Sequential (\n (0): Linear (2 -&gt; 2)\n (1): Linear (2 -&gt; 2)\n)\n1 -&gt; Linear (2 -&gt; 2)\n\n</code></pre> <pre><code>named_buffers(prefix='', recurse=True)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u6a21\u5757\u7f13\u51b2\u533a\u7684\u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u8fd4\u56de\u7684\u5143\u7d20\u662f\u7531\u7f13\u51b2\u533a\u7684\u540d\u5b57\u548c\u7f13\u51b2\u533a\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4\u3002</p> <p>named_buffers()\u51fd\u6570\u7684\u53c2\u6570: </p> <ul> <li>prefix (str) \u2013 \u8981\u6dfb\u52a0\u5728\u6240\u6709\u7f13\u51b2\u533a\u540d\u5b57\u4e4b\u524d\u7684\u524d\u7f00\u3002</li> <li>recurse (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u90a3\u6837\u8fed\u4ee3\u5668\u4e2d\u4e0d\u5149\u4f1a\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u81ea\u8eab\u76f4\u8fde\u6210\u5458\u7684\u7f13\u51b2\u533a\uff0c\u540c\u65f6\u4e5f\u4f1a\u9012\u5f52\u8fd4\u56de\u5176\u5b50\u6a21\u5757\u7684\u7f13\u51b2\u533a\u3002\u5426\u5219\uff0c\u53ea\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u76f4\u8fde\u6210\u5458\u7684\u7f13\u51b2\u533a\u3002</li> </ul> Yields: (string, torch.Tensor) \u2013 \u5305\u542b\u4e86\u7f13\u51b2\u533a\u7684\u540d\u5b57\u548c\u7f13\u51b2\u533a\u81ea\u8eab\u7684\u5143\u7ec4 <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;    if name in ['running_var']:\n&gt;&gt;&gt;        print(buf.size())\n\n</code></pre> <pre><code>named_children()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5f53\u524d\u6a21\u578b\u76f4\u8fde\u7684\u5b50\u6a21\u5757\u7684\u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u8fd4\u56de\u7684\u5143\u7d20\u662f\u7531\u5b50\u6a21\u5757\u7684\u540d\u5b57\u548c\u5b50\u6a21\u5757\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4\u3002</p> Yields: (string, Module) \u2013 \u5305\u542b\u4e86\u5b50\u6a21\u5757\u7684\u540d\u5b57\u548c\u5b50\u6a21\u5757\u81ea\u8eab\u7684\u5143\u7ec4 <p>\u4f8b\u5b50\uff1a</p> <pre><code>&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n\n</code></pre> <pre><code>named_modules(memo=None, prefix='')\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5f53\u524d\u6a21\u5757\u5185\u6240\u6709\u6a21\u5757(\u5305\u62ec\u81ea\u8eab\uff09\u7684\u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u8fd4\u56de\u7684\u5143\u7d20\u662f\u7531\u6a21\u5757\u7684\u540d\u5b57\u548c\u6a21\u5757\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4\u3002</p> Yields: (string, Module) \u2013 \u6a21\u5757\u540d\u5b57\u548c\u6a21\u5757\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4 <p>Note</p> <p>\u91cd\u590d\u7684\u6a21\u5757\u53ea\u4f1a\u88ab\u8fd4\u56de\u4e00\u6b21\u3002\u5728\u4e0b\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c<code>l</code>\u53ea\u88ab\u8fd4\u56de\u4e86\u4e00\u6b21\u3002</p> <p>\u4f8b\u5b50\uff1a</p> <pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):\n print(idx, '-&gt;', m)\n\n0 -&gt; ('', Sequential (\n (0): Linear (2 -&gt; 2)\n (1): Linear (2 -&gt; 2)\n))\n1 -&gt; ('0', Linear (2 -&gt; 2))\n\n</code></pre> <pre><code>named_parameters(prefix='', recurse=True)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5f53\u524d\u6a21\u5757\u5185\u6240\u6709\u53c2\u6570\u7684\u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u8fd4\u56de\u7684\u5143\u7d20\u662f\u7531\u53c2\u6570\u7684\u540d\u5b57\u548c\u53c2\u6570\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4\u3002</p> <p>named_parameters()\u51fd\u6570\u53c2\u6570\uff1a</p> <ul> <li>prefix (str) \u2013 \u8981\u5728\u6240\u6709\u53c2\u6570\u540d\u5b57\u524d\u9762\u6dfb\u52a0\u7684\u524d\u7f00\u3002</li> <li>recurse (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u90a3\u6837\u8fed\u4ee3\u5668\u4e2d\u4e0d\u5149\u4f1a\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u81ea\u8eab\u76f4\u8fde\u6210\u5458\u7684\u53c2\u6570\uff0c\u540c\u65f6\u4e5f\u4f1a\u8fd4\u56de\u5176\u5b50\u6a21\u5757\u7684\u53c2\u6570\u3002\u5426\u5219\uff0c\u53ea\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u76f4\u8fde\u6210\u5458\u7684\u53c2\u6570\u3002</li> </ul> Yields: (string, Parameter) \u2013 \u53c2\u6570\u540d\u5b57\u548c\u53c2\u6570\u81ea\u8eab\u7ec4\u6210\u7684\u5143\u7ec4 <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;    if name in ['bias']:\n&gt;&gt;&gt;        print(param.size())\n\n</code></pre> <pre><code>parameters(recurse=True)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u904d\u5386\u6a21\u5757\u6240\u6709\u53c2\u6570\u7684\u8fed\u4ee3\u5668\u3002 parameters()\u51fd\u6570\u4e00\u4e2a\u7ecf\u5178\u7684\u5e94\u7528\u5c31\u662f\u5b9e\u8df5\u4e2d\u7ecf\u5e38\u5c06\u6b64\u51fd\u6570\u7684\u8fd4\u56de\u503c\u4f20\u5165\u4f18\u5316\u5668\u3002</p> Parameters: recurse (bool) \u2013  \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u90a3\u6837\u8fed\u4ee3\u5668\u4e2d\u4e0d\u5149\u4f1a\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u81ea\u8eab\u76f4\u8fde\u6210\u5458\u7684\u53c2\u6570\uff0c\u540c\u65f6\u4e5f\u4f1a\u9012\u5f52\u8fd4\u56de\u5176\u5b50\u6a21\u5757\u7684\u53c2\u6570\u3002\u5426\u5219\uff0c\u53ea\u8fd4\u56de\u8fd9\u4e2a\u6a21\u5757\u76f4\u8fde\u6210\u5458\u7684\u53c2\u6570\u3002 Yields: Parameter \u2013 \u6a21\u5757\u53c2\u6570 --- --- <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; for param in model.parameters():\n&gt;&gt;&gt;     print(type(param.data), param.size())\n&lt;class 'torch.FloatTensor'&gt; (20L,)\n&lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)\n\n</code></pre> <pre><code>register_backward_hook(hook)\n</code></pre> <p>\u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a\u6302\u8f7d\u5728\u53cd\u5411\u64cd\u4f5c\u4e4b\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002(\u6302\u8f7d\u5728backward\u4e4b\u540e\u8fd9\u4e2a\u70b9\u4e0a\u7684\u94a9\u5b50\u51fd\u6570\uff09</p> <p>\u5bf9\u4e8e\u6bcf\u6b21\u8f93\u5165\uff0c\u5f53\u6a21\u5757\u5173\u4e8e\u6b64\u6b21\u8f93\u5165\u7684\u53cd\u5411\u68af\u5ea6\u7684\u8ba1\u7b97\u8fc7\u7a0b\u5b8c\u6210\uff0c\u8be5\u94a9\u5b50\u51fd\u6570\u90fd\u4f1a\u88ab\u8c03\u7528\u4e00\u6b21\u3002\u6b64\u94a9\u5b50\u51fd\u6570\u9700\u8981\u9075\u4ece\u4ee5\u4e0b\u51fd\u6570\u7b7e\u540d\uff1a</p> <pre><code>hook(module, grad_input, grad_output) -&gt; Tensor or None\n\n</code></pre> <p>\u5982\u679c\u6a21\u5757\u7684\u8f93\u5165\u6216\u8f93\u51fa\u662f\u591a\u91cd\u7684(multiple inputs or outputs\uff09\uff0c\u90a3 <code>grad_input</code> \u548c <code>grad_output</code> \u5e94\u5f53\u662f\u5143\u7ec4\u6570\u636e\u3002 \u94a9\u5b50\u51fd\u6570\u4e0d\u80fd\u5bf9\u8f93\u5165\u7684\u53c2\u6570<code>grad_input</code> \u548c <code>grad_output</code>\u8fdb\u884c\u4efb\u4f55\u66f4\u6539\uff0c\u4f46\u662f\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6839\u636e\u8f93\u5165\u7684\u53c2\u6570\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u68af\u5ea6\u56de\u53bb\uff0c\u800c\u8fd9\u4e2a\u65b0\u7684\u68af\u5ea6\u5728\u540e\u7eed\u7684\u8ba1\u7b97\u4e2d\u4f1a\u66ff\u6362\u6389<code>grad_input</code>\u3002</p> Returns: \u4e00\u4e2a\u53e5\u67c4(handle\uff09\uff0c\u8fd9\u4e2ahandle\u7684\u7279\u70b9\u5c31\u662f\u901a\u8fc7\u8c03\u7528<code>handle.remove()</code>\u51fd\u6570\u5c31\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6dfb\u52a0\u4e8e\u6a21\u5757\u4e4b\u4e0a\u7684\u94a9\u5b50\u79fb\u9664\u6389\u3002 Return type: <code>torch.utils.hooks.RemovableHandle</code> --- --- <p>Warning</p> <p>\u5bf9\u4e8e\u4e00\u4e9b\u5177\u6709\u5f88\u591a\u590d\u6742\u64cd\u4f5c\u7684<code>Module</code>\uff0c\u5f53\u524d\u7684hook\u5b9e\u73b0\u7248\u672c\u8fd8\u4e0d\u80fd\u8fbe\u5230\u5b8c\u5168\u7406\u60f3\u7684\u6548\u679c\u3002\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u6709\u4e9b\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570<code>grad_input</code> \u548c <code>grad_output</code>\u4e2d\u53ef\u80fd\u53ea\u662f\u771f\u6b63\u7684\u8f93\u5165\u548c\u8f93\u51fa\u53d8\u91cf\u7684\u4e00\u4e2a\u5b50\u96c6\u3002\u5bf9\u4e8e\u6b64\u7c7b\u7684<code>Module</code>\uff0c\u4f60\u5e94\u8be5\u4f7f\u7528[<code>torch.Tensor.register_hook()</code>]\u76f4\u63a5\u5c06\u94a9\u5b50\u6302\u8f7d\u5230\u67d0\u4e2a\u7279\u5b9a\u7684\u8f93\u5165\u8f93\u51fa\u7684\u53d8\u91cf\u4e0a\uff0c\u800c\u4e0d\u662f\u5f53\u524d\u7684\u6a21\u5757\u3002</p> <pre><code>register_buffer(name, tensor)\n</code></pre> <p>\u5f80\u6a21\u5757\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u6301\u4e45\u7f13\u51b2\u533a\u3002</p> <p>\u8fd9\u4e2a\u51fd\u6570\u7684\u7ecf\u5e38\u4f1a\u88ab\u7528\u4e8e\u5411\u6a21\u5757\u6dfb\u52a0\u4e0d\u4f1a\u88ab\u8ba4\u4e3a\u662f\u6a21\u5757\u53c2\u6570(model parameter\uff09\u7684\u7f13\u51b2\u533a\u3002\u4e3e\u4e2a\u6817\u5b50\uff0cBatchNorm\u7684<code>running_mean</code>\u5c31\u4e0d\u662f\u4e00\u4e2a\u53c2\u6570\uff0c\u4f46\u5374\u5c5e\u4e8e\u6301\u4e45\u72b6\u6001\u3002</p> <p>\u6240\u6dfb\u52a0\u7684\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u7ed9\u5b9a\u7684\u540d\u5b57(name\u53c2\u6570)\u4ee5\u8bbf\u95ee\u6a21\u5757\u7684\u5c5e\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u8bbf\u95ee\u3002</p> <p>register_buffer()\u51fd\u6570\u7684\u53c2\u6570: </p> <ul> <li>name (string) \u2013 \u8981\u6dfb\u52a0\u7684\u7f13\u51b2\u533a\u7684\u540d\u5b57\u3002\u6240\u6dfb\u52a0\u7684\u7f13\u51b2\u533a\u53ef\u4ee5\u901a\u8fc7\u6b64\u540d\u5b57\u4ee5\u8bbf\u95ee\u6a21\u5757\u7684\u5c5e\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u8bbf\u95ee\u3002</li> <li>tensor (Tensor) \u2013 \u9700\u8981\u6ce8\u518c\u5230\u6a21\u5757\u4e0a\u7684\u7f13\u51b2\u533a\u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))\n\n</code></pre> <pre><code>register_forward_hook(hook)\n</code></pre> <p>\u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a\u6302\u8f7d\u5728\u524d\u5411\u64cd\u4f5c\u4e4b\u540e\u7684\u94a9\u5b50\u51fd\u6570\u3002(\u6302\u8f7d\u5728forward\u64cd\u4f5c\u7ed3\u675f\u4e4b\u540e\u8fd9\u4e2a\u70b9\uff09</p> <p>\u6b64\u94a9\u5b50\u51fd\u6570\u5728\u6bcf\u6b21\u6a21\u5757\u7684 <code>forward()</code>\u51fd\u6570\u8fd0\u884c\u7ed3\u675f\u4ea7\u751foutput\u4e4b\u540e\u5c31\u4f1a\u88ab\u89e6\u53d1\u3002\u6b64\u94a9\u5b50\u51fd\u6570\u9700\u8981\u9075\u4ece\u4ee5\u4e0b\u51fd\u6570\u7b7e\u540d\uff1a</p> <pre><code>hook(module, input, output) -&gt; None\n\n</code></pre> <p>\u6b64\u94a9\u5b50\u51fd\u6570\u4e0d\u80fd\u8fdb\u884c\u4f1a\u4fee\u6539 input \u548c output \u8fd9\u4e24\u4e2a\u53c2\u6570\u7684\u64cd\u4f5c\u3002</p> Returns: \u4e00\u4e2a\u53e5\u67c4(handle\uff09\uff0c\u8fd9\u4e2ahandle\u7684\u7279\u70b9\u5c31\u662f\u901a\u8fc7\u8c03\u7528<code>handle.remove()</code>\u51fd\u6570\u5c31\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6dfb\u52a0\u4e8e\u6a21\u5757\u4e4b\u4e0a\u7684\u94a9\u5b50\u79fb\u9664\u6389\u3002 Return type: <code>torch.utils.hooks.RemovableHandle</code> --- --- <pre><code>register_forward_pre_hook(hook)\n</code></pre> <p>\u5728\u6a21\u5757\u4e0a\u6ce8\u518c\u4e00\u4e2a\u6302\u8f7d\u5728\u524d\u5411\u64cd\u4f5c\u4e4b\u524d\u7684\u94a9\u5b50\u51fd\u6570\u3002(\u6302\u8f7d\u5728forward\u64cd\u4f5c\u5f00\u59cb\u4e4b\u524d\u8fd9\u4e2a\u70b9\uff09</p> <p>\u6b64\u94a9\u5b50\u51fd\u6570\u5728\u6bcf\u6b21\u6a21\u5757\u7684 <code>forward()</code>\u51fd\u6570\u8fd0\u884c\u5f00\u59cb\u4e4b\u524d\u4f1a\u88ab\u89e6\u53d1\u3002\u6b64\u94a9\u5b50\u51fd\u6570\u9700\u8981\u9075\u4ece\u4ee5\u4e0b\u51fd\u6570\u7b7e\u540d\uff1a The hook will be called every time before <code>forward()</code> is invoked. It should have the following signature:</p> <pre><code>hook(module, input) -&gt; None\n\n</code></pre> <p>\u6b64\u94a9\u5b50\u51fd\u6570\u4e0d\u80fd\u8fdb\u884c\u4f1a\u4fee\u6539 input \u8fd9\u4e2a\u53c2\u6570\u7684\u64cd\u4f5c\u3002</p> Returns: \u4e00\u4e2a\u53e5\u67c4(handle\uff09\uff0c\u8fd9\u4e2ahandle\u7684\u7279\u70b9\u5c31\u662f\u901a\u8fc7\u8c03\u7528<code>handle.remove()</code>\u51fd\u6570\u5c31\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u6dfb\u52a0\u4e8e\u6a21\u5757\u4e4b\u4e0a\u7684\u94a9\u5b50\u79fb\u9664\u6389\u3002 Return type: <code>torch.utils.hooks.RemovableHandle</code> --- --- <pre><code>register_parameter(name, param)\n</code></pre> <p>\u5411\u6a21\u5757\u6dfb\u52a0\u4e00\u4e2a\u53c2\u6570(parameter\uff09\u3002</p> <p>\u6240\u6dfb\u52a0\u7684\u53c2\u6570(parameter\uff09\u53ef\u4ee5\u901a\u8fc7\u7ed9\u5b9a\u7684\u540d\u5b57(name\u53c2\u6570)\u4ee5\u8bbf\u95ee\u6a21\u5757\u7684\u5c5e\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u8bbf\u95ee\u3002</p> <p>register_parameter()\u51fd\u6570\u7684\u53c2\u6570\uff1a </p> <ul> <li>name (string) \u2013 \u6240\u6dfb\u52a0\u7684\u53c2\u6570\u7684\u540d\u5b57. \u6240\u6dfb\u52a0\u7684\u53c2\u6570(parameter\uff09\u53ef\u4ee5\u901a\u8fc7\u6b64\u540d\u5b57\u4ee5\u8bbf\u95ee\u6a21\u5757\u7684\u5c5e\u6027\u7684\u65b9\u5f0f\u8fdb\u884c\u8bbf\u95ee</li> <li>parameter (Parameter) \u2013 \u8981\u6dfb\u52a0\u5230\u6a21\u5757\u4e4b\u4e0a\u7684\u53c2\u6570\u3002</li> </ul> <pre><code>state_dict(destination=None, prefix='', keep_vars=False)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u4e86\u6a21\u5757\u5f53\u524d\u6240\u6709\u72b6\u6001(state)\u7684\u5b57\u5178(dictionary)\u3002</p> <p>\u6240\u6709\u7684\u53c2\u6570\u548c\u6301\u4e45\u7f13\u51b2\u533a\u90fd\u88ab\u56ca\u62ec\u5728\u5176\u4e2d\u3002\u5b57\u5178\u7684\u952e\u503c\u5c31\u662f\u54cd\u5e94\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684\u540d\u5b57(name)\u3002</p> Returns: \u4e00\u4e2a\u5305\u542b\u4e86\u6a21\u5757\u5f53\u524d\u6240\u6709\u72b6\u6001\u7684\u5b57\u5178 Return type: dict --- --- <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; module.state_dict().keys()\n['bias', 'weight']\n\n</code></pre> <pre><code>to(*args, **kwargs)\n</code></pre> <p>\u79fb\u52a8 \u5e76\u4e14/\u6216\u8005(and/or\uff09\u8f6c\u6362\u6240\u6709\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u3002</p> <p>\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u8fd9\u6837\u8c03\u7528\uff1a</p> <pre><code>to(device=None, dtype=None, non_blocking=False)\n</code></pre> <pre><code>to(dtype, non_blocking=False)\n</code></pre> <pre><code>to(tensor, non_blocking=False)\n</code></pre> <p>\u6b64\u51fd\u6570\u7684\u51fd\u6570\u7b7e\u540d\u8ddf<code>torch.Tensor.to()</code>\u51fd\u6570\u7684\u51fd\u6570\u7b7e\u540d\u5f88\u76f8\u4f3c\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u4e2a\u51fd\u6570<code>dtype</code>\u53c2\u6570\u53ea\u63a5\u53d7\u6d6e\u70b9\u6570\u7c7b\u578b\u7684dtype\uff0c\u5982float\uff0c double\uff0c half (floating point desired <code>dtype</code> s\uff09\u3002\u540c\u65f6\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u53ea\u4f1a\u5c06\u6d6e\u70b9\u6570\u7c7b\u578b\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a(the floating point parameters and buffers\uff09\u8f6c\u5316\u4e3a<code>dtype</code>(\u5982\u679c\u8f93\u5165\u53c2\u6570\u4e2d\u7ed9\u5b9a\u7684\u8bdd\uff09\u7684\u6570\u636e\u7c7b\u578b\u3002\u800c\u5bf9\u4e8e\u6574\u6570\u7c7b\u578b\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a(the integral parameters and buffers\uff09\uff0c\u5373\u4fbf\u8f93\u5165\u53c2\u6570\u4e2d\u7ed9\u5b9a\u4e86<code>dtype</code>\uff0c\u4e5f\u4e0d\u4f1a\u8fdb\u884c\u8f6c\u6362\u64cd\u4f5c\uff0c\u800c\u5982\u679c\u7ed9\u5b9a\u4e86 <code>device</code>\u53c2\u6570\uff0c\u79fb\u52a8\u64cd\u4f5c\u5219\u4f1a\u6b63\u5e38\u8fdb\u884c\u3002\u5f53<code>non_blocking</code>\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\u4e4b\u540e\uff0c\u6b64\u51fd\u6570\u4f1a\u5c3d\u53ef\u80fd\u5730\u76f8\u5bf9\u4e8e host \u8fdb\u884c\u5f02\u6b65\u7684 \u8f6c\u6362/\u79fb\u52a8 \u64cd\u4f5c\uff0c\u6bd4\u5982\uff0c\u5c06\u5b58\u50a8\u5728\u56fa\u5b9a\u5185\u5b58(pinned memory\uff09\u4e0a\u7684CPU Tensors\u79fb\u52a8\u5230CUDA\u8bbe\u5907\u4e0a\u8fd9\u4e00\u8fc7\u7a0b\u65e2\u662f\u5982\u6b64\u3002</p> <p>\u4f8b\u5b50\u5728\u4e0b\u9762\u3002</p> <p>Note</p> <p>\u8fd9\u4e2a\u65b9\u6cd5\u5bf9\u6a21\u5757\u7684\u4fee\u6539\u90fd\u662fin-place\u64cd\u4f5c\u3002</p> <p>to()\u51fd\u6570\u7684\u53c2\u6570: </p> <ul> <li>device (<code>torch.device</code>) \u2013 \u60f3\u8981\u5c06\u8fd9\u4e2a\u6a21\u5757\u4e2d\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u79fb\u5230\u7684\u8bbe\u5907\u3002</li> <li>dtype (<code>torch.dtype</code>) \u2013 \u60f3\u8981\u5c06\u8fd9\u4e2a\u6a21\u5757\u4e2d\u6d6e\u70b9\u6570\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u5316\u4e3a\u7684\u6d6e\u70b9\u6570\u6570\u636e\u7c7b\u578b\u3002</li> <li>tensor (torch.Tensor) \u2013 \u4e00\u4e2aTensor\uff0c\u5982\u679c\u88ab\u6307\u5b9a\uff0c\u5176dtype\u548cdevice\u4fe1\u606f\uff0c\u5c06\u5206\u522b\u8d77\u5230\u4e0a\u9762\u4e24\u4e2a\u53c2\u6570\u7684\u4f5c\u7528\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u8fd9\u4e2a\u6a21\u5757\u7684\u6d6e\u70b9\u6570\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684\u6570\u636e\u7c7b\u578b\u5c06\u4f1a\u88ab\u8f6c\u5316\u4e3a\u8fd9\u4e2aTensor\u7684dtype\u7c7b\u578b\uff0c\u540c\u65f6\u88ab\u8f6c\u79fb\u5230\u6b64Tensor\u6240\u5904\u7684\u8bbe\u5907device\u4e0a\u53bb\u3002</li> </ul> Returns: self Return type: Module --- --- <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n [-0.5112, -0.2324]], dtype=torch.float16)\n\n</code></pre> <pre><code>train(mode=True)\n</code></pre> <p>\u5c06\u6a21\u5757\u8f6c\u6362\u6210\u8bad\u7ec3\u6a21\u5f0f\u3002</p> <p>\u8fd9\u4e2a\u51fd\u6570\u53ea\u5bf9\u7279\u5b9a\u7684\u6a21\u5757\u7c7b\u578b\u6709\u6548\uff0c\u5982 <code>Dropout</code>\u548c<code>BatchNorm</code>\u7b49\u7b49\u3002\u5982\u679c\u60f3\u4e86\u89e3\u8fd9\u4e9b\u7279\u5b9a\u6a21\u5757\u5728\u8bad\u7ec3/\u6d4b\u8bd5\u6a21\u5f0f\u4e0b\u5404\u81ea\u7684\u8fd0\u4f5c\u7ec6\u8282\uff0c\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e9b\u7279\u6b8a\u6a21\u5757\u7684\u6587\u6863\u90e8\u5206\u3002</p> Returns: self Return type: Module --- --- <pre><code>type(dst_type)\n</code></pre> <p>\u5c06\u6240\u6709\u7684\u53c2\u6570\u548c\u7f13\u51b2\u533a\u8f6c\u5316\u4e3a <code>dst_type</code>\u7684\u6570\u636e\u7c7b\u578b\u3002</p> Parameters: dst_type (type or string) \u2013 \u8981\u8f6c\u5316\u7684\u6570\u636e\u7c7b\u578b Returns: self --- --- Return type: Module --- --- <pre><code>zero_grad()\n</code></pre> <p>\u8bb2\u6a21\u5757\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u8bbe\u7f6e\u4e3a0\u3002</p>"},{"location":"1.0/nn/#sequential","title":"Sequential","text":"<pre><code>class torch.nn.Sequential(*args)\n</code></pre> <p>\u4e00\u79cd\u987a\u5e8f\u5bb9\u5668\u3002\u4f20\u5165Sequential\u6784\u9020\u5668\u4e2d\u7684\u6a21\u5757\u4f1a\u88ab\u6309\u7167\u4ed6\u4eec\u4f20\u5165\u7684\u987a\u5e8f\u4f9d\u6b21\u6dfb\u52a0\u5230Sequential\u4e4b\u4e0a\u3002\u76f8\u5e94\u7684\uff0c\u4e00\u4e2a\u7531\u6a21\u5757\u7ec4\u6210\u7684\u987a\u5e8f\u8bcd\u5178\u4e5f\u53ef\u4ee5\u88ab\u4f20\u5165\u5230Sequential\u7684\u6784\u9020\u5668\u4e2d\u3002</p> <p>\u4e3a\u4e86\u65b9\u4fbf\u5927\u5bb6\u7406\u89e3\uff0c\u4e3e\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a</p> <pre><code># \u6784\u5efaSequential\u7684\u4f8b\u5b50\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# \u5229\u7528OrderedDict\u6784\u5efaSequential\u7684\u4f8b\u5b50\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n\n</code></pre>"},{"location":"1.0/nn/#modulelist","title":"ModuleList (\u6a21\u5757\u5217\u8868)","text":"<pre><code>class torch.nn.ModuleList(modules=None)\n</code></pre> <p>ModuleList\u7684\u4f5c\u7528\u662f\u5c06\u4e00\u5806\u6a21\u5757(module\uff09\u5b58\u50a8\u5728\u4e00\u4e2a\u5217\u8868\u4e4b\u4e2d\u3002</p> <p>ModuleList \u53ef\u4ee5\u6309\u4e00\u822c\u7684python\u5217\u8868\u7684\u7d22\u5f15\u65b9\u5f0f\u8fdb\u884c\u7d22\u5f15\uff0c\u4f46ModuleList\u4e2d\u7684\u6a21\u5757\u90fd\u5df2\u88ab\u6b63\u786e\u6ce8\u518c\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u7684Module method\u53ef\u89c1\u3002</p> Parameters: modules (iterable__, optional) \u2013 \u4e00\u4e2a\u8981\u6dfb\u52a0\u5230ModuleList\u4e2d\u7684\u7531\u6a21\u5757\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784(an iterable of modules) <p>\u4f8b\u5b50:</p> <pre><code>class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList\u53ef\u4ee5\u88ab\u5f53\u4f5c\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u4f7f\u7528index\u7d22\u5f15\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n</code></pre> <pre><code>append(module)\n</code></pre> <p>\u5c06\u4e00\u4e2a\u6a21\u5757\u6dfb\u52a0\u5230ModuleList\u7684\u672b\u5c3e\uff0c\u4e0epython list\u7684append()\u4e00\u81f4\u3002</p> Parameters: module (nn.Module) \u2013 \u8981\u6dfb\u52a0\u7684\u6a21\u5757 <pre><code>extend(modules)\n</code></pre> <p>\u5c06\u4e00\u4e2a\u7531\u6a21\u5757\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u6dfb\u52a0\u5230ModuleList\u7684\u672b\u5c3e\uff0c\u4e0epython list\u7684extend()\u4e00\u81f4\u3002</p> Parameters: modules (iterable) \u2013 \u8981\u6dfb\u52a0\u5230ModuleList\u672b\u5c3e\u7684\u7531\u6a21\u5757\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <pre><code>insert(index, module)\n</code></pre> <p>\u5c06\u7ed9\u5b9a\u7684<code>module</code>\u63d2\u5165\u5230ModuleList\u7684<code>index</code>\u4f4d\u7f6e\u3002</p> <p>insert()\u51fd\u6570\u7684\u53c2\u6570: </p> <ul> <li>index (int) \u2013 \u8981\u63d2\u5165\u7684\u4f4d\u7f6e</li> <li>module (nn.Module) \u2013 \u8981\u63d2\u5165\u7684\u6a21\u5757</li> </ul>"},{"location":"1.0/nn/#moduledict","title":"ModuleDict (\u6a21\u5757\u8bcd\u5178)","text":"<pre><code>class torch.nn.ModuleDict(modules=None)\n</code></pre> <p>ModuleDict\u7684\u4f5c\u7528\u662f\u5c06\u4e00\u5806\u6a21\u5757(module\uff09\u5b58\u50a8\u5728\u4e00\u4e2a\u8bcd\u5178\u4e4b\u4e2d\u3002</p> <p>ModuleDict \u53ef\u4ee5\u6309\u4e00\u822c\u7684python\u8bcd\u5178\u7684\u7d22\u5f15\u65b9\u5f0f\u8fdb\u884c\u7d22\u5f15\uff0c\u4f46ModuleDict\u4e2d\u7684\u6a21\u5757\u90fd\u5df2\u88ab\u6b63\u786e\u6ce8\u518c\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u7684Module method\u53ef\u89c1\u3002</p> Parameters: modules (iterable__, optional) \u2013 \u4e00\u4e2a\u7531(string: module)\u6620\u5c04\u7ec4\u6210\u7684\u6620\u5c04\u96c6\u5408(\u8bcd\u5178\uff09\u6216\u8005 \u4e00\u4e2a\u7531(string, module)\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <p>Example:</p> <pre><code>class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n</code></pre> <pre><code>clear()\n</code></pre> <p>\u79fb\u9664ModuleDict\u4e2d\u6240\u6709\u7684\u5143\u7d20\u3002</p> <pre><code>items()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531ModuleDict\u4e2d\u7684\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p> <pre><code>keys()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531ModuleDict\u4e2d\u7684\u952e\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p> <pre><code>pop(key)\n</code></pre> <p>\u5c06<code>key</code>\u8fd9\u4e2a\u952e\u4eceModuleDict\u4e2d\u5220\u9664\uff0c\u5e76\u5c06\u5176\u5bf9\u5e94\u7684\u6a21\u5757\u8fd4\u56de\u3002</p> Parameters: key (string) \u2013 \u8981\u4eceModuleDict\u4e2d\u5f39\u51fa\u7684\u952e <pre><code>update(modules)\n</code></pre> <p>\u901a\u8fc7\u4f20\u5165\u7684\u6620\u5c04\u6216\u8005\u7531\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u5bf9\u5f53\u524d\u7684ModuleDict\u8fdb\u884c\u66f4\u65b0\uff0c\u5982\u679c\u4f20\u5165\u5bf9\u8c61\u4e0e\u5f53\u524dModuleDict\u4e2d\u5b58\u5728\u952e\u91cd\u590d\uff0c\u5f53\u524dModuleDict\u4e2d\u8fd9\u4e9b\u91cd\u590d\u7684\u952e\u6240\u5bf9\u5e94\u7684\u503c\u5c06\u88ab\u8986\u76d6\u3002</p> Parameters: modules (iterable) \u2013 \u4e00\u4e2a\u7531(string: <code>Module</code>)\u6620\u5c04\u7ec4\u6210\u7684\u6620\u5c04\u96c6\u5408(\u8bcd\u5178\uff09\u6216\u8005 \u4e00\u4e2a\u7531(string: <code>Module</code>)\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <pre><code>values()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531ModuleDict\u4e2d\u7684\u503c\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p>"},{"location":"1.0/nn/#parameterlist","title":"ParameterList (\u53c2\u6570\u5217\u8868)","text":"<pre><code>class torch.nn.ParameterList(parameters=None)\n</code></pre> <p>ParameterList\u7684\u4f5c\u7528\u662f\u5c06\u4e00\u5806\u53c2\u6570(parameter\uff09\u5b58\u50a8\u5230\u4e00\u4e2a\u5217\u8868\u4e2d\u3002</p> <p>ParameterList \u53ef\u4ee5\u6309\u4e00\u822c\u7684python\u5217\u8868\u7684\u7d22\u5f15\u65b9\u5f0f\u8fdb\u884c\u7d22\u5f15\uff0c\u4f46ParameterList\u4e2d\u7684\u53c2\u6570(parameter\uff09\u90fd\u5df2\u88ab\u6b63\u786e\u6ce8\u518c\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u7684Module method\u53ef\u89c1\u3002</p> Parameters: parameters (iterable__, optional) \u2013 \u8981\u6dfb\u52a0\u5230ParameterList\u4e4b\u4e0a\u7684\u7531parameter\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <p>\u4f8b\u5b50:</p> <pre><code>class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList\u53ef\u4ee5\u88ab\u5f53\u4f5c\u4e00\u4e2a\u8fed\u4ee3\u5668\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u4f7f\u7528index\u7d22\u5f15\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n</code></pre> <pre><code>append(parameter)\n</code></pre> <p>\u5c06\u4e00\u4e2aparameter\u6dfb\u52a0\u5230ParameterList\u7684\u672b\u5c3e\u3002</p> Parameters: parameter (nn.Parameter) \u2013 \u8981\u6dfb\u52a0\u7684\u53c2\u6570 <pre><code>extend(parameters)\n</code></pre> <p>\u5c06\u4e00\u4e2a\u7531parameter\u7ec4\u6210\u7684Python\u53ef\u8fed\u4ee3\u7ed3\u6784\u6dfb\u52a0\u5230ParameterList\u7684\u672b\u5c3e\u3002</p> Parameters: parameters (iterable) \u2013 \u8981\u6dfb\u52a0\u5230ParameterList\u7684\u672b\u5c3e\u7684\u7531parameter\u7ec4\u6210\u7684Python\u53ef\u8fed\u4ee3\u7ed3\u6784"},{"location":"1.0/nn/#parameterdict","title":"ParameterDict (\u53c2\u6570\u8bcd\u5178)","text":"<pre><code>class torch.nn.ParameterDict(parameters=None)\n</code></pre> <p>ParameterDict\u7684\u4f5c\u7528\u662f\u5c06\u4e00\u5806\u53c2\u6570(Parameter\uff09\u5b58\u50a8\u5728\u4e00\u4e2a\u8bcd\u5178\u4e4b\u4e2d\u3002</p> <p>ParameterDict \u53ef\u4ee5\u6309\u4e00\u822c\u7684python\u8bcd\u5178\u7684\u7d22\u5f15\u65b9\u5f0f\u8fdb\u884c\u7d22\u5f15\uff0c\u4f46ParameterDictt\u4e2d\u7684\u53c2\u6570\u90fd\u5df2\u88ab\u6b63\u786e\u6ce8\u518c\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u7684Module method\u53ef\u89c1\u3002</p> Parameters: parameters (iterable__, optional) \u2013 \u4e00\u4e2a\u7531(string:<code>Parameter</code>)\u6620\u5c04\u7ec4\u6210\u7684\u6620\u5c04\u96c6\u5408(\u8bcd\u5178\uff09\u6216\u8005 \u4e00\u4e2a\u7531(string, <code>Parameter</code>)\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <p>\u4f8b\u5b50:</p> <pre><code>class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n</code></pre> <pre><code>clear()\n</code></pre> <p>\u79fb\u9664ParameterDict\u4e2d\u6240\u6709\u7684\u5143\u7d20\u3002</p> <pre><code>items()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531ParameterDict\u4e2d\u7684\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p> <pre><code>keys()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531 ParameterDict\u4e2d\u7684\u952e\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p> <pre><code>pop(key)\n</code></pre> <p>\u5c06key\u8fd9\u4e2a\u952e\u4eceParameterDict\u4e2d\u5220\u9664\uff0c\u5e76\u5c06\u5176\u5bf9\u5e94\u7684\u6a21\u5757\u8fd4\u56de\u3002</p> Parameters: key (string) \u2013 \u8981\u4eceParameterDict\u4e2d\u5f39\u51fa\u7684\u952e <pre><code>update(parameters)\n</code></pre> <p>\u901a\u8fc7\u4f20\u5165\u7684\u6620\u5c04\u6216\u8005\u7531\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u5bf9\u5f53\u524d\u7684ParameterDict\u8fdb\u884c\u66f4\u65b0\uff0c\u5982\u679c\u4f20\u5165\u5bf9\u8c61\u4e0e\u5f53\u524dParameterDict\u4e2d\u5b58\u5728\u952e\u91cd\u590d\uff0c\u5f53\u524dParameterDict\u4e2d\u8fd9\u4e9b\u91cd\u590d\u7684\u952e\u6240\u5bf9\u5e94\u7684\u503c\u5c06\u88ab\u8986\u76d6\u3002</p> Parameters: parameters (iterable) \u2013 modules (iterable) \u2013 \u4e00\u4e2a\u7531(string: <code>Parameter</code>)\u6620\u5c04\u7ec4\u6210\u7684\u6620\u5c04\u96c6\u5408(\u8bcd\u5178\uff09\u6216\u8005 \u4e00\u4e2a\u7531(string: <code>Parameter</code>)\u952e/\u503c\u5bf9\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784 <pre><code>values()\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7531ParameterDict\u4e2d\u7684\u503c\u7ec4\u6210\u7684\u53ef\u8fed\u4ee3\u7ed3\u6784\u3002</p>"},{"location":"1.0/nn/#convolution-layers","title":"Convolution layers (\u5377\u79ef\u5c42)","text":""},{"location":"1.0/nn/#conv1d","title":"Conv1d","text":"<pre><code>class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e00\u7ef4\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e00\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u4e00\u7ef4\u5377\u79ef\u64cd\u4f5c\u7684\u5377\u79ef\u5c42\u3002</p> <p>\u5728\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a\uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\u7684\u4e00\u7ef4\u5377\u79ef\u5c42\uff0c\u5176\u5377\u79ef\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u5982\u4e0b\u8868\u8ff0\uff1a</p> <p></p> <p>\u8fd9\u91cc\u7684\u7b26\u53f7\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u4e92\u76f8\u5173(cross-correlation\uff09 \u64cd\u4f5c\u7b26(\u5927\u5bb6\u53ef\u4ee5\u81ea\u5df1\u67e5\u4e00\u4e0b\u4e92\u76f8\u5173\u548c\u771f\u5377\u79ef\u7684\u533a\u522b\uff0c\u4e92\u76f8\u5173\u56e0\u4e3a\u5b9e\u73b0\u8d77\u6765\u5f88\u7b80\u5355\uff0c\u6240\u4ee5\u4e00\u822c\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u90fd\u662f\u7528\u4e92\u76f8\u5173\u64cd\u4f5c\u53d6\u4ee3\u771f\u5377\u79ef\uff09,  is a batch size,  \u4ee3\u8868\u901a\u9053\u7684\u6570\u91cf,  \u4ee3\u8868\u4fe1\u53f7\u5e8f\u5217\u7684\u957f\u5ea6\u3002</p> <ul> <li> <p><code>stride</code> \u53c2\u6570\u63a7\u5236\u4e86\u4e92\u76f8\u5173\u64cd\u4f5c(\u4f2a\u5377\u79ef\uff09\u7684\u6b65\u957f\uff0c\u53c2\u6570\u7684\u6570\u636e\u7c7b\u578b\u4e00\u822c\u662f\u5355\u4e2a\u6570\u5b57\u6216\u8005\u4e00\u4e2a\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\u3002</p> </li> <li> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u4e00\u7ef4\u5377\u79ef\u6838\u7684\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> </li> <li> <p><code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c </p> <p>&gt; *   \u5f53 groups=1, \u6b64Conv1d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv1d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv1d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= <code>in_channels</code>, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f</p> </li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note</p> <p>\u5f53<code>groups == in_channels</code> \u5e76\u4e14 <code>out_channels == K * in_channels</code>(\u5176\u4e2dK\u662f\u6b63\u6574\u6570\uff09\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u4e5f\u88ab\u79f0\u4e3a\u6df1\u5ea6\u5377\u79ef\u3002 \u4e3e\u4e2a\u521b\u5efa\u6df1\u5ea6\u5377\u79ef\u5c42\u7684\u4f8b\u5b50\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u5927\u5c0f\u4e3a  \u7684\u8f93\u5165\uff0c\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u4e58\u6570\u4e3a<code>K</code>\u7684\u6df1\u5ea6\u5377\u79ef\u5c42\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u53c2\u6570\u6765\u521b\u5efa\uff1a\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Conv1d\u7684\u53c2\u6570: </p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u8f93\u51fa\u901a\u9053\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 \u8f93\u5165\u6570\u636e\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>dilation (int or tuple, optional) \u2013 \u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  \u5176\u4e2d</p> <p></p> </li> </ul> <p>| \u5185\u90e8Variables\uff1a | </p> <ul> <li>weight (Tensor) \u2013 Conv1d\u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a(out_channels, in_channels, kernel_size)\u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c \u5176\u4e2d\u3002</li> <li>bias (Tensor) \u2013 \u6a21\u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a(out_channels)\uff0c\u53ef\u8bad\u7ec3\u5b66\u4e60\u3002\u5982\u679c\u6784\u9020Conv1d\u65f6\u6784\u9020\u51fd\u6570\u4e2d\u7684<code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c \u5176\u4e2d \u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 50)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#conv2d","title":"Conv2d","text":"<pre><code>class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e8c\u7ef4\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e8c\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u4e8c\u7ef4\u5377\u79ef\u64cd\u4f5c\u7684\u5377\u79ef\u5c42\u3002</p> <p>\u5728\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a\uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\u7684\u4e8c\u7ef4\u7ef4\u5377\u79ef\u5c42\uff0c\u5176\u5377\u79ef\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u5982\u4e0b\u8868\u8ff0\uff1a</p> <p></p> <p>\u8fd9\u91cc\u7684\u7b26\u53f7\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u4e8c\u7ef4\u4e92\u76f8\u5173(cross-correlation\uff09 \u64cd\u4f5c\u7b26(\u5927\u5bb6\u53ef\u4ee5\u81ea\u5df1\u67e5\u4e00\u4e0b\u4e92\u76f8\u5173\u548c\u771f\u5377\u79ef\u7684\u533a\u522b\uff0c\u4e92\u76f8\u5173\u56e0\u4e3a\u5b9e\u73b0\u8d77\u6765\u5f88\u7b80\u5355\uff0c\u6240\u4ee5\u4e00\u822c\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u90fd\u662f\u7528\u4e92\u76f8\u5173\u64cd\u4f5c\u53d6\u4ee3\u771f\u5377\u79ef\uff09,  is a batch size,  \u4ee3\u8868\u901a\u9053\u7684\u6570\u91cf,  \u662f\u8f93\u5165\u7684\u4e8c\u7ef4\u6570\u636e\u7684\u50cf\u7d20\u9ad8\u5ea6\uff0c \u662f\u8f93\u5165\u7684\u4e8c\u7ef4\u6570\u636e\u7684\u50cf\u7d20\u5bbd\u5ea6\u3002</p> <ul> <li> <p><code>stride</code> \u53c2\u6570\u63a7\u5236\u4e86\u4e92\u76f8\u5173\u64cd\u4f5c(\u4f2a\u5377\u79ef\uff09\u7684\u6b65\u957f\uff0c\u53c2\u6570\u7684\u6570\u636e\u7c7b\u578b\u4e00\u822c\u662f\u5355\u4e2a\u6570\u5b57\u6216\u8005\u4e00\u4e2a\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\u3002</p> </li> <li> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u4e8c\u7ef4\u5377\u79ef\u6838\u7684\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> </li> <li> <p><code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c </p> <p>&gt; *   \u5f53 groups=1, \u6b64Conv1d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv1d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv1d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= <code>in_channels</code>, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f</p> </li> </ul> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>\u8fd9\u51e0\u4e2a\u53c2\u6570\u5747\u652f\u6301\u4e00\u4e0b\u8f93\u5165\u5f62\u5f0f\uff1a</p> <ul> <li>\u4e00\u4e2a <code>int</code> \u6570\u5b57 \u2013 \u4e8c\u7ef4\u6570\u636e\u7684\u9ad8\u548c\u5bbd\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u91c7\u7528\u8fd9\u4e00\u4e2a\u6570\u5b57\u3002</li> <li>\u4e00\u4e2a\u7531\u4e24\u4e2aint\u6570\u5b57\u7ec4\u6210\u7684<code>tuple</code>\u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e8c\u7ef4\u6570\u636e\u7684\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note \u5f53<code>groups == in_channels</code> \u5e76\u4e14 <code>out_channels == K * in_channels</code>(\u5176\u4e2dK\u662f\u6b63\u6574\u6570\uff09\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u4e5f\u88ab\u79f0\u4e3a\u6df1\u5ea6\u5377\u79ef\u3002</p> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u5927\u5c0f\u4e3a\u7684\u8f93\u5165\uff0c\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u4e58\u6570\u4e3a<code>K</code>\u7684\u6df1\u5ea6\u5377\u79ef\u5c42\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u53c2\u6570\u6765\u521b\u5efa\uff1a\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Conv2d\u7684\u53c2\u6570: </p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u8f93\u51fa\u901a\u9053\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013\u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 \u8f93\u5165\u6570\u636e\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>dilation (int or tuple, optional) \u2013\u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  \u5176\u4e2d</p> <p></p> <p></p> </li> </ul> <p>| \u5185\u90e8Variables: | </p> <ul> <li>weight (Tensor) \u2013 Conv2d\u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1])\u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f \uff0c \u5176\u4e2d\u3002</li> <li>bias (Tensor) \u2013 \u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a(out_channels)\uff0c\u53ef\u8bad\u7ec3\u5b66\u4e60\u3002\u5982\u679c\u6784\u9020Conv2d\u65f6\u6784\u9020\u51fd\u6570\u4e2d\u7684<code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c\u5176\u4e2d\u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2)\n&gt;&gt;&gt; # non-square kernels and unequal stride and with padding\n&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation\n&gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#conv3d","title":"Conv3d","text":"<pre><code>class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e09\u7ef4\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e09\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u4e09\u7ef4\u5377\u79ef\u64cd\u4f5c\u7684\u5377\u79ef\u5c42\u3002</p> <p>\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a\uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a \u7684\u4e09\u7ef4\u5377\u79ef\u5c42\uff0c\u5176\u5377\u79ef\u8ba1\u7b97\u8fc7\u7a0b\u53ef\u4ee5\u5982\u4e0b\u8868\u8ff0\uff1a</p> <p></p> <p>\u8fd9\u91cc\u7684 \u7b26\u53f7\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u4e09\u7ef4\u4e92\u76f8\u5173 cross-correlation \u64cd\u4f5c\u7b26\u3002</p> <ul> <li> <p><code>stride</code> \u6570\u63a7\u5236\u4e86\u4e92\u76f8\u5173\u64cd\u4f5c(\u4f2a\u5377\u79ef\uff09\u7684\u6b65\u957f\u3002</p> </li> <li> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u4e09\u7ef4\u5377\u79ef\u6838\u7684\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> </li> <li> <p><code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c</p> </li> </ul> <p>&gt; *   \u5f53 groups=1, \u6b64Conv3d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u5bf9\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv3d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv3d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= in_channels, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f .</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code>\u8fd9\u51e0\u4e2a\u53c2\u6570\u5747\u652f\u6301\u4e00\u4e0b\u8f93\u5165\u5f62\u5f0f\uff1a</p> <ul> <li>\u4e00\u4e2a <code>int</code> \u6570\u5b57 \u2013 \u4e09\u7ef4\u7ef4\u6570\u636e\u7684\u6df1\u5ea6\uff0c\u9ad8\u548c\u5bbd\u8fd9\u4e09\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u91c7\u7528\u8fd9\u4e00\u4e2a\u6570\u5b57\u3002</li> <li>\u4e00\u4e2a\u7531\u4e09\u4e2aint\u6570\u5b57\u7ec4\u6210\u7684<code>tuple</code>\u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e09\u7ef4\u6570\u636e\u7684\u6df1\u5ea6\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e09\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note</p> <p>\u5f53<code>groups == in_channels</code> \u5e76\u4e14 <code>out_channels == K * in_channels</code>(\u5176\u4e2dK\u662f\u6b63\u6574\u6570\uff09\u7684\u65f6\u5019\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u4e5f\u88ab\u79f0\u4e3a\u6df1\u5ea6\u5377\u79ef\u3002</p> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u5927\u5c0f\u4e3a   \u7684\u8f93\u5165\uff0c\u8981\u6784\u5efa\u4e00\u4e2a\u6df1\u5ea6\u4e58\u6570\u4e3a<code>K</code>\u7684\u6df1\u5ea6\u5377\u79ef\u5c42\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u53c2\u6570\u6765\u521b\u5efa\uff1a\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Parameters: </p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u7684\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u901a\u9053\u7684\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 \u8f93\u5165\u6570\u636e\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>dilation (int or tuple, optional) \u2013 \u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  where</p> <p></p> <p></p> <p></p> </li> </ul> <p>| \u5185\u90e8Variables: | </p> <ul> <li>weight (Tensor) \u2013 Conv3d\u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2]) \u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c\u5176\u4e2d\u3002</li> <li>bias (Tensor) \u2013 \u6a21\u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a(out_channels)\uff0c\u53ef\u8bad\u7ec3\u5b66\u4e60\u3002\u5982\u679c\u6784\u9020Conv1d\u65f6\u6784\u9020\u51fd\u6570\u4e2d\u7684<code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f  \uff0c\u5176\u4e2d \u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)\n&gt;&gt;&gt; # non-square kernels and unequal stride and with padding\n&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#convtranspose1d","title":"ConvTranspose1d","text":"<pre><code>class torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e00\u7ef4\u8f6c\u7f6e\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e00\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u8f6c\u7f6e\u5377\u79ef(\u5f53\u7136\u6b64\u5377\u79ef\u4e5f\u662f\u4e92\u76f8\u5173\u64cd\u4f5c\uff0ccross-correlation\uff09\u64cd\u4f5c\u7684\u6a21\u5757\u3002</p> <p>\u8be5\u6a21\u5757\u53ef\u4ee5\u770b\u4f5c\u662fConv1d\u76f8\u5bf9\u4e8e\u5176\u8f93\u5165\u7684\u68af\u5ea6(the gradient of Conv1d with respect to its input\uff0c \u76f4\u8bd1)\uff0c \u8f6c\u7f6e\u5377\u79ef\u53c8\u88ab\u79f0\u4e3a\u5c0f\u6570\u6b65\u957f\u5377\u79ef\u6216\u662f\u53cd\u5377\u79ef(\u5c3d\u7ba1\u8fd9\u4e0d\u662f\u4e00\u4e2a\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u53cd\u5377\u79ef\uff09\u3002</p> <ul> <li> <p><code>stride</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f</p> </li> <li> <p><code>padding</code> \u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u7684\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0eConv1d\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>output_padding</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u7684\u957f\u5ea6\u589e\u91cf\uff0c\u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002\u66f4\u591a\u7684\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>dilation</code> \u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86dilation\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c</p> <p>&gt; *   \u5f53 groups=1, \u6b64Conv1d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv1d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv1d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= <code>in_channels</code>, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f\u3002</p> </li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note</p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0e\u5728Conv1d\u4e2d\u4e0d\u540c\u7684\u662f\uff0c\u5728\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c \u8fd9\u6837\u8bbe\u7f6e\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5f53\u4f7f\u7528\u76f8\u540c\u7684\u53c2\u6570\u6784\u5efa<code>Conv1d</code> \u548c<code>ConvTranspose1d</code>\u6a21\u5757\u7684\u65f6\u5019\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u80fd\u591f\u5b9e\u73b0\u4e24\u4e2a\u6a21\u5757\u6709\u6b63\u597d\u76f8\u53cd\u7684\u8f93\u5165\u8f93\u51fa\u7684\u5927\u5c0f\uff0c\u5373Conv1d\u7684\u8f93\u51fa\u5927\u5c0f\u662f\u5176\u5bf9\u5e94\u7684ConvTranspose1d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\uff0c\u800cConvTranspose1d\u7684\u8f93\u51fa\u5927\u5c0f\u53c8\u6070\u597d\u662f\u5176\u5bf9\u5e94\u7684Conv1d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\u3002\u7136\u800c\uff0c\u5f53<code>stride &gt; 1</code>\u7684\u65f6\u5019\uff0c<code>Conv1d</code> \u7684\u4e00\u4e2a\u8f93\u51fa\u5927\u5c0f\u53ef\u80fd\u4f1a\u5bf9\u5e94\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\uff0c\u4e0a\u4e00\u4e2anote\u4e2d\u5c31\u8be6\u7ec6\u7684\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b\u8981\u4fdd\u6301\u524d\u9762\u63d0\u5230\u4e24\u79cd\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u4fdd\u6301\u53cd\u5411\u4e00\u81f4\uff0c\u90a3\u5c31\u8981\u7528\u5230 <code>output_padding</code>\u53c2\u6570\u4e86\uff0c\u8fd9\u4e2a\u53c2\u6570\u53ef\u4ee5\u589e\u52a0\u8f6c\u7f6e\u5377\u79ef\u8f93\u51fa\u7684\u67d0\u4e00\u7ef4\u5ea6\u7684\u5927\u5c0f\uff0c\u4ee5\u6b64\u6765\u8fbe\u5230\u524d\u9762\u63d0\u5230\u7684\u540c\u53c2\u6570\u6784\u5efa\u7684<code>Conv1d</code> \u548c<code>ConvTranspose1d</code>\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u65b9\u5411\u4e00\u81f4\u3002 \u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5404\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Parameters: </p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u7684\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u901a\u9053\u7684\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 <code>kernel_size - 1 - padding</code> \u5c42 0 \u4f1a\u88ab\u8865\u9f50\u5230\u8f93\u5165\u6570\u636e\u7684\u5404\u8fb9\u4e0a\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>output_padding (int or tuple, optional) \u2013 \u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u8981\u589e\u52a0\u7684\u5927\u5c0f\u3002\u9ed8\u8ba4\uff1a0 </li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> <li>dilation (int or tuple, optional) \u2013 \u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  \u5176\u4e2d\uff0c</p> <p></p> </li> </ul> <p>| Variables: | </p> <ul> <li>weight (Tensor) \u2013  \u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a (in_channels, out_channels, kernel_size[0])\u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c\u5176\u4e2d \u3002</li> <li>bias (Tensor) \u2013 \u6a21\u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a (out_channels)\uff0c \u5982\u679c\u6784\u9020\u51fd\u6570\u4e2d\u7684 <code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f  \uff0c\u5176\u4e2d \u3002</li> </ul>"},{"location":"1.0/nn/#convtranspose2d","title":"ConvTranspose2d","text":"<pre><code>class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e8c\u7ef4\u8f6c\u7f6e\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e8c\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u8f6c\u7f6e\u5377\u79ef(\u5f53\u7136\u6b64\u5377\u79ef\u4e5f\u662f\u4e92\u76f8\u5173\u64cd\u4f5c\uff0ccross-correlation\uff09\u64cd\u4f5c\u7684\u6a21\u5757\u3002</p> <p>\u8be5\u6a21\u5757\u53ef\u4ee5\u770b\u4f5c\u662fConv2d\u76f8\u5bf9\u4e8e\u5176\u8f93\u5165\u7684\u68af\u5ea6(the gradient of Conv2d with respect to its input\uff0c \u76f4\u8bd1)\uff0c \u8f6c\u7f6e\u5377\u79ef\u53c8\u88ab\u79f0\u4e3a\u5c0f\u6570\u6b65\u957f\u5377\u79ef\u6216\u662f\u53cd\u5377\u79ef(\u5c3d\u7ba1\u8fd9\u4e0d\u662f\u4e00\u4e2a\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u53cd\u5377\u79ef\uff09\u3002</p> <ul> <li> <p><code>stride</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f </p> </li> <li> <p><code>padding</code> \u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u7684\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0eConv1d\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>output_padding</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u7684\u957f\u5ea6\u589e\u91cf\uff0c\u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002\u66f4\u591a\u7684\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>dilation</code> \u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86dilation\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c</p> <p>&gt; *   \u5f53 groups=1, \u6b64Conv1d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv1d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv1d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= <code>in_channels</code>, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f\u3002</p> </li> </ul> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> \u8fd9\u51e0\u4e2a\u53c2\u6570\u5747\u652f\u6301\u4e00\u4e0b\u8f93\u5165\u5f62\u5f0f\uff1a</p> <ul> <li>\u4e00\u4e2a <code>int</code> \u6570\u5b57 \u2013 \u4e8c\u7ef4\u7ef4\u6570\u636e\u7684\u9ad8\u548c\u5bbd\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u91c7\u7528\u8fd9\u4e00\u4e2a\u6570\u5b57\u3002</li> <li>\u4e00\u4e2a\u7531\u4e24\u4e2aint\u6570\u5b57\u7ec4\u6210\u7684<code>tuple</code>\u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e8c\u7ef4\u6570\u636e\u7684\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note</p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0e\u5728Conv1d\u4e2d\u4e0d\u540c\u7684\u662f\uff0c\u5728\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c \u8fd9\u6837\u8bbe\u7f6e\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5f53\u4f7f\u7528\u76f8\u540c\u7684\u53c2\u6570\u6784\u5efa<code>Conv2d</code> \u548c<code>ConvTranspose2d</code>\u6a21\u5757\u7684\u65f6\u5019\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u80fd\u591f\u5b9e\u73b0\u4e24\u4e2a\u6a21\u5757\u6709\u6b63\u597d\u76f8\u53cd\u7684\u8f93\u5165\u8f93\u51fa\u7684\u5927\u5c0f\uff0c\u5373Conv2d\u7684\u8f93\u51fa\u5927\u5c0f\u662f\u5176\u5bf9\u5e94\u7684ConvTranspose2d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\uff0c\u800cConvTranspose2d\u7684\u8f93\u51fa\u5927\u5c0f\u53c8\u6070\u597d\u662f\u5176\u5bf9\u5e94\u7684Conv2d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\u3002\u7136\u800c\uff0c\u5f53<code>stride &gt; 1</code>\u7684\u65f6\u5019\uff0c<code>Conv2d</code> \u7684\u4e00\u4e2a\u8f93\u51fa\u5927\u5c0f\u53ef\u80fd\u4f1a\u5bf9\u5e94\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\uff0c\u4e0a\u4e00\u4e2anote\u4e2d\u5c31\u8be6\u7ec6\u7684\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b\u8981\u4fdd\u6301\u524d\u9762\u63d0\u5230\u4e24\u79cd\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u4fdd\u6301\u53cd\u5411\u4e00\u81f4\uff0c\u90a3\u5c31\u8981\u7528\u5230 <code>output_padding</code>\u53c2\u6570\u4e86\uff0c\u8fd9\u4e2a\u53c2\u6570\u53ef\u4ee5\u589e\u52a0\u8f6c\u7f6e\u5377\u79ef\u8f93\u51fa\u7684\u67d0\u4e00\u7ef4\u5ea6\u7684\u5927\u5c0f\uff0c\u4ee5\u6b64\u6765\u8fbe\u5230\u524d\u9762\u63d0\u5230\u7684\u540c\u53c2\u6570\u6784\u5efa\u7684<code>Conv2d</code> \u548c<code>ConvTranspose2d</code>\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u65b9\u5411\u4e00\u81f4\u3002 \u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5404\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Parameters:</p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u7684\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u901a\u9053\u7684\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 <code>kernel_size - 1 - padding</code> \u5c42 0 \u4f1a\u88ab\u8865\u9f50\u5230\u8f93\u5165\u6570\u636e\u7684\u5404\u8fb9\u4e0a\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>output_padding (int or tuple, optional) \u2013 \u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u8981\u589e\u52a0\u7684\u5927\u5c0f\u3002\u9ed8\u8ba4\uff1a0 </li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> <li>dilation (int or tuple, optional) \u2013 \u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d</li> </ul> <p></p> <p></p> <p>| Variables: | </p> <ul> <li>weight (Tensor) \u2013  \u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1])\u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c\u5176\u4e2d \u3002</li> <li>bias (Tensor) \u2013 \u6a21\u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a (out_channels)\uff0c \u5982\u679c\u6784\u9020\u51fd\u6570\u4e2d\u7684 <code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f  \uff0c\u5176\u4e2d \u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n&gt;&gt;&gt; # non-square kernels and unequal stride and with padding\n&gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # exact output size can be also specified as an argument\n&gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)\n&gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n&gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n&gt;&gt;&gt; h = downsample(input)\n&gt;&gt;&gt; h.size()\ntorch.Size([1, 16, 6, 6])\n&gt;&gt;&gt; output = upsample(h, output_size=input.size())\n&gt;&gt;&gt; output.size()\ntorch.Size([1, 16, 12, 12])\n\n</code></pre>"},{"location":"1.0/nn/#convtranspose3d","title":"ConvTranspose3d","text":"<pre><code>class torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n</code></pre> <p>\u5229\u7528\u6307\u5b9a\u5927\u5c0f\u7684\u4e09\u7ef4\u8f6c\u7f6e\u5377\u79ef\u6838\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4e09\u7ef4\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u8f6c\u7f6e\u5377\u79ef(\u5f53\u7136\u6b64\u5377\u79ef\u4e5f\u662f\u4e92\u76f8\u5173\u64cd\u4f5c\uff0ccross-correlation\uff09\u64cd\u4f5c\u7684\u6a21\u5757\u3002\u8f6c\u7f6e\u5377\u79ef\u7684\u64cd\u4f5c\u672c\u8d28\u662f\u5c06\u5404\u901a\u9053\u8f93\u5165\u4e0e\u5377\u79ef\u6838\u505a\u4e58\u6cd5\uff0c\u7136\u540e\u8fd4\u56de\u5404\u901a\u9053\u4e0e\u6b64\u5377\u79ef\u6838\u4e58\u79ef\u7ed3\u679c\u4e4b\u548c(\u5377\u79ef\u7684\u5b9a\u4e49\uff09\u3002</p> <p>\u8be5\u6a21\u5757\u53ef\u4ee5\u770b\u4f5c\u662fConv3d\u76f8\u5bf9\u4e8e\u5176\u8f93\u5165\u7684\u68af\u5ea6(the gradient of Conv3d with respect to its input\uff0c \u76f4\u8bd1)\uff0c \u8f6c\u7f6e\u5377\u79ef\u53c8\u88ab\u79f0\u4e3a\u5c0f\u6570\u6b65\u957f\u5377\u79ef\u6216\u662f\u53cd\u5377\u79ef(\u5c3d\u7ba1\u8fd9\u4e0d\u662f\u4e00\u4e2a\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u53cd\u5377\u79ef\uff09\u3002</p> <ul> <li> <p><code>stride</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f </p> </li> <li> <p><code>padding</code> \u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u7684\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0eConv1d\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>output_padding</code> \u63a7\u5236\u4e86\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u7684\u957f\u5ea6\u589e\u91cf\uff0c\u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002\u66f4\u591a\u7684\u8be6\u60c5\u8bf7\u89c1\u4e0b\u9762\u7684note\u3002</p> </li> <li> <p><code>dilation</code> \u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86dilation\u7684\u4f5c\u7528\u3002</p> </li> <li> <p><code>groups</code> \u63a7\u5236\u4e86\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u7684\u8fde\u63a5(connections\uff09\u7684\u6570\u91cf\u3002<code>in_channels</code> \u548c <code>out_channels</code> \u5fc5\u987b\u80fd\u88ab <code>groups</code> \u6574\u9664\u3002\u4e3e\u4e2a\u6817\u5b50\uff0c</p> <p>&gt; *   \u5f53 groups=1, \u6b64Conv1d\u5c42\u4f1a\u4f7f\u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u6240\u6709\u8f93\u5165\u5230\u8f93\u51fa\u7684\u5377\u79ef\u64cd\u4f5c\u3002</p> <p>&gt; *   \u5f53 groups=2, \u6b64\u65f6Conv1d\u5c42\u4f1a\u4ea7\u751f\u4e24\u4e2a\u5e76\u5217\u7684\u5377\u79ef\u5c42\u3002\u540c\u65f6\uff0c\u8f93\u5165\u901a\u9053\u88ab\u5206\u4e3a\u4e24\u534a\uff0c\u4e24\u4e2a\u5377\u79ef\u5c42\u5206\u522b\u5904\u7406\u4e00\u534a\u7684\u8f93\u5165\u901a\u9053\uff0c\u540c\u65f6\u5404\u81ea\u4ea7\u751f\u4e00\u534a\u7684\u8f93\u51fa\u901a\u9053\u3002\u6700\u540e\u8fd9\u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u51fa\u4f1a\u88abconcatenated\u4e00\u8d77\uff0c\u4f5c\u4e3a\u6b64Conv1d\u5c42\u7684\u8f93\u51fa\u3002</p> <p>&gt; *   \u5f53 groups= <code>in_channels</code>, \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u88ab\u5355\u72ec\u7684\u4e00\u7ec4\u5377\u79ef\u5c42\u5904\u7406\uff0c\u8fd9\u4e2a\u7ec4\u7684\u5927\u5c0f\u662f\u3002</p> </li> </ul> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code> \u8fd9\u51e0\u4e2a\u53c2\u6570\u5747\u652f\u6301\u4e00\u4e0b\u8f93\u5165\u5f62\u5f0f\uff1a</p> <ul> <li>\u4e00\u4e2a <code>int</code> \u6570\u5b57 \u2013 \u4e09\u7ef4\u7ef4\u6570\u636e\u7684\u6df1\u5ea6\uff0c\u9ad8\u548c\u5bbd\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u90fd\u4f1a\u91c7\u7528\u8fd9\u4e00\u4e2a\u6570\u5b57\u3002</li> <li>\u4e00\u4e2a\u7531\u4e09\u4e2aint\u6570\u5b57\u7ec4\u6210\u7684<code>tuple</code>\u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e09\u7ef4\u6570\u636e\u7684\u6df1\u5ea6\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e09\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Note</p> <p>\u53d6\u51b3\u4e8e\u4f60\u5377\u79ef\u6838\u7684\u5927\u5c0f\uff0c\u6709\u4e9b\u65f6\u5019\u8f93\u5165\u6570\u636e\u4e2d\u67d0\u4e9b\u5217(\u6700\u540e\u51e0\u5217\uff09\u53ef\u80fd\u4e0d\u4f1a\u53c2\u4e0e\u8ba1\u7b97(\u6bd4\u5982\u5217\u6570\u6574\u9664\u5377\u79ef\u6838\u5927\u5c0f\u6709\u4f59\u6570\uff0c\u800c\u53c8\u6ca1\u6709padding\uff0c\u90a3\u6700\u540e\u7684\u4f59\u6570\u5217\u4e00\u822c\u4e0d\u4f1a\u53c2\u4e0e\u5377\u79ef\u8ba1\u7b97\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u56e0\u4e3apytorch\u4e2d\u7684\u4e92\u76f8\u5173\u64cd\u4f5ccross-correlation\u662f\u4fdd\u8bc1\u8ba1\u7b97\u6b63\u786e\u7684\u64cd\u4f5c(valid operation)\uff0c \u800c\u4e0d\u662f\u6ee1\u64cd\u4f5c(full operation)\u3002\u6240\u4ee5\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u8fd8\u662f\u8981\u4eb2\u5c3d\u91cf\u9009\u62e9\u597d\u5408\u9002\u7684padding\u53c2\u6570\u54e6\u3002</p> <p>Note</p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\uff0c\u4e0e\u5728Conv3d\u4e2d\u4e0d\u540c\u7684\u662f\uff0c\u5728\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\uff0c\u6b64padding\u53c2\u6570\u4e0e\u5b9e\u9645\u8865\u9f500\u7684\u5c42\u6570\u7684\u5173\u7cfb\u4e3a<code>\u5c42\u6570 = kernel_size - 1 - padding</code>\uff0c \u8fd9\u6837\u8bbe\u7f6e\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5f53\u4f7f\u7528\u76f8\u540c\u7684\u53c2\u6570\u6784\u5efa<code>Conv3d</code> \u548c<code>ConvTranspose3d</code>\u6a21\u5757\u7684\u65f6\u5019\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u80fd\u591f\u5b9e\u73b0\u4e24\u4e2a\u6a21\u5757\u6709\u6b63\u597d\u76f8\u53cd\u7684\u8f93\u5165\u8f93\u51fa\u7684\u5927\u5c0f\uff0c\u5373Conv3d\u7684\u8f93\u51fa\u5927\u5c0f\u662f\u5176\u5bf9\u5e94\u7684ConvTranspose3d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\uff0c\u800cConvTranspose3d\u7684\u8f93\u51fa\u5927\u5c0f\u53c8\u6070\u597d\u662f\u5176\u5bf9\u5e94\u7684Conv3d\u6a21\u5757\u7684\u8f93\u5165\u5927\u5c0f\u3002\u7136\u800c\uff0c\u5f53<code>stride &gt; 1</code>\u7684\u65f6\u5019\uff0c<code>Conv3d</code> \u7684\u4e00\u4e2a\u8f93\u51fa\u5927\u5c0f\u53ef\u80fd\u4f1a\u5bf9\u5e94\u591a\u4e2a\u8f93\u5165\u5927\u5c0f\uff0c\u4e0a\u4e00\u4e2anote\u4e2d\u5c31\u8be6\u7ec6\u7684\u4ecb\u7ecd\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u8fd9\u6837\u7684\u60c5\u51b5\u4e0b\u8981\u4fdd\u6301\u524d\u9762\u63d0\u5230\u4e24\u79cd\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u4fdd\u6301\u53cd\u5411\u4e00\u81f4\uff0c\u90a3\u5c31\u8981\u7528\u5230 <code>output_padding</code>\u53c2\u6570\u4e86\uff0c\u8fd9\u4e2a\u53c2\u6570\u53ef\u4ee5\u589e\u52a0\u8f6c\u7f6e\u5377\u79ef\u8f93\u51fa\u7684\u67d0\u4e00\u7ef4\u5ea6\u7684\u5927\u5c0f\uff0c\u4ee5\u6b64\u6765\u8fbe\u5230\u524d\u9762\u63d0\u5230\u7684\u540c\u53c2\u6570\u6784\u5efa\u7684<code>Conv3d</code> \u548c<code>ConvTranspose3d</code>\u6a21\u5757\u7684\u8f93\u5165\u8f93\u51fa\u65b9\u5411\u4e00\u81f4\u3002 \u4f46\u6ce8\u610f\u8fd9\u4e2a\u53c2\u6570\u4e0d\u662f\u8bf4\u8981\u5f80\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u4e0apad 0\uff0c\u800c\u662f\u76f4\u63a5\u63a7\u5236\u8f6c\u7f6e\u5377\u79ef\u7684\u8f93\u51fa\u5404\u7ef4\u5ea6\u7684\u5927\u5c0f\u4e3a\u6839\u636e\u6b64\u53c2\u6570pad\u540e\u7684\u5927\u5c0f\u3002</p> <p>Note</p> <p>\u5f53\u7a0b\u5e8f\u7684\u8fd0\u884c\u73af\u5883\u662f\u4f7f\u7528\u4e86CuDNN\u7684CUDA\u73af\u5883\u7684\u65f6\u5019\uff0c\u4e00\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5(nondeterministic algorithm\uff09\u53ef\u80fd\u4f1a\u88ab\u91c7\u7528\u4ee5\u63d0\u9ad8\u6574\u4e2a\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u5982\u679c\u4e0d\u60f3\u4f7f\u7528\u8fd9\u4e9b\u975e\u786e\u5b9a\u6027\u7684\u7b97\u6cd5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudnn.deterministic = True</code>\u6765\u8ba9\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\u4fdd\u6301\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u635f\u5931\u4e00\u5b9a\u7684\u8ba1\u7b97\u6027\u80fd\uff09\u3002\u5bf9\u4e8e\u540e\u7aef(background)\uff0c\u4f60\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e00\u90e8\u5206Reproducibility\u4e86\u89e3\u5176\u76f8\u5173\u4fe1\u606f\u3002</p> <p>Parameters:</p> <ul> <li>in_channels (int) \u2013 \u8f93\u5165\u901a\u9053\u7684\u4e2a\u6570</li> <li>out_channels (int) \u2013 \u5377\u79ef\u64cd\u4f5c\u8f93\u51fa\u901a\u9053\u7684\u4e2a\u6570</li> <li>kernel_size (int or tuple) \u2013 \u5377\u79ef\u6838\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u5377\u79ef\u64cd\u4f5c\u7684\u6b65\u957f\u3002 \u9ed8\u8ba4\uff1a 1</li> <li>padding (int or tuple, optional) \u2013 <code>kernel_size - 1 - padding</code> \u5c42 0 \u4f1a\u88ab\u8865\u9f50\u5230\u8f93\u5165\u6570\u636e\u7684\u5404\u8fb9\u4e0a\u3002 \u9ed8\u8ba4\uff1a 0</li> <li>output_padding (int or tuple, optional) \u2013 \u8f93\u51fa\u7684\u5404\u7ef4\u5ea6\u8981\u589e\u52a0\u7684\u5927\u5c0f\u3002\u9ed8\u8ba4\uff1a0 </li> <li>groups (int, optional) \u2013 \u8f93\u5165\u901a\u9053\u4e0e\u8f93\u51fa\u901a\u9053\u4e4b\u95f4\u76f8\u4e92\u9694\u79bb\u7684\u8fde\u63a5\u7684\u4e2a\u6570\u3002 \u9ed8\u8ba4\uff1a1</li> <li>bias (bool, optional) \u2013 \u5982\u679c\u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u5411\u8f93\u51fa\u589e\u52a0\u4e00\u4e2a\u504f\u5dee\u91cf\uff0c\u6b64\u504f\u5dee\u662f\u53ef\u5b66\u4e60\u53c2\u6570\u3002 \u9ed8\u8ba4\uff1a<code>True</code></li> <li>dilation (int or tuple, optional) \u2013 \u5377\u79ef\u6838\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u9ed8\u8ba4\uff1a 1</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d</li> </ul> <p></p> <p></p> <p></p> <p>| Variables: | </p> <ul> <li>weight (Tensor) \u2013  \u6a21\u5757\u4e2d\u7684\u4e00\u4e2a\u5927\u5c0f\u4e3a (in_channels, out_channels, kernel_size[0], kernel_size[1], kernel_size[2])\u7684\u6743\u91cd\u5f20\u91cf\uff0c\u8fd9\u4e9b\u6743\u91cd\u53ef\u8bad\u7ec3\u5b66\u4e60(learnable)\u3002\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f\uff0c\u5176\u4e2d \u3002</li> <li>bias (Tensor) \u2013 \u6a21\u5757\u7684\u504f\u5dee\u9879\uff0c\u5927\u5c0f\u4e3a (out_channels)\uff0c \u5982\u679c\u6784\u9020\u51fd\u6570\u4e2d\u7684 <code>bias</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u8fd9\u4e9b\u6743\u91cd\u7684\u521d\u59cb\u503c\u7684\u91c7\u6837\u7a7a\u95f4\u662f \uff0c\u5176\u4e2d \u3002</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2)\n&gt;&gt;&gt; # non-square kernels and unequal stride and with padding\n&gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#unfold","title":"Unfold","text":"<pre><code>class torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n</code></pre> <p>\u5c06\u4e00\u4e2abatch\u7684\u8f93\u5165\u5f20\u91cf\u5c55\u5f00\u6210\u7531\u591a\u4e2a\u6ed1\u52a8\u5c40\u90e8\u5757\u7ec4\u6210\u7684\u5f62\u5f0f\u3002(im2col\u7684\u6269\u5c55\u6a21\u5757\uff0c\u8d77\u5230\u57fa\u672c\u7c7b\u4f3cim2col\u7684\u4f5c\u7528\uff09</p> <p>\u4ee5\u4e00\u4e2a\u5927\u5c0f\u4e3a\u7684\u6279\u6b21\u5316(batched)\u8f93\u5165\u5f20\u91cf\u4e3a\u4f8b\uff0c\u5176\u4e2d\u662fbatch\u7684\u5927\u5c0f\uff0c\u662f\u901a\u9053\u6570\u91cf\uff0c\u4ee3\u8868\u4e86\u4efb\u610f\u7a7a\u95f4\u7ef4\u5ea6\u3002\u90a3Unfold\u8fd9\u4e2a\u64cd\u4f5c\u5728\u6b64\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u5c31\u662f\uff0c\u5c06\u8fd9\u4e2a\u5f20\u91cf\u5c55\u5f00\u6210\u7531\u591a\u4e2a<code>kernel_size</code>\u5927\u5c0f\u7684\u6ed1\u52a8\u5757\u7ec4\u6210\u7684\u5927\u5c0f\u4e3a\u7684\u4e09\u7ef4\u5f20\u91cf\uff0c\u5176\u4e2d\u662f\u6bcf\u4e2a\u5757\u4e2d\u6570\u7684\u4e2a\u6570(\u6bcf\u4e2a\u5757\u6709\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u5b58\u50a8\u4e00\u4e2a\u901a\u9053\u5927\u5c0f\u4e3a\u7684\u5411\u91cf\uff09\uff0c\u662f\u5757\u7684\u4e2a\u6570\uff1a</p> <p> (\u8fd9\u5f20\u56fe\u6709\u95ee\u9898\u554a\uff0c\u7f16\u8f91\u6574\u7406\u7684\u65f6\u5019\u6ce8\u610f\u4fee\u6b63\u4e00\u4e0b\uff09</p> <p>\u5176\u4e2d  \u662f\u7531\u4e0a\u9762\u4f8b\u5b50\u4e2d\u7684<code>input</code>\u5404\u7a7a\u95f4\u7ef4\u5ea6\u7ec4\u6210\u7684\uff0c\u904d\u5386\u4e86\u5404\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u3002</p> <p>\u56e0\u6b64\uff0c\u7d22\u5f15Fold\u64cd\u4f5c\u7684<code>output</code>\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u7b49\u4ef7\u4e8e\u7d22\u5f15\u67d0\u4e00\u4e2ablock\uff0c\u800c\u7d22\u5f15\u64cd\u4f5c\u7684\u8fd4\u56de\u503c\u662f\u8fd9\u4e2a\u7d22\u5f15\u5230\u7684block\u4e2d\u7684\u6240\u6709\u503c\u3002</p> <p><code>padding</code>, <code>stride</code> \u548c <code>dilation</code> \u53c2\u6570\u6307\u660e\u4e86\u6ed1\u52a8\u5757\u7684\u76f8\u5173\u6027\u8d28\u3002</p> <ul> <li><code>stride</code> \u63a7\u5236\u4e86\u6ed1\u52a8\u5757\u7684\u6b65\u957f\u3002</li> <li><code>padding</code> \u63a7\u5236\u4e86\u5728\u53d8\u6362\u4e4b\u524d\u8981\u5411input\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f50\u76840\u7684\u5c42\u6570\u3002 </li> <li><code>dilation</code> \u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86dilation\u7684\u4f5c\u7528\u3002</li> </ul> <p>Parameters: </p> <ul> <li>kernel_size (int or tuple) \u2013 \u6ed1\u52a8\u5757\u7684\u5927\u5c0f</li> <li>stride (int or tuple, optional) \u2013 \u6ed1\u52a8\u5757\u5728\u8f93\u5165\u5404\u7ef4\u5ea6\u4e0a\u7684\u6b65\u957f\u3002\u9ed8\u8ba4: 1</li> <li>padding (int or tuple, optional) \u2013 \u5728\u8f93\u5165\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\u3002</li> <li> <p>dilation (int or tuple, optional) \u2013 \u63a7\u5236\u4e86\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb(\u6ca1\u6709\u6307\u660e\u5143\u7d20\u5177\u4f53\u6307\u7684\u662f\u8c01\u7684\u5143\u7d20\uff0c\u731c\u6d4b\u662f\u8f93\u51fa\u7684\uff09\u3002\u9ed8\u8ba4\uff1a1 </p> </li> <li> <p>\u5982\u679c <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> \u6216\u8005 <code>stride</code>\u7684\u503c\u662f\u4e00\u4e2aint\uff0c\u6216\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a1\u7684int\u5143\u7ec4\uff0c\u5728\u76f8\u5173\u64cd\u4f5c\u7684\u65f6\u5019\u5404\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u90fd\u4f1a\u4f7f\u7528\u8fd9\u540c\u4e00\u4e2a\u503c\u3002 </p> </li> <li>\u5982\u679c\u8f93\u51fa\u5411\u91cf\u6709\u4e24\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u90a3\u4e48\u6b64Fold\u64cd\u4f5c\u6709\u65f6\u53c8\u88ab\u79f0\u4e3a<code>im2col</code>\u3002</li> </ul> <p>Note <code>Fold</code>\u5728\u6267\u884c\u7c7b<code>col2im</code>\u7684\u64cd\u4f5c\u7684\u65f6\u5019\uff0c\u4e3b\u8981\u662f\u662f\u901a\u8fc7\u96c6\u6210\u6b64im(\u8f93\u51fa\u5f20\u91cf\uff09\u5206\u88c2\u51fa\u6240\u6709\u5bf9\u5e94\u4f4d\u7f6e\u7684col(\u8f93\u5165\u7684\u6ed1\u52a8\u5757\uff09\u6765\u590d\u539f\u539fim\u3002\u800c<code>Unfold</code>\u5219\u662f\u901a\u8fc7\u4ece\u8f93\u5165\u5f20\u91cf\u4e2d\u4e0d\u65ad\u62f7\u8d1d\u6570\u503c\u5230\u76f8\u5e94\u7684block\u4e2d\u6765\u751f\u6210\u7531\u6ed1\u52a8\u5757\u7ec4\u6210\u7684\u8f93\u51fa\u5f20\u91cf\u3002\u6240\u4ee5\uff0c\u5982\u679c\u6ed1\u52a8\u5757\u4e4b\u95f4\u5982\u679c\u6709\u6570\u503c\u91cd\u53e0\uff0c\u90a3\u8fd9\u4e9b\u6ed1\u52a8\u5757\u4e4b\u95f4\u5e76\u4e0d\u662f\u4e92\u9006\u7684\u3002</p> <p>Warning</p> <p>\u76ee\u524d\uff0c\u53ea\u6709\u56db\u7ef4\u5f20\u91cf(\u6bd4\u5982\u6279\u6b21\u5316\u7684\u56fe\u50cf\u5f20\u91cf\uff09\u652f\u6301\u8fd9\u4e2a\u64cd\u4f5c\u3002</p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa: </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3))\n&gt;&gt;&gt; input = torch.randn(2, 5, 3, 4)\n&gt;&gt;&gt; output = unfold(input)\n&gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels)\n&gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input\n&gt;&gt;&gt; output.size()\ntorch.Size([2, 30, 4])\n\n&gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\n&gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12)\n&gt;&gt;&gt; w = torch.randn(2, 3, 4, 5)\n&gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5))\n&gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n&gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))\n&gt;&gt;&gt; # or equivalently (and avoiding a copy),\n&gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8)\n&gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max()\ntensor(1.9073e-06)\n\n</code></pre>"},{"location":"1.0/nn/#fold","title":"Fold","text":"<pre><code>class torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)\n</code></pre> <p>\u5c06\u7531\u6ed1\u52a8\u5c40\u90e8\u5757\u7ec4\u6210\u7684\u6570\u7ec4\u96c6\u5408\u4e3a\u4e00\u4e2a\u5927\u5f20\u91cf\u3002(\u7c7bcol2im)</p> <p>\u8003\u8651\u4e00\u4e2a\u5305\u542b\u4e86\u5f88\u591a\u4e2a\u6ed1\u52a8\u5c40\u90e8\u5757\u7684\u8f93\u5165\u5f20\u91cf\uff0c\u6bd4\u5982\uff0c\u4e00\u6279\u56fe\u50cf\u5206\u5272\u5757(patches of images)\u7684\u96c6\u5408\uff0c\u5927\u5c0f\u4e3a\uff0c\u5176\u4e2d\u662fbatch\u5927\u5c0f\uff0c  \u662f\u4e00\u4e2a\u5757\u4e2d\u7684\u6570\u503c\u4e2a\u6570(\u6bcf\u4e2a\u5757\u6709\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\uff0c\u6bcf\u4e2a\u7a7a\u95f4\u4f4d\u7f6e\u5b58\u50a8\u4e00\u4e2a\u901a\u9053\u5927\u5c0f\u4e3a\u7684\u5411\u91cf\uff09\uff0c\u662f\u6ed1\u52a8\u5757\u7684\u4e2a\u6570\u3002(\u8fd9\u4e9b\u5927\u5c0f\u53c2\u6570\u4e25\u683c\u9075\u5faa\u4e86<code>Unfold</code>\u64cd\u4f5c\u7684\u8f93\u51fa\u5411\u91cf\u7684\u5927\u5c0f\u89c4\u5b9a\u3002\uff09Fold\u64cd\u4f5c\u901a\u8fc7\u6c42\u548c\u91cd\u53e0\u503c\u7684\u65b9\u5f0f\u6765\u5c06\u8fd9\u4e9b\u5c40\u90e8\u5757\u96c6\u5408\u4e3a\u4e00\u4e2a\u5927\u5c0f\u4e3a\u7684<code>output</code>\u5f20\u91cf\u3002\u4e0e <code>Unfold</code>\u7c7b\u4f3c\uff0c\u8fd9\u4e9b\u53c2\u6570\u5fc5\u987b\u6ee1\u8db3\uff1a</p> <p></p> <p>\u5176\u4e2d\u904d\u5386\u4e86\u5404\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u3002</p> <ul> <li><code>output_size</code> \u63cf\u8ff0\u4e86\u8981\u751f\u6210\u7684output\u7684\u5404\u7a7a\u95f4\u7ef4\u5ea6\u7684\u5927\u5c0f\u3002\u6709\u65f6\uff0c\u540c\u6837\u6570\u91cf\u7684\u6ed1\u52a8\u5757\uff0c\u53ef\u80fd\u4f1a\u4ea7\u751f\u591a\u79cd<code>input</code>\u7684\u5f62\u72b6\uff0c\u6bd4\u5982\uff0c\u5f53<code>stride &gt; 0</code>\u7684\u65f6\u5019\uff0c\u8fd9\u65f6\u5019\uff0c\u8bbe\u7f6e<code>output_size</code>\u53c2\u6570\u5c31\u4f1a\u663e\u5f97\u6781\u4e3a\u91cd\u8981\u3002</li> </ul> <p><code>padding</code>, <code>stride</code> \u548c <code>dilation</code> \u53c2\u6570\u6307\u660e\u4e86\u6ed1\u52a8\u5757\u7684\u76f8\u5173\u6027\u8d28\u3002</p> <ul> <li><code>stride</code> \u63a7\u5236\u4e86\u6ed1\u52a8\u5757\u7684\u6b65\u957f\u3002</li> <li><code>padding</code> \u63a7\u5236\u4e86\u5728\u53d8\u6362\u4e4b\u524d\u8981\u5411input\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f50\u76840\u7684\u5c42\u6570\u3002 </li> <li><code>dilation</code> \u63a7\u5236\u4e86\u5377\u79ef\u6838\u4e2d\u5404\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86dilation\u7684\u4f5c\u7528\u3002</li> </ul> <p>Parameters: </p> <ul> <li> <p>output_size (int or tuple) \u2013  \u8f93\u51fa\u5411\u91cf\u7684\u5404\u7a7a\u95f4\u7ef4\u5ea6\u7684\u5927\u5c0f (i.e., <code>input.sizes()[2:]</code>)</p> </li> <li> <p>kernel_size (int or tuple) \u2013 \u6ed1\u52a8\u5757\u7684\u5927\u5c0f</p> </li> <li>stride (int or tuple, optional) \u2013 \u6ed1\u52a8\u5757\u5728\u8f93\u5165\u5404\u7ef4\u5ea6\u4e0a\u7684\u6b65\u957f\u3002\u9ed8\u8ba4: 1</li> <li>padding (int or tuple, optional) \u2013 \u5728\u8f93\u5165\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8865\u9f500\u7684\u5c42\u6570\u3002</li> <li> <p>dilation (int or tuple, optional) \u2013 \u63a7\u5236\u4e86\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb(\u6ca1\u6709\u6307\u660e\u5143\u7d20\u5177\u4f53\u6307\u7684\u662f\u8c01\u7684\u5143\u7d20\uff0c\u731c\u6d4b\u662f\u8f93\u51fa\u7684\uff09\u3002\u9ed8\u8ba4\uff1a1 </p> </li> <li> <p>\u5982\u679c<code>output_size</code>\uff0c <code>kernel_size</code>, <code>dilation</code>, <code>padding</code> \u6216\u8005 <code>stride</code>\u662f\u4e00\u4e2aint\u6216\u8005\u957f\u5ea6\u4e3a1\u7684int\u5143\u7ec4\uff0c\u5728\u76f8\u5173\u64cd\u4f5c\u7684\u65f6\u5019\u5404\u4e2a\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u90fd\u4f1a\u4f7f\u7528\u8fd9\u540c\u4e00\u4e2a\u503c\u3002 </p> </li> <li>\u5982\u679c\u6b64\u8f93\u51fa\u5411\u91cf\u7684\u7a7a\u95f4\u7ef4\u5ea6\u6570\u4e3a2\uff0c\u90a3\u4e48\u6b64Fold\u64cd\u4f5c\u6709\u65f6\u53c8\u88ab\u79f0\u4e3a<code>col2im</code>\u3002</li> </ul> <p>Note <code>Fold</code>\u5728\u6267\u884c\u7c7b<code>col2im</code>\u7684\u64cd\u4f5c\u7684\u65f6\u5019\uff0c\u4e3b\u8981\u662f\u662f\u901a\u8fc7\u96c6\u6210\u6b64im(\u8f93\u51fa\u5f20\u91cf\uff09\u5206\u88c2\u51fa\u6240\u6709\u5bf9\u5e94\u4f4d\u7f6e\u7684col(\u8f93\u5165\u7684\u6ed1\u52a8\u5757\uff09\u6765\u590d\u539f\u539fim\u3002\u800c<code>Unfold</code>\u5219\u662f\u901a\u8fc7\u4ece\u8f93\u5165\u5f20\u91cf\u4e2d\u4e0d\u65ad\u62f7\u8d1d\u6570\u503c\u5230\u76f8\u5e94\u7684block\u4e2d\u6765\u751f\u6210\u7531\u6ed1\u52a8\u5757\u7ec4\u6210\u7684\u8f93\u51fa\u5f20\u91cf\u3002\u6240\u4ee5\uff0c\u5982\u679c\u6ed1\u52a8\u5757\u4e4b\u95f4\u5982\u679c\u6709\u6570\u503c\u91cd\u53e0\uff0c\u90a3\u8fd9\u4e9b\u6ed1\u52a8\u5757\u4e4b\u95f4\u5e76\u4e0d\u662f\u4e92\u9006\u7684\u3002</p> <p>Warning</p> <p>\u76ee\u524d\uff0c\u53ea\u6709\u56db\u7ef4\u5f20\u91cf(\u6bd4\u5982\u6279\u6b21\u5316\u7684\u56fe\u50cf\u5f20\u91cf\uff09\u652f\u6301\u8fd9\u4e2a\u64cd\u4f5c\u3002</p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  </li> </ul> <p>\u4e3e\u4f8b:</p> <pre><code>&gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))\n&gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 1)\n&gt;&gt;&gt; output = fold(input)\n&gt;&gt;&gt; output.size()\n\n</code></pre> <p><code>\u5377\u79ef\u5c42\u90e8\u5206Fold \u4e0e Unfold \u662f1.0\u65b0\u589e\u7684\u5185\u5bb9\uff0c\u731c\u6d4b\u5176\u4e3b\u8981\u76ee\u7684\u662f\u5f00\u653ecol2im\u548cim2col\u8fd9\u4e24\u4e2a\u901a\u8fc7\u77e9\u9635\u4e58\u6cd5\u5b9e\u73b0\u5377\u79ef\u64cd\u4f5c\u7684\u524d\u5e8f\u63a5\u53e3\uff0c\u8981\u597d\u597d\u7406\u89e3\u8fd9\u90e8\u5206\u53ef\u80fd\u8981\u4e86\u89e3\u4e00\u4e0b\u73b0\u5728\u4e3b\u6d41\u6846\u67b6\u901a\u8fc7\u5927\u77e9\u9635\u4e58\u6cd5\u6765\u5b9e\u73b0\u5377\u79ef\u64cd\u4f5c\u8fd9\u4e00\u901a\u7528\u505a\u6cd5\u4e86\uff0c\u8fd9\u4e00\u7bc7\u6587\u7ae0\u5c31\u4ecb\u7ecd\u7684\u5f88\u597d[Implementing convolution as a matrix multiplication](https://buptldy.github.io/2016/10/01/2016-10-01-im2col/)\uff0c\u8fd9\u4e00\u6bb5\u5982\u679c\u611f\u89c9\u6211\u7684\u76f4\u8bd1\u6666\u6da9\u96be\u61c2\uff0c\u90a3\u6211\u6df1\u611f\u62b1\u6b49\u5e76\u5efa\u8bae\u770b\u4e00\u4e0b\u82f1\u6587\u539f\u7248\uff0c\u867d\u7136\u6211\u89c9\u5f97\u82f1\u6587\u539f\u7248\u4ecb\u7ecd\u7684\u4e5f\u662f\u6666\u6da9\u96be\u61c2</code></p>"},{"location":"1.0/nn/#pooling-layers","title":"\u6c60\u5316\u5c42(Pooling layers\uff09","text":""},{"location":"1.0/nn/#maxpool1d","title":"MaxPool1d","text":"<pre><code>class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e00\u7ef4\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a  \uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 <code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u6c60\u5316\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>dilation \u2013 \u6ed1\u52a8\u7a97\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c \u90a3\u4e48\u6b64\u6c60\u5316\u5c42\u5728\u8fd4\u56de\u8f93\u51fa\u4fe1\u53f7\u7684\u540c\u65f6\u8fd8\u4f1a\u8fd4\u56de\u4e00\u8fde\u4e32\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e\uff0c\u5373\u6bcf\u4e2a\u6ed1\u52a8\u7a97\u7684\u6700\u5927\u503c\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u53ef\u4ee5\u5728\u540e\u9762\u7684\u4e0a\u91c7\u6837<code>torch.nn.MaxUnpool1d</code>\u4e2d\u88ab\u7528\u5230\u3002</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  \u5176\u4e2d</p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of size=3, stride=2\n&gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 50)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#maxpool2d","title":"MaxPool2d","text":"<pre><code>class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e8c\u7ef4\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a  \uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\uff0c<code>kernel_size</code>\u4e3a\u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 <code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u6c60\u5316\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> \u7b49\u53c2\u6570\u5747\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\u8f93\u5165\uff1a</p> <ul> <li>\u4e00\u4e2a\u5355\u72ec\u7684 <code>int</code> \u2013 \u6b64\u65f6\u8fd9\u4e2a<code>int</code>\u4f1a\u540c\u65f6\u63a7\u5236\u6c60\u5316\u6ed1\u52a8\u7a97\u7684\u5bbd\u548c\u9ad8\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>\u4e00\u4e2a\u7531\u4e24\u4e2a<code>int</code>\u7ec4\u6210\u7684<code>tuple</code> \u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>dilation \u2013 \u6ed1\u52a8\u7a97\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c \u90a3\u4e48\u6b64\u6c60\u5316\u5c42\u5728\u8fd4\u56de\u8f93\u51fa\u4fe1\u53f7\u7684\u540c\u65f6\u8fd8\u4f1a\u8fd4\u56de\u4e00\u8fde\u4e32\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e\uff0c\u5373\u6bcf\u4e2a\u6ed1\u52a8\u7a97\u7684\u6700\u5927\u503c\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u53ef\u4ee5\u5728\u540e\u9762\u7684\u4e0a\u91c7\u6837<code>torch.nn.MaxUnpool2d</code>\u4e2d\u88ab\u7528\u5230\u3002</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2)\n&gt;&gt;&gt; # pool of non-square window\n&gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#maxpool3d","title":"MaxPool3d","text":"<pre><code>class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e09\u7ef4\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a  \uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\uff0c<code>kernel_size</code>\u4e3a  \u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002 <code>dilation</code> \u53c2\u6570\u63a7\u5236\u4e86\u6c60\u5316\u6838\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\uff1b\u8fd9\u4e5f\u88ab\u79f0\u4e3a\u591a\u5b54\u7b97\u6cd5(\u00e0 trous algorithm)\u3002\u8fd9\u4e2a\u6982\u5ff5\u6709\u70b9\u96be\u89e3\u91ca\uff0c\u8fd9\u4e2a\u94fe\u63a5link\u7528\u53ef\u89c6\u5316\u7684\u65b9\u6cd5\u5f88\u597d\u5730\u89e3\u91ca\u4e86<code>dilation</code>\u7684\u4f5c\u7528\u3002</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> \u7b49\u53c2\u6570\u5747\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\u8f93\u5165\uff1a</p> <ul> <li>\u4e00\u4e2a\u5355\u72ec\u7684 <code>int</code> \u2013 \u6b64\u65f6\u8fd9\u4e2a<code>int</code>\u4f1a\u540c\u65f6\u63a7\u5236\u6c60\u5316\u6ed1\u52a8\u7a97\u7684\u6df1\u5ea6\uff0c\u5bbd\u548c\u9ad8\u8fd9\u4e09\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>\u4e00\u4e2a\u7531\u4e09\u4e2a<code>int</code>\u7ec4\u6210\u7684<code>tuple</code> \u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6df1\u5ea6\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e09\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u6700\u5927\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>dilation \u2013 \u6ed1\u52a8\u7a97\u4e2d\u5404\u5143\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c \u90a3\u4e48\u6b64\u6c60\u5316\u5c42\u5728\u8fd4\u56de\u8f93\u51fa\u4fe1\u53f7\u7684\u540c\u65f6\u8fd8\u4f1a\u8fd4\u56de\u4e00\u8fde\u4e32\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e\uff0c\u5373\u6bcf\u4e2a\u6ed1\u52a8\u7a97\u7684\u6700\u5927\u503c\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u53ef\u4ee5\u5728\u540e\u9762\u7684\u4e0a\u91c7\u6837<code>torch.nn.MaxUnpool3d</code>\u4e2d\u88ab\u7528\u5230\u3002</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2)\n&gt;&gt;&gt; # pool of non-square window\n&gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#maxunpool1d","title":"MaxUnpool1d","text":"<pre><code>class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)\n</code></pre> <p><code>MaxPool1d</code>\u7684\u9006\u8fc7\u7a0b\uff0c\u4e0d\u8fc7\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u9006\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u5728<code>MaxPool1d</code>\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6c60\u5316\u7a97\u533a\u57df\u5185\u7684\u975e\u6700\u5927\u503c\u90fd\u5df2\u7ecf\u4e22\u5931\u3002 <code>MaxUnpool1d</code>\u7684\u8f93\u5165\u662f<code>MaxPool1d</code>\u7684\u8f93\u51fa\uff0c\u5176\u4e2d\u4e5f\u5305\u62ec\u5305\u62ec\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15(\u5373return_indices\u6240\u63a7\u5236\u7684\u8f93\u51fa\uff09\uff0c\u9006\u6c60\u5316\u64cd\u4f5c\u7684\u8fc7\u7a0b\u5c31\u662f\u5c06<code>MaxPool1d</code>\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6700\u5927\u503c\u63d2\u56de\u5230\u539f\u6765\u7684\u4f4d\u7f6e\uff0c\u5e76\u5c06\u975e\u6700\u5927\u503c\u533a\u57df\u7f6e\u4e3a0\u3002</p> <p>Note</p> <p><code>MaxPool1d</code>\u64cd\u4f5c\u53ef\u4ee5\u5c06\u591a\u4e2a\u5927\u5c0f\u4e0d\u540c\u7684\u8f93\u5165\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u56e0\u6b64\uff0c\u6c60\u5316\u64cd\u4f5c\u7684\u53cd\u8fc7\u7a0b\uff0c<code>MaxUnpool1d</code>\u7684\u4e0a\u91c7\u6837\u8fc7\u7a0b\u7684\u8f93\u51fa\u5927\u5c0f\u5c31\u4e0d\u552f\u4e00\u4e86\u3002\u4e3a\u4e86\u9002\u5e94\u8fd9\u4e00\u70b9\uff0c\u53ef\u4ee5\u5728\u8bbe\u7f6e\u63a7\u5236\u4e0a\u91c7\u6837\u8f93\u51fa\u5927\u5c0f\u7684(<code>output_size</code>\uff09\u53c2\u6570\u3002 \u5177\u4f53\u7528\u6cd5\uff0c\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b</p> <p>Parameters: </p> <ul> <li>kernel_size (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u5927\u5c0f</li> <li>stride (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u6b65\u957f\u3002\u9ed8\u8ba4<code>kernel_size</code></li> <li>padding (int or tuple) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u8981\u8865\u9f500\u7684\u5c42\u6570</li> </ul> <pre><code>Inputs:\n</code></pre> <ul> <li><code>input</code>: \u8981\u6267\u884c\u4e0a\u91c7\u6837\u64cd\u4f5c\u7684\u5f20\u91cf</li> <li><code>indices</code>: <code>MaxPool1d</code>\u6c60\u5316\u8fc7\u7a0b\u4e2d\u8f93\u51fa\u7684\u6c60\u5316\u7a97\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u7d22\u5f15</li> <li><code>output_size</code> (\u9009\u586b): \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528<code>output_size</code>\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f</p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)\n&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n&gt;&gt;&gt; # Example showcasing the use of output_size\n&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices, output_size=input.size())\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\n\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n</code></pre>"},{"location":"1.0/nn/#maxunpool2d","title":"MaxUnpool2d","text":"<pre><code>class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)\n</code></pre> <p><code>MaxPool2d</code>\u7684\u9006\u8fc7\u7a0b\uff0c\u4e0d\u8fc7\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u9006\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u5728<code>MaxPool2d</code>\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6c60\u5316\u7a97\u533a\u57df\u5185\u7684\u975e\u6700\u5927\u503c\u90fd\u5df2\u7ecf\u4e22\u5931\u3002 <code>MaxUnpool2d</code>\u7684\u8f93\u5165\u662f<code>MaxPool2d</code>\u7684\u8f93\u51fa\uff0c\u5176\u4e2d\u4e5f\u5305\u62ec\u5305\u62ec\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15(\u5373return_indices\u6240\u63a7\u5236\u7684\u8f93\u51fa\uff09\uff0c\u9006\u6c60\u5316\u64cd\u4f5c\u7684\u8fc7\u7a0b\u5c31\u662f\u5c06<code>MaxPool2d</code>\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6700\u5927\u503c\u63d2\u56de\u5230\u539f\u6765\u7684\u4f4d\u7f6e\uff0c\u5e76\u5c06\u975e\u6700\u5927\u503c\u533a\u57df\u7f6e\u4e3a0\u3002</p> <p>Note</p> <p><code>MaxPool2d</code>\u64cd\u4f5c\u53ef\u4ee5\u5c06\u591a\u4e2a\u5927\u5c0f\u4e0d\u540c\u7684\u8f93\u5165\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u56e0\u6b64\uff0c\u6c60\u5316\u64cd\u4f5c\u7684\u53cd\u8fc7\u7a0b\uff0c<code>MaxUnpool2d</code>\u7684\u4e0a\u91c7\u6837\u8fc7\u7a0b\u7684\u8f93\u51fa\u5927\u5c0f\u5c31\u4e0d\u552f\u4e00\u4e86\u3002\u4e3a\u4e86\u9002\u5e94\u8fd9\u4e00\u70b9\uff0c\u53ef\u4ee5\u5728\u8bbe\u7f6e\u63a7\u5236\u4e0a\u91c7\u6837\u8f93\u51fa\u5927\u5c0f\u7684(<code>output_size</code>\uff09\u53c2\u6570\u3002 \u5177\u4f53\u7528\u6cd5\uff0c\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b</p> <p>Parameters: </p> <ul> <li>kernel_size (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u5927\u5c0f</li> <li>stride (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u6b65\u957f\u3002\u9ed8\u8ba4<code>kernel_size</code></li> <li>padding (int or tuple) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u8981\u8865\u9f500\u7684\u5c42\u6570</li> </ul> <pre><code>Inputs:\n</code></pre> <ul> <li><code>input</code>: \u8981\u6267\u884c\u4e0a\u91c7\u6837\u64cd\u4f5c\u7684\u5f20\u91cf</li> <li><code>indices</code>: <code>MaxPool2d</code>\u6c60\u5316\u8fc7\u7a0b\u4e2d\u8f93\u51fa\u7684\u6c60\u5316\u7a97\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u7d22\u5f15</li> <li><code>output_size</code> (\u9009\u586b): \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528<code>output_size</code>\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f</p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)\n&gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],\n [ 5,  6,  7,  8],\n [ 9, 10, 11, 12],\n [13, 14, 15, 16]]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[[  0.,   0.,   0.,   0.],\n [  0.,   6.,   0.,   8.],\n [  0.,   0.,   0.,   0.],\n [  0.,  14.,   0.,  16.]]]])\n\n&gt;&gt;&gt; # specify a different output size than input size\n&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\ntensor([[[[  0.,   0.,   0.,   0.,   0.],\n [  6.,   0.,   8.,   0.,   0.],\n [  0.,   0.,   0.,  14.,   0.],\n [ 16.,   0.,   0.,   0.,   0.],\n [  0.,   0.,   0.,   0.,   0.]]]])\n\n</code></pre>"},{"location":"1.0/nn/#maxunpool3d","title":"MaxUnpool3d","text":"<pre><code>class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)\n</code></pre> <p><code>MaxPool3d</code>\u7684\u9006\u8fc7\u7a0b\uff0c\u4e0d\u8fc7\u5e76\u4e0d\u662f\u5b8c\u5168\u7684\u9006\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u5728<code>MaxPool3d</code>\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6c60\u5316\u7a97\u533a\u57df\u5185\u7684\u975e\u6700\u5927\u503c\u90fd\u5df2\u7ecf\u4e22\u5931\u3002 <code>MaxUnpool3d</code>\u7684\u8f93\u5165\u662f<code>MaxPool3d</code>\u7684\u8f93\u51fa\uff0c\u5176\u4e2d\u4e5f\u5305\u62ec\u5305\u62ec\u6ed1\u52a8\u7a97\u6700\u5927\u503c\u7684\u7d22\u5f15(\u5373return_indices\u6240\u63a7\u5236\u7684\u8f93\u51fa\uff09\uff0c\u9006\u6c60\u5316\u64cd\u4f5c\u7684\u8fc7\u7a0b\u5c31\u662f\u5c06<code>MaxPool3d</code>\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u6700\u5927\u503c\u63d2\u56de\u5230\u539f\u6765\u7684\u4f4d\u7f6e\uff0c\u5e76\u5c06\u975e\u6700\u5927\u503c\u533a\u57df\u7f6e\u4e3a0\u3002</p> <p>Note</p> <p><code>MaxPool3d</code>\u64cd\u4f5c\u53ef\u4ee5\u5c06\u591a\u4e2a\u5927\u5c0f\u4e0d\u540c\u7684\u8f93\u5165\u6620\u5c04\u5230\u76f8\u540c\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u56e0\u6b64\uff0c\u6c60\u5316\u64cd\u4f5c\u7684\u53cd\u8fc7\u7a0b\uff0c<code>MaxUnpool3d</code>\u7684\u4e0a\u91c7\u6837\u8fc7\u7a0b\u7684\u8f93\u51fa\u5927\u5c0f\u5c31\u4e0d\u552f\u4e00\u4e86\u3002\u4e3a\u4e86\u9002\u5e94\u8fd9\u4e00\u70b9\uff0c\u53ef\u4ee5\u5728\u8bbe\u7f6e\u63a7\u5236\u4e0a\u91c7\u6837\u8f93\u51fa\u5927\u5c0f\u7684(<code>output_size</code>\uff09\u53c2\u6570\u3002 \u5177\u4f53\u7528\u6cd5\uff0c\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u8f93\u5165\u548c\u793a\u4f8b</p> <p>Parameters: </p> <ul> <li>kernel_size (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u5927\u5c0f</li> <li>stride (int or tuple) \u2013 \u6700\u5927\u6c60\u5316\u7a97\u7684\u6b65\u957f\u3002\u9ed8\u8ba4<code>kernel_size</code></li> <li>padding (int or tuple) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u8981\u8865\u9f500\u7684\u5c42\u6570</li> </ul> <pre><code>Inputs:\n</code></pre> <ul> <li><code>input</code>: \u8981\u6267\u884c\u4e0a\u91c7\u6837\u64cd\u4f5c\u7684\u5f20\u91cf</li> <li><code>indices</code>: <code>MaxPool3d</code>\u6c60\u5316\u8fc7\u7a0b\u4e2d\u8f93\u51fa\u7684\u6c60\u5316\u7a97\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u7d22\u5f15</li> <li><code>output_size</code> (\u9009\u586b): \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> <p></p> <p>\u4e5f\u53ef\u4ee5\u4f7f\u7528<code>output_size</code>\u6307\u5b9a\u8f93\u51fa\u7684\u5927\u5c0f</p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)\n&gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15))\n&gt;&gt;&gt; unpooled_output = unpool(output, indices)\n&gt;&gt;&gt; unpooled_output.size()\ntorch.Size([20, 16, 51, 33, 15])\n\n</code></pre>"},{"location":"1.0/nn/#avgpool1d","title":"AvgPool1d","text":"<pre><code>class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e00\u7ef4\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a  \uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\uff0c<code>kernel_size</code>\u4e3a\u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> \u7b49\u53c2\u6570\u5747\u652f\u6301\u8f93\u5165\u4e00\u4e2aint\u6216\u8005\u7531\u4e00\u4e2aint\u7ec4\u6210\u7684tuple\u3002</p> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> <li>count_include_pad \u2013 \u5982\u679c\u88ab\u8bbe\u7f6e\u4e3aTrue, \u90a3\u4e48\u5728\u8fdb\u884c\u5e73\u5747\u8fd0\u7b97\u7684\u65f6\u5019\u4e5f\u4f1a\u5c06\u7528\u4e8e\u8865\u9f50\u76840\u52a0\u5165\u8fd0\u7b97\u3002</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool with window of size=3, stride=2\n&gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2)\n&gt;&gt;&gt; m(torch.tensor([[[1.,2,3,4,5,6,7]]]))\ntensor([[[ 2.,  4.,  6.]]])\n\n</code></pre>"},{"location":"1.0/nn/#avgpool2d","title":"AvgPool2d","text":"<pre><code>class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e8c\u7ef4\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002 \u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a   \uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\uff0c<code>kernel_size</code>\u4e3a\u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>\u7b49\u53c2\u6570\u5747\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\u8f93\u5165\uff1a</p> <ul> <li>\u4e00\u4e2a\u5355\u72ec\u7684 <code>int</code> \u2013 \u6b64\u65f6\u8fd9\u4e2a<code>int</code>\u4f1a\u540c\u65f6\u63a7\u5236\u6c60\u5316\u6ed1\u52a8\u7a97\u7684\u5bbd\u548c\u9ad8\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>\u4e00\u4e2a\u7531\u4e24\u4e2a<code>int</code>\u7ec4\u6210\u7684<code>tuple</code> \u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> <li>count_include_pad \u2013 \u5982\u679c\u88ab\u8bbe\u7f6e\u4e3aTrue, \u90a3\u4e48\u5728\u8fdb\u884c\u5e73\u5747\u8fd0\u7b97\u7684\u65f6\u5019\u4e5f\u4f1a\u5c06\u7528\u4e8e\u8865\u9f50\u76840\u52a0\u5165\u8fd0\u7b97\u3002</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2)\n&gt;&gt;&gt; # pool of non-square window\n&gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#avgpool3d","title":"AvgPool3d","text":"<pre><code>class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e09\u7ef4\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002 \u6700\u7b80\u5355\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u8f93\u5165\u5927\u5c0f\u4e3a\uff0c\u8f93\u51fa\u5927\u5c0f\u4e3a\uff0c<code>kernel_size</code>\u4e3a\u7684\u6c60\u5316\u64cd\u4f5c\uff0c\u6b64\u6c60\u5316\u8fc7\u7a0b\u53ef\u8868\u8ff0\u5982\u4e0b\uff1a</p> <p></p> <p><code>padding</code> \u53c2\u6570\u63a7\u5236\u4e86\u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570\u3002</p> <p><code>kernel_size</code>, <code>stride</code>, <code>padding</code>\u7b49\u53c2\u6570\u5747\u652f\u6301\u4ee5\u4e0b\u7c7b\u578b\u8f93\u5165\uff1a</p> <ul> <li>\u4e00\u4e2a\u5355\u72ec\u7684 <code>int</code> \u2013 \u6b64\u65f6\u8fd9\u4e2a<code>int</code>\u4f1a\u540c\u65f6\u63a7\u5236\u6c60\u5316\u6ed1\u52a8\u7a97\u7684\u6df1\u5ea6\uff0c\u5bbd\u548c\u9ad8\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>\u4e00\u4e2a\u7531\u4e09\u4e2a<code>int</code>\u7ec4\u6210\u7684<code>tuple</code> \u2013 \u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6df1\u5ea6\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e00\u4e2a<code>int</code>\u6570\u5b57\uff0c\u9ad8\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u5143\u7ec4\u4e2d\u7684\u7b2c\u4e8c\u4e2a<code>int</code>\u6570\u5b57\uff0c\u5bbd\u8fd9\u4e00\u7ef4\u5ea6\u4f1a\u91c7\u7528\u7b2c\u4e09\u4e2a<code>int</code>\u6570\u5b57\u3002</li> </ul> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u7684\u6ed1\u52a8\u7a97\u5927\u5c0f</li> <li>stride \u2013 \u6ed1\u52a8\u7a97\u7684\u6b65\u957f\uff0c\u9ed8\u8ba4\u503c\u662f <code>kernel_size</code></li> <li>padding \u2013 \u8981\u5728\u8f93\u5165\u4fe1\u53f7\u7684\u5404\u7ef4\u5ea6\u5404\u8fb9\u4e0a\u8981\u8865\u9f500\u7684\u5c42\u6570</li> <li>ceil_mode \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue\uff0c\u8ba1\u7b97\u8f93\u51fa\u4fe1\u53f7\u5927\u5c0f\u7684\u65f6\u5019\uff0c\u4f1a\u4f7f\u7528\u5411\u4e0a\u53d6\u6574\uff0c\u4ee3\u66ff\u9ed8\u8ba4\u7684\u5411\u4e0b\u53d6\u6574\u7684\u64cd\u4f5c</li> <li>count_include_pad \u2013 \u5982\u679c\u88ab\u8bbe\u7f6e\u4e3aTrue, \u90a3\u4e48\u5728\u8fdb\u884c\u5e73\u5747\u8fd0\u7b97\u7684\u65f6\u5019\u4e5f\u4f1a\u5c06\u7528\u4e8e\u8865\u9f50\u76840\u52a0\u5165\u8fd0\u7b97\u3002</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2)\n&gt;&gt;&gt; # pool of non-square window\n&gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#fractionalmaxpool2d","title":"FractionalMaxPool2d","text":"<pre><code>class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u5c0f\u6570\u7ea7\u4e8c\u7ef4\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002<code>\u5c0f\u6570\u7ea7</code>\u6307\u7684\u662f\u6b64\u64cd\u4f5c\u7684\u8f93\u51fa\u5927\u5c0f\u4e0e\u8f93\u5165\u5927\u5c0f\u6210\u6307\u5b9a\u7684\u5c0f\u6570\u500d\u6570\u5173\u7cfb\u3002</p> <p>Ben Graham\u7684\u8fd9\u7bc7\u6587\u7ae0Fractional MaxPooling\u4e2d\u8be6\u7ec6\u5730\u4ecb\u7ecd\u4e86\u5c0f\u6570\u7ea7\u4e8c\u7ef4\u6700\u5927\u6c60\u5316\u7684\u57fa\u672c\u601d\u60f3\u548c\u6280\u672f\u7ec6\u8282\u3002</p> <p>\u5c0f\u6570\u7ea7\u4e8c\u7ef4\u6700\u5927\u6c60\u5316\u7684\u57fa\u672c\u601d\u60f3\u5c31\u662f\u5c06\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u5e94\u7528\u4e8e\u4e2a\u7531\u968f\u673a\u6b65\u957f\u5927\u5c0f\u91c7\u96c6\u7684\u533a\u57df\u4e2d\uff0c\u8fd9\u4e9b\u6b65\u957f\u5927\u5c0f\u662f\u7531\u8f93\u51fa\u76ee\u6807\u7684\u5927\u5c0f\u51b3\u5b9a\u7684\u3002\u5c0f\u6570\u7ea7\u4e8c\u7ef4\u6700\u5927\u6c60\u5316\u7684\u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u7b49\u4e8e\u8f93\u5165\u901a\u9053\u7684\u6570\u91cf\u3002</p> <p>Parameters: </p> <ul> <li>kernel_size \u2013 \u6267\u884c\u6700\u5927\u64cd\u4f5c\u7684\u7a97\u53e3\u5927\u5c0f\u3002\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u5305\u62ec\u4e00\u4e2a\u5355\u72ec\u7684\u6570\u5b57k(\u751f\u6210\u4e00\u4e2a\u5927\u5c0f\u4e3ak x k\u7684\u6b63\u65b9\u5f62kernal)\uff0c\u6216\u8005\u4e00\u4e2a\u5143\u7ec4 <code>(kh x kw)</code></li> <li>output_size \u2013 \u6c60\u5316\u8f93\u51fa\u76ee\u6807\u5927\u5c0f\uff0c\u5177\u4f53\u5f62\u5f0f\u662f <code>oH x oW</code>\u3002\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u5305\u62ec\u4e00\u4e2a\u5355\u72ec\u7684\u6570\u5b57<code>oH</code>\uff0c\u6216\u8005\u4e00\u4e2a\u5143\u7ec4 <code>(oH, oW)</code>\uff0c\u6ce8\u610f\u6b64\u5904<code>oH x oW</code>\u4e0e<code>kernal_size</code>\u4e2d\u7684<code>kh x ow</code>\u76f8\u547c\u5e94\uff0c\u4e24\u8005\u6210\u4e00\u5b9a\u7684\u5c0f\u6570\u7ea7\u500d\u6570\u5173\u7cfb</li> <li>output_ratio \u2013 \u5982\u679c\u60f3\u8ba9\u8f93\u51fa\u76ee\u6807\u7684\u5927\u5c0f\u662f\u8f93\u5165\u76ee\u6807\u5927\u5c0f\u7684ratio\u500d\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u6b64\u53c2\u6570\u6765\u5b9e\u73b0\u3002\u6b64\u53c2\u6570\u53ef\u4ee5\u662f\u4e00\u4e2a\u5c0f\u6570\u6570\u5b57\u6216\u8005\u5c0f\u6570\u5143\u7ec4\uff0c\u6570\u5b57\u8303\u56f4\u662f(0, 1)</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u8bbe\u7f6e\u4e3a<code>True</code>, \u90a3\u4e48\u5728\u6c60\u5316\u64cd\u4f5c\u7ed3\u675f\u540e\uff0c\u8fd4\u56de\u6c60\u5316\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u4e5f\u4f1a\u8fd4\u56de\u6bcf\u4e2a\u6c60\u5316\u533a\u57df\u4e2d\uff0c\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u5728<code>nn.MaxUnpool2d()</code>\u53ef\u4ee5\u88ab\u7528\u5230\u3002\u6b64\u53c2\u6570\u9ed8\u8ba4\u4e3a<code>False</code></li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12\n&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n&gt;&gt;&gt; # pool of square window and target output size being half of input image size\n&gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#lppool1d","title":"LPPool1d","text":"<pre><code>class torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e00\u7ef4\u5e42\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e\u6bcf\u4e2a\u6c60\u5316\u7a97\u53e3\uff0c\u6b64\u6c60\u5316\u64cd\u4f5c\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <p></p> <ul> <li>\u5f53p\u4e3a\u65e0\u7a77\u5927\u7684\u65f6\u5019\u65f6\uff0c\u7b49\u4ef7\u4e8e\u6700\u5927\u6c60\u5316\u64cd\u4f5c</li> <li>\u5f53<code>p=1</code>\u65f6\uff0c\u7b49\u4ef7\u4e8e\u6c42\u548c\u6c60\u5316\u64cd\u4f5c(\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7b49\u4ef7\u4e8e\u5e73\u5747\u6c60\u5316\uff09</li> </ul> <p>Note</p> <p>\u5982\u679c\u67d0\u4e2a\u7279\u6b8a\u7684\u8f93\u5165\u5bfc\u81f4\u8fd9\u4e2a\u8f93\u5165\u5173\u4e8e\u5e42\u6307\u6570<code>p</code>\u7684\u6c42\u548c\u662f0\uff0c\u90a3\u4e0a\u8ff0\u6c60\u5316\u51fd\u6570\u5728\u8fd9\u4e00\u70b9\u662f\u6ca1\u6709\u610f\u4e49\u7684\u3002\u5728\u5b9e\u9645\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\uff0c\u6b64\u70b9\u7684\u68af\u5ea6\u88ab\u8bbe\u7f6e\u4e3a0\u3002</p> <p>Parameters: </p> <ul> <li>kernel_size: \u6c60\u5316\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>stride\uff1a\u6c60\u5316\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f\u3002\u9ed8\u8ba4\u503c\u662f<code>kernel_size</code></li> <li>ceil_mode: \u5f53\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\u65f6\uff0c\u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u7684\u65f6\u5019\u5c06\u4f7f\u7528\u5411\u4e0b\u53d6\u6574\u4ee3\u66ff\u5411\u4e0a\u53d6\u6574</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: \uff0c\u5176\u4e2d</p> <p></p> </li> </ul> <pre><code>\u4f8b\u5b50:\n</code></pre> <pre><code>&gt;&gt;&gt; # power-2 pool of window of length 3, with stride 2.\n&gt;&gt;&gt; m = nn.LPPool1d(2, 3, stride=2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 50)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#lppool2d","title":"LPPool2d","text":"<pre><code>class torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u6267\u884c\u4e8c\u7ef4\u5e42\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e\u6bcf\u4e2a\u6c60\u5316\u7a97\u53e3\uff0c\u6b64\u6c60\u5316\u64cd\u4f5c\u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <p></p> <ul> <li>\u5f53p\u7b49\u4e8e\u65f6\u5019\u65f6\uff0c\u7b49\u4ef7\u4e8e\u6700\u5927\u6c60\u5316\u64cd\u4f5c</li> <li>\u5f53<code>p=1</code>\u65f6\uff0c\u7b49\u4ef7\u4e8e\u6c42\u548c\u6c60\u5316\u64cd\u4f5c(\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7b49\u4ef7\u4e8e\u5e73\u5747\u6c60\u5316\uff09</li> </ul> <p>\u53c2\u6570<code>kernel_size</code>, <code>stride</code>\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\uff1a</p> <ul> <li><code>int</code>\uff0c\u6c60\u5316\u7a97\u53e3\u7684\u5bbd\u548c\u9ad8\u76f8\u7b49</li> <li><code>tuple</code>\u6570\u7ec4(\u4e24\u4e2a\u6570\u5b57\u7684\uff09\uff0c\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u6c60\u5316\u7a97\u53e3\u7684\u9ad8\uff0c\u7b2c\u4e8c\u4e2a\u662f\u5bbd</li> </ul> <p>Note</p> <p>\u5982\u679c\u67d0\u4e2a\u7279\u6b8a\u7684\u8f93\u5165\u5bfc\u81f4\u8fd9\u4e2a\u8f93\u5165\u5173\u4e8e\u5e42\u6307\u6570<code>p</code>\u7684\u6c42\u548c\u662f0\uff0c\u90a3\u4e0a\u8ff0\u6c60\u5316\u51fd\u6570\u5728\u8fd9\u4e00\u70b9\u662f\u6ca1\u6709\u610f\u4e49\u7684\u3002\u5728\u5b9e\u9645\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\uff0c\u6b64\u70b9\u7684\u68af\u5ea6\u88ab\u8bbe\u7f6e\u4e3a0\u3002</p> <p>Parameters: </p> <ul> <li>kernel_size: \u6c60\u5316\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>stride\uff1a\u6c60\u5316\u7a97\u53e3\u79fb\u52a8\u7684\u6b65\u957f\u3002\u9ed8\u8ba4\u503c\u662f<code>kernel_size</code></li> <li>ceil_mode: \u5f53\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\u65f6\uff0c\u5728\u8ba1\u7b97\u8f93\u51fa\u5927\u5c0f\u7684\u65f6\u5019\u5c06\u4f7f\u7528\u5411\u4e0b\u53d6\u6574\u4ee3\u66ff\u5411\u4e0a\u53d6\u6574</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa: , \u5176\u4e2d</p> <p></p> <p></p> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2\n&gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2)\n&gt;&gt;&gt; # pool of non-square window of power 1.2\n&gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n&gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptivemaxpool1d","title":"AdaptiveMaxPool1d","text":"<pre><code>class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c1\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fH\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fH</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u8bbe\u7f6e\u4e3a<code>True</code>, \u90a3\u4e48\u5728\u6c60\u5316\u64cd\u4f5c\u7ed3\u675f\u540e\uff0c\u8fd4\u56de\u6c60\u5316\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u4e5f\u4f1a\u8fd4\u56de\u6bcf\u4e2a\u6c60\u5316\u533a\u57df\u4e2d\uff0c\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u5728<code>nn.MaxUnpool1d()</code>\u53ef\u4ee5\u88ab\u7528\u5230\u3002\u6b64\u53c2\u6570\u9ed8\u8ba4\u4e3a<code>False</code></li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5)\n&gt;&gt;&gt; input = torch.randn(1, 64, 8)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptivemaxpool2d","title":"AdaptiveMaxPool2d","text":"<pre><code>class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fH x W\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fH x W\u3002\u6b64\u53c2\u6570\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u5143\u7ec4(H, W)\uff0c\u53c8\u6216\u8005\u662f\u4e00\u4e2a\u5355\u72ec\u7684<code>int</code> H(\u7b49\u4ef7\u4e8eH x H\uff09\u3002H \u548c W\u8fd9\u4e24\u4e2a\u53c2\u6570\u652f\u6301\u8f93\u5165\u4e00\u4e2a<code>int</code>\u53c8\u6216\u8005\u662f<code>None</code>, <code>None</code>\u8868\u793a\u6b64\u8f93\u51fa\u7ef4\u5ea6\u7684\u5927\u5c0f\u7b49\u4ef7\u4e8e\u8f93\u5165\u6570\u636e\u6b64\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u8bbe\u7f6e\u4e3a<code>True</code>, \u90a3\u4e48\u5728\u6c60\u5316\u64cd\u4f5c\u7ed3\u675f\u540e\uff0c\u8fd4\u56de\u6c60\u5316\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u4e5f\u4f1a\u8fd4\u56de\u6bcf\u4e2a\u6c60\u5316\u533a\u57df\u4e2d\uff0c\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u5728<code>nn.MaxUnpool2d()</code>\u53ef\u4ee5\u88ab\u7528\u5230\u3002\u6b64\u53c2\u6570\u9ed8\u8ba4\u4e3a<code>False</code></li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5x7\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7))\n&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x7 (square)\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7)\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 10x7\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptivemaxpool3d","title":"AdaptiveMaxPool3d","text":"<pre><code>class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c3\u7ef4\u7684\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fD x H x W\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fD x H x W\u3002\u6b64\u53c2\u6570\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u5143\u7ec4(D, H, W)\uff0c\u53c8\u6216\u8005\u662f\u4e00\u4e2a\u5355\u72ec\u7684<code>int</code> D(\u7b49\u4ef7\u4e8eD x D x D)\u3002D, H \u548c W\u8fd9\u4e09\u4e2a\u53c2\u6570\u652f\u6301\u8f93\u5165\u4e00\u4e2a<code>int</code>\u53c8\u6216\u8005\u662f<code>None</code>, <code>None</code>\u8868\u793a\u6b64\u8f93\u51fa\u7ef4\u5ea6\u7684\u5927\u5c0f\u7b49\u4ef7\u4e8e\u8f93\u5165\u6570\u636e\u6b64\u7ef4\u5ea6\u7684\u5927\u5c0f</li> <li>return_indices \u2013 \u5982\u679c\u6b64\u53c2\u6570\u8bbe\u7f6e\u4e3a<code>True</code>, \u90a3\u4e48\u5728\u6c60\u5316\u64cd\u4f5c\u7ed3\u675f\u540e\uff0c\u8fd4\u56de\u6c60\u5316\u8f93\u51fa\u7ed3\u679c\u7684\u540c\u65f6\u4e5f\u4f1a\u8fd4\u56de\u6bcf\u4e2a\u6c60\u5316\u533a\u57df\u4e2d\uff0c\u6700\u5927\u503c\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002\u8fd9\u4e9b\u4fe1\u606f\u5728<code>nn.MaxUnpool3d()</code>\u53ef\u4ee5\u88ab\u7528\u5230\u3002\u6b64\u53c2\u6570\u9ed8\u8ba4\u4e3a<code>False</code></li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5x7x9\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9))\n&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x7x7 (cube)\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7)\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x9x8\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptiveavgpool1d","title":"AdaptiveAvgPool1d","text":"<pre><code>class torch.nn.AdaptiveAvgPool1d(output_size)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c1\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fH\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fH</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5\n&gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5)\n&gt;&gt;&gt; input = torch.randn(1, 64, 8)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptiveavgpool2d","title":"AdaptiveAvgPool2d","text":"<pre><code>class torch.nn.AdaptiveAvgPool2d(output_size)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c2\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fH x W\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fH x W\u3002\u6b64\u53c2\u6570\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u5143\u7ec4(H, W)\uff0c\u53c8\u6216\u8005\u662f\u4e00\u4e2a\u5355\u72ec\u7684<code>int</code> H(\u7b49\u4ef7\u4e8eH x H\uff09\u3002H \u548c W\u8fd9\u4e24\u4e2a\u53c2\u6570\u652f\u6301\u8f93\u5165\u4e00\u4e2a<code>int</code>\u53c8\u6216\u8005\u662f<code>None</code>, <code>None</code>\u8868\u793a\u6b64\u8f93\u51fa\u7ef4\u5ea6\u7684\u5927\u5c0f\u7b49\u4ef7\u4e8e\u8f93\u5165\u6570\u636e\u6b64\u7ef4\u5ea6\u7684\u5927\u5c0f</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5x7\n&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7))\n&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x7 (square)\n&gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7)\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 10x7\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7))\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptiveavgpool3d","title":"AdaptiveAvgPool3d","text":"<pre><code>class torch.nn.AdaptiveAvgPool3d(output_size)\n</code></pre> <p>\u5bf9\u8f93\u5165\u7684\u591a\u901a\u9053\u4fe1\u53f7\u8fdb\u884c3\u7ef4\u7684\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002</p> <p>\u6b64\u6c60\u5316\u5c42\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u8f93\u51fa\u5927\u5c0fD x H x W\uff0c\u5c06\u4efb\u610f\u8f93\u5165\u5927\u5c0f\u7684\u8f93\u5165\u5f3a\u884c\u7684\u6c60\u5316\u5230\u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0f\u3002\u4e0d\u8fc7\u8f93\u5165\u548c\u8f93\u51fa\u7279\u5f81\u7684\u901a\u9053\u6570\u4e0d\u4f1a\u53d8\u5316\u3002</p> <p>Parameters: </p> <ul> <li>output_size \u2013 \u6307\u5b9a\u7684\u8f93\u51fa\u5927\u5c0fD x H x W\u3002\u6b64\u53c2\u6570\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\u53ef\u4ee5\u662f\u4e00\u4e2a\u5143\u7ec4(D, H, W)\uff0c\u53c8\u6216\u8005\u662f\u4e00\u4e2a\u5355\u72ec\u7684<code>int</code> D(\u7b49\u4ef7\u4e8eD x D x D)\u3002D, H \u548c W\u8fd9\u4e09\u4e2a\u53c2\u6570\u652f\u6301\u8f93\u5165\u4e00\u4e2a<code>int</code>\u53c8\u6216\u8005\u662f<code>None</code>, <code>None</code>\u8868\u793a\u6b64\u8f93\u51fa\u7ef4\u5ea6\u7684\u5927\u5c0f\u7b49\u4ef7\u4e8e\u8f93\u5165\u6570\u636e\u6b64\u7ef4\u5ea6\u7684\u5927\u5c0f</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; # target output size of 5x7x9\n&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9))\n&gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x7x7 (cube)\n&gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7)\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # target output size of 7x9x8\n&gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None))\n&gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#padding-layers","title":"\u586b\u5145\u5c42(Padding layers\uff09","text":""},{"location":"1.0/nn/#reflectionpad1d","title":"ReflectionPad1d","text":"<pre><code>class torch.nn.ReflectionPad1d(padding)\n</code></pre> <p>\u4ee5\u8f93\u5165\u5f20\u91cf\u7684\u5404\u8fb9\u754c\u4e3a\u8f74\uff0c\u901a\u8fc7\u5bf9\u8f93\u5165\u5f20\u91cf\u6570\u636e\u7684\u8fdb\u884c\u955c\u50cf\u590d\u5236\u7684\u65b9\u5f0f\u6765\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u4e24\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167 (, )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReflectionPad1d(2)\n&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n&gt;&gt;&gt; input\ntensor([[[0., 1., 2., 3.],\n [4., 5., 6., 7.]]])\n&gt;&gt;&gt; m(input)\ntensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n [6., 5., 4., 5., 6., 7., 6., 5.]]])\n&gt;&gt;&gt; m(input)\ntensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n [6., 5., 4., 5., 6., 7., 6., 5.]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1))\n&gt;&gt;&gt; m(input)\ntensor([[[3., 2., 1., 0., 1., 2., 3., 2.],\n [7., 6., 5., 4., 5., 6., 7., 6.]]])\n\n</code></pre>"},{"location":"1.0/nn/#reflectionpad2d","title":"ReflectionPad2d","text":"<pre><code>class torch.nn.ReflectionPad2d(padding)\n</code></pre> <p>\u4ee5\u8f93\u5165\u5f20\u91cf\u7684\u5404\u8fb9\u754c\u4e3a\u8f74\uff0c\u901a\u8fc7\u5bf9\u8f93\u5165\u5f20\u91cf\u6570\u636e\u7684\u8fdb\u884c\u955c\u50cf\u590d\u5236\u7684\u65b9\u5f0f\u6765\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u56db\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li> <p>\u8f93\u5165: </p> </li> <li> <p>\u8f93\u51fa:  \u5176\u4e2d</p> <p> </p> </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReflectionPad2d(2)\n&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n&gt;&gt;&gt; input\ntensor([[[[0., 1., 2.],\n [3., 4., 5.],\n [6., 7., 8.]]]])\n&gt;&gt;&gt; m(input)\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\n [5., 4., 3., 4., 5., 4., 3.],\n [2., 1., 0., 1., 2., 1., 0.],\n [5., 4., 3., 4., 5., 4., 3.],\n [8., 7., 6., 7., 8., 7., 6.],\n [5., 4., 3., 4., 5., 4., 3.],\n [2., 1., 0., 1., 2., 1., 0.]]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0))\n&gt;&gt;&gt; m(input)\ntensor([[[[7., 6., 7., 8., 7.],\n [4., 3., 4., 5., 4.],\n [1., 0., 1., 2., 1.],\n [4., 3., 4., 5., 4.],\n [7., 6., 7., 8., 7.]]]])\n\n</code></pre>"},{"location":"1.0/nn/#replicationpad1d","title":"ReplicationPad1d","text":"<pre><code>class torch.nn.ReplicationPad1d(padding)\n</code></pre> <p>\u901a\u8fc7\u590d\u5236\u8f93\u5165\u5f20\u91cf\u8fb9\u754c\u5143\u7d20\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u4e24\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167 (, )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReplicationPad1d(2)\n&gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n&gt;&gt;&gt; input\ntensor([[[0., 1., 2., 3.],\n [4., 5., 6., 7.]]])\n&gt;&gt;&gt; m(input)\ntensor([[[0., 0., 0., 1., 2., 3., 3., 3.],\n [4., 4., 4., 5., 6., 7., 7., 7.]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1))\n&gt;&gt;&gt; m(input)\ntensor([[[0., 0., 0., 0., 1., 2., 3., 3.],\n [4., 4., 4., 4., 5., 6., 7., 7.]]])\n\n</code></pre>"},{"location":"1.0/nn/#replicationpad2d","title":"ReplicationPad2d","text":"<pre><code>class torch.nn.ReplicationPad2d(padding)\n</code></pre> <p>\u901a\u8fc7\u590d\u5236\u8f93\u5165\u5f20\u91cf\u8fb9\u754c\u5143\u7d20\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u56db\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d  </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReplicationPad2d(2)\n&gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n&gt;&gt;&gt; input\ntensor([[[[0., 1., 2.],\n [3., 4., 5.],\n [6., 7., 8.]]]])\n&gt;&gt;&gt; m(input)\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\n [0., 0., 0., 1., 2., 2., 2.],\n [0., 0., 0., 1., 2., 2., 2.],\n [3., 3., 3., 4., 5., 5., 5.],\n [6., 6., 6., 7., 8., 8., 8.],\n [6., 6., 6., 7., 8., 8., 8.],\n [6., 6., 6., 7., 8., 8., 8.]]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0))\n&gt;&gt;&gt; m(input)\ntensor([[[[0., 0., 1., 2., 2.],\n [0., 0., 1., 2., 2.],\n [0., 0., 1., 2., 2.],\n [3., 3., 4., 5., 5.],\n [6., 6., 7., 8., 8.]]]])\n\n</code></pre>"},{"location":"1.0/nn/#replicationpad3d","title":"ReplicationPad3d","text":"<pre><code>class torch.nn.ReplicationPad3d(padding)\n</code></pre> <p>\u901a\u8fc7\u590d\u5236\u8f93\u5165\u5f20\u91cf\u8fb9\u754c\u5143\u7d20\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u516d\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d  </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReplicationPad3d(3)\n&gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#zeropad2d","title":"ZeroPad2d","text":"<pre><code>class torch.nn.ZeroPad2d(padding)\n</code></pre> <p>\u901a\u8fc7\u5728\u5404\u8fb9\u4e0a\u586b\u51450\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c\u3002</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u56db\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d  </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ZeroPad2d(2)\n&gt;&gt;&gt; input = torch.randn(1, 1, 3, 3)\n&gt;&gt;&gt; input\ntensor([[[[-0.1678, -0.4418,  1.9466],\n [ 0.9604, -0.4219, -0.5241],\n [-0.9162, -0.5436, -0.6446]]]])\n&gt;&gt;&gt; m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],\n [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0))\n&gt;&gt;&gt; m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],\n [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],\n [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])\n\n</code></pre>"},{"location":"1.0/nn/#constantpad1d","title":"ConstantPad1d","text":"<pre><code>class torch.nn.ConstantPad1d(padding, value)\n</code></pre> <p>\u901a\u8fc7\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u56fa\u5b9a\u6570\u5b57\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u4e24\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167 (, )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)\n&gt;&gt;&gt; input = torch.randn(1, 2, 4)\n&gt;&gt;&gt; input\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\n [-1.3287,  1.8966,  0.1466, -0.2771]]])\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,\n 3.5000],\n [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,\n 3.5000]]])\n&gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5)\n&gt;&gt;&gt; input = torch.randn(1, 2, 3)\n&gt;&gt;&gt; input\ntensor([[[ 1.6616,  1.4523, -1.1255],\n [-3.6372,  0.1182, -1.8652]]])\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],\n [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5)\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],\n [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\n\n</code></pre>"},{"location":"1.0/nn/#constantpad2d","title":"ConstantPad2d","text":"<pre><code>class torch.nn.ConstantPad2d(padding, value)\n</code></pre> <p>\u901a\u8fc7\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u56fa\u5b9a\u6570\u5b57\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u56db\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d  </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5)\n&gt;&gt;&gt; input = torch.randn(1, 2, 2)\n&gt;&gt;&gt; input\ntensor([[[ 1.6585,  0.4320],\n [-0.8701, -0.4649]]])\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\n [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\n [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\n&gt;&gt;&gt; m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],\n [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],\n [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n\n</code></pre>"},{"location":"1.0/nn/#constantpad3d","title":"ConstantPad3d","text":"<pre><code>class torch.nn.ConstantPad3d(padding, value)\n</code></pre> <p>\u901a\u8fc7\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u56fa\u5b9a\u6570\u5b57\u7684\u65b9\u5f0f\u5bf9\u8f93\u5165\u5f20\u91cf\u8fdb\u884c\u586b\u5145\u64cd\u4f5c</p> <p>\u5bf9\u4e8e<code>N</code>\u7ef4\u7684\u586b\u5145\u64cd\u4f5c\uff0c\u8c03\u7528<code>torch.nn.functional.pad()</code>\u3002</p> Parameters: padding (int, tuple) \u2013 \u8981\u586b\u5145\u7684\u8303\u56f4\u5927\u5c0f\u3002\u5982\u679c\u8f93\u5165\u6570\u636e\u662f\u4e00\u4e2a<code>int</code>, \u90a3\u5404\u4e2a\u8fb9\u754c\u4e0a\u90fd\u4f1a\u586b\u5145\u540c\u6837\u5927\u5c0f\u7684\u6570\u636e\u3002\u5982\u679c\u662f\u4e00\u4e2a\u516d\u4e2a\u5143\u7d20\u7684\u5143\u7ec4\uff0c\u90a3\u4e48\u6309\u7167(, , , , , )\u7684\u5927\u5c0f\u8bbe\u5b9a\u6765\u5728\u5404\u8fb9\u4e0a\u586b\u5145\u3002 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165: </li> <li>\u8f93\u51fa:  \u5176\u4e2d  </li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5)\n&gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; # using different paddings for different sides\n&gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#non-linear-activations-weighted-sum-nonlinearity","title":"\u975e\u7ebf\u6027\u6fc0\u6d3b(\u52a0\u6743\u6c42\u548c\uff0c\u975e\u7ebf\u6027) ( Non-linear activations (weighted sum, nonlinearity))","text":""},{"location":"1.0/nn/#elu","title":"ELU","text":"<pre><code>class torch.nn.ELU(alpha=1.0, inplace=False)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> <p>Parameters: </p> <ul> <li>alpha \u2013 ELU\u64cd\u4f5c\u7684\u503c\u3002 \u9ed8\u8ba4\uff1a 1.0</li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ELU()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#hardshrink","title":"Hardshrink","text":"<pre><code>class torch.nn.Hardshrink(lambd=0.5)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7hard shrinkage\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> Parameters: lambd \u2013 Hardshrink\u8fd0\u7b97\u4e2d\u7684  \u503c\u3002 \u9ed8\u8ba4: 0.5 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Hardshrink()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#hardtanh","title":"Hardtanh","text":"<pre><code>class torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7HardTanh\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\u3002</p> <p>HardTanh\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b:</p> <p></p> <p>\u7ebf\u6027\u533a\u57df \u7684\u5927\u5c0f\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>min_val</code> \u53c2\u6570 <code>max_val</code>\u6765\u8fdb\u884c\u8c03\u6574\u3002</p> <p></p> <p>\u53c2\u6570: </p> <ul> <li>min_val \u2013 \u7ebf\u6027\u533a\u57df\u7684\u4e0b\u9650. \u9ed8\u8ba4: -1</li> <li>max_val \u2013 \u7ebf\u6027\u533a\u57df\u7684\u4e0a\u9650. \u9ed8\u8ba4: 1</li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <p>\u4e4b\u524d\u7248\u672c\u7684<code>min_value</code> \u548c <code>max_value</code> \u53c2\u6570\u5df2\u7ecf\u88ab\u5e9f\u5f03\u6389\u4e86\uff0c\u6539\u4e3a<code>min_val</code> \u548c <code>max_val</code>\u53c2\u6570\u3002</p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Hardtanh(-2, 2)\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#leakyrelu","title":"LeakyReLU","text":"<pre><code>class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> <p>\u6216</p> <p></p> <p>Parameters: </p> <ul> <li>negative_slope \u2013 \u63a7\u5236\u8d1f\u6570\u8303\u56f4\u51fd\u6570\u7684\u659c\u7387\u3002 \u9ed8\u8ba4: 1e-2</li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.LeakyReLU(0.1)\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#logsigmoid","title":"LogSigmoid","text":"<pre><code>class torch.nn.LogSigmoid\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570LogSigmoid\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a(\u6b64\u5904\u6f0f\u4e86\u4e00\u5f20\u56fe\uff0c\u540e\u671f\u8865\u4e00\u4e0b)</p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.LogSigmoid()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#prelu","title":"PReLU","text":"<pre><code>class torch.nn.PReLU(num_parameters=1, init=0.25)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> <p>\u6216</p> <p></p> <p>\u6b64\u5904  \u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u3002 \u5982\u679c\u5728\u8c03\u7528<code>nn.PReLU()</code>\u51fd\u6570\u7684\u65f6\u5019\u6ca1\u6709\u4f20\u5165\u53c2\u6570\uff0c\u90a3\u4e48\u4f1a\u9ed8\u8ba4\u5728\u6240\u6709\u7684\u8f93\u5165\u901a\u9053\u4e0a\u5e94\u7528\u540c\u4e00\u4e2a \u53c2\u6570\u3002 \u5982\u679c\u4ee5<code>nn.PReLU(nChannels)</code>\u8fd9\u79cd\u65b9\u5f0f\u8c03\u7528\uff0c \u6bcf\u4e2a\u8f93\u5165\u901a\u9053\u90fd\u4f1a\u6709\u4e00\u4e2a\u5355\u72ec\u7684 \u53c2\u6570\u3002</p> <p>Note</p> <p>\u60f3\u8981\u5b66\u4e00\u4e2a\u597d\u7684\u53c2\u6570\uff0c\u6700\u597d\u4e0d\u8981\u7528weight decay\u3002</p> <p>Note</p> <p>\u901a\u9053\u7ef4\u5ea6\u662f\u8f93\u5165\u5f20\u91cf\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u3002\u5f53\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6\u6570 &lt; 2\u7684\u65f6\u5019\uff0c\u90a3\u6b64\u8f93\u5165\u5f20\u91cf\u5c31\u6ca1\u6709\u901a\u9053\u7ef4\u5ea6\uff0c\u800c\u6b64\u65f6\u5176\u901a\u9053\u6570\u88ab\u8ba4\u4e3a\u662f1\u3002</p> <p>Parameters: </p> <ul> <li>num_parameters (int) \u2013 \u8981\u8fdb\u884c\u8bad\u7ec3\u5b66\u4e60\u7684  \u53c2\u6570\u7684\u6570\u91cf\u3002\u5c3d\u7ba1\u6b64\u51fd\u6570\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u6574\u5f62\uff0c\u4f46\u6b64\u51fd\u6570\u8981\u6c42\u8f93\u5165\u7684\u6574\u5f62\u53ea\u80fd\u4e3a\u4e24\u4e2a\u503c\uff0c1\u6216\u8005\u8f93\u5165\u5f20\u91cf\u7684\u901a\u9053\u6570\u3002\u9ed8\u8ba4\uff1a1 </li> <li>init (float) \u2013 \u7684\u521d\u59cb\u503c\uff0c\u9ed8\u8ba4: 0.25</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> Variables: weight (Tensor) \u2013 \u5927\u5c0f\u4e3a<code>num_parameters</code>\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u3002The attr:<code>dtype</code> is default to(\u8fd9\u53e5\u8bdd\u6709\u70b9\u95ee\u9898\uff0c to\u540e\u9762\u6f0f\u6389\u4e86\uff09 <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.PReLU()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#relu","title":"ReLU","text":"<pre><code>class torch.nn.ReLU(inplace=False)\n</code></pre> <p>\u5c06\u5143\u7d20\u7ea7\u7ebf\u6027\u6574\u6d41\u51fd\u6570\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a Applies the rectified linear unit function element-wise </p> <p></p> Parameters: inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReLU()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#relu6","title":"ReLU6","text":"<pre><code>class torch.nn.ReLU6(inplace=False)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> Parameters: inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.ReLU6()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#rrelu","title":"RReLU","text":"<pre><code>class torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)\n</code></pre> <p>\u5c06\u5143\u7d20\u7ea7\u968f\u673a\u7ebf\u6027\u6574\u6d41\u51fd\u6570\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff0c\u8be6\u60c5\u6b64\u6587\u7ae0\uff1a</p> <p>Empirical Evaluation of Rectified Activations in Convolutional Network.</p> <p>\u6b64\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b:</p> <p></p> <p>\u5176\u4e2d  \u662f\u4ece\u6b64\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u800c\u6765\uff1a.</p> <p>\u8be6\u89c1: https://arxiv.org/pdf/1505.00853.pdf</p> <p>Parameters: </p> <ul> <li>lower \u2013 \u5747\u5300\u5206\u5e03\u4e0b\u9650\uff0c \u9ed8\u8ba4: </li> <li>upper \u2013 \u5747\u5300\u5206\u5e03\u4e0a\u9650\uff0c\u9ed8\u8ba4: </li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.RReLU(0.1, 0.3)\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#selu","title":"SELU","text":"<pre><code>class torch.nn.SELU(inplace=False)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> <p>\u5176\u4e2d  \u800c\u4e14 .</p> <p></p> <p>More details can be found in the paper Self-Normalizing Neural Networks .</p> Parameters: inplace (bool, optional) \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.SELU()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#celu","title":"CELU","text":"<pre><code>class torch.nn.CELU(alpha=1.0, inplace=False)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> <p>\u66f4\u591a\u7ec6\u8282\u8bf7\u89c1paper: Continuously Differentiable Exponential Linear Units .</p> <p>Parameters: </p> <ul> <li>alpha \u2013 CELU\u64cd\u4f5c\u4e2d\u7684  \u503c\uff0c\u9ed8\u8ba4: 1.0</li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.CELU()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#sigmoid","title":"Sigmoid","text":"<pre><code>class torch.nn.Sigmoid\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a\uff1a</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Sigmoid()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#softplus","title":"Softplus","text":"<pre><code>class torch.nn.Softplus(beta=1, threshold=20)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> <p>SoftPlus\u662f\u4e00\u4e2a\u5e73\u6ed1\u7684\u7c7bReLU\u51fd\u6570\uff0c\u53ef\u4ee5\u7528\u4e8e\u5c06\u8f93\u51fa\u7ed3\u679c\u89c4\u8303\u5230\u5168\u6b63\u3002</p> <p>\u4e3a\u4e86\u6570\u503c\u7a33\u5b9a\u6027\uff0c\u5728\u5b9e\u73b0\u6b64\u51fd\u6570\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5f53 <code>x</code> \u8d85\u8fc7\u67d0\u4e2a\u7279\u5b9a\u503c\u4e4b\u540e\uff0c\u6211\u4eec\u4f1a\u5c06\u6b64\u51fd\u6570\u8f6c\u5316\u4e3a\u4e00\u4e2a\u7ebf\u6027\u51fd\u6570\u3002</p> <p>Parameters: </p> <ul> <li>beta \u2013 Softplus\u64cd\u4f5c\u7684  \u503c\uff0c\u9ed8\u8ba4: 1</li> <li>threshold \u2013 \u5c06\u51fd\u6570\u8f6c\u5316\u4e3a\u7ebf\u6027\u51fd\u6570\u7684\u9608\u503c\uff0c \u9ed8\u8ba4: 20</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Softplus()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#softshrink","title":"Softshrink","text":"<pre><code>class torch.nn.Softshrink(lambd=0.5)\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u8f6f\u6536\u7f29\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> Parameters: lambd \u2013 \u8f6f\u6536\u7f29\u8fd0\u7b97\u7684\u503c\uff0c\u9ed8\u8ba4: 0.5 <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Softshrink()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#softsign","title":"Softsign","text":"<pre><code>class torch.nn.Softsign\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Softsign()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#tanh","title":"Tanh","text":"<pre><code>class torch.nn.Tanh\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Tanh()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#tanhshrink","title":"Tanhshrink","text":"<pre><code>class torch.nn.Tanhshrink\n</code></pre> <p>\u5c06\u4e0b\u9762\u7684\u5143\u7d20\u7ea7\u51fd\u6570\u5e94\u7528\u5230\u8f93\u5165\u5f20\u91cf\u4e0a:</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p></p> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Tanhshrink()\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#threshold","title":"Threshold","text":"<pre><code>class torch.nn.Threshold(threshold, value, inplace=False)\n</code></pre> <p>\u4f7f\u7528\u9608\u503c\u8fc7\u6ee4\u8f93\u5165\u5f20\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20</p> <p>\u9608\u503c\u88ab\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <p></p> <p>Parameters: </p> <ul> <li>threshold \u2013 \u9608\u503c\u5927\u5c0f </li> <li>value \u2013 \u5c0f\u4e8e\u9608\u503c\u7684\u5143\u7d20\u7684\u66ff\u6362\u503c</li> <li>inplace \u2013 \u662f\u5426\u8fdb\u884c\u539f\u4f4d\u64cd\u4f5c\u3002 \u9ed8\u8ba4\uff1a <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>\u8f93\u5165:  \u5176\u4e2d <code>*</code> \u4ee3\u8868\u652f\u6301\u4efb\u610f\u5927\u5c0f\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>\u8f93\u51fa: , \u4e0e\u8f93\u5165\u5411\u91cf\u4fdd\u6301\u4e00\u6837\u7684\u5f62\u72b6\u5927\u5c0f</li> </ul> <p>\u793a\u4f8b:</p> <pre><code>&gt;&gt;&gt; m = nn.Threshold(0.1, 20)\n&gt;&gt;&gt; input = torch.randn(2)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#non-linear-activations-other","title":"Non-linear activations (other)","text":""},{"location":"1.0/nn/#softmin","title":"Softmin","text":"<pre><code>class torch.nn.Softmin(dim=None)\n</code></pre> <p>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range <code>(0, 1)</code> and sum to 1</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: any shape</li> <li>Output: same as input</li> </ul> Parameters: dim (int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input, with values in the range [0, 1] --- --- <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Softmin()\n&gt;&gt;&gt; input = torch.randn(2, 3)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#softmax","title":"Softmax","text":"<pre><code>class torch.nn.Softmax(dim=None)\n</code></pre> <p>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range (0,1) and sum to 1</p> <p>Softmax is defined as:</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: any shape</li> <li>Output: same as input</li> </ul> Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] Parameters: dim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1). --- --- <p>Note</p> <p>This module doesn't work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use <code>LogSoftmax</code> instead (it's faster and has better numerical properties).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Softmax()\n&gt;&gt;&gt; input = torch.randn(2, 3)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#softmax2d","title":"Softmax2d","text":"<pre><code>class torch.nn.Softmax2d\n</code></pre> <p>Applies SoftMax over features to each spatial location.</p> <p>When given an image of <code>Channels x Height x Width</code>, it will apply <code>Softmax</code> to each location </p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> Returns: a Tensor of the same dimension and shape as the input with values in the range [0, 1] <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Softmax2d()\n&gt;&gt;&gt; # you softmax over the 2nd dimension\n&gt;&gt;&gt; input = torch.randn(2, 3, 12, 13)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#logsoftmax","title":"LogSoftmax","text":"<pre><code>class torch.nn.LogSoftmax(dim=None)\n</code></pre> <p>Applies the  function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:</p> <p></p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: any shape</li> <li>Output: same as input</li> </ul> Parameters: dim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1). Returns: a Tensor of the same dimension and shape as the input with values in the range [-inf, 0) --- --- <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.LogSoftmax()\n&gt;&gt;&gt; input = torch.randn(2, 3)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#adaptivelogsoftmaxwithloss","title":"AdaptiveLogSoftmaxWithLoss","text":"<pre><code>class torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False)\n</code></pre> <p>Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.</p> <p>Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf's law.</p> <p>Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.</p> <p>The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels.</p> <p>We highly recommend taking a look at the original paper for more details.</p> <ul> <li><code>cutoffs</code> should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting <code>cutoffs = [10, 100, 1000]</code> means that first <code>10</code> targets will be assigned to the 'head' of the adaptive softmax, targets <code>11, 12, \u2026, 100</code> will be assigned to the first cluster, and targets <code>101, 102, \u2026, 1000</code> will be assigned to the second cluster, while targets <code>1001, 1002, \u2026, n_classes - 1</code> will be assigned to the last, third cluster</li> <li><code>div_value</code> is used to compute the size of each additional cluster, which is given as , where  is the cluster index (with clusters for less frequent words having larger indices, and indices starting from ).</li> <li><code>head_bias</code> if set to True, adds a bias term to the 'head' of the adaptive softmax. See paper for details. Set to False in the official implementation.</li> </ul> <p>Warning</p> <p>Labels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index <code>0</code>, and the least frequent label should be represented by the index <code>n_classes - 1</code>.</p> <p>Note</p> <p>This module returns a <code>NamedTuple</code> with <code>output</code> and <code>loss</code> fields. See further documentation for details.</p> <p>Note</p> <p>To compute log-probabilities for all classes, the <code>log_prob</code> method can be used.</p> <p>Parameters: </p> <ul> <li>in_features (int) \u2013 Number of features in the input tensor</li> <li>n_classes (int) \u2013 Number of classes in the dataset.</li> <li>cutoffs (Sequence) \u2013 Cutoffs used to assign targets to their buckets.</li> <li>div_value (float, optional) \u2013 value used as an exponent to compute sizes of the clusters. Default: 4.0</li> </ul> <p>| Returns: | </p> <ul> <li>output is a Tensor of size <code>N</code> containing computed target log probabilities for each example</li> <li>loss is a Scalar representing the computed negative log likelihood loss</li> </ul> Return type: <code>NamedTuple</code> with <code>output</code> and <code>loss</code> fields <pre><code>Shape:\n</code></pre> <ul> <li>input: </li> <li>target:  where each value satisfies </li> <li>output: </li> <li>loss: <code>Scalar</code></li> </ul> <pre><code>log_prob(input)\n</code></pre> <p>Computes log probabilities for all </p> Parameters: input (Tensor) \u2013 a minibatch of examples Returns: log-probabilities of for each class  in range , where  is a parameter passed to <code>AdaptiveLogSoftmaxWithLoss</code> constructor. --- --- <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output: </li> </ul> <pre><code>predict(input)\n</code></pre> <p>This is equivalent to <code>self.log_pob(input).argmax(dim=1)</code>, but is more efficient in some cases.</p> Parameters: input (Tensor) \u2013 a minibatch of examples Returns: a class with the highest probability for each example --- --- Return type: output (Tensor) --- --- <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output: </li> </ul>"},{"location":"1.0/nn/#normalization-layers","title":"Normalization layers","text":""},{"location":"1.0/nn/#batchnorm1d","title":"BatchNorm1d","text":"<pre><code>class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n</code></pre> <p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension over the mini-batches and  and  are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of  are sampled from  and the elements of  are set to 0.</p> <p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, L)</code> slices, it's common terminology to call this Temporal Batch Normalization.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size  or  from input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  or </li> <li>Output:  or  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm1d(100)\n&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False)\n&gt;&gt;&gt; input = torch.randn(20, 100)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#batchnorm2d","title":"BatchNorm2d","text":"<pre><code>class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n</code></pre> <p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension over the mini-batches and  and  are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of  are sampled from  and the elements of  are set to 0.</p> <p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, H, W)</code> slices, it's common terminology to call this Spatial Batch Normalization.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm2d(100)\n&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#batchnorm3d","title":"BatchNorm3d","text":"<pre><code>class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n</code></pre> <p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension over the mini-batches and  and  are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of  are sampled from  and the elements of  are set to 0.</p> <p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics on <code>(N, D, H, W)</code> slices, it's common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code></li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm3d(100)\n&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False)\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#groupnorm","title":"GroupNorm","text":"<pre><code>class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)\n</code></pre> <p>Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization .</p> <p></p> <p>The input channels are separated into <code>num_groups</code> groups, each containing <code>num_channels / num_groups</code> channels. The mean and standard-deviation are calculated separately over the each group.  and  are learnable per-channel affine transform parameter vectorss of size <code>num_channels</code> if <code>affine</code> is <code>True</code>.</p> <p>This layer uses statistics computed from input data in both training and evaluation modes.</p> <p>Parameters: </p> <ul> <li>num_groups (int) \u2013 number of groups to separate the channels into</li> <li>num_channels (int) \u2013 number of channels expected in input</li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: <code>True</code>.</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(20, 6, 10, 10)\n&gt;&gt;&gt; # Separate 6 channels into 3 groups\n&gt;&gt;&gt; m = nn.GroupNorm(3, 6)\n&gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n&gt;&gt;&gt; m = nn.GroupNorm(6, 6)\n&gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm)\n&gt;&gt;&gt; m = nn.GroupNorm(1, 6)\n&gt;&gt;&gt; # Activating the module\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#instancenorm1d","title":"InstanceNorm1d","text":"<pre><code>class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n</code></pre> <p>Applies Instance Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.  and  are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size) if <code>affine</code> is <code>True</code>.</p> <p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p> <p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Note</p> <p><code>InstanceNorm1d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm1d</code> is applied on each channel of channeled data like multidimensional time series, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionaly, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm1d</code> usually don't apply affine transform.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size  or  from input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm1d(100)\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True)\n&gt;&gt;&gt; input = torch.randn(20, 100, 40)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#instancenorm2d","title":"InstanceNorm2d","text":"<pre><code>class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n</code></pre> <p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.  and  are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size) if <code>affine</code> is <code>True</code>.</p> <p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p> <p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Note</p> <p><code>InstanceNorm2d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm2d</code> is applied on each channel of channeled data like RGB images, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionaly, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm2d</code> usually don't apply affine transform.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm2d(100)\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True)\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#instancenorm3d","title":"InstanceNorm3d","text":"<pre><code>class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n</code></pre> <p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .</p> <p></p> <p>The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch.  and  are learnable parameter vectors of size C (where C is the input size) if <code>affine</code> is <code>True</code>.</p> <p>By default, this layer uses instance statistics computed from input data in both training and evaluation modes.</p> <p>If <code>track_running_stats</code> is set to <code>True</code>, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is , where  is the estimated statistic and  is the new observed value.</p> <p>Note</p> <p><code>InstanceNorm3d</code> and <code>LayerNorm</code> are very similar, but have some subtle differences. <code>InstanceNorm3d</code> is applied on each channel of channeled data like 3D models with RGB color, but <code>LayerNorm</code> is usually applied on entire sample and often in NLP tasks. Additionaly, <code>LayerNorm</code> applies elementwise affine transform, while <code>InstanceNorm3d</code> usually don't apply affine transform.</p> <p>Parameters: </p> <ul> <li>num_features \u2013  from an expected input of size </li> <li>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</li> <li>momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1</li> <li>affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: <code>False</code>.</li> <li>track_running_stats \u2013 a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm3d(100)\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True)\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#layernorm","title":"LayerNorm","text":"<pre><code>class torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)\n</code></pre> <p>Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization .</p> <p></p> <p>The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by <code>normalized_shape</code>.  and  are learnable affine transform parameters of <code>normalized_shape</code> if <code>elementwise_affine</code> is <code>True</code>.</p> <p>Note</p> <p>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the <code>affine</code> option, Layer Normalization applies per-element scale and bias with <code>elementwise_affine</code>.</p> <p>This layer uses statistics computed from input data in both training and evaluation modes.</p> <p>Parameters: </p> <ul> <li> <p>normalized_shape (int or list or torch.Size) \u2013</p> <p>input shape from an expected input of size</p> <p></p> <p>If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</p> </li> <li> <p>eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5</p> </li> <li>elementwise_affine \u2013 a boolean value that when set to <code>True</code>, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: <code>True</code>.</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(20, 5, 10, 10)\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:])\n&gt;&gt;&gt; # Without Learnable Parameters\n&gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n&gt;&gt;&gt; # Normalize over last two dimensions\n&gt;&gt;&gt; m = nn.LayerNorm([10, 10])\n&gt;&gt;&gt; # Normalize over last dimension of size 10\n&gt;&gt;&gt; m = nn.LayerNorm(10)\n&gt;&gt;&gt; # Activating the module\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#localresponsenorm","title":"LocalResponseNorm","text":"<pre><code>class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)\n</code></pre> <p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</p> <p></p> <p>Parameters: </p> <ul> <li>size \u2013 amount of neighbouring channels used for normalization</li> <li>alpha \u2013 multiplicative factor. Default: 0.0001</li> <li>beta \u2013 exponent. Default: 0.75</li> <li>k \u2013 additive factor. Default: 1</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lrn = nn.LocalResponseNorm(2)\n&gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24)\n&gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7)\n&gt;&gt;&gt; output_2d = lrn(signal_2d)\n&gt;&gt;&gt; output_4d = lrn(signal_4d)\n\n</code></pre>"},{"location":"1.0/nn/#recurrent-layers","title":"Recurrent layers","text":""},{"location":"1.0/nn/#rnn","title":"RNN","text":"<pre><code>class torch.nn.RNN(*args, **kwargs)\n</code></pre> <p>Applies a multi-layer Elman RNN with  or  non-linearity to an input sequence.</p> <p>For each element in the input sequence, each layer computes the following function:</p> <p></p> <p>where  is the hidden state at time <code>t</code>,  is the input at time <code>t</code>, and  is the hidden state of the previous layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>. If <code>nonlinearity</code> is <code>'relu'</code>, then <code>ReLU</code> is used instead of <code>tanh</code>.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>num_layers \u2013 Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two RNNs together to form a <code>stacked RNN</code>, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</li> <li>nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'</li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> <li>batch_first \u2013 If <code>True</code>, then the input and output tensors are provided as <code>(batch, seq, feature)</code>. Default: <code>False</code></li> <li>dropout \u2013 If non-zero, introduces a <code>Dropout</code> layer on the outputs of each RNN layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li> <li>bidirectional \u2013 If <code>True</code>, becomes a bidirectional RNN. Default: <code>False</code></li> </ul> <pre><code>Inputs: input, h_0\n</code></pre> <ul> <li>input of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <code>torch.nn.utils.rnn.pack_padded_sequence()</code> or <code>torch.nn.utils.rnn.pack_sequence()</code> for details.</li> <li>h_0 of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</li> </ul> <pre><code>Outputs: output, h_n\n</code></pre> <ul> <li> <p>output of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features (<code>h_k</code>) from the last layer of the RNN, for each <code>k</code>. If a <code>torch.nn.utils.rnn.PackedSequence</code> has been given as the input, the output will also be a packed sequence.</p> <p>For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively. Similarly, the directions can be separated in the packed case.</p> </li> <li> <p>h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for <code>k = seq_len</code>.</p> <p>Like output, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>.</p> </li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih_l[k] \u2013 the learnable input-hidden weights of the k-th layer, of shape <code>(hidden_size * input_size)</code> for <code>k = 0</code>. Otherwise, the shape is <code>(hidden_size * hidden_size)</code></li> <li>weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the k-th layer, of shape <code>(hidden_size * hidden_size)</code></li> <li>bias_ih_l[k] \u2013 the learnable input-hidden bias of the k-th layer, of shape <code>(hidden_size)</code></li> <li>bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the k-th layer, of shape <code>(hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Note</p> <p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.RNN(10, 20, 2)\n&gt;&gt;&gt; input = torch.randn(5, 3, 10)\n&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)\n&gt;&gt;&gt; output, hn = rnn(input, h0)\n\n</code></pre>"},{"location":"1.0/nn/#lstm","title":"LSTM","text":"<pre><code>class torch.nn.LSTM(*args, **kwargs)\n</code></pre> <p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p> <p>For each element in the input sequence, each layer computes the following function:</p> <p></p> <p>where  is the hidden state at time <code>t</code>,  is the cell state at time <code>t</code>,  is the input at time <code>t</code>,  is the hidden state of the layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>, and , , ,  are the input, forget, cell, and output gates, respectively.  is the sigmoid function.</p> <p>In a multilayer LSTM, the input  of the  -th layer () is the hidden state  of the previous layer multiplied by dropout  where each  is a Bernoulli random variable which is  with probability <code>dropout</code>.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>num_layers \u2013 Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two LSTMs together to form a <code>stacked LSTM</code>, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1</li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> <li>batch_first \u2013 If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></li> <li>dropout \u2013 If non-zero, introduces a <code>Dropout</code> layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li> <li>bidirectional \u2013 If <code>True</code>, becomes a bidirectional LSTM. Default: <code>False</code></li> </ul> <pre><code>Inputs: input, (h_0, c_0)\n</code></pre> <ul> <li> <p>input of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <code>torch.nn.utils.rnn.pack_padded_sequence()</code> or <code>torch.nn.utils.rnn.pack_sequence()</code> for details.</p> </li> <li> <p>h_0 of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</p> </li> <li> <p>c_0 of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial cell state for each element in the batch.</p> <p>If <code>(h_0, c_0)</code> is not provided, both h_0 and c_0 default to zero.</p> </li> </ul> <pre><code>Outputs: output, (h_n, c_n)\n</code></pre> <ul> <li> <p>output of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features <code>(h_t)</code> from the last layer of the LSTM, for each t. If a <code>torch.nn.utils.rnn.PackedSequence</code> has been given as the input, the output will also be a packed sequence.</p> <p>For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively. Similarly, the directions can be separated in the packed case.</p> </li> <li> <p>h_n of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the hidden state for <code>t = seq_len</code>.</p> <p>Like output, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code> and similarly for c_n.</p> </li> <li> <p>c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for <code>t = seq_len</code></p> </li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih_l[k] \u2013 the learnable input-hidden weights of the  layer <code>(W_ii&amp;#124;W_if&amp;#124;W_ig&amp;#124;W_io)</code>, of shape <code>(4*hidden_size x input_size)</code></li> <li>weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the  layer <code>(W_hi&amp;#124;W_hf&amp;#124;W_hg&amp;#124;W_ho)</code>, of shape <code>(4*hidden_size x hidden_size)</code></li> <li>bias_ih_l[k] \u2013 the learnable input-hidden bias of the  layer <code>(b_ii&amp;#124;b_if&amp;#124;b_ig&amp;#124;b_io)</code>, of shape <code>(4*hidden_size)</code></li> <li>bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the  layer <code>(b_hi&amp;#124;b_hf&amp;#124;b_hg&amp;#124;b_ho)</code>, of shape <code>(4*hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Note</p> <p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)\n&gt;&gt;&gt; input = torch.randn(5, 3, 10)\n&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)\n&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)\n&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))\n\n</code></pre>"},{"location":"1.0/nn/#gru","title":"GRU","text":"<pre><code>class torch.nn.GRU(*args, **kwargs)\n</code></pre> <p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p> <p>For each element in the input sequence, each layer computes the following function:</p> <p></p> <p>where  is the hidden state at time <code>t</code>,  is the input at time <code>t</code>,  is the hidden state of the layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>, and , ,  are the reset, update, and new gates, respectively.  is the sigmoid function.</p> <p>In a multilayer GRU, the input  of the  -th layer () is the hidden state  of the previous layer multiplied by dropout  where each  is a Bernoulli random variable which is  with probability <code>dropout</code>.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>num_layers \u2013 Number of recurrent layers. E.g., setting <code>num_layers=2</code> would mean stacking two GRUs together to form a <code>stacked GRU</code>, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1</li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> <li>batch_first \u2013 If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code></li> <li>dropout \u2013 If non-zero, introduces a <code>Dropout</code> layer on the outputs of each GRU layer except the last layer, with dropout probability equal to <code>dropout</code>. Default: 0</li> <li>bidirectional \u2013 If <code>True</code>, becomes a bidirectional GRU. Default: <code>False</code></li> </ul> <pre><code>Inputs: input, h_0\n</code></pre> <ul> <li>input of shape <code>(seq_len, batch, input_size)</code>: tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See <code>torch.nn.utils.rnn.pack_padded_sequence()</code> for details.</li> <li>h_0 of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</li> </ul> <pre><code>Outputs: output, h_n\n</code></pre> <ul> <li> <p>output of shape <code>(seq_len, batch, num_directions * hidden_size)</code>: tensor containing the output features h_t from the last layer of the GRU, for each t. If a <code>torch.nn.utils.rnn.PackedSequence</code> has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using <code>output.view(seq_len, batch, num_directions, hidden_size)</code>, with forward and backward being direction <code>0</code> and <code>1</code> respectively.</p> <p>Similarly, the directions can be separated in the packed case.</p> </li> <li> <p>h_n of shape <code>(num_layers * num_directions, batch, hidden_size)</code>: tensor containing the hidden state for <code>t = seq_len</code></p> <p>Like output, the layers can be separated using <code>h_n.view(num_layers, num_directions, batch, hidden_size)</code>.</p> </li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih_l[k] \u2013 the learnable input-hidden weights of the  layer (W_ir|W_iz|W_in), of shape <code>(3*hidden_size x input_size)</code></li> <li>weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the  layer (W_hr|W_hz|W_hn), of shape <code>(3*hidden_size x hidden_size)</code></li> <li>bias_ih_l[k] \u2013 the learnable input-hidden bias of the  layer (b_ir|b_iz|b_in), of shape <code>(3*hidden_size)</code></li> <li>bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the  layer (b_hr|b_hz|b_hn), of shape <code>(3*hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Note</p> <p>If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype <code>torch.float16</code> 4) V100 GPU is used, 5) input data is not in <code>PackedSequence</code> format persistent algorithm can be selected to improve performance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)\n&gt;&gt;&gt; input = torch.randn(5, 3, 10)\n&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)\n&gt;&gt;&gt; output, hn = rnn(input, h0)\n\n</code></pre>"},{"location":"1.0/nn/#rnncell","title":"RNNCell","text":"<pre><code>class torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')\n</code></pre> <p>An Elman RNN cell with tanh or ReLU non-linearity.</p> <p></p> <p>If <code>nonlinearity</code> is <code>'relu'</code>, then ReLU is used in place of tanh.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> <li>nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'</li> </ul> <pre><code>Inputs: input, hidden\n</code></pre> <ul> <li>input of shape <code>(batch, input_size)</code>: tensor containing input features</li> <li>hidden of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</li> </ul> <pre><code>Outputs: h'\n</code></pre> <ul> <li>h' of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih \u2013 the learnable input-hidden weights, of shape <code>(hidden_size x input_size)</code></li> <li>weight_hh \u2013 the learnable hidden-hidden weights, of shape <code>(hidden_size x hidden_size)</code></li> <li>bias_ih \u2013 the learnable input-hidden bias, of shape <code>(hidden_size)</code></li> <li>bias_hh \u2013 the learnable hidden-hidden bias, of shape <code>(hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)\n&gt;&gt;&gt; input = torch.randn(6, 3, 10)\n&gt;&gt;&gt; hx = torch.randn(3, 20)\n&gt;&gt;&gt; output = []\n&gt;&gt;&gt; for i in range(6):\n hx = rnn(input[i], hx)\n output.append(hx)\n\n</code></pre>"},{"location":"1.0/nn/#lstmcell","title":"LSTMCell","text":"<pre><code>class torch.nn.LSTMCell(input_size, hidden_size, bias=True)\n</code></pre> <p>A long short-term memory (LSTM) cell.</p> <p></p> <p>where  is the sigmoid function.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> </ul> <pre><code>Inputs: input, (h_0, c_0)\n</code></pre> <ul> <li> <p>input of shape <code>(batch, input_size)</code>: tensor containing input features</p> </li> <li> <p>h_0 of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch.</p> </li> <li> <p>c_0 of shape <code>(batch, hidden_size)</code>: tensor containing the initial cell state for each element in the batch.</p> <p>If <code>(h_0, c_0)</code> is not provided, both h_0 and c_0 default to zero.</p> </li> </ul> <pre><code>Outputs: h_1, c_1\n</code></pre> <ul> <li>h_1 of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li> <li>c_1 of shape <code>(batch, hidden_size)</code>: tensor containing the next cell state for each element in the batch</li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih \u2013 the learnable input-hidden weights, of shape <code>(4*hidden_size x input_size)</code></li> <li>weight_hh \u2013 the learnable hidden-hidden weights, of shape <code>(4*hidden_size x hidden_size)</code></li> <li>bias_ih \u2013 the learnable input-hidden bias, of shape <code>(4*hidden_size)</code></li> <li>bias_hh \u2013 the learnable hidden-hidden bias, of shape <code>(4*hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)\n&gt;&gt;&gt; input = torch.randn(6, 3, 10)\n&gt;&gt;&gt; hx = torch.randn(3, 20)\n&gt;&gt;&gt; cx = torch.randn(3, 20)\n&gt;&gt;&gt; output = []\n&gt;&gt;&gt; for i in range(6):\n hx, cx = rnn(input[i], (hx, cx))\n output.append(hx)\n\n</code></pre>"},{"location":"1.0/nn/#grucell","title":"GRUCell","text":"<pre><code>class torch.nn.GRUCell(input_size, hidden_size, bias=True)\n</code></pre> <p>A gated recurrent unit (GRU) cell</p> <p></p> <p>where  is the sigmoid function.</p> <p>Parameters: </p> <ul> <li>input_size \u2013 The number of expected features in the input <code>x</code></li> <li>hidden_size \u2013 The number of features in the hidden state <code>h</code></li> <li>bias \u2013 If <code>False</code>, then the layer does not use bias weights <code>b_ih</code> and <code>b_hh</code>. Default: <code>True</code></li> </ul> <pre><code>Inputs: input, hidden\n</code></pre> <ul> <li>input of shape <code>(batch, input_size)</code>: tensor containing input features</li> <li>hidden of shape <code>(batch, hidden_size)</code>: tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.</li> </ul> <pre><code>Outputs: h'\n</code></pre> <ul> <li>h' of shape <code>(batch, hidden_size)</code>: tensor containing the next hidden state for each element in the batch</li> </ul> <p>| Variables: | </p> <ul> <li>weight_ih \u2013 the learnable input-hidden weights, of shape <code>(3*hidden_size x input_size)</code></li> <li>weight_hh \u2013 the learnable hidden-hidden weights, of shape <code>(3*hidden_size x hidden_size)</code></li> <li>bias_ih \u2013 the learnable input-hidden bias, of shape <code>(3*hidden_size)</code></li> <li>bias_hh \u2013 the learnable hidden-hidden bias, of shape <code>(3*hidden_size)</code></li> </ul> <p>Note</p> <p>All the weights and biases are initialized from  where </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rnn = nn.GRUCell(10, 20)\n&gt;&gt;&gt; input = torch.randn(6, 3, 10)\n&gt;&gt;&gt; hx = torch.randn(3, 20)\n&gt;&gt;&gt; output = []\n&gt;&gt;&gt; for i in range(6):\n hx = rnn(input[i], hx)\n output.append(hx)\n\n</code></pre>"},{"location":"1.0/nn/#linear-layers","title":"Linear layers","text":""},{"location":"1.0/nn/#linear","title":"Linear","text":"<pre><code>class torch.nn.Linear(in_features, out_features, bias=True)\n</code></pre> <p>Applies a linear transformation to the incoming data: </p> <p>Parameters: </p> <ul> <li>in_features \u2013 size of each input sample</li> <li>out_features \u2013 size of each output sample</li> <li>bias \u2013 If set to False, the layer will not learn an additive bias. Default: <code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where  means any number of additional dimensions</li> <li>Output:  where all but the last dimension are the same shape as the input.</li> </ul> <p>| Variables: | </p> <ul> <li>weight \u2013 the learnable weights of the module of shape . The values are initialized from , where </li> <li>bias \u2013 the learnable bias of the module of shape . If <code>bias</code> is <code>True</code>, the values are initialized from  where </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\n</code></pre>"},{"location":"1.0/nn/#bilinear","title":"Bilinear","text":"<pre><code>class torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)\n</code></pre> <p>Applies a bilinear transformation to the incoming data: </p> <p>Parameters: </p> <ul> <li>in1_features \u2013 size of each first input sample</li> <li>in2_features \u2013 size of each second input sample</li> <li>out_features \u2013 size of each output sample</li> <li>bias \u2013 If set to False, the layer will not learn an additive bias. Default: <code>True</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: ,  where  means any number of additional dimensions. All but the last dimension of the inputs should be the same.</li> <li>Output:  where all but the last dimension are the same shape as the input.</li> </ul> <p>| Variables: | </p> <ul> <li>weight \u2013 the learnable weights of the module of shape . The values are initialized from , where </li> <li>bias \u2013 the learnable bias of the module of shape  If <code>bias</code> is <code>True</code>, the values are initialized from , where </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Bilinear(20, 30, 40)\n&gt;&gt;&gt; input1 = torch.randn(128, 20)\n&gt;&gt;&gt; input2 = torch.randn(128, 30)\n&gt;&gt;&gt; output = m(input1, input2)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 40])\n\n</code></pre>"},{"location":"1.0/nn/#dropout-layers","title":"Dropout layers","text":""},{"location":"1.0/nn/#dropout","title":"Dropout","text":"<pre><code>class torch.nn.Dropout(p=0.5, inplace=False)\n</code></pre> <p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.</p> <p>This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .</p> <p>Furthermore, the outputs are scaled by a factor of  during training. This means that during evaluation the module simply computes an identity function.</p> <p>Parameters: </p> <ul> <li>p \u2013 probability of an element to be zeroed. Default: 0.5</li> <li>inplace \u2013 If set to <code>True</code>, will do this operation in-place. Default: <code>False</code></li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: <code>Any</code>. Input can be of any shape</li> <li>Output: <code>Same</code>. Output is of the same shape as input</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Dropout(p=0.2)\n&gt;&gt;&gt; input = torch.randn(20, 16)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#dropout2d","title":"Dropout2d","text":"<pre><code>class torch.nn.Dropout2d(p=0.5, inplace=False)\n</code></pre> <p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the -th channel of the -th sample in the batched input is a 2D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability <code>p</code> using samples from a Bernoulli distribution.</p> <p>Usually the input comes from <code>nn.Conv2d</code> modules.</p> <p>As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</p> <p>In this case, <code>nn.Dropout2d()</code> will help promote independence between feature maps and should be used instead.</p> <p>Parameters: </p> <ul> <li>p (float, optional) \u2013 probability of an element to be zero-ed.</li> <li>inplace (bool, optional) \u2013 If set to <code>True</code>, will do this operation in-place</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Dropout2d(p=0.2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 32, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#dropout3d","title":"Dropout3d","text":"<pre><code>class torch.nn.Dropout3d(p=0.5, inplace=False)\n</code></pre> <p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the -th channel of the -th sample in the batched input is a 3D tensor ) of the input tensor). Each channel will be zeroed out independently on every forward call. with probability <code>p</code> using samples from a Bernoulli distribution.</p> <p>Usually the input comes from <code>nn.Conv3d</code> modules.</p> <p>As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.</p> <p>In this case, <code>nn.Dropout3d()</code> will help promote independence between feature maps and should be used instead.</p> <p>Parameters: </p> <ul> <li>p (float, optional) \u2013 probability of an element to be zeroed.</li> <li>inplace (bool, optional) \u2013 If set to <code>True</code>, will do this operation in-place</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  (same shape as input)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Dropout3d(p=0.2)\n&gt;&gt;&gt; input = torch.randn(20, 16, 4, 32, 32)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#alphadropout","title":"AlphaDropout","text":"<pre><code>class torch.nn.AlphaDropout(p=0.5, inplace=False)\n</code></pre> <p>Applies Alpha Dropout over the input.</p> <p>Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.</p> <p>During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.</p> <p>During evaluation the module simply computes an identity function.</p> <p>More details can be found in the paper Self-Normalizing Neural Networks .</p> <p>Parameters: </p> <ul> <li>p (float) \u2013 probability of an element to be dropped. Default: 0.5</li> <li>inplace (bool, optional) \u2013 If set to <code>True</code>, will do this operation in-place</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: <code>Any</code>. Input can be of any shape</li> <li>Output: <code>Same</code>. Output is of the same shape as input</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.AlphaDropout(p=0.2)\n&gt;&gt;&gt; input = torch.randn(20, 16)\n&gt;&gt;&gt; output = m(input)\n\n</code></pre>"},{"location":"1.0/nn/#sparse-layers","title":"Sparse layers","text":""},{"location":"1.0/nn/#embedding","title":"Embedding","text":"<pre><code>class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)\n</code></pre> <p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p> <p>This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.</p> <p>Parameters: </p> <ul> <li>num_embeddings (int) \u2013 size of the dictionary of embeddings</li> <li>embedding_dim (int) \u2013 the size of each embedding vector</li> <li>padding_idx (int, optional) \u2013 If given, pads the output with the embedding vector at <code>padding_idx</code> (initialized to zeros) whenever it encounters the index.</li> <li>max_norm (float, optional) \u2013 If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.</li> <li>norm_type (float, optional) \u2013 The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li> <li>scale_grad_by_freq (boolean__, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>.</li> <li>sparse (bool, optional) \u2013 If <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.</li> </ul> Variables: weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from  <p>Shape:</p> <ul> <li>Input: LongTensor of arbitrary shape containing the indices to extract</li> <li>Output: <code>(*, embedding_dim)</code>, where <code>*</code> is the input shape</li> </ul> <p>Note</p> <p>Keep in mind that only a limited number of optimizers support sparse gradients: currently it's <code>optim.SGD</code> (<code>CUDA</code> and <code>CPU</code>), <code>optim.SparseAdam</code> (<code>CUDA</code> and <code>CPU</code>) and <code>optim.Adagrad</code> (<code>CPU</code>)</p> <p>Note</p> <p>With <code>padding_idx</code> set, the embedding vector at <code>padding_idx</code> is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from <code>Embedding</code> is always zero.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n&gt;&gt;&gt; embedding = nn.Embedding(10, 3)\n&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n&gt;&gt;&gt; embedding(input)\ntensor([[[-0.0251, -1.6902,  0.7172],\n [-0.6431,  0.0748,  0.6969],\n [ 1.4970,  1.3448, -0.9685],\n [-0.3677, -2.7265, -0.1685]],\n\n [[ 1.4970,  1.3448, -0.9685],\n [ 0.4362, -0.4004,  0.9400],\n [-0.6431,  0.0748,  0.6969],\n [ 0.9124, -2.3616,  1.1151]]])\n\n&gt;&gt;&gt; # example with padding_idx\n&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)\n&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])\n&gt;&gt;&gt; embedding(input)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n [ 0.1535, -2.0309,  0.9315],\n [ 0.0000,  0.0000,  0.0000],\n [-0.1655,  0.9897,  0.0635]]])\n\n</code></pre> <pre><code>classmethod from_pretrained(embeddings, freeze=True, sparse=False)\n</code></pre> <p>Creates Embedding instance from given 2-dimensional FloatTensor.</p> <p>Parameters: </p> <ul> <li>embeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as 'num_embeddings', second as 'embedding_dim'.</li> <li>freeze (boolean__, optional) \u2013 If <code>True</code>, the tensor does not get updated in the learning process. Equivalent to <code>embedding.weight.requires_grad = False</code>. Default: <code>True</code></li> <li>sparse (bool, optional) \u2013 if <code>True</code>, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # FloatTensor containing pretrained weights\n&gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n&gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)\n&gt;&gt;&gt; # Get embeddings for index 1\n&gt;&gt;&gt; input = torch.LongTensor([1])\n&gt;&gt;&gt; embedding(input)\ntensor([[ 4.0000,  5.1000,  6.3000]])\n\n</code></pre>"},{"location":"1.0/nn/#embeddingbag","title":"EmbeddingBag","text":"<pre><code>class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False)\n</code></pre> <p>Computes sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.</p> <p>For bags of constant length, this class</p> <ul> <li>with <code>mode=\"sum\"</code> is equivalent to <code>Embedding</code> followed by <code>torch.sum(dim=1)</code>,</li> <li>with <code>mode=\"mean\"</code> is equivalent to <code>Embedding</code> followed by <code>torch.mean(dim=1)</code>,</li> <li>with <code>mode=\"max\"</code> is equivalent to <code>Embedding</code> followed by <code>torch.max(dim=1)</code>.</li> </ul> <p>However, <code>EmbeddingBag</code> is much more time and memory efficient than using a chain of these operations.</p> <p>Parameters: </p> <ul> <li>num_embeddings (int) \u2013 size of the dictionary of embeddings</li> <li>embedding_dim (int) \u2013 the size of each embedding vector</li> <li>max_norm (float, optional) \u2013 If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>.</li> <li>norm_type (float, optional) \u2013 The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li> <li>scale_grad_by_freq (boolean__, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>. Note: this option is not supported when <code>mode=\"max\"</code>.</li> <li>mode (string__, optional) \u2013 <code>\"sum\"</code>, <code>\"mean\"</code> or <code>\"max\"</code>. Specifies the way to reduce the bag. Default: <code>\"mean\"</code></li> <li>sparse (bool, optional) \u2013 if <code>True</code>, gradient w.r.t. <code>weight</code> matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when <code>mode=\"max\"</code>.</li> </ul> Variables: weight (Tensor) \u2013 the learnable weights of the module of shape <code>(num_embeddings x embedding_dim)</code> initialized from . <p>Inputs: <code>input</code> (LongTensor) and <code>offsets</code> (LongTensor, optional)</p> <ul> <li> <p>If <code>input</code> is 2D of shape <code>B x N</code>,</p> <p>it will be treated as <code>B</code> bags (sequences) each of fixed length <code>N</code>, and this will return <code>B</code> values aggregated in a way depending on the <code>mode</code>. <code>offsets</code> is ignored and required to be <code>None</code> in this case.</p> </li> <li> <p>If <code>input</code> is 1D of shape <code>N</code>,</p> <p>it will be treated as a concatenation of multiple bags (sequences). <code>offsets</code> is required to be a 1D tensor containing the starting index positions of each bag in <code>input</code>. Therefore, for <code>offsets</code> of shape <code>B</code>, <code>input</code> will be viewed as having <code>B</code> bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.</p> </li> </ul> <p>Output shape: <code>B x embedding_dim</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n&gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\n&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.LongTensor([1,2,4,5,4,3,2,9])\n&gt;&gt;&gt; offsets = torch.LongTensor([0,4])\n&gt;&gt;&gt; embedding_sum(input, offsets)\ntensor([[-0.8861, -5.4350, -0.0523],\n [ 1.1306, -2.5798, -1.0044]])\n\n</code></pre>"},{"location":"1.0/nn/#distance-functions","title":"Distance functions","text":""},{"location":"1.0/nn/#cosinesimilarity","title":"CosineSimilarity","text":"<pre><code>class torch.nn.CosineSimilarity(dim=1, eps=1e-08)\n</code></pre> <p>Returns cosine similarity between  and , computed along dim.</p> <p></p> <p>Parameters: </p> <ul> <li>dim (int, optional) \u2013 Dimension where cosine similarity is computed. Default: 1</li> <li>eps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input1:  where D is at position <code>dim</code></li> <li>Input2: , same shape as the Input1</li> <li>Output: </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input1 = torch.randn(100, 128)\n&gt;&gt;&gt; input2 = torch.randn(100, 128)\n&gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n&gt;&gt;&gt; output = cos(input1, input2)\n\n</code></pre>"},{"location":"1.0/nn/#pairwisedistance","title":"PairwiseDistance","text":"<pre><code>class torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)\n</code></pre> <p>Computes the batchwise pairwise distance between vectors ,  using the p-norm:</p> <p></p> <p>Parameters: </p> <ul> <li>p (real) \u2013 the norm degree. Default: 2</li> <li>eps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-6</li> <li>keepdim (bool, optional) \u2013 Determines whether or not to keep the batch dimension. Default: False</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input1:  where <code>D = vector dimension</code></li> <li>Input2: , same shape as the Input1</li> <li>Output: . If <code>keepdim</code> is <code>False</code>, then .</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2)\n&gt;&gt;&gt; input1 = torch.randn(100, 128)\n&gt;&gt;&gt; input2 = torch.randn(100, 128)\n&gt;&gt;&gt; output = pdist(input1, input2)\n\n</code></pre>"},{"location":"1.0/nn/#loss-functions","title":"Loss functions","text":""},{"location":"1.0/nn/#l1loss","title":"L1Loss","text":"<pre><code>class torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the mean absolute error (MAE) between each element in the input <code>x</code> and target <code>y</code>.</p> <p>The loss can be described as:</p> <p></p> <p>where  is the batch size. If reduce is <code>True</code>, then:</p> <p></p> <p><code>x</code> and <code>y</code> are tensors of arbitrary shapes with a total of <code>n</code> elements each.</p> <p>The sum operation still operates over all the elements, and divides by <code>n</code>.</p> <p>The division by <code>n</code> can be avoided if one sets the constructor argument <code>size_average=False</code>.</p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>*</code> means, any number of additional dimensions</li> <li>Target: , same shape as the input</li> <li>Output: scalar. If reduce is <code>False</code>, then , same shape as the input</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = nn.L1Loss()\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#mseloss","title":"MSELoss","text":"<pre><code>class torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input <code>x</code> and target <code>y</code>.</p> <p>The loss can be described as:</p> <p></p> <p>where  is the batch size. If reduce is <code>True</code>, then:</p> <p></p> <p>The sum operation still operates over all the elements, and divides by <code>n</code>.</p> <p>The division by <code>n</code> can be avoided if one sets <code>size_average</code> to <code>False</code>.</p> <p>To get a batch of losses, a loss per batch element, set <code>reduce</code> to <code>False</code>. These losses are not averaged and are not affected by <code>size_average</code>.</p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>*</code> means, any number of additional dimensions</li> <li>Target: , same shape as the input</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = nn.MSELoss()\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#crossentropyloss","title":"CrossEntropyLoss","text":"<pre><code>class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n</code></pre> <p>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p> <p>It is useful when training a classification problem with <code>C</code> classes. If provided, the optional argument <code>weight</code> should be a 1D <code>Tensor</code> assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</p> <p>The <code>input</code> is expected to contain scores for each class.</p> <p><code>input</code> has to be a Tensor of size either  or  with  for the <code>K</code>-dimensional case (described later).</p> <p>This criterion expects a class index (0 to <code>C-1</code>) as the <code>target</code> for each value of a 1D tensor of size <code>minibatch</code></p> <p>The loss can be described as:</p> <p></p> <p>or in the case of the <code>weight</code> argument being specified:</p> <p></p> <p>The losses are averaged across observations for each minibatch.</p> <p>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size  with , where  is the number of dimensions, and a target of appropriate shape (see below).</p> <p>Parameters: </p> <ul> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size <code>C</code></li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>ignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets.</li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p><code>py     Input: \\((N, C)\\) where C = number of classes, or</code></p> <p> with  in the case of <code>K</code>-dimensional loss. *   <code>py Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or</code></p> <p> with  in the case of K-dimensional loss. *   <code>py Output: scalar. If reduce is False, then the same size</code></p> <p>as the target: , or  with  in the case of K-dimensional loss.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = nn.CrossEntropyLoss()\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#ctcloss","title":"CTCLoss","text":"<pre><code>class torch.nn.CTCLoss(blank=0, reduction='mean')\n</code></pre> <p>The Connectionist Temporal Classification loss.</p> <p>Parameters: </p> <ul> <li>blank (int, optional) \u2013 blank label. Default .</li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: 'mean'</li> </ul> <pre><code>Inputs:\n</code></pre> <pre><code>log_probs: Tensor of size \\((T, N, C)\\) where C = number of characters in alphabet including blank,\n</code></pre> <p>T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with <code>torch.nn.functional.log_softmax()</code>).</p> <pre><code>targets: Tensor of size \\((N, S)\\) or (sum(target_lengths)).\n</code></pre> <p>Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.</p> <pre><code>input_lengths: Tuple or tensor of size \\((N)\\).\n</code></pre> <p>Lengths of the inputs (must each be )</p> <pre><code>target_lengths: Tuple or tensor of size  \\((N)\\).\n</code></pre> <p>Lengths of the targets</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; ctc_loss = nn.CTCLoss()\n&gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n&gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n&gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)\n&gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n&gt;&gt;&gt; loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n&gt;&gt;&gt; loss.backward()\n\n</code></pre> <pre><code>Reference:\n</code></pre> <p>A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf</p> <p>Note</p> <p>In order to use CuDNN, the following must be satisfied: <code>targets</code> must be in concatenated format, all <code>input_lengths</code> must be <code>T</code>. , <code>target_lengths</code> , the integer arguments must be of dtype <code>torch.int32</code>.</p> <p>The regular implementation uses the (more common in PyTorch) <code>torch.long</code> dtype.</p> <p>Note</p> <p>In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. Please see the notes on Reproducibility for background.</p>"},{"location":"1.0/nn/#nllloss","title":"NLLLoss","text":"<pre><code>class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n</code></pre> <p>The negative log likelihood loss. It is useful to train a classification problem with <code>C</code> classes.</p> <p>If provided, the optional argument <code>weight</code> should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.</p> <p>The input given through a forward call is expected to contain log-probabilities of each class. <code>input</code> has to be a Tensor of size either  or  with  for the <code>K</code>-dimensional case (described later).</p> <p>Obtaining log-probabilities in a neural network is easily achieved by adding a <code>LogSoftmax</code> layer in the last layer of your network. You may use <code>CrossEntropyLoss</code> instead, if you prefer not to add an extra layer.</p> <p>The target that this loss expects is a class index <code>(0 to C-1, where C = number of classes)</code></p> <p>If <code>reduce</code> is <code>False</code>, the loss can be described as:</p> <p></p> <p>where  is the batch size. If <code>reduce</code> is <code>True</code> (default), then</p> <p></p> <p>Can also be used for higher dimension inputs, such as 2D images, by providing an input of size  with , where  is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.</p> <p>Parameters: </p> <ul> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>ignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets.</li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li> <p><code>py     Input: \\((N, C)\\) where C = number of classes, or</code></p> <p> with  in the case of <code>K</code>-dimensional loss. *   <code>py Target: \\((N)\\) where each value is \\(0 \\leq \\text{targets}[i] \\leq C-1\\), or</code></p> <p> with  in the case of K-dimensional loss. *   <code>py Output: scalar. If reduce is False, then the same size</code></p> <p>as the target: , or  with  in the case of K-dimensional loss.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.LogSoftmax()\n&gt;&gt;&gt; loss = nn.NLLLoss()\n&gt;&gt;&gt; # input is of size N x C = 3 x 5\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C\n&gt;&gt;&gt; target = torch.tensor([1, 0, 4])\n&gt;&gt;&gt; output = loss(m(input), target)\n&gt;&gt;&gt; output.backward()\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)\n&gt;&gt;&gt; N, C = 5, 4\n&gt;&gt;&gt; loss = nn.NLLLoss()\n&gt;&gt;&gt; # input is of size N x C x height x width\n&gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)\n&gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))\n&gt;&gt;&gt; m = nn.LogSoftmax()\n&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C\n&gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n&gt;&gt;&gt; output = loss(m(conv(data)), target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#poissonnllloss","title":"PoissonNLLLoss","text":"<pre><code>class torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n</code></pre> <p>Negative log likelihood loss with Poisson distribution of target.</p> <p>The loss can be described as:</p> <p></p> <p>The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.</p> <p>Parameters: </p> <ul> <li>log_input (bool, optional) \u2013 if <code>True</code> the loss is computed as , if <code>False</code> the loss is .</li> <li> <p>full (bool, optional) \u2013</p> <p>whether to compute full loss, i. e. to add the Stirling approximation term</p> <p></p> </li> <li> <p>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></p> </li> <li>eps (float, optional) \u2013 Small value to avoid evaluation of  when <code>log_input == False</code>. Default: 1e-8</li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loss = nn.PoissonNLLLoss()\n&gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(5, 2)\n&gt;&gt;&gt; output = loss(log_input, target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#kldivloss","title":"KLDivLoss","text":"<pre><code>class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>The Kullback-Leibler divergence Loss</p> <p>KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.</p> <p>As with <code>NLLLoss</code>, the <code>input</code> given is expected to contain log-probabilities. However, unlike <code>NLLLoss</code>, <code>input</code> is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).</p> <p>This criterion expects a <code>target</code> <code>Tensor</code> of the same size as the <code>input</code> <code>Tensor</code>.</p> <p>The unreduced (i.e. with <code>reduce</code> set to <code>False</code>) loss can be described as:</p> <p></p> <p>where the index  spans all dimensions of <code>input</code> and  has the same shape as <code>input</code>. If <code>reduce</code> is <code>True</code> (the default), then:</p> <p></p> <p>In default reduction mode 'mean', the losses are averaged for each minibatch over observations as well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses are averaged over batch dimension only. 'mean' mode's behavior will be changed to the same as 'batchmean' in the next major release.</p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied. 'batchmean': the sum of the output will be divided by batchsize. 'sum': the output will be summed. 'mean': the output will be divided by the number of elements in the output. Default: 'mean'</li> </ul> <p>:param .. note:: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated,: and in the meantime, specifying either of those two args will override <code>reduction</code>. :param .. note:: <code>reduction='mean'</code> doesn't return the true kl divergence value, please use: <code>reduction='batchmean'</code> which aligns with KL math definition.</p> <p>In the next major release, 'mean' will be changed to be the same as 'batchmean'.</p> <pre><code>Shape:\n</code></pre> <ul> <li>input:  where <code>*</code> means, any number of additional dimensions</li> <li>target: , same shape as the input</li> <li> <p><code>py     output: scalar by default. If reduce is False, then \\((N, *)\\),</code></p> <p>the same shape as the input</p> </li> </ul>"},{"location":"1.0/nn/#bceloss","title":"BCELoss","text":"<pre><code>class torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</p> <p>The loss can be described as:</p> <p></p> <p>where  is the batch size. If reduce is <code>True</code>, then</p> <p></p> <p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets <code>y</code> should be numbers between 0 and 1.</p> <p>Parameters: </p> <ul> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size \u201cnbatch\u201d.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>*</code> means, any number of additional dimensions</li> <li>Target: , same shape as the input</li> <li>Output: scalar. If <code>reduce</code> is False, then <code>(N, *)</code>, same shape as input.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = nn.Sigmoid()\n&gt;&gt;&gt; loss = nn.BCELoss()\n&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3).random_(2)\n&gt;&gt;&gt; output = loss(m(input), target)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#bcewithlogitsloss","title":"BCEWithLogitsLoss","text":"<pre><code>class torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n</code></pre> <p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single class. This version is more numerically stable than using a plain <code>Sigmoid</code> followed by a <code>BCELoss</code> as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.</p> <p>The loss can be described as:</p> <p></p> <p>where  is the batch size. If reduce is <code>True</code>, then</p> <p></p> <p>This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets <code>t[i]</code> should be numbers between 0 and 1.</p> <p>It's possible to trade off recall and precision by adding weights to positive examples. In this case the loss can be described as:</p> <p></p> <p>where  is the positive weight of class .  increases the recall,  increases the precision.</p> <p>For example, if a dataset contains 100 positive and 300 negative examples of a single class, then <code>pos_weight</code> for the class should be equal to . The loss would act as if the dataset contains  positive examples.</p> <p>Parameters: </p> <ul> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size \u201cnbatch\u201d.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> <li>pos_weight \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.</li> </ul>"},{"location":"1.0/nn/#marginrankingloss","title":"MarginRankingLoss","text":"<pre><code>class torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the loss given inputs <code>x1</code>, <code>x2</code>, two 1D mini-batch <code>Tensor</code>s, and a label 1D mini-batch tensor <code>y</code> with values (<code>1</code> or <code>-1</code>).</p> <p>If <code>y == 1</code> then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for <code>y == -1</code>.</p> <p>The loss function for each sample in the mini-batch is:</p> <p></p> <p>Parameters: </p> <ul> <li>margin (float, optional) \u2013 Has a default value of <code>0</code>.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>N</code> is the batch size and <code>D</code> is the size of a sample.</li> <li>Target: </li> <li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li> </ul>"},{"location":"1.0/nn/#hingeembeddingloss","title":"HingeEmbeddingLoss","text":"<pre><code>class torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Measures the loss given an input tensor <code>x</code> and a labels tensor <code>y</code> containing values (<code>1</code> or <code>-1</code>). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as <code>x</code>, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p> <p>The loss function for -th sample in the mini-batch is</p> <p></p> <p>and the total loss functions is</p> <p></p> <p>where .</p> <p>Parameters: </p> <ul> <li>margin (float, optional) \u2013 Has a default value of <code>1</code>.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: Tensor of arbitrary shape. The sum operation operates over all the elements.</li> <li>Target: Same shape as input.</li> <li>Output: scalar. If reduce is <code>False</code>, then same shape as the input</li> </ul>"},{"location":"1.0/nn/#multilabelmarginloss","title":"MultiLabelMarginLoss","text":"<pre><code>class torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input <code>x</code> (a 2D mini-batch <code>Tensor</code>) and output <code>y</code> (which is a 2D <code>Tensor</code> of target class indices). For each sample in the mini-batch:</p> <p></p> <p>where  to ,  to , , and  for all  and .</p> <p><code>y</code> and <code>x</code> must have the same size.</p> <p>The criterion only considers a contiguous block of non-negative targets that starts at the front.</p> <p>This allows for different samples to have variable amounts of target classes</p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  or  where <code>N</code> is the batch size and <code>C</code> is the number of classes.</li> <li>Target:  or , same shape as the input.</li> <li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li> </ul>"},{"location":"1.0/nn/#smoothl1loss","title":"SmoothL1Loss","text":"<pre><code>class torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the <code>MSELoss</code> and in some cases prevents exploding gradients (e.g. see \u201cFast R-CNN\u201d paper by Ross Girshick). Also known as the Huber loss:</p> <p></p> <p>where  is given by:</p> <p></p> <p><code>x</code> and <code>y</code> arbitrary shapes with a total of <code>n</code> elements each the sum operation still operates over all the elements, and divides by <code>n</code>.</p> <p>The division by <code>n</code> can be avoided if one sets <code>size_average</code> to <code>False</code></p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>*</code> means, any number of additional dimensions</li> <li>Target: , same shape as the input</li> <li>Output: scalar. If reduce is <code>False</code>, then , same shape as the input</li> </ul>"},{"location":"1.0/nn/#softmarginloss","title":"SoftMarginLoss","text":"<pre><code>class torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor <code>x</code> and target tensor <code>y</code> (containing 1 or -1).</p> <p></p> <p>Parameters: </p> <ul> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: Tensor of arbitrary shape.</li> <li>Target: Same shape as input.</li> <li>Output: scalar. If reduce is <code>False</code>, then same shape as the input</li> </ul>"},{"location":"1.0/nn/#multilabelsoftmarginloss","title":"MultiLabelSoftMarginLoss","text":"<pre><code>class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input <code>x</code> and target <code>y</code> of size <code>(N, C)</code>. For each sample in the minibatch:</p> <p></p> <p>where <code>i == 0</code> to <code>x.nElement()-1</code>, <code>y[i] in {0,1}</code>.</p> <p>Parameters: </p> <ul> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>N</code> is the batch size and <code>C</code> is the number of classes.</li> <li>Target: , same shape as the input.</li> <li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li> </ul>"},{"location":"1.0/nn/#cosineembeddingloss","title":"CosineEmbeddingLoss","text":"<pre><code>class torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the loss given input tensors ,  and a <code>Tensor</code> label <code>y</code> with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.</p> <p>The loss function for each sample is:</p> <p></p> <p>Parameters: </p> <ul> <li>margin (float, optional) \u2013 Should be a number from <code>-1</code> to <code>1</code>, <code>0</code> to <code>0.5</code> is suggested. If <code>margin</code> is missing, the default value is <code>0</code>.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul>"},{"location":"1.0/nn/#multimarginloss","title":"MultiMarginLoss","text":"<pre><code>class torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input <code>x</code> (a 2D mini-batch <code>Tensor</code>) and output <code>y</code> (which is a 1D tensor of target class indices, ):</p> <p>For each mini-batch sample, the loss in terms of the 1D input <code>x</code> and scalar output <code>y</code> is:</p> <p></p> <p>where <code>i == 0</code> to <code>x.size(0)</code> and .</p> <p>Optionally, you can give non-equal weighting on the classes by passing a 1D <code>weight</code> tensor into the constructor.</p> <p>The loss function then becomes:</p> <p></p> <p>Parameters: </p> <ul> <li>p (int, optional) \u2013 Has a default value of <code>1</code>. <code>1</code> and <code>2</code> are the only supported values</li> <li>margin (float, optional) \u2013 Has a default value of <code>1</code>.</li> <li>weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size <code>C</code>. Otherwise, it is treated as if having all ones.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul>"},{"location":"1.0/nn/#tripletmarginloss","title":"TripletMarginLoss","text":"<pre><code>class torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. This is used for measuring a relative similarity between samples. A triplet is composed by <code>a</code>, <code>p</code> and <code>n</code>: anchor, positive examples and negative example respectively. The shapes of all input tensors should be .</p> <p>The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al.</p> <p>The loss function for each sample in the mini-batch is:</p> <p></p> <p>where</p> <p></p> <p>Parameters: </p> <ul> <li>margin (float, optional) \u2013 Default: <code>1</code>.</li> <li>p (int, optional) \u2013 The norm degree for pairwise distance. Default: <code>2</code>.</li> <li>swap (float, optional) \u2013 The distance swap is described in detail in the paper <code>Learning shallow convolutional feature descriptors with triplet losses</code> by V. Balntas, E. Riba et al. Default: <code>False</code>.</li> <li>size_average (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code></li> <li>reduce (bool, optional) \u2013 Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code></li> <li>reduction (string__, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: 'mean'</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input:  where <code>D</code> is the vector dimension.</li> <li>Output: scalar. If <code>reduce</code> is False, then <code>(N)</code>.</li> </ul> <pre><code>&gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n&gt;&gt;&gt; input1 = torch.randn(100, 128, requires_grad=True)\n&gt;&gt;&gt; input2 = torch.randn(100, 128, requires_grad=True)\n&gt;&gt;&gt; input3 = torch.randn(100, 128, requires_grad=True)\n&gt;&gt;&gt; output = triplet_loss(input1, input2, input3)\n&gt;&gt;&gt; output.backward()\n\n</code></pre>"},{"location":"1.0/nn/#vision-layers","title":"Vision layers","text":""},{"location":"1.0/nn/#pixelshuffle","title":"PixelShuffle","text":"<pre><code>class torch.nn.PixelShuffle(upscale_factor)\n</code></pre> <p>Rearranges elements in a tensor of shape  to a tensor of shape .</p> <p>This is useful for implementing efficient sub-pixel convolution with a stride of .</p> <p>Look at the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.</p> Parameters: upscale_factor (int) \u2013 factor to increase spatial resolution by <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output: </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pixel_shuffle = nn.PixelShuffle(3)\n&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)\n&gt;&gt;&gt; output = pixel_shuffle(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([1, 1, 12, 12])\n\n</code></pre>"},{"location":"1.0/nn/#upsample","title":"Upsample","text":"<pre><code>class torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)\n</code></pre> <p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p> <p>The input data is assumed to be of the form <code>minibatch x channels x [optional depth] x [optional height] x width</code>. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.</p> <p>The algorithms available for upsampling are nearest neighbor and linear, bilinear and trilinear for 3D, 4D and 5D input Tensor, respectively.</p> <p>One can either give a <code>scale_factor</code> or the target output <code>size</code> to calculate the output size. (You cannot give both, as it is ambiguous)</p> <p>Parameters: </p> <ul> <li>size (tuple, optional) \u2013 a tuple of ints <code>([optional D_out], [optional H_out], W_out)</code> output sizes</li> <li>scale_factor (int / tuple of python:ints__, optional) \u2013 the multiplier for the image height / width / depth</li> <li>mode (string__, optional) \u2013 the upsampling algorithm: one of <code>nearest</code>, <code>linear</code>, <code>bilinear</code> and <code>trilinear</code>. Default: <code>nearest</code></li> <li>align_corners (bool, optional) \u2013 if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when <code>mode</code> is <code>linear</code>, <code>bilinear</code>, or <code>trilinear</code>. Default: False</li> </ul> <pre><code>Shape:\n</code></pre> <ul> <li>Input: ,  or </li> <li>Output: ,  or , where</li> </ul> <p></p> <p></p> <p></p> <p>Warning</p> <p>With <code>align_corners = True</code>, the linearly interpolating modes (<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>) don't proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is <code>align_corners = False</code>. See below for concrete examples on how this affects the outputs.</p> <p>Note</p> <p>If you want downsampling/general resizing, you should use <code>interpolate()</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2).float()\n&gt;&gt;&gt; input\ntensor([[[[ 1.,  2.],\n [ 3.,  4.]]]])\n\n&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='nearest')\n&gt;&gt;&gt; m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n [ 1.,  1.,  2.,  2.],\n [ 3.,  3.,  4.,  4.],\n [ 3.,  3.,  4.,  4.]]]])\n\n&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n&gt;&gt;&gt; m(input)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],\n [ 1.5000,  1.7500,  2.2500,  2.5000],\n [ 2.5000,  2.7500,  3.2500,  3.5000],\n [ 3.0000,  3.2500,  3.7500,  4.0000]]]])\n\n&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n&gt;&gt;&gt; m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n [ 1.6667,  2.0000,  2.3333,  2.6667],\n [ 2.3333,  2.6667,  3.0000,  3.3333],\n [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n\n&gt;&gt;&gt; # Try scaling the same data in a larger tensor\n&gt;&gt;&gt;\n&gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)\n&gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input)\ntensor([[[[ 1.,  2.],\n [ 3.,  4.]]]])\n&gt;&gt;&gt; input_3x3\ntensor([[[[ 1.,  2.,  0.],\n [ 3.,  4.,  0.],\n [ 0.,  0.,  0.]]]])\n\n&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n&gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary)\n&gt;&gt;&gt; m(input_3x3)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],\n [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],\n [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],\n [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],\n [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n\n&gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n&gt;&gt;&gt; # Notice that values in top left corner are now changed\n&gt;&gt;&gt; m(input_3x3)\ntensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],\n [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],\n [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],\n [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],\n [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n\n</code></pre>"},{"location":"1.0/nn/#upsamplingnearest2d","title":"UpsamplingNearest2d","text":"<pre><code>class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)\n</code></pre> <p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</p> <p>To specify the scale, it takes either the <code>size</code> or the <code>scale_factor</code> as it's constructor argument.</p> <p>When <code>size</code> is given, it is the output size of the image <code>(h, w)</code>.</p> <p>Parameters: </p> <ul> <li>size (tuple, optional) \u2013 a tuple of ints <code>(H_out, W_out)</code> output sizes</li> <li>scale_factor (int, optional) \u2013 the multiplier for the image height or width</li> </ul> <p>Warning</p> <p>This class is deprecated in favor of <code>interpolate()</code>.</p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  where</li> </ul> <p></p> <p></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)\n&gt;&gt;&gt; input\ntensor([[[[ 1.,  2.],\n [ 3.,  4.]]]])\n\n&gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2)\n&gt;&gt;&gt; m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n [ 1.,  1.,  2.,  2.],\n [ 3.,  3.,  4.,  4.],\n [ 3.,  3.,  4.,  4.]]]])\n\n</code></pre>"},{"location":"1.0/nn/#upsamplingbilinear2d","title":"UpsamplingBilinear2d","text":"<pre><code>class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)\n</code></pre> <p>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</p> <p>To specify the scale, it takes either the <code>size</code> or the <code>scale_factor</code> as it's constructor argument.</p> <p>When <code>size</code> is given, it is the output size of the image <code>(h, w)</code>.</p> <p>Parameters: </p> <ul> <li>size (tuple, optional) \u2013 a tuple of ints <code>(H_out, W_out)</code> output sizes</li> <li>scale_factor (int, optional) \u2013 the multiplier for the image height or width</li> </ul> <p>Warning</p> <p>This class is deprecated in favor of <code>interpolate()</code>. It is equivalent to <code>nn.functional.interpolate(..., mode='bilinear', align_corners=True)</code>.</p> <pre><code>Shape:\n</code></pre> <ul> <li>Input: </li> <li>Output:  where</li> </ul> <p></p> <p></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; input = torch.arange(1, 5).view(1, 1, 2, 2)\n&gt;&gt;&gt; input\ntensor([[[[ 1.,  2.],\n [ 3.,  4.]]]])\n\n&gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2)\n&gt;&gt;&gt; m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n [ 1.6667,  2.0000,  2.3333,  2.6667],\n [ 2.3333,  2.6667,  3.0000,  3.3333],\n [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n\n</code></pre>"},{"location":"1.0/nn/#dataparallel-layers-multi-gpu-distributed","title":"DataParallel layers (multi-GPU, distributed)","text":""},{"location":"1.0/nn/#dataparallel","title":"DataParallel","text":"<pre><code>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n</code></pre> <p>Implements data parallelism at the module level.</p> <p>This container parallelizes the application of the given <code>module</code> by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.</p> <p>The batch size should be larger than the number of GPUs used.</p> <p>See also: Use nn.DataParallel instead of multiprocessing</p> <p>Arbitrary positional and keyword inputs are allowed to be passed into DataParallel EXCEPT Tensors. All tensors will be scattered on dim specified (default 0). Primitive types will be broadcasted, but all other types will be a shallow copy and can be corrupted if written to in the model's forward pass.</p> <p>The parallelized <code>module</code> must have its parameters and buffers on <code>device_ids[0]</code> before running this <code>DataParallel</code> module.</p> <p>Warning</p> <p>In each forward, <code>module</code> is replicated on each device, so any updates to the runing module in <code>forward</code> will be lost. For example, if <code>module</code> has a counter attribute that is incremented in each <code>forward</code>, it will always stay at the initial value becasue the update is done on the replicas which are destroyed after <code>forward</code>. However, <code>DataParallel</code> guarantees that the replica on <code>device[0]</code> will have its parameters and buffers sharing storage with the base parallelized <code>module</code>. So in-place updates to the parameters or buffers on <code>device[0]</code> will be recorded. E.g., <code>BatchNorm2d</code> and <code>spectral_norm()</code> rely on this behavior to update the buffers.</p> <p>Warning</p> <p>Forward and backward hooks defined on <code>module</code> and its submodules will be invoked <code>len(device_ids)</code> times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via <code>register_forward_pre_hook()</code> be executed before <code>all</code> <code>len(device_ids)</code> <code>forward()</code> calls, but that each such hook be executed before the corresponding <code>forward()</code> call of that device.</p> <p>Warning</p> <p>When <code>module</code> returns a scalar (i.e., 0-dimensional tensor) in <code>forward()</code>, this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</p> <p>Note</p> <p>There is a subtlety in using the <code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code> pattern in a <code>Module</code> wrapped in <code>DataParallel</code>. See My recurrent network doesn't work with data parallelism section in FAQ for details.</p> <p>Parameters: </p> <ul> <li>module (Module) \u2013 module to be parallelized</li> <li>device_ids (list of python:int or torch.device) \u2013 CUDA devices (default: all devices)</li> <li>output_device (int or torch.device) \u2013 device location of output (default: device_ids[0])</li> </ul> Variables: module (Module) \u2013 the module to be parallelized <p>Example:</p> <pre><code>&gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n&gt;&gt;&gt; output = net(input_var)\n\n</code></pre>"},{"location":"1.0/nn/#distributeddataparallel","title":"DistributedDataParallel","text":"<pre><code>class torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, check_reduction=False)\n</code></pre> <p>Implements distributed data parallelism that is based on torch.distributed package at the module level.</p> <p>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</p> <p>The batch size should be larger than the number of GPUs used locally. It should also be an integer multiple of the number of GPUs so that each chunk is the same size (so that each GPU processes the same number of samples).</p> <p>See also: Basics and Use nn.DataParallel instead of multiprocessing. The same constraints on input as in <code>torch.nn.DataParallel</code> apply.</p> <p>Creation of this class requires that <code>torch.distributed</code> to be already initialized, by calling <code>torch.distributed.init_process_group()</code></p> <p><code>DistributedDataParallel</code> can be used in the following two ways:</p> <ol> <li>Single-Process Multi-GPU</li> </ol> <p>In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it's running. To use <code>DistributedDataParallel</code> in this way, you can simply construct the model as the following:</p> <pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend=\"nccl\")\n&gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices be default\n\n</code></pre> <ol> <li>Multi-Process Single-GPU</li> </ol> <p>This is the highly recommended way to use <code>DistributedDataParallel</code>, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than <code>torch.nn.DataParallel</code> for single-node multi-GPU data parallel training.</p> <p>Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process invidually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling:</p> <pre><code>&gt;&gt;&gt; torch.cuda.set_device(i)\n\n</code></pre> <p>where i is from 0 to N-1. In each process, you should refer the following to construct this module:</p> <pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n&gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)\n\n</code></pre> <p>In order to spawn up multiple processes per node, you can use either <code>torch.distributed.launch</code> or <code>torch.multiprocessing.spawn</code></p> <p>Note</p> <p><code>nccl</code> backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training</p> <p>Warning</p> <p>This module works only with the <code>gloo</code> and <code>nccl</code> backends.</p> <p>Warning</p> <p>Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.</p> <p>Warning</p> <p>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.</p> <p>Warning</p> <p>This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other wise, it is users' responsibility to ensure that each distributed process has the exact same model and thus the exact parameter registeration order.</p> <p>Warning</p> <p>This module assumes all buffers and gradients are dense.</p> <p>Warning</p> <p>This module doesn't work with <code>torch.autograd.grad()</code> (i.e. it will only work if gradients are to be accumulated in <code>.grad</code> attributes of parameters).</p> <p>Warning</p> <p>If you plan on using this module with a <code>nccl</code> backend or a <code>gloo</code> backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to <code>forkserver</code> (Python 3 only) or <code>spawn</code>. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don't change this setting.</p> <p>Warning</p> <p>Forward and backward hooks defined on <code>module</code> and its submodules won't be invoked anymore, unless the hooks are initialized in the <code>forward()</code> method.</p> <p>Warning</p> <p>You should never try to change your model's parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model's parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters' gradient reduction functions might not get called.</p> <p>Note</p> <p>Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.</p> <p>Parameters: </p> <ul> <li>module (Module) \u2013 module to be parallelized</li> <li>device_ids (list of python:int or torch.device) \u2013 CUDA devices (default: all devices)</li> <li>output_device (int or torch.device) \u2013 device location of output (default: device_ids[0])</li> <li>broadcast_buffers (bool) \u2013 flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True)</li> <li>process_group \u2013 the process group to be used for distributed data all-reduction. If None, the default process group, which is created by <code>torch.distributed.init_process_group</code>, will be used. (default: None)</li> <li>bucket_cap_mb \u2013 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25)</li> <li>check_reduction \u2013 when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration's backward reductions were successfully issued at the beginning of every iteration's forward function. You normally don't need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is corrected used. (default: False)</li> </ul> Variables: module (Module) \u2013 the module to be parallelized <pre><code>Example::\n</code></pre> <pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n&gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)\n\n</code></pre>"},{"location":"1.0/nn/#distributeddataparallelcpu","title":"DistributedDataParallelCPU","text":"<pre><code>class torch.nn.parallel.DistributedDataParallelCPU(module)\n</code></pre> <p>Implements distributed data parallelism for CPU at the module level.</p> <p>This module supports the <code>mpi</code> and <code>gloo</code> backends.</p> <p>This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.</p> <p>This module could be used in conjunction with the DistributedSampler, (see :class <code>torch.utils.data.distributed.DistributedSampler</code>) which will load a subset of the original datset for each node with the same batch size. So strong scaling should be configured like this:</p> <p>n = 1, batch size = 12</p> <p>n = 2, batch size = 64</p> <p>n = 4, batch size = 32</p> <p>n = 8, batch size = 16</p> <p>Creation of this class requires the distributed package to be already initialized in the process group mode (see <code>torch.distributed.init_process_group()</code>).</p> <p>Warning</p> <p>Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different node might be executing different code.</p> <p>Warning</p> <p>This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later.</p> <p>Warning</p> <p>This module assumes all gradients are dense.</p> <p>Warning</p> <p>This module doesn't work with <code>torch.autograd.grad()</code> (i.e. it will only work if gradients are to be accumulated in <code>.grad</code> attributes of parameters).</p> <p>Warning</p> <p>Forward and backward hooks defined on <code>module</code> and its submodules won't be invoked anymore, unless the hooks are initialized in the <code>forward()</code> method.</p> <p>Note</p> <p>Parameters are broadcast between nodes in the init() function. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all nodes in the same way.</p> Parameters: module \u2013 module to be parallelized <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.distributed.init_process_group(world_size=4, init_method='...')\n&gt;&gt;&gt; net = torch.nn.DistributedDataParallelCPU(model)\n\n</code></pre>"},{"location":"1.0/nn/#utilities","title":"Utilities","text":""},{"location":"1.0/nn/#clip_grad_norm_","title":"clip_grad_norm_","text":"<pre><code>torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2)\n</code></pre> <p>Clips gradient norm of an iterable of parameters.</p> <p>The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.</p> <p>Parameters: </p> <ul> <li>parameters (Iterable__[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized</li> <li>max_norm (float or int) \u2013 max norm of the gradients</li> <li>norm_type (float or int) \u2013 type of the used p-norm. Can be <code>'inf'</code> for infinity norm.</li> </ul> Returns: Total norm of the parameters (viewed as a single vector)."},{"location":"1.0/nn/#clip_grad_value_","title":"clip_grad_value_","text":"<pre><code>torch.nn.utils.clip_grad_value_(parameters, clip_value)\n</code></pre> <p>Clips gradient of an iterable of parameters at specified value.</p> <p>Gradients are modified in-place.</p> <p>Parameters: </p> <ul> <li>parameters (Iterable__[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized</li> <li>clip_value (float or int) \u2013 maximum allowed value of the gradients The gradients are clipped in the range [-clip_value, clip_value]</li> </ul>"},{"location":"1.0/nn/#parameters_to_vector","title":"parameters_to_vector","text":"<pre><code>torch.nn.utils.parameters_to_vector(parameters)\n</code></pre> <p>Convert parameters to one vector</p> Parameters: parameters (Iterable__[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model. Returns: The parameters represented by a single vector --- ---"},{"location":"1.0/nn/#vector_to_parameters","title":"vector_to_parameters","text":"<pre><code>torch.nn.utils.vector_to_parameters(vec, parameters)\n</code></pre> <p>Convert one vector to the parameters</p> <p>Parameters: </p> <ul> <li>vec (Tensor) \u2013 a single vector represents the parameters of a model.</li> <li>parameters (Iterable__[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.</li> </ul>"},{"location":"1.0/nn/#weight_norm","title":"weight_norm","text":"<pre><code>torch.nn.utils.weight_norm(module, name='weight', dim=0)\n</code></pre> <p>Applies weight normalization to a parameter in the given module.</p> <p></p> <p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by <code>name</code> (e.g. \u201cweight\u201d) with two parameters: one specifying the magnitude (e.g. \u201cweight_g\u201d) and one specifying the direction (e.g. \u201cweight_v\u201d). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every <code>forward()</code> call.</p> <p>By default, with <code>dim=0</code>, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use <code>dim=None</code>.</p> <p>See https://arxiv.org/abs/1602.07868</p> <p>Parameters: </p> <ul> <li>module (nn.Module) \u2013 containing module</li> <li>name (str, optional) \u2013 name of weight parameter</li> <li>dim (int, optional) \u2013 dimension over which to compute the norm</li> </ul> Returns: The original module with the weight norm hook <p>Example:</p> <pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')\nLinear (20 -&gt; 40)\n&gt;&gt;&gt; m.weight_g.size()\ntorch.Size([40, 1])\n&gt;&gt;&gt; m.weight_v.size()\ntorch.Size([40, 20])\n\n</code></pre>"},{"location":"1.0/nn/#remove_weight_norm","title":"remove_weight_norm","text":"<pre><code>torch.nn.utils.remove_weight_norm(module, name='weight')\n</code></pre> <p>Removes the weight normalization reparameterization from a module.</p> <p>Parameters: </p> <ul> <li>module (nn.Module) \u2013 containing module</li> <li>name (str, optional) \u2013 name of weight parameter</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40))\n&gt;&gt;&gt; remove_weight_norm(m)\n\n</code></pre>"},{"location":"1.0/nn/#spectral_norm","title":"spectral_norm","text":"<pre><code>torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None)\n</code></pre> <p>Applies spectral normalization to a parameter in the given module.</p> <p></p> <p>Spectral normalization stabilizes the training of discriminators (critics) in Generaive Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm  of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every <code>forward()</code> call.</p> <p>See Spectral Normalization for Generative Adversarial Networks .</p> <p>Parameters: </p> <ul> <li>module (nn.Module) \u2013 containing module</li> <li>name (str, optional) \u2013 name of weight parameter</li> <li>n_power_iterations (int, optional) \u2013 number of power iterations to calculate spectal norm</li> <li>eps (float, optional) \u2013 epsilon for numerical stability in calculating norms</li> <li>dim (int, optional) \u2013 dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose1/2/3d, when it is 1</li> </ul> Returns: The original module with the spectal norm hook <p>Example:</p> <pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40))\nLinear (20 -&gt; 40)\n&gt;&gt;&gt; m.weight_u.size()\ntorch.Size([20])\n\n</code></pre>"},{"location":"1.0/nn/#remove_spectral_norm","title":"remove_spectral_norm","text":"<pre><code>torch.nn.utils.remove_spectral_norm(module, name='weight')\n</code></pre> <p>Removes the spectral normalization reparameterization from a module.</p> <p>Parameters: </p> <ul> <li>module (nn.Module) \u2013 containing module</li> <li>name (str, optional) \u2013 name of weight parameter</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; m = spectral_norm(nn.Linear(40, 10))\n&gt;&gt;&gt; remove_spectral_norm(m)\n\n</code></pre>"},{"location":"1.0/nn/#packedsequence","title":"PackedSequence","text":"<pre><code>torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None)\n</code></pre> <p>Holds the data and list of <code>batch_sizes</code> of a packed sequence.</p> <p>All RNN modules accept packed sequences as inputs.</p> <p>Note</p> <p>Instances of this class should never be created manually. They are meant to be instantiated by functions like <code>pack_padded_sequence()</code>.</p> <p>Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to <code>pack_padded_sequence()</code>. For instance, given data <code>abc</code> and <code>x</code> the <code>PackedSequence</code> would contain data <code>axbc</code> with <code>batch_sizes=[2,1,1]</code>.</p> <p>| Variables: | </p> <ul> <li>data (Tensor) \u2013 Tensor containing packed sequence</li> <li>batch_sizes (Tensor) \u2013 Tensor of integers holding information about the batch size at each sequence step</li> </ul>"},{"location":"1.0/nn/#pack_padded_sequence","title":"pack_padded_sequence","text":"<pre><code>torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)\n</code></pre> <p>Packs a Tensor containing padded sequences of variable length.</p> <p>Input can be of size <code>T x B x *</code> where <code>T</code> is the length of the longest sequence (equal to <code>lengths[0]</code>), <code>B</code> is the batch size, and <code>*</code> is any number of dimensions (including 0). If <code>batch_first</code> is True <code>B x T x *</code> inputs are expected.</p> <p>The sequences should be sorted by length in a decreasing order, i.e. <code>input[:,0]</code> should be the longest sequence, and <code>input[:,B-1]</code> the shortest one.</p> <p>Note</p> <p>This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a <code>PackedSequence</code> object by accessing its <code>.data</code> attribute.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 padded batch of variable length sequences.</li> <li>lengths (Tensor) \u2013 list of sequences lengths of each batch element.</li> <li>batch_first (bool, optional) \u2013 if <code>True</code>, the input is expected in <code>B x T x *</code> format.</li> </ul> Returns: a <code>PackedSequence</code> object"},{"location":"1.0/nn/#pad_packed_sequence","title":"pad_packed_sequence","text":"<pre><code>torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)\n</code></pre> <p>Pads a packed batch of variable length sequences.</p> <p>It is an inverse operation to <code>pack_padded_sequence()</code>.</p> <p>The returned Tensor's data will be of size <code>T x B x *</code>, where <code>T</code> is the length of the longest sequence and <code>B</code> is the batch size. If <code>batch_first</code> is True, the data will be transposed into <code>B x T x *</code> format.</p> <p>Batch elements will be ordered decreasingly by their length.</p> <p>Note</p> <p><code>total_length</code> is useful to implement the <code>pack sequence -&amp;gt; recurrent network -&amp;gt; unpack sequence</code> pattern in a <code>Module</code> wrapped in <code>DataParallel</code>. See this FAQ section for details.</p> <p>Parameters: </p> <ul> <li>sequence (PackedSequence) \u2013 batch to pad</li> <li>batch_first (bool, optional) \u2013 if <code>True</code>, the output will be in <code>B x T x *</code> format.</li> <li>padding_value (float, optional) \u2013 values for padded elements.</li> <li>total_length (int, optional) \u2013 if not <code>None</code>, the output will be padded to have length <code>total_length</code>. This method will throw <code>ValueError</code> if <code>total_length</code> is less than the max sequence length in <code>sequence</code>.</li> </ul> Returns: Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch."},{"location":"1.0/nn/#pad_sequence","title":"pad_sequence","text":"<pre><code>torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0)\n</code></pre> <p>Pad a list of variable length Tensors with zero</p> <p><code>pad_sequence</code> stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size <code>L x *</code> and if batch_first is False, and <code>T x B x *</code> otherwise.</p> <p><code>B</code> is batch size. It is equal to the number of elements in <code>sequences</code>. <code>T</code> is length of the longest sequence. <code>L</code> is length of the sequence. <code>*</code> is any number of trailing dimensions, including none.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence\n&gt;&gt;&gt; a = torch.ones(25, 300)\n&gt;&gt;&gt; b = torch.ones(22, 300)\n&gt;&gt;&gt; c = torch.ones(15, 300)\n&gt;&gt;&gt; pad_sequence([a, b, c]).size()\ntorch.Size([25, 3, 300])\n\n</code></pre> <p>Note</p> <p>This function returns a Tensor of size <code>T x B x *</code> or <code>B x T x *</code> where <code>T</code> is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.</p> <p>Parameters: </p> <ul> <li>sequences (list[Tensor]) \u2013 list of variable length sequences.</li> <li>batch_first (bool, optional) \u2013 output will be in <code>B x T x *</code> if True, or in <code>T x B x *</code> otherwise</li> <li>padding_value (float, optional) \u2013 value for padded elements. Default: 0.</li> </ul> Returns: Tensor of size <code>T x B x *</code> if <code>batch_first</code> is <code>False</code>. Tensor of size <code>B x T x *</code> otherwise"},{"location":"1.0/nn/#pack_sequence","title":"pack_sequence","text":"<pre><code>torch.nn.utils.rnn.pack_sequence(sequences)\n</code></pre> <p>Packs a list of variable length Tensors</p> <p><code>sequences</code> should be a list of Tensors of size <code>L x *</code>, where <code>L</code> is the length of a sequence and <code>*</code> is any number of trailing dimensions, including zero. They should be sorted in the order of decreasing length.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence\n&gt;&gt;&gt; a = torch.tensor([1,2,3])\n&gt;&gt;&gt; b = torch.tensor([4,5])\n&gt;&gt;&gt; c = torch.tensor([6])\n&gt;&gt;&gt; pack_sequence([a, b, c])\nPackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))\n\n</code></pre> Parameters: sequences (list[Tensor]) \u2013 A list of sequences of decreasing length. Returns: a <code>PackedSequence</code> object --- ---"},{"location":"1.0/nn_functional/","title":"torch.nn.functional","text":"<p>\u8bd1\u8005\uff1ahijkzzz</p>"},{"location":"1.0/nn_functional/#_1","title":"\u5377\u79ef\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#conv1d","title":"conv1d","text":"<pre><code>torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u4e00\u7ef4\u5377\u79ef. </p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1<code>Conv1d</code>. </p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013 \u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c: <code>None</code> *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4<code>(sW,)</code>. \u9ed8\u8ba4\u503c: 1 *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4<code>(padW, )</code>. \u9ed8\u8ba4\u503c:  0 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4<code>(dW,)</code>. \u9ed8\u8ba4\u503c:  1 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; filters = torch.randn(33, 16, 3)\n&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)\n&gt;&gt;&gt; F.conv1d(inputs, filters)\n</code></pre>"},{"location":"1.0/nn_functional/#conv2d","title":"conv2d","text":"<pre><code>torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u56fe\u50cf\u5e94\u7528\u4e8c\u7ef4\u5377\u79ef.</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1<code>Conv2d</code>.</p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013 \u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c:  <code>None</code> *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(sH, sW)</code>. \u9ed8\u8ba4\u503c:  1 *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(padH, padW)</code>. \u9ed8\u8ba4\u503c:  0 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4 <code>(dH, dW)</code>. \u9ed8\u8ba4\u503c:  1 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; filters = torch.randn(8,4,3,3)\n&gt;&gt;&gt; inputs = torch.randn(1,4,5,5)\n&gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)\n</code></pre>"},{"location":"1.0/nn_functional/#conv3d","title":"conv3d","text":"<pre><code>torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u56fe\u50cf\u5e94\u7528\u4e09\u7ef4\u5377\u79ef.</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>Conv3d</code>.</p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013 \u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c:  None *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(sT, sH, sW)</code>. \u9ed8\u8ba4\u503c:  1 *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(padT, padH, padW)</code>. \u9ed8\u8ba4\u503c:  0 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4 <code>(dT, dH, dW)</code>. \u9ed8\u8ba4\u503c:  1 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)\n&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)\n&gt;&gt;&gt; F.conv3d(inputs, filters)\n</code></pre>"},{"location":"1.0/nn_functional/#conv_transpose1d","title":"conv_transpose1d","text":"<pre><code>torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u5e94\u7528\u4e00\u7ef4\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c, \u6709\u65f6\u4e5f\u79f0\u4e3a\u53cd\u5377\u79ef. </p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>ConvTranspose1d</code> </p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013 \u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c:  None *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(sW,)</code>. \u9ed8\u8ba4\u503c:  1 *   padding \u2013 \u8f93\u5165\u4e2d\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e24\u8fb9\u90fd\u5c06\u6dfb\u52a0\u96f6\u586b\u5145<code>kernel_size - 1 - padding</code>. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(padW,)</code>. \u9ed8\u8ba4\u503c:  0 *   output_padding \u2013 \u6dfb\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\u4e2d\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e00\u4fa7\u7684\u989d\u5916\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(out_padW)</code>. \u9ed8\u8ba4\u503c:  0 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4 <code>(dW,)</code>. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)\n&gt;&gt;&gt; weights = torch.randn(16, 33, 5)\n&gt;&gt;&gt; F.conv_transpose1d(inputs, weights)\n</code></pre>"},{"location":"1.0/nn_functional/#conv_transpose2d","title":"conv_transpose2d","text":"<pre><code>torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u56fe\u50cf\u5e94\u7528\u4e8c\u7ef4\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c, \u6709\u65f6\u4e5f\u79f0\u4e3a\u53cd\u5377\u79ef.</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>ConvTranspose2d</code>.</p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013\u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c:  None *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(sH, sW)</code>. \u9ed8\u8ba4\u503c:  1 *   padding \u2013 \u8f93\u5165\u4e2d\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e24\u8fb9\u90fd\u5c06\u6dfb\u52a0\u96f6\u586b\u5145<code>kernel_size - 1 - padding</code>. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(padH, padW)</code>. \u9ed8\u8ba4\u503c:  0 *   output_padding \u2013 \u6dfb\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\u4e2d\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e00\u4fa7\u7684\u989d\u5916\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(out_padH, out_padW)</code>. \u9ed8\u8ba4\u503c:  0 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4 <code>(dH, dW)</code>. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)\n&gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)\n&gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)\n</code></pre>"},{"location":"1.0/nn_functional/#conv_transpose3d","title":"conv_transpose3d","text":"<pre><code>torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u56fe\u50cf\u5e94\u7528\u4e00\u4e2a\u4e09\u7ef4\u8f6c\u7f6e\u5377\u79ef\u64cd\u4f5c, \u6709\u65f6\u4e5f\u79f0\u4e3a\u53cd\u5377\u79ef</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>ConvTranspose3d</code>.</p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   weight \u2013 \u5377\u79ef\u6838, \u5f62\u72b6\u4e3a  *   bias \u2013\u53ef\u9009\u7684\u504f\u7f6e, \u5f62\u72b6\u4e3a . \u9ed8\u8ba4\u503c:  None *   stride \u2013 \u5377\u79ef\u6838\u7684\u6b65\u5e45, \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(sT, sH, sW)</code>. \u9ed8\u8ba4\u503c:  1 *   padding \u2013 \u8f93\u5165\u4e2d\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e24\u8fb9\u90fd\u5c06\u6dfb\u52a0\u96f6\u586b\u5145<code>kernel_size - 1 - padding</code>. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(padT, padH, padW)</code>. \u9ed8\u8ba4\u503c:  0 *   output_padding \u2013 \u6dfb\u52a0\u5230\u8f93\u51fa\u5f62\u72b6\u4e2d\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u4e00\u4fa7\u7684\u989d\u5916\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(out_padT, out_padH, out_padW)</code>. \u9ed8\u8ba4\u503c:  0 *   groups \u2013 \u5c06\u8f93\u5165\u5206\u7ec4,  \u5e94\u8be5\u53ef\u4ee5\u88ab\u7ec4\u7684\u6570\u76ee\u6574\u9664. \u9ed8\u8ba4\u503c:  1 *   dilation \u2013 \u6838\u5143\u7d20\u4e4b\u95f4\u7684\u7a7a\u6d1e. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5355\u5143\u7d20\u5143\u7ec4 <code>(dT, dH, dW)</code>. \u9ed8\u8ba4\u503c:  1</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)\n&gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)\n&gt;&gt;&gt; F.conv_transpose3d(inputs, weights)\n</code></pre>"},{"location":"1.0/nn_functional/#unfold","title":"unfold","text":"<pre><code>torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)\n</code></pre> <p>\u4ece\u6279\u91cf\u7684\u8f93\u5165\u5f20\u91cf\u4e2d\u63d0\u53d6\u6ed1\u52a8\u5c40\u90e8\u5757.</p> <p>\u8b66\u544a</p> <p>\u76ee\u524d, \u4ec5\u652f\u6301\u56db\u7ef4(4D\uff09\u7684\u8f93\u5165\u5f20\u91cf(\u6279\u91cf\u7684\u7c7b\u4f3c\u56fe\u50cf\u7684\u5f20\u91cf).</p> <p>\u7ec6\u8282\u8bf7\u53c2\u9605 <code>torch.nn.Unfold</code></p>"},{"location":"1.0/nn_functional/#fold","title":"fold","text":"<pre><code>torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)\n</code></pre> <p>\u5c06\u4e00\u7ec4\u6ed1\u52a8\u5c40\u90e8\u5757\u6570\u7ec4\u5408\u6210\u4e00\u4e2a\u5927\u7684\u5f20\u91cf.</p> <p>\u8b66\u544a</p> <p>\u76ee\u524d, \u4ec5\u652f\u6301\u56db\u7ef4(4D\uff09\u7684\u8f93\u5165\u5f20\u91cf(\u6279\u91cf\u7684\u7c7b\u4f3c\u56fe\u50cf\u7684\u5f20\u91cf).</p> <p>\u7ec6\u8282\u8bf7\u53c2\u9605 <code>torch.nn.Fold</code> </p>"},{"location":"1.0/nn_functional/#_2","title":"\u6c60\u5316\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#avg_pool1d","title":"avg_pool1d","text":"<pre><code>torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u2192 Tensor\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u5e94\u7528\u4e00\u7ef4\u5e73\u5747\u6c60\u5316.</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>AvgPool1d</code>.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a  *   kernel_size \u2013 \u7a97\u53e3\u7684\u5927\u5c0f. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4  *   stride \u2013 \u7a97\u6237\u7684\u6b65\u5e45. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u5143\u7ec4 <code>(sW,)</code>. \u9ed8\u8ba4\u503c:  <code>kernel_size</code> *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(padW,)</code>. \u9ed8\u8ba4\u503c:  0 *   ceil_mode \u2013 \u5982\u679c <code>True</code>, \u5c06\u7528 <code>ceil</code> \u4ee3\u66ff <code>floor</code>\u8ba1\u7b97\u8f93\u51fa\u5f62\u72b6. \u9ed8\u8ba4\u503c:  <code>False</code> *   count_include_pad \u2013 \u5982\u679c <code>True</code>, \u5c06\u5728\u5e73\u5747\u8ba1\u7b97\u4e2d\u5305\u62ec\u96f6\u586b\u5145. \u9ed8\u8ba4\u503c:  <code>True</code></p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; input = torch.tensor([[[1,2,3,4,5,6,7]]])\n&gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)\ntensor([[[ 2.,  4.,  6.]]])\n</code></pre>"},{"location":"1.0/nn_functional/#avg_pool2d","title":"avg_pool2d","text":"<pre><code>torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u2192 Tensor\n</code></pre> <p>\u5728 \u533a\u57df\u5e94\u7528\u4e8c\u7ef4\u5e73\u5747\u6c60\u5316, \u6b65\u5e45\u4e3a  . \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u7b49\u4e8e\u8f93\u5165\u5e73\u9762\u7684\u6570\u91cf.</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1 <code>AvgPool2d</code>.</p> <p>\u53c2\u6570: *   input \u2013 input tensor  *   kernel_size \u2013 \u6c60\u5316\u533a\u57df\u7684\u5927\u5c0f, \u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216\u8005\u5143\u7ec4  *   stride \u2013 \u6c60\u5316\u6b65\u5e45, \u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216\u8005\u5143\u7ec4 <code>(sH, sW)</code>. \u9ed8\u8ba4\u503c:  <code>kernel_size</code> *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(padH, padW)</code>. \u9ed8\u8ba4\u503c:  0 *   ceil_mode \u2013 \u5982\u679c <code>True</code>, \u5c06\u7528 <code>ceil</code> \u4ee3\u66ff <code>floor</code>\u8ba1\u7b97\u8f93\u51fa\u5f62\u72b6. \u9ed8\u8ba4\u503c:  <code>False</code> *   count_include_pad \u2013 \u5982\u679c <code>True</code>, \u5c06\u5728\u5e73\u5747\u8ba1\u7b97\u4e2d\u5305\u62ec\u96f6\u586b\u5145. \u9ed8\u8ba4\u503c:  <code>True</code></p>"},{"location":"1.0/nn_functional/#avg_pool3d","title":"avg_pool3d","text":"<pre><code>torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u2192 Tensor\n</code></pre> <p>\u5e94 \u533a\u57df\u5e94\u7528\u4e09\u7ef4\u5e73\u5747\u6c60\u5316, \u6b65\u5e45\u4e3a  . \u8f93\u51fa\u7279\u5f81\u7684\u6570\u91cf\u7b49\u4e8e .</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\u548c\u8f93\u51fa\u5f62\u72b6, \u8bf7\u53c2\u89c1  <code>AvgPool3d</code>.</p> <p>\u53c2\u6570: *   input \u2013 \u8f93\u5165\u5f20\u91cf  *   kernel_size \u2013 \u6c60\u5316\u533a\u57df\u7684\u5927\u5c0f, \u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216\u8005\u5143\u7ec4  *   stride \u2013 \u6c60\u5316\u6b65\u5e45, \u53ef\u4ee5\u662f\u4e00\u4e2a\u6570\u5b57\u6216\u8005\u5143\u7ec4 <code>(sT, sH, sW)</code>. \u9ed8\u8ba4\u503c:  <code>kernel_size</code> *   padding \u2013 \u5728\u8f93\u5165\u7684\u4e24\u8fb9\u9690\u5f0f\u52a0\u96f6. \u53ef\u4ee5\u662f\u5355\u4e2a\u6570\u5b57\u6216\u4e00\u4e2a\u5143\u7d20\u5143\u7ec4 <code>(padT, padH, padW)</code>, \u9ed8\u8ba4\u503c:  0 *   ceil_mode \u2013 \u5982\u679c <code>True</code>, \u5c06\u7528 <code>ceil</code> \u4ee3\u66ff <code>floor</code>\u8ba1\u7b97\u8f93\u51fa\u5f62\u72b6. \u9ed8\u8ba4\u503c:  <code>False</code> *   count_include_pad \u2013 \u5982\u679c <code>True</code>, \u5c06\u5728\u5e73\u5747\u8ba1\u7b97\u4e2d\u5305\u62ec\u96f6\u586b\u5145. \u9ed8\u8ba4\u503c:  <code>True</code></p>"},{"location":"1.0/nn_functional/#max_pool1d","title":"max_pool1d","text":"<pre><code>torch.nn.functional.max_pool1d(*args, **kwargs)\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u5e94\u7528\u4e00\u7ef4\u6700\u5927\u6c60\u5316.</p> <p>\u8be6\u60c5\u89c1 <code>MaxPool1d</code>.</p>"},{"location":"1.0/nn_functional/#max_pool2d","title":"max_pool2d","text":"<pre><code>torch.nn.functional.max_pool2d(*args, **kwargs)\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u5e94\u7528\u4e8c\u7ef4\u6700\u5927\u6c60\u5316.</p> <p>\u8be6\u60c5\u89c1 <code>MaxPool2d</code>.</p>"},{"location":"1.0/nn_functional/#max_pool3d","title":"max_pool3d","text":"<pre><code>torch.nn.functional.max_pool3d(*args, **kwargs)\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e09\u7ef4\u6700\u5927\u6c60\u5316.</p> <p>\u8be6\u60c5\u89c1 <code>MaxPool3d</code>.</p>"},{"location":"1.0/nn_functional/#max_unpool1d","title":"max_unpool1d","text":"<pre><code>torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)\n</code></pre> <p>\u8ba1\u7b97<code>MaxPool1d</code>\u7684\u504f\u9006.</p> <p>\u8bf7\u53c2\u89c1 <code>MaxUnpool1d</code>.</p>"},{"location":"1.0/nn_functional/#max_unpool2d","title":"max_unpool2d","text":"<pre><code>torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)\n</code></pre> <p>\u8ba1\u7b97<code>MaxPool2d</code>\u7684\u504f\u9006.</p> <p>\u8be6\u60c5\u89c1 <code>MaxUnpool2d</code>.</p>"},{"location":"1.0/nn_functional/#max_unpool3d","title":"max_unpool3d","text":"<pre><code>torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)\n</code></pre> <p>\u8ba1\u7b97\u7684<code>MaxPool3d</code>\u504f\u9006.</p> <p>\u8be6\u60c5\u89c1 <code>MaxUnpool3d</code>.</p>"},{"location":"1.0/nn_functional/#lp_pool1d","title":"lp_pool1d","text":"<pre><code>torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e00\u7ef4\u5e42\u5e73\u5747\u6c60\u5316. \u5982\u679c\u6240\u6709\u8f93\u5165\u7684p\u6b21\u65b9\u7684\u548c\u4e3a\u96f6, \u68af\u5ea6\u4e5f\u4e3a\u96f6. </p> <p>\u8be6\u60c5\u89c1 <code>LPPool1d</code>.</p>"},{"location":"1.0/nn_functional/#lp_pool2d","title":"lp_pool2d","text":"<pre><code>torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e8c\u7ef4\u5e42\u5e73\u5747\u6c60\u5316. \u5982\u679c\u6240\u6709\u8f93\u5165\u7684p\u6b21\u65b9\u7684\u548c\u4e3a\u96f6, \u68af\u5ea6\u4e5f\u4e3a\u96f6. </p> <p>\u8be6\u60c5\u89c1 <code>LPPool2d</code>.</p>"},{"location":"1.0/nn_functional/#adaptive_max_pool1d","title":"adaptive_max_pool1d","text":"<pre><code>torch.nn.functional.adaptive_max_pool1d(*args, **kwargs)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e00\u7ef4\u81ea\u9002\u5e94\u6700\u5927\u6c60\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveMaxPool1d</code>\u548c\u8f93\u51fa\u5f62\u72b6.</p> <p>\u53c2\u6570: *   output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5927\u5c0f(\u5355\u4e2a\u6574\u6570) *   return_indices \u2013 \u662f\u5426\u8fd4\u56de\u6c60\u5316\u7d22\u5f15. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#adaptive_max_pool2d","title":"adaptive_max_pool2d","text":"<pre><code>torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e8c\u7ef4\u81ea\u9002\u5e94\u6700\u5927\u6c60.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveMaxPool2d</code> \u548c\u8f93\u51fa\u5f62\u72b6.</p> <p>\u53c2\u6570: *   output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5927\u5c0f(\u5355\u4e2a\u6574\u6570 \u6216\u8005 \u53cc\u6574\u6570\u5143\u7ec4) *   return_indices \u2013 \u662f\u5426\u8fd4\u56de\u6c60\u5316\u7d22\u5f15. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#adaptive_max_pool3d","title":"adaptive_max_pool3d","text":"<pre><code>torch.nn.functional.adaptive_max_pool3d(*args, **kwargs)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e09\u7ef4\u81ea\u9002\u5e94\u6700\u5927\u6c60.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveMaxPool3d</code>\u548c\u8f93\u51fa\u5f62\u72b6.</p> <p>\u53c2\u6570: *   output_size \u2013 \u76ee\u6807\u8f93\u51fa\u7684\u5927\u5c0f(\u5355\u4e2a\u6574\u6570 \u6216\u8005 \u4e09\u6574\u6570\u5143\u7ec4) *   return_indices \u2013 \u662f\u5426\u8fd4\u56de\u6c60\u5316\u7d22\u5f15. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#adaptive_avg_pool1d","title":"adaptive_avg_pool1d","text":"<pre><code>torch.nn.functional.adaptive_avg_pool1d(input, output_size) \u2192 Tensor\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e00\u7ef4\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveAvgPool1d</code>  \u4e86\u89e3\u8be6\u60c5\u548c\u8f93\u51fa\u7684\u5f62\u72b6.</p> <p>\u53c2\u6570: * output_size \u2013 \u8f93\u51fa\u76ee\u6807\u5927\u5c0f(\u5355\u4e2a\u6574\u6570) </p>"},{"location":"1.0/nn_functional/#adaptive_avg_pool2d","title":"adaptive_avg_pool2d","text":"<pre><code>torch.nn.functional.adaptive_avg_pool2d(input, output_size)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e8c\u7ef4\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveAvgPool2d</code>  \u4e86\u89e3\u8be6\u60c5\u548c\u8f93\u51fa\u7684\u5f62\u72b6.</p> <p>\u53c2\u6570: * output_size \u2013 \u8f93\u51fa\u76ee\u6807\u5927\u5c0f(\u5355\u4e2a\u6574\u6570 \u6216\u8005 \u53cc\u6574\u6570\u5143\u7ec4) </p>"},{"location":"1.0/nn_functional/#adaptive_avg_pool3d","title":"adaptive_avg_pool3d","text":"<pre><code>torch.nn.functional.adaptive_avg_pool3d(input, output_size)\n</code></pre> <p>\u5728\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u4e0a\u5e94\u7528\u4e09\u7ef4\u81ea\u9002\u5e94\u5e73\u5747\u6c60\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>AdaptiveAvgPool3d</code>  \u4e86\u89e3\u8be6\u60c5\u548c\u8f93\u51fa\u7684\u5f62\u72b6.</p> <p>\u53c2\u6570: * output_size \u2013 \u8f93\u51fa\u76ee\u6807\u5927\u5c0f(\u5355\u4e2a\u6574\u6570 \u6216\u8005 \u4e09\u6574\u6570\u5143\u7ec4) </p>"},{"location":"1.0/nn_functional/#_3","title":"\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#threshold","title":"threshold","text":"<pre><code>torch.nn.functional.threshold(input, threshold, value, inplace=False)\n</code></pre> <p>\u4e3a\u8f93\u5165\u5143\u7d20\u7684\u6bcf\u4e2a\u5143\u7d20\u8bbe\u7f6e\u9608\u503c.</p> <p>\u8bf7\u53c2\u89c1 <code>Threshold</code>.</p> <pre><code>torch.nn.functional.threshold_(input, threshold, value) \u2192 Tensor\n</code></pre> <p>\u5c31\u5730\u7248\u7684 <code>threshold()</code>.</p>"},{"location":"1.0/nn_functional/#relu","title":"relu","text":"<pre><code>torch.nn.functional.relu(input, inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528\u6574\u6d41\u7ebf\u6027\u5355\u5143\u51fd\u6570. \u8bf7\u53c2\u89c1 <code>ReLU</code>.</p> <pre><code>torch.nn.functional.relu_(input) \u2192 Tensor\n</code></pre> <p>\u5c31\u5730\u7248\u7684 <code>relu()</code>.</p>"},{"location":"1.0/nn_functional/#hardtanh","title":"hardtanh","text":"<pre><code>torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528hardtanh\u51fd\u6570. \u8bf7\u53c2\u89c1 <code>Hardtanh</code>.</p> <pre><code>torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u7684 <code>hardtanh()</code>.</p>"},{"location":"1.0/nn_functional/#relu6","title":"relu6","text":"<pre><code>torch.nn.functional.relu6(input, inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528\u51fd\u6570 .</p> <p>\u8bf7\u53c2\u89c1 <code>ReLU6</code>.</p>"},{"location":"1.0/nn_functional/#elu","title":"elu","text":"<pre><code>torch.nn.functional.elu(input, alpha=1.0, inplace=False)\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 .</p> <p>\u8bf7\u53c2\u89c1 <code>ELU</code>.</p> <pre><code>torch.nn.functional.elu_(input, alpha=1.) \u2192 Tensor\n</code></pre> <p>\u5c31\u5730\u7248\u7684 <code>elu()</code>.</p>"},{"location":"1.0/nn_functional/#selu","title":"selu","text":"<pre><code>torch.nn.functional.selu(input, inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 , \u5176\u4e2d \u5e76\u4e14 .</p> <p>\u8bf7\u53c2\u89c1 <code>SELU</code>.</p>"},{"location":"1.0/nn_functional/#celu","title":"celu","text":"<pre><code>torch.nn.functional.celu(input, alpha=1., inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 .</p> <p>\u8bf7\u53c2\u89c1 <code>CELU</code>.</p>"},{"location":"1.0/nn_functional/#leaky_relu","title":"leaky_relu","text":"<pre><code>torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 </p> <p>\u8bf7\u53c2\u89c1 <code>LeakyReLU</code>.</p> <pre><code>torch.nn.functional.leaky_relu_(input, negative_slope=0.01) \u2192 Tensor\n</code></pre> <p>\u5c31\u5730\u7248\u7684 <code>leaky_relu()</code>.</p>"},{"location":"1.0/nn_functional/#prelu","title":"prelu","text":"<pre><code>torch.nn.functional.prelu(input, weight) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528\u51fd\u6570  \u5176\u4e2d\uff0c\u6743\u91cd\u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570.</p> <p>\u8bf7\u53c2\u89c1 <code>PReLU</code>.</p>"},{"location":"1.0/nn_functional/#rrelu","title":"rrelu","text":"<pre><code>torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) \u2192 Tensor\n</code></pre> <p>\u968f\u673a\u7684 leaky ReLU.</p> <p>\u8bf7\u53c2\u89c1 <code>RReLU</code>.</p> <pre><code>torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) \u2192 Tensor\n</code></pre> <p>\u5c31\u5730\u7248\u7684 <code>rrelu()</code>.</p>"},{"location":"1.0/nn_functional/#glu","title":"glu","text":"<pre><code>torch.nn.functional.glu(input, dim=-1) \u2192 Tensor\n</code></pre> <p>\u95e8\u63a7\u7ebf\u6027\u5355\u5143. \u8ba1\u7b97: </p> <p>\u5176\u4e2d<code>inpuy</code>\u6cbf<code>dim</code>\u5206\u6210\u4e24\u534a, \u5f62\u6210<code>A</code>\u548c<code>B</code>. </p> <p>\u89c1 Language Modeling with Gated Convolutional Networks.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165\u5f20\u91cf *   dim (int) \u2013 \u7528\u4e8e\u5206\u5272\u8f93\u5165\u7684\u7ef4\u5ea6</p>"},{"location":"1.0/nn_functional/#logsigmoid","title":"logsigmoid","text":"<pre><code>torch.nn.functional.logsigmoid(input) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 </p> <p>\u8bf7\u53c2\u89c1 <code>LogSigmoid</code>.</p>"},{"location":"1.0/nn_functional/#hardshrink","title":"hardshrink","text":"<pre><code>torch.nn.functional.hardshrink(input, lambd=0.5) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528hardshrink\u51fd\u6570</p> <p>\u8bf7\u53c2\u89c1 <code>Hardshrink</code>.</p>"},{"location":"1.0/nn_functional/#tanhshrink","title":"tanhshrink","text":"<pre><code>torch.nn.functional.tanhshrink(input) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528, </p> <p>\u8bf7\u53c2\u89c1 <code>Tanhshrink</code>.</p>"},{"location":"1.0/nn_functional/#softsign","title":"softsign","text":"<pre><code>torch.nn.functional.softsign(input) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528, the function </p> <p>\u8bf7\u53c2\u89c1 <code>Softsign</code>.</p>"},{"location":"1.0/nn_functional/#softplus","title":"softplus","text":"<pre><code>torch.nn.functional.softplus(input, beta=1, threshold=20) \u2192 Tensor\n</code></pre>"},{"location":"1.0/nn_functional/#softmin","title":"softmin","text":"<pre><code>torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)\n</code></pre> <p>\u5e94\u7528 softmin \u51fd\u6570.</p> <p>\u6ce8\u610f . \u6570\u5b66\u516c\u5f0f\u89c1softmax\u5b9a\u4e49</p> <p>\u8bf7\u53c2\u89c1 <code>Softmin</code>.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165 *   dim (int) \u2013 \u8ba1\u7b97softmin\u7684\u7ef4\u5ea6(\u56e0\u6b64dim\u4e0a\u6bcf\u4e2a\u5207\u7247\u7684\u548c\u4e3a1). *   dtype (<code>torch.dtype</code>, \u53ef\u9009\u7684) \u2013 \u8fd4\u56detenosr\u7684\u671f\u671b\u6570\u636e\u7c7b\u578b.</p> <p>\u5982\u679c\u6307\u5b9a\u4e86\u53c2\u6570, \u8f93\u5165\u5f20\u91cf\u5728\u6267\u884c::param\u64cd\u4f5c\u4e4b\u524d\u88ab\u8f6c\u6362\u4e3a<code>dtype</code>. \u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u975e\u5e38\u6709\u7528. \u9ed8\u8ba4\u503c:  None.</p>"},{"location":"1.0/nn_functional/#softmax","title":"softmax","text":"<pre><code>torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n</code></pre> <p>\u5e94\u7528 softmax \u51fd\u6570.</p> <p>Softmax\u5b9a\u4e49\u4e3a: </p> <p>\u5b83\u5e94\u7528\u4e8edim\u4e0a\u7684\u6240\u6709\u5207\u7247, \u5e76\u5c06\u5bf9\u5b83\u4eec\u8fdb\u884c\u91cd\u65b0\u7f29\u653e, \u4f7f\u5143\u7d20\u4f4d\u4e8e<code>(0,1)</code>\u8303\u56f4\u5185, \u548c\u4e3a1.</p> <p>\u8bf7\u53c2\u89c1 <code>Softmax</code>.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165 *   dim (int) \u2013 \u5c06\u8ba1\u7b97softmax\u7684\u7ef4\u5ea6. *   dtype (<code>torch.dtype</code>, \u53ef\u9009\u7684) \u2013 \u8fd4\u56detenosr\u7684\u671f\u671b\u6570\u636e\u7c7b\u578b.</p> <p>:\u5982\u679c\u6307\u5b9a\u4e86\u53c2\u6570, \u8f93\u5165\u5f20\u91cf\u5728\u6267\u884c::param\u64cd\u4f5c\u4e4b\u524d\u88ab\u8f6c\u6362\u4e3a<code>dtype</code>. \u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u975e\u5e38\u6709\u7528. \u9ed8\u8ba4\u503c:  None.</p> <p>\u6ce8\u610f</p> <p>\u8fd9\u4e2a\u51fd\u6570\u4e0d\u80fd\u76f4\u63a5\u5904\u7406NLLLoss, NLLLoss\u8981\u6c42\u65e5\u5fd7\u5728Softmax\u548c\u5b83\u81ea\u5df1\u4e4b\u95f4\u8ba1\u7b97. \u4f7f\u7528log_softmax\u6765\u4ee3\u66ff(\u5b83\u66f4\u5feb\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u6570\u503c\u5c5e\u6027).</p>"},{"location":"1.0/nn_functional/#softshrink","title":"softshrink","text":"<pre><code>torch.nn.functional.softshrink(input, lambd=0.5) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 soft shrinkage \u51fd\u6570</p> <p>\u8bf7\u53c2\u89c1 <code>Softshrink</code>.</p>"},{"location":"1.0/nn_functional/#gumbel_softmax","title":"gumbel_softmax","text":"<pre><code>torch.nn.functional.gumbel_softmax(logits, tau=1.0, hard=False, eps=1e-10)\n</code></pre> <p>\u91c7\u6837\u81eaGumbel-Softmax\u5206\u5e03, \u5e76\u53ef\u9009\u62e9\u79bb\u6563\u5316.</p> <p>\u53c2\u6570: *   logits \u2013 <code>[batch_size, num_features]</code> \u975e\u89c4\u8303\u5316\u5bf9\u6570\u6982\u7387 *   tau \u2013 \u975e\u8d1f\u7684\u5bf9\u6297\u5f3a\u5ea6 *   hard \u2013 \u5982\u679c <code>True</code>, \u8fd4\u56de\u7684\u6837\u672c\u5c06\u4f1a\u79bb\u6563\u4e3a one-hot \u5411\u91cf, \u4f46\u5c06\u4f1a\u662f\u53ef\u5fae\u5206\u7684\uff0c\u5c31\u50cf\u662f\u5728\u81ea\u52a8\u6c42\u5bfc\u7684soft\u6837\u672c\u4e00\u6837</p> <p>\u8fd4\u56de\u503c: * \u4ece Gumbel-Softmax \u5206\u5e03\u91c7\u6837\u7684 tensor, \u5f62\u72b6\u4e3a <code>batch_size x num_features</code> . \u5982\u679c <code>hard=True</code>, \u8fd4\u56de\u503c\u662f one-hot \u7f16\u7801, \u5426\u5219, \u5b83\u4eec\u5c31\u662f\u7279\u5f81\u548c\u4e3a1\u7684\u6982\u7387\u5206\u5e03 </p> <p>\u7ea6\u675f: *   \u76ee\u524d\u4ec5\u652f\u6301\u4e8c\u7ef4\u7684 <code>logits</code> \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a <code>batch_size x num_features</code></p> <p>\u57fa\u4e8e https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb , (MIT license)</p>"},{"location":"1.0/nn_functional/#log_softmax","title":"log_softmax","text":"<pre><code>torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)\n</code></pre> <p>\u5e94\u7528 softmax \u548c\u5bf9\u6570\u8fd0\u7b97.</p> <p>\u867d\u7136\u5728\u6570\u5b66\u4e0a\u7b49\u4ef7\u4e8elog(softmax(x)), \u4f46\u5206\u5f00\u6267\u884c\u8fd9\u4e24\u4e2a\u64cd\u4f5c\u6bd4\u8f83\u6162, \u800c\u4e14\u5728\u6570\u503c\u4e0a\u4e0d\u7a33\u5b9a. \u8fd9\u4e2a\u51fd\u6570\u4f7f\u7528\u53e6\u4e00\u79cd\u516c\u5f0f\u6765\u6b63\u786e\u8ba1\u7b97\u8f93\u51fa\u548c\u68af\u5ea6.</p> <p>\u8bf7\u53c2\u89c1 <code>LogSoftmax</code>.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165 *   dim (int) \u2013 \u8ba1\u7b97log_softmax\u7684\u7ef4\u5ea6. *   dtype (<code>torch.dtype</code>, \u53ef\u9009\u7684) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u671f\u671b\u6570\u636e\u7c7b\u578b.</p> <p>:\u5982\u679c\u6307\u5b9a\u4e86\u53c2\u6570, \u8f93\u5165\u5f20\u91cf\u5728\u6267\u884c::param\u64cd\u4f5c\u4e4b\u524d\u88ab\u8f6c\u6362\u4e3a<code>dtype</code>. \u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u975e\u5e38\u6709\u7528. \u9ed8\u8ba4\u503c:  None.</p>"},{"location":"1.0/nn_functional/#tanh","title":"tanh","text":"<pre><code>torch.nn.functional.tanh(input) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528 </p> <p>\u8bf7\u53c2\u89c1 <code>Tanh</code>.</p>"},{"location":"1.0/nn_functional/#sigmoid","title":"sigmoid","text":"<pre><code>torch.nn.functional.sigmoid(input) \u2192 Tensor\n</code></pre> <p>\u9010\u5143\u7d20\u5e94\u7528\u51fd\u6570 </p> <p>\u8bf7\u53c2\u89c1 <code>Sigmoid</code>.</p>"},{"location":"1.0/nn_functional/#_4","title":"\u89c4\u8303\u5316\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#batch_norm","title":"batch_norm","text":"<pre><code>torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)\n</code></pre> <p>\u5bf9\u4e00\u6279\u6570\u636e\u4e2d\u7684\u6bcf\u4e2a\u901a\u9053\u5e94\u7528\u6279\u91cf\u6807\u51c6\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>BatchNorm1d</code>, <code>BatchNorm2d</code>, <code>BatchNorm3d</code>.</p>"},{"location":"1.0/nn_functional/#instance_norm","title":"instance_norm","text":"<pre><code>torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)\n</code></pre> <p>\u5bf9\u6279\u4e2d\u6bcf\u4e2a\u6570\u636e\u6837\u672c\u4e2d\u7684\u6bcf\u4e2a\u901a\u9053\u5e94\u7528\u5b9e\u4f8b\u89c4\u8303\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>InstanceNorm1d</code>, <code>InstanceNorm2d</code>, <code>InstanceNorm3d</code>.</p>"},{"location":"1.0/nn_functional/#layer_norm","title":"layer_norm","text":"<pre><code>torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)\n</code></pre> <p>\u5bf9\u6700\u540e\u7279\u5b9a\u6570\u91cf\u7684\u7ef4\u5ea6\u5e94\u7528layer\u89c4\u8303\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>LayerNorm</code>.</p>"},{"location":"1.0/nn_functional/#local_response_norm","title":"local_response_norm","text":"<pre><code>torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)\n</code></pre> <p>\u5bf9\u7531\u591a\u4e2a\u8f93\u5165\u5e73\u9762\u7ec4\u6210\u7684\u8f93\u5165\u4fe1\u53f7\u8fdb\u884c\u5c40\u90e8\u54cd\u5e94\u5f52\u4e00\u5316, \u5176\u4e2d\u901a\u9053\u5360\u636e\u7b2c\u4e8c\u7ef4. \u8de8\u901a\u9053\u5e94\u7528\u6807\u51c6\u5316.</p> <p>\u8bf7\u53c2\u89c1 <code>LocalResponseNorm</code>.</p>"},{"location":"1.0/nn_functional/#normalize","title":"normalize","text":"<pre><code>torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)\n</code></pre> <p>\u5bf9\u6307\u5b9a\u7ef4\u5ea6\u6267\u884c  \u89c4\u8303\u5316.</p> <p>\u5bf9\u4e8e\u4e00\u4e2a\u5c3a\u5bf8\u4e3a \u7684\u8f93\u5165\u5f20\u91cf, \u6bcf\u4e00  -\u5143\u7d20\u5411\u91cf \u6cbf\u7740\u7ef4\u5ea6 <code>dim</code> \u88ab\u8f6c\u6362\u4e3a </p> <p>\u5bf9\u4e8e\u9ed8\u8ba4\u53c2\u6570, \u5b83\u4f7f\u7528\u6cbf\u7ef4\u5ea6\u7684\u6b27\u51e0\u91cc\u5f97\u8303\u6570\u8fdb\u884c\u6807\u51c6\u5316.</p> <p>\u53c2\u6570: *   input \u2013 \u4efb\u610f\u5f62\u72b6\u7684\u8f93\u5165\u5f20\u91cf *   p (float) \u2013 \u8303\u6570\u516c\u5f0f\u4e2d\u7684\u6307\u6570\u503c. \u9ed8\u8ba4\u503c:  2 *   dim (int) \u2013 \u8fdb\u884c\u89c4\u7ea6\u7684\u7ef4\u5ea6. \u9ed8\u8ba4\u503c:  1 *   eps (float) \u2013 \u907f\u514d\u9664\u4ee5\u96f6\u7684\u5c0f\u503c. \u9ed8\u8ba4\u503c:  1e-12 *   out (Tensor, \u53ef\u9009\u7684) \u2013 \u8f93\u51fa\u5f20\u91cf. \u5982\u679c <code>out</code> \u88ab\u8bbe\u7f6e, \u6b64\u64cd\u4f5c\u4e0d\u53ef\u5fae\u5206.</p>"},{"location":"1.0/nn_functional/#_5","title":"\u7ebf\u6027\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#linear","title":"linear","text":"<pre><code>torch.nn.functional.linear(input, weight, bias=None)\n</code></pre> <p>\u5bf9\u4f20\u5165\u6570\u636e\u5e94\u7528\u7ebf\u6027\u8f6c\u6362: .</p> <p>\u5f62\u72b6:</p> <ul> <li>Input:  <code>*</code> \u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u9644\u52a0\u7ef4\u5ea6</li> <li>Weight: </li> <li>Bias: </li> <li>Output: </li> </ul>"},{"location":"1.0/nn_functional/#bilinear","title":"bilinear","text":"<pre><code>torch.nn.functional.bilinear(input1, input2, weight, bias=None)\n</code></pre>"},{"location":"1.0/nn_functional/#dropout","title":"Dropout \u51fd\u6570","text":""},{"location":"1.0/nn_functional/#dropout_1","title":"dropout","text":"<pre><code>torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n</code></pre> <p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d, \u4f7f\u7528\u4f2f\u52aa\u5229\u5206\u5e03\u7684\u6837\u672c, \u968f\u673a\u5730\u7528\u6982\u7387<code>p</code>\u5c06\u8f93\u5165\u5f20\u91cf\u7684\u4e00\u4e9b\u5143\u7d20\u5f52\u96f6.</p> <p>\u8bf7\u53c2\u89c1 <code>Dropout</code>.</p> <p>\u53c2\u6570: *   p \u2013 \u6e05\u96f6\u6982\u7387. \u9ed8\u8ba4\u503c:  0.5 *   training \u2013 \u5982\u679c <code>True</code> \u4f7f\u7528 dropout. \u9ed8\u8ba4\u503c:  <code>True</code> *   inplace \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a <code>True</code>, \u5c06\u4f1a\u539f\u5730\u64cd\u4f5c. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#alpha_dropout","title":"alpha_dropout","text":"<pre><code>torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)\n</code></pre> <p>\u5bf9\u8f93\u5165\u5e94\u7528 alpha dropout.</p> <p>\u8bf7\u53c2\u89c1 <code>AlphaDropout</code>.</p>"},{"location":"1.0/nn_functional/#dropout2d","title":"dropout2d","text":"<pre><code>torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)\n</code></pre> <p>\u968f\u673a\u5f52\u96f6\u8f93\u5165\u5f20\u91cf\u7684\u6574\u4e2a\u901a\u9053 (\u4e00\u4e2a\u901a\u9053\u662f\u4e00\u4e2a\u4e8c\u7ef4\u7279\u5f81\u56fe, \u4f8b\u5982, \u5728\u6279\u91cf\u8f93\u5165\u4e2d\u7b2cj\u4e2a\u901a\u9053\u7684\u7b2ci\u4e2a\u6837\u672c\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u7684\u8f93\u5165[i,j]). \u6bcf\u6b21\u524d\u5411\u4f20\u9012\u65f6, \u6bcf\u4e2a\u4fe1\u9053\u90fd\u5c06\u88ab\u72ec\u7acb\u6e05\u96f6. \u7528\u6982\u7387 <code>p</code> \u4ece Bernoulli \u5206\u5e03\u91c7\u6837.</p> <p>\u8bf7\u53c2\u89c1 <code>Dropout2d</code>.</p> <p>\u53c2\u6570: *   p \u2013 \u901a\u9053\u6e05\u96f6\u7684\u6982\u7387. \u9ed8\u8ba4\u503c:  0.5 *   training \u2013 \u4f7f\u7528 dropout \u5982\u679c\u8bbe\u4e3a <code>True</code>. \u9ed8\u8ba4\u503c:  <code>True</code> *   inplace \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a <code>True</code>, \u5c06\u4f1a\u505a\u539f\u5730\u64cd\u4f5c. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#dropout3d","title":"dropout3d","text":"<pre><code>torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)\n</code></pre> <p>\u968f\u673a\u5f52\u96f6\u8f93\u5165\u5f20\u91cf\u7684\u6574\u4e2a\u901a\u9053 (\u4e00\u4e2a\u901a\u9053\u662f\u4e00\u4e2a\u4e09\u7ef4\u7279\u5f81\u56fe, \u4f8b\u5982, \u5728\u6279\u91cf\u8f93\u5165\u4e2d\u7b2cj\u4e2a\u901a\u9053\u7684\u7b2ci\u4e2a\u6837\u672c\u662f\u4e00\u4e2a\u4e09\u7ef4\u5f20\u91cf\u7684\u8f93\u5165[i,j]). \u6bcf\u6b21\u524d\u5411\u4f20\u9012\u65f6, \u6bcf\u4e2a\u4fe1\u9053\u90fd\u5c06\u88ab\u72ec\u7acb\u6e05\u96f6. \u7528\u6982\u7387 <code>p</code> \u4ece Bernoulli \u5206\u5e03\u91c7\u6837.</p> <p>\u8bf7\u53c2\u89c1 <code>Dropout3d</code>.</p> <p>\u53c2\u6570: *   p \u2013 \u901a\u9053\u6e05\u96f6\u7684\u6982\u7387. \u9ed8\u8ba4\u503c:  0.5 *   training \u2013 \u4f7f\u7528 dropout \u5982\u679c\u8bbe\u4e3a <code>True</code>. \u9ed8\u8ba4\u503c:  <code>True</code> *   inplace \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3a <code>True</code>, \u5c06\u4f1a\u505a\u539f\u5730\u64cd\u4f5c. \u9ed8\u8ba4\u503c:  <code>False</code></p>"},{"location":"1.0/nn_functional/#_6","title":"\u7a00\u758f\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#embedding","title":"embedding","text":"<pre><code>torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)\n</code></pre> <p>\u4e00\u4e2a\u7b80\u5355\u7684\u67e5\u627e\u8868, \u67e5\u627e\u56fa\u5b9a\u5b57\u5178\u4e2d\u7684embedding(\u5d4c\u5165)\u5185\u5bb9\u548c\u5927\u5c0f.</p> <p>\u8fd9\u4e2a\u6a21\u5757\u901a\u5e38\u7528\u4e8e\u4f7f\u7528\u7d22\u5f15\u68c0\u7d22\u5355\u8bcd\u5d4c\u5165. \u6a21\u5757\u7684\u8f93\u5165\u662f\u7d22\u5f15\u5217\u8868\u548c\u5d4c\u5165\u77e9\u9635, \u8f93\u51fa\u662f\u76f8\u5e94\u7684\u5355\u8bcd\u5d4c\u5165.</p> <p>\u8bf7\u53c2\u89c1 <code>torch.nn.Embedding</code>.</p> <p>\u53c2\u6570: *   input (LongTensor) \u2013  \u5305\u542b\u5d4c\u5165\u77e9\u9635\u4e2d\u7684\u7d22\u5f15\u7684\u5f20\u91cf *   weight (Tensor) \u2013 \u5d4c\u5165\u77e9\u9635\u7684\u884c\u6570\u7b49\u4e8e\u53ef\u80fd\u7684\u6700\u5927\u7d22\u5f15\u6570+ 1, \u5217\u6570\u7b49\u4e8e\u5d4c\u5165\u5927\u5c0f *   padding_idx (int, \u53ef\u9009\u7684) \u2013  \u5982\u679c\u7ed9\u5b9a, \u6bcf\u5f53\u9047\u5230\u7d22\u5f15\u65f6, \u5728<code>padding_idx</code> (\u521d\u59cb\u5316\u4e3a\u96f6)\u7528\u5d4c\u5165\u5411\u91cf\u586b\u5145\u8f93\u51fa. *   max_norm (float, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u7ed9\u5b9a, \u5219\u5c06\u8303\u6570\u5927\u4e8e<code>max_norm</code>\u7684\u6bcf\u4e2a\u5d4c\u5165\u5411\u91cf\u91cd\u65b0\u89c4\u8303\u5316, \u5f97\u5230\u8303\u6570<code>max_norm</code>. \u6ce8\u610f:\u8fd9\u5c06\u4fee\u6539\u9002\u5f53\u7684<code>weight</code>. *   norm_type (float, \u53ef\u9009\u7684) \u2013 \u7528\u4e8e\u8ba1\u7b97<code>max_norm</code>\u9009\u9879\u7684p\u8303\u6570\u7684p. \u9ed8\u8ba4 <code>2</code>. *   scale_grad_by_freq (boolean__, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u7ed9\u5b9a, \u8fd9\u5c06\u901a\u8fc7\u5c0f\u6279\u5904\u7406\u4e2d\u5355\u8bcd\u9891\u7387\u7684\u5012\u6570\u6765\u7f29\u653e\u68af\u5ea6. \u9ed8\u8ba4 <code>False</code>. *   sparse (bool, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u503c\u4e3a <code>True</code>, \u68af\u5ea6 w.r.t. <code>weight</code> \u5c06\u4f1a\u662f\u4e00\u4e2a\u7a00\u758f tensor. \u8bf7\u770b <code>torch.nn.Embedding</code>\u6709\u5173\u7a00\u758f\u68af\u5ea6\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f.</p> <p>\u5f62\u72b6:</p> <ul> <li>Input:  \u5305\u542b\u8981\u63d0\u53d6\u7684\u7d22\u5f15\u7684\u4efb\u610f\u5f62\u72b6\u7684\u957f\u5f20\u91cf</li> <li>Weight: \u6d6e\u70b9\u578b\u5d4c\u5165\u77e9\u9635, \u5f62\u72b6\u4e3a (V, embedding_dim),    V = maximum index + 1 \u5e76\u4e14 embedding_dim = the embedding size</li> <li>Output: <code>(*, embedding_dim)</code>,  <code>*</code> \u662f\u8f93\u5165\u5f62\u72b6</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n&gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3\n&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)\n&gt;&gt;&gt; F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,  0.9625,  0.6753],\n [ 0.9666,  0.7761,  0.6108],\n [ 0.6246,  0.9751,  0.3618],\n [ 0.4161,  0.2419,  0.7383]],\n\n [[ 0.6246,  0.9751,  0.3618],\n [ 0.0237,  0.7794,  0.0528],\n [ 0.9666,  0.7761,  0.6108],\n [ 0.3385,  0.8612,  0.1867]]])\n\n&gt;&gt;&gt; # example with padding_idx\n&gt;&gt;&gt; weights = torch.rand(10, 3)\n&gt;&gt;&gt; weights[0, :].zero_()\n&gt;&gt;&gt; embedding_matrix = weights\n&gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])\n&gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n [ 0.5609,  0.5384,  0.8720],\n [ 0.0000,  0.0000,  0.0000],\n [ 0.6262,  0.2438,  0.7471]]])\n</code></pre>"},{"location":"1.0/nn_functional/#embedding_bag","title":"embedding_bag","text":"<pre><code>torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False)\n</code></pre> <p>\u8ba1\u7b97\u5d4c\u5165<code>bags</code>\u7684\u548c\u3001\u5e73\u5747\u503c\u6216\u6700\u5927\u503c, \u800c\u4e0d\u5b9e\u4f8b\u5316\u4e2d\u95f4\u5d4c\u5165.</p> <p>\u8bf7\u53c2\u89c1 <code>torch.nn.EmbeddingBag</code></p> <p>\u53c2\u6570:</p> <ul> <li>input (LongTensor) \u2013 \u5305\u542b\u5d4c\u5165\u77e9\u9635\u7684\u7d22\u5f15\u7684<code>bags</code>\u5f20\u91cf</li> <li>weight (Tensor) \u2013 \u5d4c\u5165\u77e9\u9635\u7684\u884c\u6570\u7b49\u4e8e\u53ef\u80fd\u7684\u6700\u5927\u7d22\u5f15\u6570+ 1, \u5217\u6570\u7b49\u4e8e\u5d4c\u5165\u5927\u5c0f</li> <li>offsets (LongTensor__, \u53ef\u9009\u7684) \u2013 \u4ec5\u5f53<code>input</code>\u4e3a\u4e00\u7ef4\u65f6\u4f7f\u7528. <code>offsets</code>\u786e\u5b9a\u8f93\u5165\u4e2d\u6bcf\u4e2a<code>bag</code>(\u5e8f\u5217)\u7684\u8d77\u59cb\u7d22\u5f15\u4f4d\u7f6e</li> <li>max_norm (float, \u53ef\u9009\u7684) \u2013  \u5982\u679c\u7ed9\u5b9a\u6b64\u53c2\u6570, \u8303\u6570\u5927\u4e8e<code>max_norm</code>\u7684\u6bcf\u4e2a\u5d4c\u5165\u5411\u91cf\u5c06\u88ab\u91cd\u65b0\u89c4\u683c\u5316\u4e3a\u8303\u6570<code>max_norm</code>. \u6ce8\u610f:\u8fd9\u5c06\u5c31\u5730\u4fee\u6539<code>weight</code></li> <li>norm_type (float, \u53ef\u9009\u7684) \u2013 The <code>p</code> in the <code>p</code>-norm to compute for the <code>max_norm</code> option. \u9ed8\u8ba4 <code>2</code>.</li> <li>scale_grad_by_freq (boolean__, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u7ed9\u5b9a\u6b64\u53c2\u6570, \u8fd9\u5c06\u901a\u8fc7\u5c0f\u6279\u5904\u7406\u4e2d\u5355\u8bcd\u9891\u7387\u7684\u5012\u6570\u6765\u7f29\u653e\u68af\u5ea6. \u9ed8\u8ba4\u503c False. \u6ce8\u610f:\u5f53<code>mode=\"max\"</code>\u65f6\u4e0d\u652f\u6301\u6b64\u9009\u9879.</li> <li>mode (string__, \u53ef\u9009\u7684) \u2013 <code>\"sum\"</code>, <code>\"mean\"</code> or <code>\"max\"</code>. \u6307\u5b9a\u51cf\u5c11<code>bag</code>\u7684\u65b9\u6cd5. \u9ed8\u8ba4\u503c: <code>\"mean\"</code></li> <li>sparse (bool, \u53ef\u9009\u7684) \u2013 \u5982\u679c<code>True</code>, \u68af\u5ea6w.r.t.\u6743\u503c\u5c31\u662f\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf.\u8bf7\u53c2\u89c1 <code>torch.nn.Embedding</code> \u5173\u4e8e\u7a00\u758f\u68af\u5ea6. \u6ce8\u610f: \u6b64\u9009\u9879\u4e0d\u652f\u6301 <code>mode=\"max\"</code>.</li> </ul> <p>\u5f62\u72b6:</p> <ul> <li><code>input</code> (LongTensor) \u548c <code>offsets</code> (LongTensor, \u53ef\u9009\u7684)  <ul> <li>\u5982\u679c <code>input</code> \u662f\u4e8c\u7ef4\u7684, \u5f62\u72b6\u4e3a <code>B x N</code>,        \u5b83\u5c06\u88ab\u89c6\u4e3a\u6bcf\u4e2a\u56fa\u5b9a\u957f\u5ea6<code>N</code>\u7684<code>B</code>\u4e2abag(\u5e8f\u5217), \u8fd9\u5c06\u6839\u636e\u6a21\u5f0f\u4ee5\u67d0\u79cd\u65b9\u5f0f\u8fd4\u56de<code>B</code>\u4e2a\u805a\u5408\u503c. \u5728\u672c\u4f8b\u4e2d, <code>offsets</code>\u88ab\u5ffd\u7565, \u5e76\u4e14\u8981\u6c42\u4e3a<code>None</code> </li> <li>\u5982\u679c <code>input</code> \u662f\u4e00\u7ef4\u7684, \u5f62\u72b6\u4e3a <code>N</code>     \u5b83\u5c06\u88ab\u89c6\u4e3a\u591a\u4e2a<code>bag</code>(\u5e8f\u5217)\u7684\u4e32\u8054. <code>offsets</code>\u5fc5\u987b\u662f\u4e00\u4e2a\u4e00\u7ef4tensor, \u5176\u4e2d\u5305\u542b<code>input</code>\u4e2d\u6bcf\u4e2a<code>bag</code>\u7684\u8d77\u59cb\u7d22\u5f15\u4f4d\u7f6e. \u56e0\u6b64, \u5bf9\u4e8e\u5f62\u72b6<code>B</code>\u7684\u504f\u79fb\u91cf, \u8f93\u5165\u5c06\u88ab\u89c6\u4e3a\u6709<code>B</code>\u4e2abag. \u7a7abags( \u5373, \u5177\u67090\u957f\u5ea6)\u5c06\u8fd4\u56de\u75310\u586b\u5145\u7684\u5411\u91cf</li> </ul> </li> <li><code>weight</code> (Tensor): \u6a21\u5757\u7684\u53ef\u5b66\u4e60\u6743\u91cd, \u5f62\u72b6 <code>(num_embeddings x embedding_dim)</code></li> <li><code>output</code>: \u805a\u5408\u7684\u5d4c\u5165\u503c, \u5f62\u72b6 <code>B x embedding_dim</code></li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)\n&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])\n&gt;&gt;&gt; offsets = torch.tensor([0,4])\n&gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)\ntensor([[ 0.3397,  0.3552,  0.5545],\n [ 0.5893,  0.4386,  0.5882]])\n</code></pre>"},{"location":"1.0/nn_functional/#_7","title":"\u8ddd\u79bb\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#pairwise_distance","title":"pairwise_distance","text":"<pre><code>torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>torch.nn.PairwiseDistance</code></p>"},{"location":"1.0/nn_functional/#cosine_similarity","title":"cosine_similarity","text":"<pre><code>torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56dex1\u548cx2\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6, \u6cbfdim\u8ba1\u7b97 </p> <p>\u53c2\u6570: *   x1 (Tensor) \u2013 \u7b2c\u4e00\u4e2a\u8f93\u5165. *   x2 (Tensor) \u2013 \u7b2c\u4e8c\u4e2a\u8f93\u5165(\u5927\u5c0f\u548c x1 \u5339\u914d). *   dim (int, \u53ef\u9009\u7684) \u2013 \u7ef4\u5ea6. \u9ed8\u8ba4\u503c:  1 *   eps (float, \u53ef\u9009\u7684) \u2013 \u975e\u5e38\u5c0f\u7684\u503c\u907f\u514d\u9664\u4ee50. \u9ed8\u8ba4\u503c:  1e-8</p> <p>\u5f62\u72b6: *   Input:  \u5176\u4e2dD\u5728<code>dim</code>\u4f4d\u7f6e. *   Output:  \u5176\u4e2d1\u5728<code>dim</code>\u4f4d\u7f6e.</p> <p>\u4f8b\u5b50: </p> <pre><code>&gt;&gt;&gt; input1 = torch.randn(100, 128)\n&gt;&gt;&gt; input2 = torch.randn(100, 128)\n&gt;&gt;&gt; output = F.cosine_similarity(input1, input2)\n&gt;&gt;&gt; print(output)\n</code></pre>"},{"location":"1.0/nn_functional/#pdist","title":"pdist","text":"<pre><code>torch.nn.functional.pdist(input, p=2) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u8f93\u5165\u4e2d\u6bcf\u5bf9\u884c\u5411\u91cf\u4e4b\u95f4\u7684p\u8303\u6570\u8ddd\u79bb.  \u8fd9\u4e0e<code>torch.norm(input[:, None] - input, dim=2, p=p)</code>\u7684\u4e0a\u4e09\u89d2\u5f62\u90e8\u5206(\u4e0d\u5305\u62ec\u5bf9\u89d2\u7ebf\uff09\u76f8\u540c.  \u5982\u679c\u884c\u662f\u8fde\u7eed\u7684, \u5219\u6b64\u51fd\u6570\u5c06\u66f4\u5feb</p> <p>\u5982\u679c\u8f93\u5165\u5177\u6709\u5f62\u72b6  \u5219\u8f93\u51fa\u5c06\u5177\u6709\u5f62\u72b6 .</p> <p>\u8fd9\u4e2a\u51fd\u6570\u76f8\u5f53\u4e8e <code>scipy.spatial.distance.pdist(input, 'minkowski', p=p)</code> \u5982\u679c . \u5f53  \u5b83\u7b49\u4ef7\u4e8e <code>scipy.spatial.distance.pdist(input, 'hamming') * M</code>. \u5f53 , \u6700\u76f8\u8fd1\u7684scipy\u51fd\u6570\u662f <code>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</code>.</p> <p>\u53c2\u6570:</p> <ul> <li>input \u2013 \u8f93\u5165\u5f20\u91cf, \u5f62\u72b6\u4e3a .</li> <li>p \u2013 \u8ba1\u7b97\u6bcf\u4e2a\u5411\u91cf\u5bf9\u4e4b\u95f4\u7684p\u8303\u6570\u8ddd\u79bb\u7684p\u503c .</li> </ul>"},{"location":"1.0/nn_functional/#_8","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#binary_cross_entropy","title":"binary_cross_entropy","text":"<pre><code>torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u8ba1\u7b97\u76ee\u6807\u548c\u8f93\u51fa\u4e4b\u95f4\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u7684\u51fd\u6570.</p> <p>\u8bf7\u53c2\u89c1 <code>BCELoss</code>.</p> <p>\u53c2\u6570: *   input \u2013 \u4efb\u610f\u5f62\u72b6\u7684\u5f20\u91cf *   target \u2013 \u4e0e\u8f93\u5165\u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf *   weight (Tensor, \u53ef\u9009\u7684) \u2013 \u624b\u52a8\u91cd\u65b0\u8c03\u6574\u6743\u91cd, \u5982\u679c\u63d0\u4f9b, \u5b83\u91cd\u590d\u6765\u5339\u914d\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6 *   size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code> *   reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code> *   reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u6ca1\u6709reduction, 'mean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570\u91cf 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u6c42\u548c.  \u6ce8\u610f\uff1a<code>size_average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aargs\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u5c06\u8986\u76d6reduce.  \u9ed8\u8ba4\u503c\uff1a'mean', \u9ed8\u8ba4\u503c:  'mean'</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)\n&gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)\n&gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"1.0/nn_functional/#binary_cross_entropy_with_logits","title":"binary_cross_entropy_with_logits","text":"<pre><code>torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n</code></pre> <p>\u8ba1\u7b97\u76ee\u6807\u548c\u8f93\u51falogits\u4e4b\u95f4\u7684\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u7684\u51fd\u6570.</p> <p>\u8bf7\u53c2\u89c1 <code>BCEWithLogitsLoss</code>.</p> <p>\u53c2\u6570: *   input \u2013 \u4efb\u610f\u5f62\u72b6\u7684\u5f20\u91cf *   target \u2013 \u4e0e\u8f93\u5165\u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf *   weight (Tensor, \u53ef\u9009\u7684) \u2013 \u624b\u52a8\u91cd\u65b0\u8c03\u6574\u6743\u91cd, \u5982\u679c\u63d0\u4f9b, \u5b83\u91cd\u590d\u6765\u5339\u914d\u8f93\u5165\u5f20\u91cf\u7684\u5f62\u72b6 *   size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code> *   reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code> *   reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u6ca1\u6709reduction, 'mean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570\u91cf 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u6c42\u548c.  \u6ce8\u610f\uff1a<code>size_average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aargs\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u5c06\u8986\u76d6reduce.  \u9ed8\u8ba4\u503c\uff1a'mean', \u9ed8\u8ba4\u503c:  'mean' *   pos_weight (Tensor, \u53ef\u9009\u7684) \u2013 \u6b63\u4f8b\u6837\u672c\u7684\u6743\u91cd. \u5fc5\u987b\u662f\u957f\u5ea6\u7b49\u4e8e\u7c7b\u6570\u7684\u5411\u91cf.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3).random_(2)\n&gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"1.0/nn_functional/#poisson_nll_loss","title":"poisson_nll_loss","text":"<pre><code>torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n</code></pre> <p>\u6cca\u677e\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931.</p> <p>\u8bf7\u53c2\u89c1 <code>PoissonNLLLoss</code>.</p> <p>\u53c2\u6570: *   input \u2013 \u6f5c\u5728\u6cca\u677e\u5206\u5e03\u7684\u671f\u671b. *   target \u2013 \u968f\u673a\u62bd\u6837 . *   log_input \u2013 \u5982\u679c\u4e3a<code>True</code>, \u5219\u635f\u5931\u8ba1\u7b97\u4e3a , \u5982\u679c\u4e3a<code>False</code>, \u5219\u635f\u5931\u8ba1\u7b97\u4e3a . \u9ed8\u8ba4\u503c:  <code>True</code> *   full \u2013 \u662f\u5426\u8ba1\u7b97\u5168\u90e8\u635f\u5931, \u5373. \u52a0\u5165Stirling\u8fd1\u4f3c\u9879. \u9ed8\u8ba4\u503c:  <code>False</code> . *   size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code> *   eps (float, \u53ef\u9009\u7684) \u2013 \u4e00\u4e2a\u5c0f\u503c\u907f\u514d\u6c42\u503c  \u5f53 <code>log_input</code>=<code>False</code>. \u9ed8\u8ba4\u503c:  1e-8 *   reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code> *   reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u6ca1\u6709reduction, 'mean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570\u91cf 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u6c42\u548c.  \u6ce8\u610f\uff1a<code>size_average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aargs\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u5c06\u8986\u76d6reduce.  \u9ed8\u8ba4\u503c\uff1a'mean', \u9ed8\u8ba4\u503c:  'mean'</p>"},{"location":"1.0/nn_functional/#cosine_embedding_loss","title":"cosine_embedding_loss","text":"<pre><code>torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>CosineEmbeddingLoss</code>.</p>"},{"location":"1.0/nn_functional/#cross_entropy","title":"cross_entropy","text":"<pre><code>torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n</code></pre> <p>\u6b64\u51fd\u6570\u7ed3\u5408\u4e86 <code>log_softmax</code> \u548c <code>nll_loss</code>.</p> <p>\u8bf7\u53c2\u89c1 <code>CrossEntropyLoss</code>.</p> <p>\u53c2\u6570:</p> <ul> <li>input (Tensor) \u2013  \u5176\u4e2d <code>C = \u7c7b\u522b\u6570</code> \u6216\u8005\u5728\u4e8c\u7ef4\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u4e3a , \u6216\u8005  \u5f53  \u5728k\u7ef4\u635f\u5931\u7684\u60c5\u51b5\u4e0b</li> <li>target (Tensor) \u2013  \u5176\u4e2d\u6bcf\u4e2a\u503c\u90fd\u5728 \u8303\u56f4\u5185, \u6216\u8005  \u5176\u4e2d  \u5728k\u7ef4\u635f\u5931\u60c5\u51b5\u4e0b.</li> <li>weight (Tensor, \u53ef\u9009\u7684) \u2013 \u7ed9\u6bcf\u4e2a\u7c7b\u522b\u7684\u624b\u52a8\u91cd\u5b9a\u6743\u91cd. \u5982\u679c\u7ed9\u5b9a, \u5fc5\u987b\u662f\u5927\u5c0f\u4e3a<code>C</code>\u7684\u5f20\u91cf</li> <li>size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>ignore_index (int, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u4e00\u4e2a\u88ab\u5ffd\u7565\u7684\u76ee\u6807\u503c\uff0c\u8be5\u76ee\u6807\u503c\u4e0d\u5f71\u54cd\u8f93\u5165\u68af\u5ea6\u3002\u5f53 <code>size_average</code> \u53d6\u503c\u4e3a <code>True</code>, \u635f\u5931\u5e73\u5747\u5728\u4e0d\u53ef\u5ffd\u7565\u7684\u76ee\u6807\u4e0a. \u9ed8\u8ba4\u503c:  -100</li> <li>reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u6ca1\u6709reduction, 'mean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570\u91cf 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u6c42\u548c.  \u6ce8\u610f\uff1a<code>size_average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aargs\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u5c06\u8986\u76d6reduce.  \u9ed8\u8ba4\u503c\uff1a'mean', \u9ed8\u8ba4\u503c:  'mean'</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)\n&gt;&gt;&gt; loss = F.cross_entropy(input, target)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"1.0/nn_functional/#ctc_loss","title":"ctc_loss","text":"<pre><code>torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean')\n</code></pre> <p>\u8054\u7ed3\u4e3b\u4e49\u65f6\u95f4\u5206\u7c7b\u635f\u5931.</p> <p>\u8bf7\u53c2\u89c1 <code>CTCLoss</code>.</p> <p>\u6ce8\u610f</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b, \u5f53\u4f7f\u7528CUDA\u540e\u7aef\u4e0eCuDNN\u65f6, \u8be5\u64cd\u4f5c\u7b26\u53ef\u80fd\u4f1a\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u7b97\u6cd5\u6765\u63d0\u9ad8\u6027\u80fd. \u5982\u679c\u8fd9\u4e0d\u662f\u60a8\u5e0c\u671b\u7684, \u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e<code>torch.backends.cudn .deterministic = True</code>\u6765\u5c1d\u8bd5\u4f7f\u64cd\u4f5c\u5177\u6709\u786e\u5b9a\u6027(\u53ef\u80fd\u4f1a\u4ee5\u6027\u80fd\u4e3a\u4ee3\u4ef7). \u8bf7\u53c2\u9605\u5173\u4e8e Reproducibility \u4e86\u89e3\u80cc\u666f.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p> <p>\u53c2\u6570:</p> <ul> <li>log_probs \u2013  \u5176\u4e2d <code>C = \u5b57\u6bcd\u8868\u4e2d\u5305\u62ec\u7a7a\u683c\u5728\u5185\u7684\u5b57\u7b26\u6570</code>, <code>T = \u8f93\u5165\u957f\u5ea6</code>, and <code>N = \u6279\u6b21\u6570\u91cf</code>. \u8f93\u51fa\u7684\u5bf9\u6570\u6982\u7387(e.g. \u83b7\u5f97\u4e8e<code>torch.nn.functional.log_softmax()</code>).</li> <li>targets \u2013  or <code>(sum(target_lengths))</code>. \u76ee\u6807(\u4e0d\u80fd\u4e3a\u7a7a\uff09. \u5728\u7b2c\u4e8c\u79cd\u5f62\u5f0f\u4e2d\uff0c\u5047\u5b9a\u76ee\u6807\u662f\u4e32\u8054\u7684\u3002</li> <li>input_lengths \u2013 . \u8f93\u5165\u7684\u957f\u5ea6 (\u5fc5\u987b )</li> <li>target_lengths \u2013 . \u76ee\u6807\u7684\u957f\u5ea6</li> <li>blank (int, \u53ef\u9009\u7684) \u2013 \u7a7a\u767d\u7684\u6807\u7b7e. \u9ed8\u8ba4 .</li> <li>reduction (string__, \u53ef\u9009\u7684)  - \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u4e0d\u4f1a\u5e94\u7528<code>reduce</code>, 'mean'\uff1a\u8f93\u51fa\u635f\u5931\u5c06\u9664\u4ee5\u76ee\u6807\u957f\u5ea6, \u7136\u540e\u5f97\u5230\u6279\u6b21\u7684\u5e73\u5747\u503c.  \u9ed8\u8ba4\u503c\uff1a'mean'</li> </ul> <p>\u4f8b\u5b50: </p> <pre><code>&gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n&gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n&gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)\n&gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n&gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"1.0/nn_functional/#hinge_embedding_loss","title":"hinge_embedding_loss","text":"<pre><code>torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>HingeEmbeddingLoss</code>.</p>"},{"location":"1.0/nn_functional/#kl_div","title":"kl_div","text":"<pre><code>torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>Kullback-Leibler divergence \u635f\u5931.</p> <p>\u8bf7\u53c2\u89c1 <code>KLDivLoss</code></p> <p>\u53c2\u6570:</p> <ul> <li>input \u2013 \u4efb\u610f\u5f62\u72b6\u7684\u5f20\u91cf</li> <li>target \u2013 \u548c\u8f93\u5165\u5f62\u72b6\u76f8\u540c\u7684\u5f20\u91cf</li> <li>size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684\u7f29\u51cf\uff1a'none'| 'batchmean'| 'sum'| 'mean'.  'none'\uff1a\u4e0d\u4f1a\u5e94\u7528<code>reduction</code> 'batchmean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5batchsize 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u52a0\u603b 'mean'\uff1a\u8f93\u51fa\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570 \u9ed8\u8ba4\u503c\uff1a'mean'</li> </ul> <p>:param  \u6ce8::<code>size average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aarg\u4e2d\u7684\u4e00\u4e2a\u5c06\u8986\u76d6reduce.  :param  \u6ce8::<code>reduce = mean</code>\u4e0d\u8fd4\u56de\u771f\u5b9e\u7684kl\u6563\u5ea6\u503c, \u8bf7\u4f7f\u7528:<code>reduce = batchmean</code>, \u5b83\u7b26\u5408kl\u7684\u6570\u5b66\u5b9a\u4e49. </p> <p>\u5728\u4e0b\u4e00\u4e2a\u4e3b\u8981\u7248\u672c\u4e2d, \u201cmean\u201d\u5c06\u88ab\u4fee\u6539\u4e3a\u4e0e\u201cbatchmean\u201d\u76f8\u540c.</p>"},{"location":"1.0/nn_functional/#l1_loss","title":"l1_loss","text":"<pre><code>torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8be5\u51fd\u6570\u53d6\u5143\u7d20\u7684\u7edd\u5bf9\u503c\u5dee\u7684\u5e73\u5747\u503c\u3002</p> <p>\u8bf7\u53c2\u89c1 <code>L1Loss</code>.</p>"},{"location":"1.0/nn_functional/#mse_loss","title":"mse_loss","text":"<pre><code>torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u5143\u7d20\u7684\u5747\u65b9\u8bef\u5dee.</p> <p>\u8bf7\u53c2\u89c1 <code>MSELoss</code>.</p>"},{"location":"1.0/nn_functional/#margin_ranking_loss","title":"margin_ranking_loss","text":"<pre><code>torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>MarginRankingLoss</code>.</p>"},{"location":"1.0/nn_functional/#multilabel_margin_loss","title":"multilabel_margin_loss","text":"<pre><code>torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>MultiLabelMarginLoss</code>.</p>"},{"location":"1.0/nn_functional/#multilabel_soft_margin_loss","title":"multilabel_soft_margin_loss","text":"<pre><code>torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>MultiLabelSoftMarginLoss</code>.</p>"},{"location":"1.0/nn_functional/#multi_margin_loss","title":"multi_margin_loss","text":"<pre><code>torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n</code></pre> <pre><code>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None, reduce=None, reduction='mean') -&gt; Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>MultiMarginLoss</code>.</p>"},{"location":"1.0/nn_functional/#nll_loss","title":"nll_loss","text":"<pre><code>torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n</code></pre> <p>\u8d1f\u7684\u5bf9\u6570\u4f3c\u7136\u51fd\u6570.</p> <p>\u8bf7\u53c2\u89c1 <code>NLLLoss</code>.</p> <p>\u53c2\u6570:</p> <ul> <li>input \u2013  <code>C = \u7c7b\u522b\u7684\u6570\u91cf</code> \u6216\u8005  \u5728\u4e8c\u7ef4\u635f\u5931\u7684\u60c5\u51b5\u4e0b, \u6216\u8005   \u5728K\u7ef4\u635f\u5931\u7684\u60c5\u51b5\u4e0b.</li> <li>target \u2013  \u6bcf\u4e2a\u503c\u662f , \u6216\u8005   K\u7ef4\u635f\u5931.</li> <li>weight (Tensor, \u53ef\u9009\u7684) \u2013  \u7ed9\u6bcf\u4e2a\u7c7b\u522b\u7684\u624b\u52a8\u91cd\u5b9a\u6743\u91cd. \u5982\u679c\u7ed9\u5b9a, \u5fc5\u987b\u662f\u5927\u5c0f\u4e3a<code>C</code>\u7684\u5f20\u91cf</li> <li>size_average (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6279\u5904\u7406\u4e2d\u7684\u6bcf\u4e2a\u635f\u5931\u5143\u7d20\u7684\u5e73\u5747\u635f\u5931. \u6ce8\u610f, \u5bf9\u4e8e\u67d0\u4e9b\u635f\u5931, \u6bcf\u4e2a\u6837\u672c\u6709\u591a\u4e2a\u5143\u7d20. \u5982\u679c<code>size_average</code>\u8bbe\u7f6e\u4e3a<code>False</code>, \u5219\u5bf9\u6bcf\u4e2a\u5c0f\u6279\u7684\u635f\u5931\u8fdb\u884c\u6c47\u603b. reduce\u4e3aFalse\u65f6\u5ffd\u7565. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>ignore_index (int, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u4e00\u4e2a\u88ab\u5ffd\u7565\u7684\u76ee\u6807\u503c, \u8be5\u503c\u4e0d\u4f1a\u5f71\u54cd\u8f93\u5165\u68af\u5ea6. \u5f53<code>size_average</code>\u4e3a<code>True</code>\u65f6, \u635f\u8017\u5728\u672a\u5ffd\u7565\u7684\u76ee\u6807\u4e0a\u5e73\u5747. \u9ed8\u8ba4\u503c: -100</li> <li>reduce (bool, \u53ef\u9009\u7684) \u2013 \u5e9f\u5f03\u7684 (\u89c1 <code>reduction</code>). \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u6839\u636esize_average, \u5bf9\u6bcf\u4e2a\u5c0f\u6279\u91cf\u7684\u89c2\u5bdf\u7ed3\u679c\u7684\u635f\u5931\u8fdb\u884c\u5e73\u5747\u6216\u6c42\u548c.  \u5f53reduce\u4e3aFalse\u65f6, \u8fd4\u56de\u6bcf\u6279\u5143\u7d20\u7684\u635f\u5931\u5e76\u5ffd\u7565<code>size_average</code>. \u9ed8\u8ba4\u503c:  <code>True</code></li> <li>reduction (string__, \u53ef\u9009\u7684) \u2013 \u6307\u5b9a\u8981\u5e94\u7528\u4e8e\u8f93\u51fa\u7684<code>reduction</code>\uff1a'none'| 'mean'| 'sum'.  'none'\uff1a\u6ca1\u6709reduction, 'mean'\uff1a\u8f93\u51fa\u7684\u603b\u548c\u5c06\u9664\u4ee5\u8f93\u51fa\u4e2d\u7684\u5143\u7d20\u6570\u91cf 'sum'\uff1a\u8f93\u51fa\u5c06\u88ab\u6c42\u548c.  \u6ce8\u610f\uff1a<code>size_average</code>\u548c<code>reduce</code>\u6b63\u5728\u88ab\u5f03\u7528, \u540c\u65f6, \u6307\u5b9a\u8fd9\u4e24\u4e2aargs\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u90fd\u5c06\u8986\u76d6reduce.  \u9ed8\u8ba4\u503c\uff1a'mean', \u9ed8\u8ba4\u503c:  'mean'</li> </ul> <p>\u4f8b\u5b50: </p> <pre><code>&gt;&gt;&gt; # input is of size N x C = 3 x 5\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C\n&gt;&gt;&gt; target = torch.tensor([1, 0, 4])\n&gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)\n&gt;&gt;&gt; output.backward()\n</code></pre>"},{"location":"1.0/nn_functional/#smooth_l1_loss","title":"smooth_l1_loss","text":"<pre><code>torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u5982\u679c\u7edd\u5bf9\u5143\u7d20\u8bef\u5dee\u4f4e\u4e8e1, \u5219\u4f7f\u7528\u5e73\u65b9\u9879, \u5426\u5219\u4f7f\u7528L1\u9879\u7684\u51fd\u6570.</p> <p>\u8bf7\u53c2\u89c1 <code>SmoothL1Loss</code>.</p>"},{"location":"1.0/nn_functional/#soft_margin_loss","title":"soft_margin_loss","text":"<pre><code>torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>SoftMarginLoss</code>.</p>"},{"location":"1.0/nn_functional/#triplet_margin_loss","title":"triplet_margin_loss","text":"<pre><code>torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>TripletMarginLoss</code></p>"},{"location":"1.0/nn_functional/#_9","title":"\u89c6\u89c9\u51fd\u6570","text":""},{"location":"1.0/nn_functional/#pixel_shuffle","title":"pixel_shuffle","text":"<pre><code>torch.nn.functional.pixel_shuffle()\n</code></pre> <p>\u91cd\u65b0\u6392\u5217\u5f20\u91cf\u4e2d\u7684\u5143\u7d20, \u4ece\u5f62\u72b6  \u5230 .</p> <p>\u8bf7\u53c2\u89c1 <code>PixelShuffle</code>.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165\u5f20\u91cf *   upscale_factor (int) \u2013 \u63d0\u9ad8\u7a7a\u95f4\u89e3\u6790\u5ea6\u7684\u53c2\u6570</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)\n&gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([1, 1, 12, 12])\n</code></pre>"},{"location":"1.0/nn_functional/#pad","title":"pad","text":"<pre><code>torch.nn.functional.pad(input, pad, mode='constant', value=0)\n</code></pre> <p>\u7528\u4e8e\u586b\u5145\u5f20\u91cf.</p> <pre><code>Pading size:\n</code></pre> <p>\u8981\u586b\u5145\u7684\u7ef4\u5ea6\u6570\u4e3a \u586b\u5145\u7684\u7ef4\u5ea6\u4ece\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u5f00\u59cb\u5411\u524d\u79fb\u52a8. \u4f8b\u5982,  \u586b\u5145\u8f93\u5165tensor\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6, \u6240\u4ee5 pad \u5f62\u5982 (padLeft, padRight); \u586b\u5145\u6700\u540e 2 \u4e2a\u7ef4\u5ea6, \u4f7f\u7528 (padLeft, padRight, padTop, padBottom); \u586b\u5145\u6700\u540e 3 \u4e2a\u7ef4\u5ea6, \u4f7f\u7528 (padLeft, padRight, padTop, padBottom, padFront, padBack).</p> <pre><code>Padding mode:\n</code></pre> <p>\u8bf7\u53c2\u89c1 <code>torch.nn.ConstantPad2d</code>, <code>torch.nn.ReflectionPad2d</code>, and <code>torch.nn.ReplicationPad2d</code> \u6709\u5173\u6bcf\u4e2a\u586b\u5145\u6a21\u5f0f\u5982\u4f55\u5de5\u4f5c\u7684\u5177\u4f53\u793a\u4f8b. Constant padding \u5df2\u7ecf\u5b9e\u73b0\u4e8e\u4efb\u610f\u7ef4\u5ea6. \u590d\u5236\u586b\u5145\u7528\u4e8e\u586b\u51455D\u8f93\u5165\u5f20\u91cf\u7684\u6700\u540e3\u4e2a\u7ef4\u5ea6, \u62164D\u8f93\u5165\u5f20\u91cf\u7684\u6700\u540e2\u4e2a\u7ef4\u5ea6, \u62163D\u8f93\u5165\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6. \u53cd\u5c04\u586b\u5145\u4ec5\u7528\u4e8e\u586b\u51454D\u8f93\u5165\u5f20\u91cf\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6, \u6216\u80053D\u8f93\u5165\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p> <p>\u53c2\u6570: *   input (Tensor) \u2013 N\u7ef4\u5f20\u91cf *   pad (tuple) \u2013 m\u4e2a\u5143\u7d20\u7684\u5143\u7ec4, \u5176\u4e2d  \u8f93\u5165\u7ef4\u6570\uff0c\u4e14m\u662f\u5076\u6570 *   mode \u2013 'constant', 'reflect' or 'replicate'. \u9ed8\u8ba4\u503c:  'constant' *   value \u2013 \u7528\u201c\u5e38\u91cf\u201d\u586b\u5145\u6765\u586b\u5145\u503c. \u9ed8\u8ba4\u503c:  0</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)\n&gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side\n&gt;&gt;&gt; out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n&gt;&gt;&gt; print(out.data.size())\ntorch.Size([3, 3, 4, 4])\n&gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n&gt;&gt;&gt; out = F.pad(t4d, p2d, \"constant\", 0)\n&gt;&gt;&gt; print(out.data.size())\ntorch.Size([3, 3, 8, 4])\n&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)\n&gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n&gt;&gt;&gt; out = F.pad(t4d, p3d, \"constant\", 0)\n&gt;&gt;&gt; print(out.data.size())\ntorch.Size([3, 9, 7, 3])\n</code></pre>"},{"location":"1.0/nn_functional/#interpolate","title":"interpolate","text":"<pre><code>torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)\n</code></pre> <p>\u5411\u4e0b/\u5411\u4e0a\u91c7\u6837\u8f93\u5165\u5230\u7ed9\u5b9a\u7684<code>size</code>\u6216\u7ed9\u5b9a\u7684<code>scale_factor</code></p> <p>\u7531 <code>mode</code> \u6307\u5b9a\u63d2\u503c\u7684\u7b97\u6cd5.</p> <p>\u76ee\u524d\u652f\u6301\u65f6\u95f4, \u7a7a\u95f4\u548c\u4f53\u79ef\u4e0a\u91c7\u6837, \u5373\u9884\u671f\u8f93\u5165\u4e3a\u4e09\u7ef4\u3001\u56db\u7ef4\u6216\u4e94\u7ef4\u5f62\u72b6.</p> <p>\u8f93\u5165\u7ef4\u5ea6\u5f62\u5f0f: <code>mini-batch x channels x [\u53ef\u9009\u7684 depth] x [\u53ef\u9009\u7684 height] x width</code>.</p> <p>\u53ef\u7528\u4e8e\u4e0a\u91c7\u6837\u7684\u6a21\u5f0f\u662f: <code>nearest</code>, <code>linear</code> (\u4ec5\u4e09\u7ef4), <code>bilinear</code> (\u4ec5\u56db\u7ef4), <code>trilinear</code> (\u4ec5\u4e94\u7ef4), <code>area</code></p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165\u5f20\u91cf *   size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) \u2013 \u8f93\u51fa\u5c3a\u5bf8. *   scale_factor (float or Tuple__[float]) \u2013  \u7a7a\u95f4\u5927\u5c0f\u7684\u4e58\u6570. \u5982\u679c\u662f\u5143\u7ec4, \u5219\u5fc5\u987b\u5339\u914d\u8f93\u5165\u5927\u5c0f. *   mode (string) \u2013 \u4e0a\u91c7\u6837\u7b97\u6cd5: 'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. \u9ed8\u8ba4\u503c:  'nearest' *   align_corners (bool, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u4e3aTrue, \u5219\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u89d2\u50cf\u7d20\u5bf9\u9f50, \u4ece\u800c\u4fdd\u7559\u8fd9\u4e9b\u50cf\u7d20\u7684\u503c. \u4ec5\u5728 <code>mode</code> \u662f <code>linear</code>, <code>bilinear</code>, \u6216\u8005 <code>trilinear</code> \u65f6\u751f\u6548. \u9ed8\u8ba4\u503c:  False</p> <p>\u8b66\u544a</p> <p><code>align_corners = True</code>\u65f6, \u7ebf\u6027\u63d2\u503c\u6a21\u5f0f(<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>)\u4e0d\u4f1a\u6309\u6bd4\u4f8b\u5bf9\u9f50\u8f93\u51fa\u548c\u8f93\u5165\u50cf\u7d20, \u56e0\u6b64\u8f93\u51fa\u503c\u53ef\u80fd\u53d6\u51b3\u4e8e\u8f93\u5165\u5927\u5c0f. \u8fd9\u662f0.3.1\u7248\u4e4b\u524d\u8fd9\u4e9b\u6a21\u5f0f\u7684\u9ed8\u8ba4\u884c\u4e3a.\u6b64\u540e, \u9ed8\u8ba4\u884c\u4e3a\u4e3a<code>align_corners = False</code>. \u6709\u5173\u8fd9\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684\u5177\u4f53\u793a\u4f8b, \u8bf7\u53c2\u89c1\u4e0a\u4f8b. </p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p>"},{"location":"1.0/nn_functional/#upsample","title":"upsample","text":"<pre><code>torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)\n</code></pre> <p>\u5c06\u8f93\u5165\u91c7\u6837\u5230\u7ed9\u5b9a<code>size</code>\u6216\u7ed9\u5b9a\u7684<code>scale_factor</code></p> <p>\u8b66\u544a</p> <p>\u6b64\u51fd\u6570\u5df2\u88ab\u5f03\u7528, \u53d6\u800c\u4ee3\u4e4b\u7684\u662f <code>torch.nn.functional.interpolate()</code>. \u7b49\u4ef7\u4e8e <code>nn.functional.interpolate(...)</code>.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p> <p>\u7528\u4e8e\u4e0a\u91c7\u6837\u7684\u7b97\u6cd5\u7531 <code>mode</code> \u786e\u5b9a.</p> <p>\u76ee\u524d\u652f\u6301\u65f6\u95f4, \u7a7a\u95f4\u548c\u4f53\u79ef\u4e0a\u91c7\u6837, \u5373\u9884\u671f\u8f93\u5165\u4e3a\u4e09\u7ef4\u3001\u56db\u7ef4\u6216\u4e94\u7ef4\u5f62\u72b6.</p> <p>\u8f93\u5165\u7ef4\u5ea6\u5f62\u5f0f: <code>mini-batch x channels x [\u53ef\u9009\u7684 depth] x [\u53ef\u9009\u7684 height] x width</code>.</p> <p>\u53ef\u7528\u4e8e\u4e0a\u91c7\u6837\u7684\u6a21\u5f0f\u662f: <code>nearest</code>, <code>linear</code> (\u4ec5\u4e09\u7ef4), <code>bilinear</code> (\u4ec5\u56db\u7ef4), <code>trilinear</code> (\u4ec5\u4e94\u7ef4), <code>area</code></p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165\u5f20\u91cf *   size (int or Tuple__[int] or Tuple__[int, int] or Tuple__[int, int, int]) \u2013 \u8f93\u51fa\u5c3a\u5bf8. *   scale_factor (int) \u2013 \u7a7a\u95f4\u5927\u5c0f\u7684\u4e58\u6570. \u5fc5\u987b\u662f\u6574\u6570. *   mode (string) \u2013 \u4e0a\u91c7\u6837\u7b97\u6cd5: 'nearest' | 'linear'| 'bilinear' | 'trilinear'. \u9ed8\u8ba4\u503c:  'nearest' *   align_corners (bool, \u53ef\u9009\u7684) \u2013 \u5982\u679c\u4e3aTrue, \u5219\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\u7684\u89d2\u50cf\u7d20\u5bf9\u9f50, \u4ece\u800c\u4fdd\u7559\u8fd9\u4e9b\u50cf\u7d20\u7684\u503c. \u4ec5\u5728 <code>mode</code> \u662f <code>linear</code>, <code>bilinear</code>, \u6216\u8005 <code>trilinear</code> \u65f6\u751f\u6548. \u9ed8\u8ba4\u503c:  False</p> <p>\u8b66\u544a</p> <p><code>align_corners = True</code>\u65f6, \u7ebf\u6027\u63d2\u503c\u6a21\u5f0f(<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>)\u4e0d\u4f1a\u6309\u6bd4\u4f8b\u5bf9\u9f50\u8f93\u51fa\u548c\u8f93\u5165\u50cf\u7d20, \u56e0\u6b64\u8f93\u51fa\u503c\u53ef\u80fd\u53d6\u51b3\u4e8e\u8f93\u5165\u5927\u5c0f. \u8fd9\u662f0.3.1\u7248\u4e4b\u524d\u8fd9\u4e9b\u6a21\u5f0f\u7684\u9ed8\u8ba4\u884c\u4e3a.\u6b64\u540e, \u9ed8\u8ba4\u884c\u4e3a\u4e3a<code>align_corners = False</code>. \u6709\u5173\u8fd9\u5982\u4f55\u5f71\u54cd\u8f93\u51fa\u7684\u5177\u4f53\u793a\u4f8b, \u8bf7\u53c2\u89c1 <code>Upsample</code> </p>"},{"location":"1.0/nn_functional/#upsample_nearest","title":"upsample_nearest","text":"<pre><code>torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)\n</code></pre> <p>\u4f7f\u7528\u6700\u8fd1\u90bb\u7684\u50cf\u7d20\u503c\u5bf9\u8f93\u5165\u8fdb\u884c\u4e0a\u91c7\u6837.</p> <p>\u8b66\u544a</p> <p>\u4e0d\u63a8\u8350\u4f7f\u7528\u6b64\u51fd\u6570, \u800c\u4f7f\u7528 <code>torch.nn.functional.interpolate()</code>. \u7b49\u4ef7\u4e8eh <code>nn.functional.interpolate(..., mode='nearest')</code>.</p> <p>\u76ee\u524d\u652f\u6301\u7a7a\u95f4\u548c\u4f53\u79ef\u4e0a\u91c7\u6837 (\u5373 inputs \u662f 4 \u6216\u8005 5 \u7ef4\u7684).</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165 *   size (int or Tuple__[int, int] or Tuple__[int, int, int]) \u2013 \u8f93\u51fa\u7a7a\u95f4\u5927\u5c0f. *   scale_factor (int) \u2013 \u7a7a\u95f4\u5927\u5c0f\u4e58\u6cd5\u5668\u3002\u5fc5\u987b\u662f\u6574\u6570\u3002</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p>"},{"location":"1.0/nn_functional/#upsample_bilinear","title":"upsample_bilinear","text":"<pre><code>torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)\n</code></pre> <p>\u4f7f\u7528\u53cc\u7ebf\u6027\u4e0a\u91c7\u6837\u5bf9\u8f93\u5165\u8fdb\u884c\u4e0a\u91c7\u6837.</p> <p>\u8b66\u544a</p> <p>\u4e0d\u63a8\u8350\u4f7f\u7528\u6b64\u51fd\u6570, \u800c\u4f7f\u7528 <code>torch.nn.functional.interpolate()</code>. \u7b49\u4ef7\u4e8e <code>nn.functional.interpolate(..., mode='bilinear', align_corners=True)</code>.</p> <p>\u671f\u671b\u8f93\u5165\u662f\u7a7a\u95f4\u7684 (\u56db\u7ef4). \u7528 <code>upsample_trilinear</code> \u5bf9\u4f53\u79ef (\u4e94\u7ef4) \u8f93\u5165.</p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u8f93\u5165 *   size (int or Tuple__[int, int] or Tuple__[int, int, int]) \u2013 \u8f93\u51fa\u7a7a\u95f4\u5927\u5c0f. *   scale_factor (int) \u2013 \u7a7a\u95f4\u5927\u5c0f\u4e58\u6cd5\u5668\u3002</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p>"},{"location":"1.0/nn_functional/#grid_sample","title":"grid_sample","text":"<pre><code>torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')\n</code></pre> <p>\u7ed9\u5b9a<code>input</code> \u548c\u6d41\u573a <code>grid</code>, \u4f7f\u7528 <code>input</code> \u548c <code>grid</code> \u4e2d\u7684\u50cf\u7d20\u4f4d\u7f6e\u8ba1\u7b97<code>output</code>.</p> <p>\u76ee\u524d, \u4ec5\u652f\u6301 spatial (\u56db\u7ef4) \u548c volumetric (\u4e94\u7ef4) <code>input</code>.</p> <p>\u5728 spatial (4\u56db\u7ef4) \u7684\u60c5\u51b5\u4e0b, \u5bf9\u4e8e <code>input</code> \u5f62\u5982  \u548c <code>grid</code> \u5f62\u5982 , \u8f93\u51fa\u7684\u5f62\u72b6\u4e3a .</p> <p>\u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u51fa\u4f4d\u7f6e <code>output[n, :, h, w]</code>, \u5927\u5c0f\u4e3a2\u7684\u5411\u91cf <code>grid[n, h, w]</code> \u6307\u5b9a <code>input</code> \u7684\u50cf\u7d20\u4f4d\u7f6e <code>x</code> \u548c <code>y</code>, \u7528\u4e8e\u63d2\u503c\u8f93\u51fa\u503c <code>output[n, :, h, w]</code>. \u5bf9\u4e8e 5D \u7684 inputs, <code>grid[n, d, h, w]</code> \u6307\u5b9a <code>x</code>, <code>y</code>, <code>z</code> \u50cf\u7d20\u4f4d\u7f6e\u7528\u4e8e\u63d2\u503c <code>output[n, :, d, h, w]</code>. <code>mode</code> \u53c2\u6570\u6307\u5b9a <code>nearest</code> or <code>bilinear</code> \u63d2\u503c\u65b9\u6cd5.</p> <p><code>grid</code> \u5927\u591a\u6570\u503c\u5e94\u8be5\u5904\u4e8e <code>[-1, 1]</code>.  \u8fd9\u662f\u56e0\u4e3a\u50cf\u7d20\u4f4d\u7f6e\u7531<code>input</code> \u7a7a\u95f4\u7ef4\u5ea6\u6807\u51c6\u5316.\u4f8b\u5982, \u503c <code>x = -1, y = -1</code> \u662f <code>input</code> \u7684\u5de6\u4e0a\u89d2, \u503c <code>x = 1, y = 1</code> \u662f <code>input</code> \u7684\u53f3\u4e0b\u89d2.</p> <p>\u5982\u679c <code>grid</code> \u6709 <code>[-1, 1]</code> \u4e4b\u5916\u7684\u503c, \u90a3\u4e9b\u5750\u6807\u5c06\u7531 <code>padding_mode</code> \u5b9a\u4e49. \u9009\u9879\u5982\u4e0b</p> <ul> <li><code>padding_mode=\"zeros\"</code>: \u7528 <code>0</code> \u4ee3\u66ff\u8fb9\u754c\u5916\u7684\u503c,</li> <li><code>padding_mode=\"border\"</code>: \u7528 border \u503c\u4ee3\u66ff,</li> <li><code>padding_mode=\"reflection\"</code>: \u5bf9\u4e8e\u8d85\u51fa\u8fb9\u754c\u7684\u503c, \u7528\u53cd\u5c04\u7684\u503c. \u5bf9\u4e8e\u8ddd\u79bb\u8fb9\u754c\u8f83\u8fdc\u7684\u4f4d\u7f6e, \u5b83\u4f1a\u4e00\u76f4\u88ab\u53cd\u5c04, \u76f4\u5230\u5230\u8fbe\u8fb9\u754c, \u4f8b\u5982(\u5f52\u4e00\u5316)\u50cf\u7d20\u4f4d\u7f6e<code>x = -3.5</code>\u88ab<code>-1</code>\u53cd\u5c04, \u53d8\u6210<code>x' = 2.5</code>, \u7136\u540e\u88ab\u8fb9\u754c1\u53cd\u5c04, \u53d8\u6210<code>x'' = -0.5</code>.</li> </ul> <p>\u6ce8\u610f</p> <p>\u8be5\u529f\u80fd\u5e38\u7528\u4e8e\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc\u7684\u6784\u5efa.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528CUDA\u540e\u7aef\u65f6, \u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u786e\u5b9a\u7684\u5411\u540e\u884c\u4e3a, \u5e76\u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u53c2\u9605\u5173\u4e8eReproducibility\u7684\u6ce8\u91ca. </p> <p>\u53c2\u6570: *   input (Tensor) \u2013 \u5f62\u72b6\u4e3a \u7684\u8f93\u5165 (\u56db\u7ef4\u60c5\u5f62) \u6216\u5f62\u72b6\u4e3a \u7684\u8f93\u5165(\u4e94\u7ef4\u60c5\u5f62\uff09 *   grid (Tensor) \u2013 \u5f62\u72b6\u4e3a \u7684\u6d41\u573a(\u56db\u7ef4\u60c5\u5f62) \u6216\u8005  (\u4e94\u7ef4\u60c5\u5f62\uff09 *   mode (str) \u2013 \u63d2\u503c\u6a21\u5f0f\u8ba1\u7b97\u8f93\u51fa\u503c'\u53cc\u7ebf\u6027' | '\u6700\u63a5\u8fd1'. \u9ed8\u8ba4\u503c:  'bilinear' *   padding_mode (str) \u2013 \u5916\u90e8\u7f51\u683c\u503c' zeros ' | ' border ' | ' reflection '\u7684\u586b\u5145\u6a21\u5f0f. \u9ed8\u8ba4\u503c:  'zeros'</p> <p>\u8fd4\u56de\u503c: *   \u8f93\u51fa\u5f20\u91cf</p> <p>\u8fd4\u56de\u7c7b\u578b: *   \u8f93\u51fa (Tensor) </p>"},{"location":"1.0/nn_functional/#affine_grid","title":"affine_grid","text":"<pre><code>torch.nn.functional.affine_grid(theta, size)\n</code></pre> <p>\u5728\u7ed9\u5b9a\u4e00\u6279\u4eff\u5c04\u77e9\u9635<code>theta</code>\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4e8c\u7ef4\u6d41\u573a. \u901a\u5e38\u4e0e<code>grid_sample()</code>\u4e00\u8d77\u4f7f\u7528\u4ee5\u5b9e\u73b0<code>\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc</code>. </p> <p>\u53c2\u6570: *   theta (Tensor) \u2013 \u8f93\u5165\u7684\u4eff\u5c04\u77e9\u9635 () *   size (torch.Size) \u2013 \u76ee\u6807\u56fe\u50cf\u8f93\u51fa\u7684\u5927\u5c0f () \u4f8b\u5b50:  torch.Size((32, 3, 24, 24))</p> <p>\u8fd4\u56de\u503c: *   \u8f93\u51fatensor, \u5f62\u72b6\u4e3a () </p> <p>\u8fd4\u56de\u7c7b\u578b:  *   output (Tensor) </p>"},{"location":"1.0/nn_functional/#multi-gpu-distributed","title":"\u6570\u636e\u5e76\u884c\u51fd\u6570 (multi-GPU, distributed)","text":""},{"location":"1.0/nn_functional/#data_parallel","title":"data_parallel","text":"<pre><code>torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)\n</code></pre> <p>\u5728\u8bbe\u5907id\u4e2d\u7ed9\u5b9a\u7684gpu\u4e0a\u5e76\u884c\u8ba1\u7b97\u6a21\u5757(\u8f93\u5165).</p> <p>\u8fd9\u662fDataParallel\u6a21\u5757\u7684\u51fd\u6570\u7248\u672c.</p> <p>\u53c2\u6570: *   module (Module) \u2013 \u8981\u5e76\u884c\u8bc4\u4f30\u7684\u6a21\u5757 *   inputs (tensor) \u2013  \u6a21\u5757\u7684\u8f93\u5165 *   device_ids (list of python:int or torch.device) \u2013 \u7528\u4e8e\u590d\u5236\u6a21\u5757\u7684GPU id *   output_device (list of python:int or torch.device) \u2013  \u8f93\u51fa\u7684GPU\u4f4d\u7f6e\u4f7f\u7528 -1\u8868\u793aCPU. (\u9ed8\u8ba4\u503c:  device_ids[0])</p> <p>\u8fd4\u56de\u503c: *   \u4e00\u4e2a\u5f20\u91cf, \u5305\u542b\u4f4d\u4e8e\u8f93\u51fa\u8bbe\u5907\u4e0a\u7684\u6a21\u5757(\u8f93\u5165)\u7684\u7ed3\u679c</p>"},{"location":"1.0/nn_init/","title":"torch.nn.init","text":"<p>\u8bd1\u8005\uff1aGeneZC</p> <pre><code>torch.nn.init.calculate_gain(nonlinearity, param=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u975e\u7ebf\u6027\u51fd\u6570\u7684\u63a8\u8350\u7684\u589e\u76ca\u503c\u3002\u5bf9\u5e94\u5173\u7cfb\u5982\u4e0b\u8868\uff1a</p> \u975e\u7ebf\u6027\u51fd\u6570 \u589e\u76ca Linear / Identity Conv{1,2,3}D Sigmoid Tanh ReLU Leaky Relu <p>\u53c2\u6570\uff1a</p> <ul> <li>nonlinearity \u2013 \u975e\u7ebf\u6027\u51fd\u6570 (<code>nn.functional</code> \u4e2d\u7684\u540d\u5b57)</li> <li>param \u2013 \u5bf9\u5e94\u975e\u7ebf\u6027\u51fd\u6570\u7684\u53ef\u9009\u53c2\u6570</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; gain = nn.init.calculate_gain('leaky_relu')\n\n</code></pre> <pre><code>torch.nn.init.uniform_(tensor, a=0, b=1)\n</code></pre> <p>\u7528\u5747\u5300\u5206\u5e03  \u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>a \u2013 \u5747\u5300\u5206\u5e03\u7684\u4e0b\u754c</li> <li>b \u2013 \u5747\u5300\u5206\u5e03\u7684\u4e0a\u754c</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.uniform_(w)\n\n</code></pre> <pre><code>torch.nn.init.normal_(tensor, mean=0, std=1)\n</code></pre> <p>\u7528\u6b63\u6001\u5206\u5e03  \u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>mean \u2013 \u6b63\u6001\u5206\u5e03\u7684\u5747\u503c</li> <li>std \u2013 \u6b63\u6001\u5206\u5e03\u7684\u6807\u51c6\u5dee</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.normal_(w)\n\n</code></pre> <pre><code>torch.nn.init.constant_(tensor, val)\n</code></pre> <p>\u7528\u5e38\u6570  \u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>val \u2013 \u7528\u4ee5\u586b\u5165\u5f20\u91cf\u7684\u5e38\u6570</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.constant_(w, 0.3)\n\n</code></pre> <pre><code>torch.nn.init.eye_(tensor)\n</code></pre> <p>\u7528\u5355\u4f4d\u77e9\u9635\u521d\u59cb\u5316 2 \u7ef4\u8f93\u5165 <code>Tensor</code>\u3002 \u4fdd\u6301\u8f93\u5165\u5f20\u91cf\u8f93\u5165 <code>Linear</code> \u65f6\u7684\u72ec\u4e00\u6027\uff0c\u5e76\u4e14\u8d8a\u591a\u8d8a\u597d.</p> <p>\u53c2\u6570\uff1a  </p> <ul> <li>tensor \u2013 2 \u7ef4 <code>torch.Tensor</code> </li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.eye_(w)\n\n</code></pre> <pre><code>torch.nn.init.dirac_(tensor)\n</code></pre> <p>\u7528\u72c4\u62c9\u514b\u03b4\u51fd\u6570\u521d\u59cb\u5316 {3, 4, 5} \u7ef4\u8f93\u5165 <code>Tensor</code>\u3002 \u4fdd\u6301\u8f93\u5165\u5f20\u91cf\u8f93\u5165 <code>Convolutional</code> \u65f6\u7684\u72ec\u4e00\u6027\uff0c\u5e76\u4e14\u8d8a\u591a\u901a\u9053\u8d8a\u597d\u3002</p> <p>\u53c2\u6570\uff1a  </p> <ul> <li>tensor \u2013 {3, 4, 5} \u7ef4 <code>torch.Tensor</code> </li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 16, 5, 5)\n&gt;&gt;&gt; nn.init.dirac_(w)\n\n</code></pre> <pre><code>torch.nn.init.xavier_uniform_(tensor, gain=1)\n</code></pre> <p>\u7528\u8bba\u6587 \u201cUnderstanding the difficulty of training deep feedforward neural networks\u201d - Glorot, X. &amp; Bengio, Y. (2010) \u4e2d\u63d0\u53ca\u7684\u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002\u521d\u59cb\u5316\u540e\u7684\u5f20\u91cf\u4e2d\u7684\u503c\u91c7\u6837\u81ea  \u4e14</p> <p></p> <p>\u4e5f\u88ab\u79f0\u4f5c Glorot \u521d\u59cb\u5316\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>gain \u2013 \u53ef\u9009\u7f29\u653e\u56e0\u5b50</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n\n</code></pre> <pre><code>torch.nn.init.xavier_normal_(tensor, gain=1)\n</code></pre> <p>\u7528\u8bba\u6587 \u201cUnderstanding the difficulty of training deep feedforward neural networks\u201d - Glorot, X. &amp; Bengio, Y. (2010) \u4e2d\u63d0\u53ca\u7684\u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002\u521d\u59cb\u5316\u540e\u7684\u5f20\u91cf\u4e2d\u7684\u503c\u91c7\u6837\u81ea  \u4e14</p> <p></p> <p>\u4e5f\u88ab\u79f0\u4f5c Glorot initialization\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>gain \u2013 \u53ef\u9009\u7f29\u653e\u56e0\u5b50</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.xavier_normal_(w)\n\n</code></pre> <pre><code>torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n</code></pre> <p>\u7528\u8bba\u6587 \u201cDelving deep into rectifiers: Surpassing human-level performance on ImageNet classification\u201d - He, K. et al. (2015) \u4e2d\u63d0\u53ca\u7684\u5747\u5300\u5206\u5e03\u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002\u521d\u59cb\u5316\u540e\u7684\u5f20\u91cf\u4e2d\u7684\u503c\u91c7\u6837\u81ea  \u4e14</p> <p></p> <p>\u4e5f\u88ab\u79f0\u4f5c He initialization\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>a \u2013 \u8be5\u5c42\u540e\u9762\u4e00\u5c42\u7684\u6574\u6d41\u51fd\u6570\u4e2d\u8d1f\u7684\u659c\u7387 (\u9ed8\u8ba4\u4e3a 0\uff0c\u6b64\u65f6\u4e3a Relu)</li> <li>mode \u2013 'fan_in' (default) \u6216\u8005 'fan_out'\u3002\u4f7f\u7528fan_in\u4fdd\u6301weights\u7684\u65b9\u5dee\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u4e0d\u53d8\uff1b\u4f7f\u7528fan_out\u4fdd\u6301weights\u7684\u65b9\u5dee\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4e0d\u53d8\u3002</li> <li>nonlinearity \u2013 \u975e\u7ebf\u6027\u51fd\u6570 (<code>nn.functional</code> \u4e2d\u7684\u540d\u5b57)\uff0c\u63a8\u8350\u53ea\u4f7f\u7528 'relu' \u6216 'leaky_relu' (default)\u3002</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n\n</code></pre> <pre><code>torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n</code></pre> <p>\u7528\u8bba\u6587 \u201cDelving deep into rectifiers: Surpassing human-level performance on ImageNet classification\u201d - He, K. et al. (2015) \u4e2d\u63d0\u53ca\u7684\u6b63\u6001\u5206\u5e03\u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002\u521d\u59cb\u5316\u540e\u7684\u5f20\u91cf\u4e2d\u7684\u503c\u91c7\u6837  \u4e14</p> <p></p> <p>\u4e5f\u88ab\u79f0\u4f5c He initialization\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>a \u2013 \u8be5\u5c42\u540e\u9762\u4e00\u5c42\u7684\u6574\u6d41\u51fd\u6570\u4e2d\u8d1f\u7684\u659c\u7387 (\u9ed8\u8ba4\u4e3a 0\uff0c\u6b64\u65f6\u4e3a Relu)</li> <li>mode \u2013 'fan_in' (default) \u6216\u8005 'fan_out'\u3002\u4f7f\u7528fan_in\u4fdd\u6301weights\u7684\u65b9\u5dee\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u4e0d\u53d8\uff1b\u4f7f\u7528fan_out\u4fdd\u6301weights\u7684\u65b9\u5dee\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4e0d\u53d8\u3002</li> <li>nonlinearity \u2013 \u975e\u7ebf\u6027\u51fd\u6570 (<code>nn.functional</code> \u4e2d\u7684\u540d\u5b57)\uff0c\u63a8\u8350\u53ea\u4f7f\u7528 'relu' \u6216 'leaky_relu' (default)\u3002</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n\n</code></pre> <pre><code>torch.nn.init.orthogonal_(tensor, gain=1)\n</code></pre> <p>\u7528\u8bba\u6587 \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks\u201d - Saxe, A. et al. (2013) \u4e2d\u63cf\u8ff0\u7684(\u534a\uff09\u6b63\u5b9a\u77e9\u9635\u521d\u59cb\u5316\u8f93\u5165 <code>Tensor</code>\u3002\u8f93\u5165\u5f20\u91cf\u5fc5\u987b\u81f3\u5c11\u6709 2 \u7ef4\uff0c\u5982\u679c\u8f93\u5165\u5f20\u91cf\u7684\u7ef4\u5ea6\u5927\u4e8e 2\uff0c \u5219\u5bf9\u540e\u7eed\u7ef4\u5ea6\u8fdb\u884c\u653e\u5e73\u64cd\u4f5c\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code>\uff0c\u4e14 </li> <li>gain \u2013 \u53ef\u9009\u7f29\u653e\u56e0\u5b50</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.orthogonal_(w)\n\n</code></pre> <pre><code>torch.nn.init.sparse_(tensor, sparsity, std=0.01)\n</code></pre> <p>\u7528\u8bba\u6587 \u201cDeep learning via Hessian-free optimization\u201d - Martens, J. (2010). \u63d0\u53ca\u7684\u7a00\u758f\u77e9\u9635\u521d\u59cb\u5316 2 \u7ef4\u8f93\u5165 <code>Tensor</code>\uff0c\u4e14\u4f7f\u7528\u6b63\u6001\u5206\u5e03  \u521d\u59cb\u5316\u975e\u96f6\u5143\u7d20\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor \u2013 n \u7ef4 <code>torch.Tensor</code></li> <li>sparsity \u2013 \u6bcf\u4e00\u884c\u7f6e\u96f6\u5143\u7d20\u7684\u6bd4\u4f8b</li> <li>std \u2013 \u521d\u59cb\u5316\u975e\u96f6\u5143\u7d20\u65f6\u4f7f\u7528\u6b63\u6001\u5206\u5e03\u7684\u6807\u51c6\u5dee</li> </ul> <p>\u4f8b\u5b50</p> <pre><code>&gt;&gt;&gt; w = torch.empty(3, 5)\n&gt;&gt;&gt; nn.init.sparse_(w, sparsity=0.1)\n\n</code></pre>"},{"location":"1.0/nn_tutorial/","title":"torch.nn \u5230\u5e95\u662f\u4ec0\u4e48\uff1f","text":"<p>\u8bd1\u8005\uff1alhc741</p> <p>\u4f5c\u8005\uff1aJeremy Howard\uff0cfast.ai\u3002\u611f\u8c22Rachel Thomas\u548cFrancisco Ingham\u7684\u5e2e\u52a9\u548c\u652f\u6301\u3002</p> <p>\u6211\u4eec\u63a8\u8350\u4f7f\u7528notebook\u6765\u8fd0\u884c\u8fd9\u4e2a\u6559\u7a0b\uff0c\u800c\u4e0d\u662f\u811a\u672c\uff0c\u70b9\u51fb\u8fd9\u91cc\u4e0b\u8f7dnotebook(.ipynb)\u6587\u4ef6\u3002</p> <p>Pytorch\u63d0\u4f9b\u4e86torch.nn\u3001torch.optim\u3001Dataset\u548cDataLoader\u8fd9\u4e9b\u8bbe\u8ba1\u4f18\u96c5\u7684\u6a21\u5757\u548c\u7c7b\u4ee5\u5e2e\u52a9\u4f7f\u7528\u8005\u521b\u5efa\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002 \u4e3a\u4e86\u6700\u5927\u5316\u5229\u7528\u8fd9\u4e9b\u6a21\u5757\u548c\u7c7b\u7684\u529f\u80fd\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u505a\u51fa\u9002\u7528\u4e8e\u4f60\u6240\u7814\u7a76\u95ee\u9898\u7684\u6a21\u578b\uff0c\u4f60\u9700\u8981\u771f\u6b63\u7406\u89e3\u4ed6\u4eec\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002 \u4e3a\u4e86\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u9996\u5148\u57fa\u4e8eMNIST\u6570\u636e\u96c6\u8bad\u7ec3\u4e00\u4e2a\u6ca1\u6709\u4efb\u4f55\u7279\u5f81\u7684\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u3002 \u6700\u5f00\u59cb\u6211\u4eec\u53ea\u4f1a\u7528\u5230PyTorch\u4e2d\u6700\u57fa\u672c\u7684tensor\u529f\u80fd\uff0c\u7136\u540e\u6211\u4eec\u5c06\u4f1a\u9010\u6e10\u7684\u4ece<code>torch.nn</code>\uff0c<code>torch.optim</code>\uff0c<code>Dataset</code>\uff0c<code>DataLoader</code>\u4e2d\u9009\u62e9\u4e00\u4e2a\u7279\u5f81\u52a0\u5165\u5230\u6a21\u578b\u4e2d\uff0c\u6765\u5c55\u793a\u65b0\u52a0\u5165\u7684\u7279\u5f81\u4f1a\u5bf9\u6a21\u578b\u4ea7\u751f\u4ec0\u4e48\u6837\u7684\u6548\u679c\uff0c\u4ee5\u53ca\u5b83\u662f\u5982\u4f55\u4f7f\u6a21\u578b\u53d8\u5f97\u66f4\u7b80\u6d01\u6216\u66f4\u7075\u6d3b\u3002</p> <p>\u5728\u8fd9\u4e2a\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5047\u8bbe\u4f60\u5df2\u7ecf\u5b89\u88c5\u597d\u4e86PyTorch\uff0c\u5e76\u4e14\u5df2\u7ecf\u719f\u6089\u4e86\u57fa\u672c\u7684tensor\u8fd0\u7b97\u3002(\u5982\u679c\u4f60\u719f\u6089Numpy\u7684\u6570\u7ec4\u8fd0\u7b97\uff0c\u4f60\u5c06\u4f1a\u53d1\u73b0\u8fd9\u91cc\u7528\u5230\u7684PyTorch tensor\u8fd0\u7b97\u548cnumpy\u51e0\u4e4e\u662f\u4e00\u6837\u7684)</p>"},{"location":"1.0/nn_tutorial/#mnist","title":"MNIST\u6570\u636e\u5b89\u88c5","text":"<p>\u6211\u4eec\u5c06\u8981\u4f7f\u7528\u7ecf\u5178\u7684MNIST\u6570\u636e\u96c6\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u7531\u624b\u5199\u6570\u5b57(0\u52309\uff09\u7684\u9ed1\u767d\u56fe\u7247\u7ec4\u6210\u3002</p> <p>\u6211\u4eec\u5c06\u4f7f\u7528pathlib\u6765\u5904\u7406\u6587\u4ef6\u8def\u5f84\u7684\u76f8\u5173\u64cd\u4f5c(python3\u4e2d\u7684\u4e00\u4e2a\u6807\u51c6\u5e93\uff09\uff0c\u4f7f\u7528request\u6765\u4e0b\u8f7d\u6570\u636e\u96c6\u3002 \u6211\u4eec\u53ea\u4f1a\u5728\u7528\u5230\u76f8\u5173\u5e93\u7684\u65f6\u5019\u8fdb\u884c\u5f15\u7528\uff0c\u8fd9\u6837\u4f60\u5c31\u53ef\u4ee5\u660e\u786e\u5728\u6bcf\u4e2a\u64cd\u4f5c\u4e2d\u7528\u5230\u4e86\u54ea\u4e9b\u5e93\u3002</p> <pre><code>from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"http://deeplearning.net/data/mnist/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)`\n</code></pre> <p>\u8be5\u6570\u636e\u96c6\u91c7\u7528numpy\u6570\u7ec4\u683c\u5f0f\uff0c\u5e76\u5df2\u4f7f\u7528pickle\u5b58\u50a8\uff0cpickle\u662f\u4e00\u4e2a\u7528\u6765\u628a\u6570\u636e\u5e8f\u5217\u5316\u4e3apython\u7279\u5b9a\u683c\u5f0f\u7684\u5e93\u3002</p> <pre><code>import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n</code></pre> <p>\u6bcf\u4e00\u5e45\u56fe\u50cf\u90fd\u662f28 x 28\u7684\uff0c\u5e76\u88ab\u62c9\u5e73\u6210\u957f\u5ea6\u4e3a784(=28x28)\u7684\u4e00\u884c\u3002 \u6211\u4eec\u4ee5\u5176\u4e2d\u4e00\u4e2a\u4e3a\u4f8b\u5c55\u793a\u4e00\u4e0b\uff0c\u9996\u5148\u9700\u8981\u5c06\u8fd9\u4e2a\u4e00\u884c\u7684\u6570\u636e\u91cd\u65b0\u53d8\u6362\u4e3a\u4e00\u4e2a2d\u7684\u6570\u636e\u3002</p> <pre><code>from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\nprint(x_train.shape)\n</code></pre> <p></p> <p>\u8f93\u51fa\uff1a</p> <pre><code>(50000, 784)\n</code></pre> <p>PyTorch\u4f7f\u7528<code>torch.tensor</code>\uff0c\u800c\u4e0d\u662fnumpy\u6570\u7ec4\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5c06\u6570\u636e\u8f6c\u6362\u3002</p> <pre><code>import torch\n\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\nx_train, x_train.shape, y_train.min(), y_train.max()\nprint(x_train, y_train)\nprint(x_train.shape)\nprint(y_train.min(), y_train.max())\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)\n</code></pre>"},{"location":"1.0/nn_tutorial/#torchnn_1","title":"\u795e\u7ecf\u7f51\u7edc\u4ece\u96f6\u5f00\u59cb(\u4e0d\u4f7f\u7528torch.nn\uff09","text":"<p>\u6211\u4eec\u5148\u6765\u5efa\u7acb\u4e00\u4e2a\u53ea\u4f7f\u7528PyTorch\u5f20\u91cf\u8fd0\u7b97\u7684\u6a21\u578b\u3002 \u6211\u4eec\u5047\u8bbe\u4f60\u5df2\u7ecf\u719f\u6089\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840\u3002(\u5982\u679c\u4f60\u8fd8\u4e0d\u719f\u6089\uff0c\u53ef\u4ee5\u8bbf\u95eecourse.fast.ai\u8fdb\u884c\u5b66\u4e60\uff09\u3002</p> <p>PyTorch\u63d0\u4f9b\u521b\u5efa\u968f\u673a\u6570\u586b\u5145\u6216\u5168\u96f6\u586b\u5145\u5f20\u91cf\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u4f7f\u7528\u8be5\u65b9\u6cd5\u521d\u59cb\u5316\u4e00\u4e2a\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u7684\u6743\u91cd\u548c\u504f\u7f6e\u3002 \u8fd9\u4e24\u4e2a\u90fd\u662f\u666e\u901a\u7684\u5f20\u91cf\uff0c\u4f46\u5b83\u4eec\u6709\u4e00\u4e2a\u7279\u6b8a\u7684\u9644\u52a0\u6761\u4ef6\uff1a\u8bbe\u7f6e\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u7684\u53c2\u6570\u4e3aTrue\u3002\u8fd9\u6837PyTorch\u5c31\u4f1a\u8bb0\u5f55\u6240\u6709\u4e0e\u8fd9\u4e2a\u5f20\u91cf\u76f8\u5173\u7684\u8fd0\u7b97\uff0c\u4f7f\u5176\u80fd\u5728\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6\u3002</p> <p>\u5bf9\u4e8eweights\u800c\u8a00\uff0c\u7531\u4e8e\u6211\u4eec\u5e0c\u671b\u521d\u59cb\u5316\u5f20\u91cf\u8fc7\u7a0b\u4e2d\u5b58\u5728\u68af\u5ea6\uff0c\u6240\u4ee5\u6211\u4eec\u5728\u521d\u59cb\u5316\u4e4b\u540e\u8bbe\u7f6e<code>requires_grad</code>\u3002(\u6ce8\u610f\uff1a\u5c3e\u7f00\u4e3a<code>_</code>\u7684\u65b9\u6cd5\u5728PyTorch\u4e2d\u8868\u793a\u8fd9\u4e2a\u64cd\u4f5c\u4f1a\u88ab\u7acb\u5373\u88ab\u6267\u884c\u3002\uff09</p> <ul> <li>\u6ce8\u610f\uff1a</li> </ul> <p>\u6211\u4eec\u4ee5Xavier\u521d\u59cb\u5316\u65b9\u6cd5(\u6bcf\u4e2a\u5143\u7d20\u90fd\u9664\u4ee51/sqrt(n)\uff09\u4e3a\u4f8b\u6765\u5bf9\u6743\u91cd\u8fdb\u884c\u521d\u59cb\u5316\u3002</p> <pre><code>import math\n\nweights = torch.randn(784, 10) / math.sqrt(784)\nweights.requires_grad_()\nbias = torch.zeros(10, requires_grad=True)\n</code></pre> <p>\u591a\u4e8f\u4e86PyTorch\u5177\u6709\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u529f\u80fd\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528Python\u4e2d\u4efb\u4f55\u6807\u51c6\u51fd\u6570(\u6216\u8005\u53ef\u8c03\u7528\u5bf9\u8c61\uff09\u6765\u521b\u5efa\u6a21\u578b\uff01 \u56e0\u6b64\uff0c\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u666e\u901a\u7684\u77e9\u9635\u4e58\u6cd5\u548c\u5e7f\u64ad\u52a0\u6cd5\u5efa\u7acb\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\u3002 \u6211\u4eec\u8fd8\u9700\u8981\u4e00\u4e2a\u6fc0\u6d3b\u51fd\u6570\uff0c\u6240\u4ee5\u6211\u4eec\u7f16\u5199\u5e76\u4f7f\u7528\u4e00\u4e2alog_softmax\u51fd\u6570\u3002 \u8bf7\u8bb0\u4f4f\uff1a\u5c3d\u7ba1Pytorch\u63d0\u4f9b\u4e86\u8bb8\u591a\u9884\u5148\u7f16\u5199\u597d\u7684\u635f\u5931\u51fd\u6570\u3001\u6fc0\u6d3b\u51fd\u6570\u7b49\u7b49\uff0c\u4f60\u4ecd\u7136\u53ef\u4ee5\u4f7f\u7528\u7eafpython\u8f7b\u677e\u5b9e\u73b0\u4f60\u81ea\u5df1\u7684\u51fd\u6570\u3002 Pytorch\u751a\u81f3\u53ef\u4ee5\u81ea\u52a8\u5730\u4e3a\u4f60\u7684\u51fd\u6570\u521b\u5efa\u5feb\u901f\u7684GPU\u4ee3\u7801\u6216\u5411\u91cf\u5316\u7684CPU\u4ee3\u7801\u3002</p> <pre><code>def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model(xb):\n    return log_softmax(xb @ weights + bias)\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u4e00\u6bb5\u4ee3\u7801\u4e2d\uff0c<code>@</code>\u8868\u793a\u70b9\u79ef\u8fd0\u7b97\u7b26\u3002\u6211\u4eec\u5c06\u8c03\u7528\u6211\u4eec\u7684\u51fd\u6570\u8ba1\u7b97\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e(\u672c\u4f8b\u4e2d\u4e3a64\u5e45\u56fe\u50cf\uff09\u3002 \u8fd9\u662f\u4e00\u6b21\u6a21\u578b\u524d\u5411\u4f20\u9012\u7684\u8fc7\u7a0b\u3002 \u8bf7\u6ce8\u610f\uff0c\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u4e86\u968f\u673a\u6570\u6765\u521d\u59cb\u5316\u6743\u91cd\uff0c\u6240\u4ee5\u5728\u8fd9\u4e2a\u9636\u6bb5\u6211\u4eec\u7684\u9884\u6d4b\u503c\u5e76\u4e0d\u4f1a\u6bd4\u968f\u673a\u7684\u66f4\u597d\u3002</p> <pre><code>bs = 64  # \u4e00\u6279\u6570\u636e\u4e2a\u6570\n\nxb = x_train[0:bs]  # \u4ecex\u83b7\u53d6\u4e00\u5c0f\u6279\u6570\u636e\npreds = model(xb)  # \u9884\u6d4b\u503c\npreds[0], preds.shape\nprint(preds[0], preds.shape)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor([-2.4513, -2.5024, -2.0599, -3.1052, -3.2918, -2.2665, -1.9007, -2.2588,\n        -2.0149, -2.0287], grad_fn=&lt;SelectBackward&gt;) torch.Size([64, 10])\n</code></pre> <p>\u53ef\u4ee5\u4ece\u4e0a\u9762\u7684\u7ed3\u679c\u4e0d\u96be\u770b\u51fa\uff0c\u5f20\u91cf<code>preds</code>\u4e0d\u4ec5\u5305\u62ec\u4e86\u5f20\u91cf\u503c\uff0c\u8fd8\u5305\u62ec\u4e86\u68af\u5ea6\u51fd\u6570\u3002\u8fd9\u4e2a\u68af\u5ea6\u51fd\u6570\u6211\u4eec\u53ef\u4ee5\u5728\u540e\u9762\u7684\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u7528\u5230\u3002</p> <p>\u4e0b\u9762\u6211\u4eec\u6765\u5b9e\u73b0\u4e00\u4e2a\u8d1f\u7684\u5bf9\u6570\u4f3c\u7136\u51fd\u6570(Negative log-likehood\uff09\u4f5c\u4e3a\u635f\u5931\u51fd\u6570(\u540c\u6837\u4e5f\u4f7f\u7528\u7eafpython\u5b9e\u73b0\uff09\uff1a</p> <pre><code>def nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\n\nloss_func = nll\n</code></pre> <p>\u8ba9\u6211\u4eec\u67e5\u770b\u4e0b\u968f\u673a\u6a21\u578b\u7684\u635f\u5931\u503c\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u786e\u8ba4\u5728\u6267\u884c\u53cd\u5411\u4f20\u64ad\u7684\u6b65\u9aa4\u540e\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u6548\u679c\u662f\u5426\u6709\u4e86\u6539\u8fdb\u3002</p> <pre><code>yb = y_train[0:bs]\nprint(loss_func(preds, yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(2.3620, grad_fn=&lt;NegBackward&gt;)\n</code></pre> <p>\u6211\u4eec\u518d\u6765\u5b9e\u73b0\u4e00\u4e2a\u7528\u6765\u8ba1\u7b97\u6a21\u578b\u51c6\u786e\u7387\u7684\u51fd\u6570\u3002\u5bf9\u4e8e\u6bcf\u6b21\u9884\u6d4b\uff0c\u6211\u4eec\u89c4\u5b9a\u5982\u679c\u9884\u6d4b\u7ed3\u679c\u4e2d\u6982\u7387\u6700\u5927\u7684\u6570\u5b57\u548c\u56fe\u7247\u5b9e\u9645\u5bf9\u5e94\u7684\u6570\u5b57\u662f\u76f8\u540c\u7684\uff0c\u90a3\u4e48\u8fd9\u6b21\u9884\u6d4b\u5c31\u662f\u6b63\u786e\u7684\u3002</p> <pre><code>def accuracy(out, yb):\n    preds = torch.argmax(out, dim=1)\n    return (preds == yb).float().mean()\n</code></pre> <p>\u6211\u4eec\u5148\u6765\u770b\u4e00\u4e0b\u88ab\u968f\u673a\u521d\u59cb\u5316\u7684\u6a21\u578b\u7684\u51c6\u786e\u7387\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u770b\u5230\u635f\u5931\u503c\u964d\u4f4e\u7684\u65f6\u5019\u51c6\u786e\u7387\u662f\u5426\u63d0\u9ad8\u4e86\u3002</p> <pre><code>print(accuracy(preds, yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0938)\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u8fd0\u884c\u4e00\u4e2a\u5b8c\u6574\u7684\u8bad\u7ec3\u6b65\u9aa4\u4e86\uff0c\u6bcf\u6b21\u8fed\u4ee3\uff0c\u6211\u4eec\u4f1a\u8fdb\u884c\u4ee5\u4e0b\u51e0\u4e2a\u64cd\u4f5c\uff1a</p> <ul> <li>\u4ece\u5168\u90e8\u6570\u636e\u4e2d\u9009\u62e9\u4e00\u5c0f\u6279\u6570\u636e(\u5927\u5c0f\u4e3a<code>bs</code>\uff09</li> <li>\u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u9884\u6d4b</li> <li>\u8ba1\u7b97\u5f53\u524d\u9884\u6d4b\u7684\u635f\u5931\u503c</li> <li>\u4f7f\u7528<code>loss.backward()</code>\u66f4\u65b0\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\uff0c\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u66f4\u65b0\u7684\u662f<code>weights</code>\u548c<code>bias</code></li> </ul> <p>\u73b0\u5728\uff0c\u6211\u4eec\u6765\u5229\u7528\u8ba1\u7b97\u51fa\u7684\u68af\u5ea6\u5bf9\u6743\u503c\u548c\u504f\u7f6e\u9879\u8fdb\u884c\u66f4\u65b0\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u5e0c\u671b\u8fd9\u4e00\u6b65\u7684\u64cd\u4f5c\u88ab\u7528\u4e8e\u4e0b\u4e00\u6b21\u8fed\u4ee3\u7684\u68af\u5ea6\u8ba1\u7b97\uff0c\u6240\u4ee5\u6211\u4eec\u5728<code>torch.no_grad()</code>\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u4e2d\u5b8c\u6210\u3002\u60f3\u8981\u4e86\u89e3\u66f4\u591aPyTorch Autograd\u8bb0\u5f55\u64cd\u4f5c\u73b0\uff0c\u53ef\u4ee5\u70b9\u51fb\u8fd9\u91cc\u3002</p> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u68af\u5ea6\u8bbe\u7f6e\u4e3a0\uff0c\u6765\u4e3a\u4e0b\u4e00\u6b21\u5faa\u73af\u505a\u51c6\u5907\u3002\u5426\u5219\u6211\u4eec\u7684\u68af\u5ea6\u5c06\u4f1a\u8bb0\u5f55\u6240\u6709\u5df2\u7ecf\u6267\u884c\u8fc7\u7684\u8fd0\u7b97(\u5982\uff0c<code>loss.backward()</code>\u4f1a\u5c06\u68af\u5ea6\u53d8\u5316\u503c\u76f4\u63a5\u4e0e\u53d8\u91cf\u5df2\u6709\u503c\u8fdb\u884c\u7d2f\u52a0\uff0c\u800c\u4e0d\u662f\u66ff\u6362\u53d8\u91cf\u539f\u6709\u7684\u503c\uff09\u3002</p> <p>\u5c0f\u8d34\u58eb</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u6807\u51c6python\u8c03\u8bd5\u5668\u5bf9PyTorch\u4ee3\u7801\u8fdb\u884c\u5355\u6b65\u8c03\u8bd5\uff0c\u4ece\u800c\u5728\u6bcf\u4e00\u6b65\u68c0\u67e5\u4e0d\u540c\u7684\u53d8\u91cf\u503c\u3002\u53d6\u6d88\u4e0b\u9762\u7684<code>set_trace()</code>\u6765\u5c1d\u8bd5\u8be5\u529f\u80fd\u3002</p> <pre><code>from IPython.core.debugger import set_trace\n\nlr = 0.5  # \u5b66\u4e60\u7387\nepochs = 2  # \u8bad\u7ec3\u7684\u8f6e\u6570\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        with torch.no_grad():\n            weights -= weights.grad * lr\n            bias -= bias.grad * lr\n            weights.grad.zero_()\n            bias.grad.zero_()\n</code></pre> <p>\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u5df2\u7ecf\u4ece\u96f6\u5f00\u59cb\u5b8c\u6210\u4e86\u5efa\u7acb\u548c\u8bad\u7ec3\u4e00\u4e2a\u6700\u5c0f\u7684\u795e\u7ecf\u7f51\u7edc(\u56e0\u4e3a\u6211\u4eec\u5efa\u7acb\u7684logistic\u56de\u5f52\u6a21\u578b\u4e0d\u5305\u542b\u9690\u5c42\uff09\uff01</p> <p>\u73b0\u5728\uff0c\u6211\u4eec\u6765\u770b\u4e00\u4e0b\u6a21\u578b\u7684\u635f\u5931\u503c\u548c\u51c6\u786e\u7387\uff0c\u5e76\u4e8e\u6211\u4eec\u4e4b\u524d\u8f93\u51fa\u7684\u503c\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u6b63\u5982\u6211\u4eec\u9884\u671f\u7684\uff0c\u635f\u5931\u503c\u4e0b\u964d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u3002</p> <pre><code>print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0822, grad_fn=&lt;NegBackward&gt;) tensor(1.)\n</code></pre>"},{"location":"1.0/nn_tutorial/#torchnnfunctional","title":"torch.nn.functional\u7684\u4f7f\u7528","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u8981\u5bf9\u524d\u9762\u7684\u4ee3\u7801\u8fdb\u884c\u91cd\u6784\uff0c\u4f7f\u4ee3\u7801\u5728\u5b8c\u6210\u76f8\u540c\u529f\u80fd\u7684\u540c\u65f6\uff0c\u7528PyTorch\u7684<code>nn</code>\u6765\u4f7f\u4ee3\u7801\u53d8\u5f97\u66f4\u52a0\u7b80\u6d01\u548c\u7075\u6d3b\u3002 \u4ece\u73b0\u5728\u5f00\u59cb\uff0c\u63a5\u4e0b\u6765\u7684\u6bcf\u4e00\u6b65\u6211\u4eec\u90fd\u4f1a\u4f7f\u4ee3\u7801\u53d8\u5f97\u66f4\u77ed\uff0c\u66f4\u597d\u7406\u89e3\u6216\u66f4\u7075\u6d3b\u3002</p> <p>\u8981\u8fdb\u884c\u7684\u7b2c\u4e00\u6b65\u4e5f\u662f\u6700\u7b80\u5355\u7684\u4e00\u6b65\uff0c\u662f\u4f7f\u7528<code>torch.nn.functional</code>(\u901a\u8fc7\u4f1a\u5728\u5f15\u7528\u65f6\u7528F\u8868\u793a\uff09\u4e2d\u7684\u51fd\u6570\u66ff\u6362\u6211\u4eec\u81ea\u5df1\u7684\u6fc0\u6d3b\u51fd\u6570\u548c\u635f\u5931\u51fd\u6570\u4f7f\u4ee3\u7801\u53d8\u5f97\u66f4\u77ed\u3002 \u8fd9\u4e2a\u6a21\u5757\u5305\u542b\u4e86<code>torch.nn</code>\u5e93\u4e2d\u7684\u6240\u6709\u51fd\u6570(\u8fd9\u4e2a\u5e93\u7684\u5176\u5b83\u90e8\u5206\u662f\u5404\u79cd\u7c7b\uff09\uff0c\u6240\u4ee5\u5728\u8fd9\u4e2a\u6a21\u5757\u4e2d\u8fd8\u4f1a\u627e\u5230\u5176\u5b83\u4fbf\u4e8e\u5efa\u7acb\u795e\u7ecf\u7f51\u7edc\u7684\u51fd\u6570\uff0c\u6bd4\u5982\u6c60\u5316\u51fd\u6570\u3002(\u6a21\u5757\u4e2d\u8fd8\u5305\u542b\u5377\u79ef\u51fd\u6570\uff0c\u7ebf\u6027\u51fd\u6570\u7b49\u7b49\uff0c\u4e0d\u8fc7\u5728\u540e\u9762\u7684\u5185\u5bb9\u4e2d\u6211\u4eec\u4f1a\u770b\u5230\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u4f7f\u7528\u5e93\u4e2d\u7684\u5176\u5b83\u90e8\u5206\u4f1a\u66f4\u597d\u3002\uff09</p> <p>\u5982\u679c\u4f60\u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u548c\u5bf9\u6570\u67d4\u6027\u6700\u5927\u503c(softmax)\u6fc0\u6d3b\u51fd\u6570\uff0cPyTorch\u6709\u4e00\u4e2a\u7ed3\u5408\u4e86\u8fd9\u4e24\u4e2a\u51fd\u6570\u7684\u7b80\u5355\u51fd\u6570<code>F.cross_entropy</code>\u4f9b\u4f60\u4f7f\u7528\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5220\u6389\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\u3002</p> <pre><code>import torch.nn.functional as F\n\nloss_func = F.cross_entropy\n\ndef model(xb):\n    return xb @ weights + bias\n</code></pre> <p>\u6ce8\u610f\u5728<code>model</code>\u51fd\u6570\u4e2d\u6211\u4eec\u4e0d\u518d\u8c03\u7528<code>log_softmax</code>\u3002\u73b0\u5728\u6211\u4eec\u6765\u786e\u8ba4\u4e00\u4e0b\u635f\u5931\u503c\u548c\u51c6\u786e\u7387\u4e0e\u4e4b\u524d\u76f8\u540c\u3002</p> <pre><code>print(loss_func(model(xb), yb), accuracy(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0822, grad_fn=&lt;NllLossBackward&gt;) tensor(1.)\n</code></pre>"},{"location":"1.0/nn_tutorial/#nnmodule","title":"\u4f7f\u7528nn.Module\u8fdb\u884c\u91cd\u6784","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u4f1a\u7528\u5230<code>nn.Model</code>\u548c<code>nn.Parameter</code>\u6765\u5b8c\u6210\u4e00\u4e2a\u66f4\u52a0\u6e05\u6670\u7b80\u6d01\u7684\u8bad\u7ec3\u5faa\u73af\u3002\u6211\u4eec\u7ee7\u627f<code>nn.Module</code>(\u5b83\u662f\u4e00\u4e2a\u80fd\u591f\u8ddf\u8e2a\u72b6\u6001\u7684\u7c7b)\u3002\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u60f3\u8981\u65b0\u5efa\u4e00\u4e2a\u7c7b\uff0c\u5b9e\u73b0\u5b58\u50a8\u6743\u91cd\uff0c\u504f\u7f6e\u548c\u524d\u5411\u4f20\u64ad\u6b65\u9aa4\u4e2d\u6240\u6709\u7528\u5230\u65b9\u6cd5\u3002<code>nn.Module</code>\u5305\u542b\u4e86\u8bb8\u591a\u5c5e\u6027\u548c\u65b9\u6cd5(\u6bd4\u5982<code>.parameters()</code>\u548c<code>.zero_grad()</code>\uff09\uff0c\u6211\u4eec\u4f1a\u5728\u540e\u9762\u7528\u5230\u3002</p> <p>\u6ce8\u610f</p> <p><code>nn.Module</code>(M\u5927\u5199\uff09\u662f\u4e00\u4e2aPyTorch\u4e2d\u7279\u6709\u7684\u6982\u5ff5\uff0c\u5b83\u662f\u4e00\u4e2a\u4f1a\u7ecf\u5e38\u7528\u5230\u7684\u7c7b\u3002\u4e0d\u8981\u548cPython\u4e2dmodule(<code>m</code>\u5c0f\u5199\uff09\u6df7\u6dc6\uff0cmodule\u662f\u4e00\u4e2a\u53ef\u4ee5\u88ab\u5f15\u5165\u7684Python\u4ee3\u7801\u6587\u4ef6\u3002</p> <pre><code>from torch import nn\n\nclass Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n        self.bias = nn.Parameter(torch.zeros(10))\n\n    def forward(self, xb):\n        return xb @ self.weights + self.bias\n</code></pre> <p>\u65e2\u7136\u73b0\u5728\u6211\u4eec\u8981\u4f7f\u7528\u4e00\u4e2a\u5bf9\u8c61\u800c\u4e0d\u662f\u51fd\u6570\uff0c\u6211\u4eec\u8981\u5148\u5bf9\u6a21\u578b\u8fdb\u884c\u5b9e\u4f8b\u5316\u3002</p> <pre><code>model = Mnist_Logistic()\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u90a3\u6837\u8ba1\u7b97\u635f\u5931\u503c\u4e86\u3002\u6ce8\u610f<code>nn.Module</code>\u5bf9\u8c61\u7684\u4f7f\u7528\u65b9\u5f0f\u5f88\u50cf\u51fd\u6570(\u4f8b\u5982\u5b83\u4eec\u662f\u53ef\u8c03\u7528\u7684\uff09\uff0c\u4f46\u662fPyTorch\u5c06\u4f1a\u81ea\u52a8\u8c03\u7528\u6211\u4eec\u7684<code>forward</code>\u51fd\u6570</p> <pre><code>print(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(2.2082, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre> <p>\u4e4b\u524d\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u53d8\u91cf\u540d\u5bf9\u6bcf\u4e2a\u53d8\u91cf\u7684\u503c\u8fdb\u884c\u66f4\u65b0\uff0c\u5e76\u624b\u52a8\u7684\u5c06\u6bcf\u4e2a\u53d8\u91cf\u7684\u68af\u5ea6\u7f6e\u4e3a0\uff0c\u50cf\u8fd9\u6837\uff1a</p> <pre><code>with torch.no_grad():\n    weights -= weights.grad * lr\n    bias -= bias.grad * lr\n    weights.grad.zero_()\n    bias.grad.zero_()\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5229\u7528<code>model.parameters()</code>\u548c<code>model.zero_grad()</code>(\u8fd9\u4e24\u4e2a\u90fd\u662fPyTorch\u5b9a\u4e49\u5728<code>nn.Module</code>\u4e2d\u7684\uff09\u4f7f\u8fd9\u4e9b\u6b65\u9aa4\u53d8\u5f97\u66f4\u52a0\u7b80\u6d01\u5e76\u4e14\u66f4\u4e0d\u5bb9\u6613\u5fd8\u8bb0\u66f4\u65b0\u90e8\u5206\u53c2\u6570\uff0c\u5c24\u5176\u662f\u6a21\u578b\u5f88\u590d\u6742\u7684\u60c5\u51b5\uff1a</p> <pre><code>with torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()\n</code></pre> <p>\u4e0b\u9762\u6211\u4eec\u628a\u8bad\u7ec3\u5faa\u73af\u5c01\u88c5\u8fdb<code>fit</code>\u51fd\u6570\u4e2d\uff0c\u8fd9\u6837\u5c31\u80fd\u5728\u540e\u9762\u518d\u6b21\u8fd0\u884c\u3002</p> <pre><code>def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n            xb = x_train[start_i:end_i]\n            yb = y_train[start_i:end_i]\n            pred = model(xb)\n            loss = loss_func(pred, yb)\n\n            loss.backward()\n            with torch.no_grad():\n                for p in model.parameters():\n                    p -= p.grad * lr\n                model.zero_grad()\n\nfit()\n</code></pre> <p>\u6211\u4eec\u6765\u518d\u6b21\u68c0\u67e5\u4e00\u4e0b\u6211\u4eec\u7684\u635f\u5931\u503c\u662f\u5426\u4e0b\u964d\u3002</p> <pre><code>print(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0812, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre>"},{"location":"1.0/nn_tutorial/#nnlinear","title":"\u4f7f\u7528nn.Linear\u8fdb\u884c\u91cd\u6784","text":"<p>\u6211\u4eec\u7ee7\u7eed\u5bf9\u4ee3\u7801\u8fdb\u884c\u91cd\u6784\u3002\u6211\u4eec\u5c06\u7528PyTorch\u4e2d\u7684nn.Linear\u4ee3\u66ff\u624b\u52a8\u5b9a\u4e49\u548c\u521d\u59cb\u5316<code>self.weights</code>\u548c<code>self.bias</code>\u4ee5\u53ca\u8ba1\u7b97<code>xb @ self.weights + self.bias</code>, \u56e0\u4e3a<code>nn.Linear</code>\u53ef\u4ee5\u5b8c\u6210\u8fd9\u4e9b\u64cd\u4f5c\u3002 PyTorch\u4e2d\u9884\u8bbe\u4e86\u5f88\u591a\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u4f7f\u7528\u5b83\u4eec\u53ef\u4ee5\u6781\u5927\u7684\u7b80\u5316\u6211\u4eec\u7684\u4ee3\u7801\uff0c\u901a\u5e38\u8fd8\u4f1a\u5e26\u6765\u901f\u5ea6\u4e0a\u7684\u63d0\u5347\u3002</p> <pre><code>class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10)\n\n    def forward(self, xb):\n        return self.lin(xb)\n</code></pre> <p>\u6211\u4eec\u521d\u59cb\u5316\u6a21\u578b\u5e76\u50cf\u4e4b\u524d\u90a3\u6837\u8ba1\u7b97\u635f\u5931\u503c\uff1a</p> <pre><code>model = Mnist_Logistic()\nprint(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(2.2731, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre> <p>\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u50cf\u4e4b\u524d\u90a3\u6837\u4f7f\u7528<code>fit</code>\u51fd\u6570\uff1a</p> <pre><code>fit()\n\nprint(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0820, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre>"},{"location":"1.0/nn_tutorial/#optim","title":"\u4f7f\u7528optim\u8fdb\u884c\u91cd\u6784","text":"<p>PyTorch\u8fd8\u6709\u4e00\u4e2a\u5305\u542b\u5f88\u591a\u4f18\u5316\u7b97\u6cd5\u7684\u5305\u2014\u2014\u2014\u2014<code>torch.optim</code>\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4f18\u5316\u5668\u4e2d\u7684<code>step</code>\u65b9\u6cd5\u6267\u884c\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u7684\u6b65\u9aa4\u6765\u66ff\u6362\u624b\u52a8\u66f4\u65b0\u6bcf\u4e2a\u53c2\u6570\u3002</p> <p>\u8fd9\u4e2a\u65b9\u6cd5\u5c06\u5141\u8bb8\u6211\u4eec\u66ff\u6362\u4e4b\u524d\u624b\u52a8\u7f16\u5199\u7684\u4f18\u5316\u6b65\u9aa4\uff1a</p> <pre><code>with torch.no_grad():\n    for p in model.parameters(): p -= p.grad * lr\n    model.zero_grad()\n</code></pre> <p>\u66ff\u6362\u540e\u5982\u4e0b\uff1a</p> <pre><code>opt.step()\nopt.zero_grad()\n</code></pre> <p>(<code>optim.zero_grad()</code>\u5c06\u68af\u5ea6\u91cd\u7f6e\u4e3a0\uff0c\u6211\u4eec\u9700\u8981\u5728\u8ba1\u7b97\u4e0b\u4e00\u6b21\u68af\u5ea6\u4e4b\u524d\u8c03\u7528\u5b83\uff09</p> <pre><code>from torch import optim\n</code></pre> <p>\u6211\u4eec\u5c06\u5efa\u7acb\u6a21\u578b\u548c\u4f18\u5316\u5668\u7684\u6b65\u9aa4\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5c0f\u51fd\u6570\u65b9\u4fbf\u5c06\u6765\u590d\u7528\u3002</p> <pre><code>def get_model():\n    model = Mnist_Logistic()\n    return model, optim.SGD(model.parameters(), lr=lr)\n\nmodel, opt = get_model()\nprint(loss_func(model(xb), yb))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n        xb = x_train[start_i:end_i]\n        yb = y_train[start_i:end_i]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(2.3785, grad_fn=&lt;NllLossBackward&gt;)\ntensor(0.0802, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre>"},{"location":"1.0/nn_tutorial/#dataset","title":"\u4f7f\u7528Dataset\u8fdb\u884c\u91cd\u6784","text":"<p>Pytorch\u5305\u542b\u4e00\u4e2aDataset\u62bd\u8c61\u7c7b\u3002Dataset\u53ef\u4ee5\u662f\u4efb\u4f55\u4e1c\u897f\uff0c\u4f46\u5b83\u59cb\u7ec8\u5305\u542b\u4e00\u4e2a<code>__len__</code>\u51fd\u6570(\u901a\u8fc7Python\u4e2d\u7684\u6807\u51c6\u51fd\u6570<code>len</code>\u8c03\u7528\uff09\u548c\u4e00\u4e2a\u7528\u6765\u7d22\u5f15\u5230\u5185\u5bb9\u4e2d\u7684<code>__getitem__</code>\u51fd\u6570\u3002 \u8fd9\u7bc7\u6559\u7a0b\u4ee5\u521b\u5efa<code>Dataset</code>\u7684\u81ea\u5b9a\u4e49\u5b50\u7c7b<code>FacialLandmarkDataset</code>\u4e3a\u4f8b\u8fdb\u884c\u4ecb\u7ecd\u3002</p> <p>PyTorch\u4e2d\u7684TensorDataset\u662f\u4e00\u4e2a\u5c01\u88c5\u4e86\u5f20\u91cf\u7684Dataset\u3002\u901a\u8fc7\u5b9a\u4e49\u957f\u5ea6\u548c\u7d22\u5f15\u7684\u65b9\u5f0f\uff0c\u662f\u6211\u4eec\u53ef\u4ee5\u5bf9\u5f20\u91cf\u7684\u7b2c\u4e00\u7ef4\u8fdb\u884c\u8fed\u4ee3\uff0c\u7d22\u5f15\u548c\u5207\u7247\u3002\u8fd9\u5c06\u4f7f\u6211\u4eec\u5728\u8bad\u7ec3\u4e2d\uff0c\u83b7\u53d6\u540c\u4e00\u884c\u4e2d\u7684\u81ea\u53d8\u91cf\u548c\u56e0\u53d8\u91cf\u66f4\u52a0\u5bb9\u6613\u3002</p> <pre><code>from torch.utils.data import TensorDataset\n</code></pre> <p>\u53ef\u4ee5\u628a<code>x_train</code>\u548c<code>y_train</code>\u4e2d\u7684\u6570\u636e\u5408\u5e76\u6210\u4e00\u4e2a\u7b80\u5355\u7684<code>TensorDataset</code>\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u65b9\u4fbf\u7684\u8fdb\u884c\u8fed\u4ee3\u548c\u5207\u7247\u64cd\u4f5c\u3002</p> <pre><code>train_ds = TensorDataset(x_train, y_train)\n</code></pre> <p>\u4e4b\u524d\uff0c\u6211\u4eec\u4e0d\u5f97\u4e0d\u5206\u522b\u5bf9x\u548cy\u7684\u503c\u8fdb\u884c\u8fed\u4ee3\u5faa\u73af\u3002</p> <pre><code>xb = x_train[start_i:end_i]\nyb = y_train[start_i:end_i]\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e24\u6b65\u5408\u4e8c\u4e3a\u4e00\u4e86\u3002</p> <pre><code>xb,yb = train_ds[i*bs : i*bs+bs]\n</code></pre> <pre><code>model, opt = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        xb, yb = train_ds[i * bs: i * bs + bs]\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre>"},{"location":"1.0/nn_tutorial/#dataloader","title":"\u4f7f\u7528DataLoader\u8fdb\u884c\u91cd\u6784","text":"<p>PyTorch\u7684<code>DataLoader</code>\u8d1f\u8d23\u6279\u91cf\u6570\u636e\u7ba1\u7406\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u4efb\u610f\u7684<code>Dataset</code>\u521b\u5efa\u4e00\u4e2a<code>DataLoader</code>\u3002<code>DataLoader</code>\u4f7f\u5f97\u5bf9\u6279\u91cf\u6570\u636e\u7684\u8fed\u4ee3\u66f4\u5bb9\u6613\u3002<code>DataLoader</code>\u81ea\u52a8\u7684\u4e3a\u6211\u4eec\u63d0\u4f9b\u6bcf\u4e00\u5c0f\u6279\u91cf\u7684\u6570\u636e\u6765\u4ee3\u66ff\u5207\u7247\u7684\u65b9\u5f0f<code>train_ds[i*bs : i*bs+bs]</code>\u3002</p> <pre><code>from torch.utils.data import DataLoader\n\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n</code></pre> <p>\u4e4b\u524d\u6211\u4eec\u50cf\u4e0b\u9762\u8fd9\u6837\u6309\u6279(xb,yb)\u5bf9\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\uff1a</p> <pre><code>for i in range((n-1)//bs + 1):\n    xb,yb = train_ds[i*bs : i*bs+bs]\n    pred = model(xb)\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u7684\u5faa\u73af\u53d8\u5f97\u66f4\u52a0\u7b80\u6d01\uff0c\u56e0\u4e3a\u4f7f\u7528\u4e86data loader\u6765\u81ea\u52a8\u83b7\u53d6\u6570\u636e\u3002</p> <pre><code>for xb,yb in train_dl:\n    pred = model(xb)\n</code></pre> <pre><code>model, opt = get_model()\n\nfor epoch in range(epochs):\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\nprint(loss_func(model(xb), yb))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>tensor(0.0817, grad_fn=&lt;NllLossBackward&gt;)\n</code></pre> <p>\u591a\u4e8fPyTorch\u4e2d\u7684<code>nn.Module</code>\uff0c<code>nn.Parameter</code>\uff0c<code>Dataset</code>\u548c<code>DataLoader</code>\uff0c\u6211\u4eec\u7684\u8bad\u7ec3\u4ee3\u7801\u53d8\u5f97\u975e\u5e38\u7b80\u6d01\u6613\u61c2\u3002\u4e0b\u9762\u6211\u4eec\u6765\u8bd5\u7740\u589e\u52a0\u4e00\u4e9b\u7528\u4e8e\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u6240\u5fc5\u9700\u7684\u7684\u57fa\u672c\u7279\u5f81\u3002</p>"},{"location":"1.0/nn_tutorial/#_1","title":"\u589e\u52a0\u9a8c\u8bc1\u96c6","text":"<p>\u5728\u7b2c\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u4ec5\u4ec5\u662f\u8bd5\u7740\u4e3a\u6211\u4eec\u7684\u8bad\u7ec3\u96c6\u6784\u5efa\u4e00\u4e2a\u5408\u7406\u7684\u8bad\u7ec3\u6b65\u9aa4\uff0c\u4f46\u5b9e\u9645\u4e0a\uff0c\u6211\u4eec\u59cb\u7ec8\u5e94\u8be5\u6709\u4e00\u4e2a\u9a8c\u8bc1\u96c6\u6765\u786e\u8ba4\u6a21\u578b\u662f\u5426\u8fc7\u62df\u5408\u3002</p> <p>\u6253\u4e71\u8bad\u7ec3\u6570\u636e\u7684\u987a\u5e8f\u901a\u5e38\u662f\u907f\u514d\u4e0d\u540c\u6279\u6570\u636e\u4e2d\u5b58\u5728\u76f8\u5173\u6027\u548c\u8fc7\u62df\u5408\u7684\u91cd\u8981\u6b65\u9aa4\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u65e0\u8bba\u662f\u5426\u6253\u4e71\u987a\u5e8f\u8ba1\u7b97\u51fa\u7684\u9a8c\u8bc1\u96c6\u635f\u5931\u503c\u90fd\u662f\u4e00\u6837\u7684\u3002\u9274\u4e8e\u6253\u4e71\u987a\u5e8f\u8fd8\u4f1a\u6d88\u8017\u989d\u5916\u7684\u65f6\u95f4\uff0c\u6240\u4ee5\u6253\u4e71\u9a8c\u8bc1\u96c6\u6570\u636e\u662f\u6ca1\u6709\u4efb\u4f55\u610f\u4e49\u7684\u3002</p> <p>\u6211\u4eec\u5728\u9a8c\u8bc1\u96c6\u4e0a\u7528\u5230\u7684\u6bcf\u6279\u6570\u636e\u7684\u6570\u91cf\u662f\u8bad\u7ec3\u96c6\u7684\u4e24\u500d\uff0c\u8fd9\u662f\u56e0\u4e3a\u5728\u9a8c\u8bc1\u96c6\u4e0a\u4e0d\u9700\u8981\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u8fd9\u6837\u5c31\u4f1a\u5360\u7528\u8f83\u5c0f\u7684\u5185\u5b58(\u56e0\u4e3a\u5b83\u5e76\u4e0d\u9700\u8981\u50a8\u5b58\u68af\u5ea6\uff09\u3002\u6211\u4eec\u5229\u7528\u4e86\u8fd9\u4e00\u70b9\uff0c\u4f7f\u7528\u4e86\u66f4\u5927\u7684batchsize\uff0c\u66f4\u5feb\u7684\u8ba1\u7b97\u51fa\u4e86\u635f\u5931\u503c\u3002</p> <pre><code>train_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nvalid_ds = TensorDataset(x_valid, y_valid)\nvalid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n</code></pre> <p>\u6211\u4eec\u5c06\u4f1a\u5728\u6bcf\u8f6e(epoch)\u7ed3\u675f\u540e\u8ba1\u7b97\u5e76\u8f93\u51fa\u9a8c\u8bc1\u96c6\u4e0a\u7684\u635f\u5931\u503c\u3002</p> <p>(\u6ce8\u610f\uff1a\u5728\u8bad\u7ec3\u524d\u6211\u4eec\u603b\u662f\u4f1a\u8c03\u7528<code>model.train()</code>\u51fd\u6570\uff0c\u5728\u63a8\u65ad\u4e4b\u524d\u8c03\u7528<code>model.eval()</code>\u51fd\u6570\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4f1a\u88ab<code>nn.BatchNorm2d</code>\uff0c<code>nn.Dropout</code>\u7b49\u5c42\u4f7f\u7528\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u9636\u6bb5\u7684\u51c6\u786e\u6027\u3002\uff09</p> <pre><code>model, opt = get_model()\n\nfor epoch in range(epochs):\n    model.train()\n    for xb, yb in train_dl:\n        pred = model(xb)\n        loss = loss_func(pred, yb)\n\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    model.eval()\n    with torch.no_grad():\n        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n\n    print(epoch, valid_loss / len(valid_dl))\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>0 tensor(0.2999)\n1 tensor(0.2742)\n</code></pre>"},{"location":"1.0/nn_tutorial/#fitget_data","title":"\u7f16\u5199fit()\u548cget_data()\u51fd\u6570","text":"<p>\u73b0\u5728\u6211\u4eec\u6765\u91cd\u6784\u4e00\u4e0b\u6211\u4eec\u81ea\u5df1\u7684\u51fd\u6570\u3002 \u6211\u4eec\u5728\u8ba1\u7b97\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u635f\u5931\u503c\u65f6\u6267\u884c\u4e86\u5dee\u4e0d\u591a\u7684\u8fc7\u7a0b\u4e24\u6b21\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u8fd9\u90e8\u5206\u4ee3\u7801\u63d0\u70bc\u6210\u4e00\u4e2a\u51fd\u6570<code>loss_batch</code>\uff0c\u7528\u6765\u8ba1\u7b97\u6bcf\u4e2a\u6279\u7684\u635f\u5931\u503c\u3002</p> <p>\u6211\u4eec\u4e3a\u8bad\u7ec3\u96c6\u4f20\u9012\u4e00\u4e2a\u4f18\u5316\u5668\u53c2\u6570\u6765\u6267\u884c\u53cd\u5411\u4f20\u64ad\u3002\u5bf9\u4e8e\u9a8c\u8bc1\u96c6\u6211\u4eec\u4e0d\u4f20\u4f18\u5316\u5668\u53c2\u6570\uff0c\u8fd9\u6837\u5c31\u4e0d\u4f1a\u6267\u884c\u53cd\u5411\u4f20\u64ad\u3002</p> <pre><code>def loss_batch(model, loss_func, xb, yb, opt=None):\n    loss = loss_func(model(xb), yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n</code></pre> <p><code>fit</code>\u6267\u884c\u4e86\u8bad\u7ec3\u6a21\u578b\u7684\u5fc5\u8981\u64cd\u4f5c\uff0c\u5e76\u5728\u6bcf\u8f6e(epoch)\u7ed3\u675f\u540e\u8ba1\u7b97\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u635f\u5931\u3002</p> <pre><code>import numpy as np\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)\n</code></pre> <p><code>get_data</code>\u8fd4\u56de\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u9700\u8981\u4f7f\u7528\u5230\u7684dataloaders\u3002</p> <pre><code>def get_data(train_ds, valid_ds, bs):\n    return (\n        DataLoader(train_ds, batch_size=bs, shuffle=True),\n        DataLoader(valid_ds, batch_size=bs * 2),\n    )\n</code></pre> <p>\u73b0\u5728\uff0c\u6211\u4eec\u53ea\u9700\u8981\u4e09\u884c\u4ee3\u7801\u5c31\u53ef\u4ee5\u83b7\u53d6\u6570\u636e\u3001\u62df\u5408\u6a21\u578b\u4e86\u3002</p> <pre><code>    train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n    model, opt = get_model()\n    fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>    0 0.2961075816631317\n    1 0.28558296990394594\n</code></pre> <p>\u73b0\u5728\u4f60\u80fd\u7528\u8fd9\u4e09\u884c\u4ee3\u7801\u8bad\u7ec3\u5404\u79cd\u5404\u6837\u7684\u6a21\u578b\u3002\u6211\u4eec\u6765\u770b\u4e00\u4e0b\u80fd\u5426\u7528\u5b83\u4eec\u6765\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN\uff09\u5427\uff01</p>"},{"location":"1.0/nn_tutorial/#_2","title":"\u5e94\u7528\u5230\u5377\u79ef\u795e\u7ecf\u7f51\u7edc","text":"<p>\u6211\u4eec\u73b0\u5728\u5c06\u8981\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u5377\u79ef\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u56e0\u4e3a\u524d\u9762\u7ae0\u8282\u4e2d\u6ca1\u6709\u4e00\u4e2a\u51fd\u6570\u6d89\u53ca\u5230\u6a21\u578b\u7684\u5177\u4f53\u5f62\u5f0f\uff0c\u6240\u4ee5\u6211\u4eec\u4e0d\u9700\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u5c31\u53ef\u4ee5\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528PyTorch\u4e2d\u9884\u5148\u5b9a\u4e49\u597d\u7684Conv2d\u7c7b\u4f5c\u4e3a\u6211\u4eec\u7684\u5377\u79ef\u5c42\u3002\u6211\u4eec\u5b9a\u4e49\u4e00\u4e2a\u6709\u4e09\u4e2a\u5377\u79ef\u5c42\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002\u6bcf\u4e2a\u5377\u79ef\u5c42\u4e4b\u540e\u4f1a\u6267\u884cReLu\u3002\u5728\u6700\u540e\uff0c\u6211\u4eec\u4f1a\u6267\u884c\u4e00\u4e2a\u5e73\u5747\u6c60\u5316\u64cd\u4f5c\u3002 (\u6ce8\u610f\uff1a<code>view</code>\u662fPyTorch\u7248\u7684numpy <code>reshape</code>\uff09</p> <pre><code>class Mnist_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, xb):\n        xb = xb.view(-1, 1, 28, 28)\n        xb = F.relu(self.conv1(xb))\n        xb = F.relu(self.conv2(xb))\n        xb = F.relu(self.conv3(xb))\n        xb = F.avg_pool2d(xb, 4)\n        return xb.view(-1, xb.size(1))\n\nlr = 0.1\n</code></pre> <p>Momentum\u662f\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7684\u4e00\u4e2a\u53d8\u578b\uff0c\u5b83\u5c06\u524d\u9762\u6b65\u9aa4\u7684\u66f4\u65b0\u4e5f\u8003\u8651\u5728\u5185\uff0c\u901a\u5e38\u80fd\u591f\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\u3002</p> <pre><code>model = Mnist_CNN()\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>0 0.3829730714321136\n1 0.2258522843360901\n</code></pre>"},{"location":"1.0/nn_tutorial/#nnsequential","title":"nn.Sequential","text":"<p><code>torch.nn</code>\u4e2d\u8fd8\u6709\u53e6\u4e00\u4e2a\u7c7b\u53ef\u4ee5\u65b9\u4fbf\u7684\u7528\u6765\u7b80\u5316\u6211\u4eec\u7684\u4ee3\u7801\uff1aSequential\u3002\u4e00\u4e2a<code>Sequential</code>\u5bf9\u8c61\u53ef\u4ee5\u5e8f\u5217\u5316\u8fd0\u884c\u5b83\u5305\u542b\u7684\u6a21\u5757\u3002\u8fd9\u662f\u4e00\u4e2a\u66f4\u7b80\u5355\u7684\u642d\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u5f0f\u3002</p> <p>\u60f3\u8981\u5145\u5206\u5229\u7528\u8fd9\u4e00\u4f18\u52bf\uff0c\u6211\u4eec\u8981\u80fd\u591f\u4f7f\u7528\u7ed9\u5b9a\u7684\u51fd\u6570\u8f7b\u677e\u7684\u5b9a\u4e49\u4e00\u4e2a\u81ea\u5b9a\u4e49\u5c42\u3002\u6bd4\u5982\u8bf4\uff0cPyTorch\u4e2d\u6ca1\u6709<code>view</code>\u5c42\uff0c\u6211\u4eec\u9700\u8981\u4e3a\u6211\u4eec\u7684\u7f51\u7edc\u5b9a\u4e49\u4e00\u4e2a\u3002 <code>Lambda</code>\u51fd\u6570\u5c06\u4f1a\u521b\u5efa\u4e00\u4e2a\u5c42\uff0c\u5e76\u5728\u540e\u9762\u4f7f\u7528<code>Sequential</code>\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u65f6\u5019\u7528\u5230\u3002</p> <pre><code>class Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)\n</code></pre> <p>\u4f7f\u7528<code>Sequential</code>\u521b\u5efa\u6a21\u578b\u975e\u5e38\u7b80\u5355\uff1a</p> <pre><code>model = nn.Sequential(\n    Lambda(preprocess),\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AvgPool2d(4),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nfit(epochs, model, loss_func, opt, train_dl, valid_dl)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>0 0.32739396529197695\n1 0.25574398956298827\n</code></pre>"},{"location":"1.0/nn_tutorial/#dataloader_1","title":"\u5bf9DataLoader\u8fdb\u884c\u5c01\u88c5","text":"<p>\u6211\u4eec\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5df2\u7ecf\u975e\u5e38\u7b80\u6d01\u4e86\uff0c\u4f46\u662f\u5b83\u53ea\u80fd\u8fd0\u884c\u5728MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u539f\u56e0\u5982\u4e0b\uff1a - \u5b83\u5047\u5b9a\u8f93\u5165\u662f\u957f\u5ea6\u4e3a2828\u7684\u5411\u91cf - \u5b83\u5047\u5b9a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6700\u7ec8\u8f93\u51fa\u662f\u5927\u5c0f\u4e3a44\u7684\u7f51\u683c(\u56e0\u4e3a\u8fd9\u662f\u5e73\u5747\u503c\u6c60\u5316\u64cd\u4f5c\u65f6\u6211\u4eec\u4f7f\u7528\u7684\u6838\u5927\u5c0f\uff09</p> <p>\u8ba9\u6211\u4eec\u6446\u8131\u8fd9\u4e24\u79cd\u5047\u5b9a\uff0c\u8fd9\u6837\u6211\u4eec\u7684\u6a21\u578b\u5c31\u53ef\u4ee5\u8fd0\u884c\u5728\u4efb\u610f\u76842d\u5355\u901a\u9053\u56fe\u50cf\u4e0a\u3002 \u9996\u5148\uff0c\u6211\u4eec\u53ef\u4ee5\u5220\u9664\u6700\u521d\u7684Lambda\u5c42\uff0c\u5e76\u5c06\u6570\u636e\u9884\u5904\u7406\u653e\u5728\u4e00\u4e2a\u751f\u6210\u5668\u4e2d\u3002</p> <pre><code>def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u7528<code>nn.AdaptiveAvgPool2d</code>\u66ff\u6362<code>nn.AvgPool2d</code>\uff0c\u8fd9\u4e2a\u51fd\u6570\u5141\u8bb8\u6211\u4eec\u5b9a\u4e49\u671f\u671b\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f\uff0c\u800c\u4e0d\u662f\u5b9a\u4e49\u5df2\u6709\u8f93\u5165\u7684\u5927\u5c0f\u3002 \u8fd9\u6837\u6211\u4eec\u7684\u6a21\u578b\u5c31\u53ef\u4ee5\u5904\u7406\u4efb\u610f\u5927\u5c0f\u7684\u8f93\u5165\u4e86\u3002</p> <pre><code>model = nn.Sequential(\n    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),\n    Lambda(lambda x: x.view(x.size(0), -1)),\n)\n\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n</code></pre> <p>\u6211\u4eec\u6765\u8bd5\u4e00\u4e0b\u65b0\u7684\u6a21\u578b\uff1a</p> <pre><code>fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>0 0.32888883714675904\n1 0.31000419993400574\n</code></pre>"},{"location":"1.0/nn_tutorial/#gpu","title":"\u4f7f\u7528\u4f60\u7684GPU","text":"<p>\u5982\u679c\u4f60\u6709\u5e78\u62e5\u6709\u652f\u6301CUDA\u7684GPU(\u4f60\u53ef\u4ee5\u79df\u4e00\u4e2a\uff0c\u5927\u90e8\u5206\u4e91\u670d\u52a1\u63d0\u4f9b\u5546\u7684\u4ef7\u683c\u4f7f0.5$/\u6bcf\u5c0f\u65f6\uff09\uff0c\u90a3\u4f60\u53ef\u4ee5\u7528GPU\u6765\u52a0\u901f\u4f60\u7684\u4ee3\u7801\u3002 \u9996\u5148\u68c0\u67e5\u4e00\u4e0b\u7684GPU\u662f\u5426\u53ef\u4ee5\u88abPyTorch\u8c03\u7528\uff1a</p> <pre><code>print(torch.cuda.is_available())\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>True\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u65b0\u5efa\u4e00\u4e2a\u8bbe\u5907\u5bf9\u8c61\uff1a</p> <pre><code>dev = torch.device(\n    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n</code></pre> <p>\u7136\u540e\u66f4\u65b0\u4e00\u4e0b<code>preprocess</code>\u51fd\u6570\u5c06\u6279\u8fd0\u7b97\u79fb\u5230GPU\u4e0a\u8ba1\u7b97</p> <pre><code>def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n\n\ntrain_dl, valid_dl = get_data(train_ds, valid_ds, bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)\n</code></pre> <p>\u6700\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u6a21\u578b\u79fb\u52a8\u5230GPU\u4e0a\u3002</p> <pre><code>model.to(dev)\nopt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n</code></pre> <p>\u4f60\u73b0\u5728\u5e94\u8be5\u80fd\u53d1\u73b0\u6a21\u578b\u8fd0\u7b97\u53d8\u5feb\u4e86\u3002</p> <pre><code>fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>0 0.21190375366210937\n1 0.18018000435829162\n</code></pre>"},{"location":"1.0/nn_tutorial/#_3","title":"\u603b\u7ed3","text":"<p>\u73b0\u5728\u6211\u4eec\u6709\u4e00\u4e2a\u7528PyTorch\u6784\u5efa\u7684\u901a\u7528\u6570\u636e\u7ba1\u9053\u548c\u8bad\u7ec3\u5faa\u73af\u53ef\u4ee5\u7528\u6765\u8bad\u7ec3\u5f88\u591a\u7c7b\u578b\u7684\u6a21\u578b\u3002 \u60f3\u8981\u77e5\u9053\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u6709\u591a\u4e48\u7b80\u5355\uff0c\u53ef\u4ee5\u53c2\u7167<code>mnist_sample</code>\u8fd9\u4e2a\u4f8b\u5b50\u3002</p> <p>\u5f53\u7136\u4e86\uff0c\u4f60\u53ef\u80fd\u8fd8\u60f3\u8981\u5728\u6a21\u578b\u4e2d\u52a0\u5165\u5f88\u591a\u5176\u5b83\u7684\u4e1c\u897f\uff0c\u6bd4\u5982\u6570\u636e\u6269\u5145\uff0c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u8bad\u7ec3\u76d1\u63a7\uff0c\u8fc1\u79fb\u5b66\u4e60\u7b49\u3002\u8fd9\u4e9b\u7279\u6027\u53ef\u4ee5\u5728fastai\u5e93\u4e2d\u83b7\u53d6\u5230\uff0c\u8be5\u5e93\u4f7f\u7528\u4e0e\u672c\u6559\u7a0b\u4e2d\u4ecb\u7ecd\u7684\u76f8\u540c\u8bbe\u8ba1\u65b9\u6cd5\u5f00\u53d1\u7684\uff0c\u4e3a\u60f3\u8981\u6269\u5c55\u6a21\u578b\u7684\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u5408\u7406\u7684\u540e\u7eed\u6b65\u9aa4\u3002</p> <p>\u5728\u6559\u7a0b\u7684\u5f00\u59cb\u90e8\u5206\uff0c\u6211\u4eec\u8bf4\u4e86\u8981\u901a\u8fc7\u4f8b\u5b50\u5bf9<code>torch.nn</code>\uff0c<code>torch.optim</code>\uff0c<code>Dataset</code>\u548c<code>DataLoader</code>\u8fdb\u884c\u8bf4\u660e\u3002 \u73b0\u5728\u6211\u4eec\u6765\u603b\u7ed3\u4e00\u4e0b\uff0c\u6211\u4eec\u90fd\u8bb2\u4e86\u4e9b\u4ec0\u4e48\uff1a - torch.nn     - <code>Module</code>\uff1a\u521b\u5efa\u4e00\u4e2a\u53ef\u8c03\u7528\u7684\uff0c\u5176\u8868\u73b0\u7c7b\u4f3c\u4e8e\u51fd\u6570\uff0c\u4f46\u53c8\u53ef\u4ee5\u5305\u542b\u72b6\u6001(\u6bd4\u5982\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u6743\u91cd\uff09\u7684\u5bf9\u8c61\u3002\u8be5\u5bf9\u8c61\u77e5\u9053\u5b83\u5305\u542b\u7684<code>Parameter</code>(s\uff09\uff0c\u5e76\u53ef\u4ee5\u5c06\u68af\u5ea6\u7f6e\u4e3a0\uff0c\u4ee5\u53ca\u5bf9\u68af\u5ea6\u8fdb\u884c\u5faa\u73af\u4ee5\u66f4\u65b0\u6743\u91cd\u7b49\u3002     - <code>Parameter</code>\uff1a\u662f\u4e00\u4e2a\u5bf9\u5f20\u91cf\u7684\u5c01\u88c5\uff0c\u5b83\u544a\u8bc9<code>Module</code>\u5728\u53cd\u5411\u4f20\u64ad\u9636\u6bb5\u66f4\u65b0\u6743\u91cd\u3002\u53ea\u6709\u8bbe\u7f6e\u4e86requires_grad\u5c5e\u6027\u7684\u5f20\u91cf\u4f1a\u88ab\u66f4\u65b0\u3002     - <code>functional</code>\uff1a\u4e00\u4e2a\u5305\u542b\u4e86\u68af\u5ea6\u51fd\u6570\u3001\u635f\u5931\u51fd\u6570\u7b49\u4ee5\u53ca\u4e00\u4e9b\u65e0\u72b6\u6001\u7684\u5c42\uff0c\u5982\u5377\u79ef\u5c42\u548c\u7ebf\u6027\u5c42\u7684\u6a21\u5757(\u901a\u5e38\u4f7f\u7528<code>F</code>\u4f5c\u4e3a\u5bfc\u5165\u7684\u522b\u540d\uff09\u3002 - <code>torch.optim</code>\uff1a\u5305\u542b\u4e86\u4f18\u5316\u5668\uff0c\u6bd4\u5982\u5728\u53cd\u5411\u9636\u6bb5\u66f4\u65b0<code>Parameter</code>\u4e2d\u6743\u91cd\u7684<code>SGD</code>\u3002 - <code>Dataset</code>\uff1a\u4e00\u4e2a\u62bd\u8c61\u63a5\u53e3\uff0c\u5305\u542b\u4e86<code>__len__</code>\u548c<code>__getitem__</code>\uff0c\u8fd8\u5305\u542b\u4e86PyTorch\u63d0\u4f9b\u7684\u7c7b\uff0c\u5982<code>TensorDataset</code>\u3002 - <code>DataLoader</code>\uff1a\u63a5\u53d7\u4efb\u610f\u7684<code>Dataset</code>\u5e76\u751f\u6210\u4e00\u4e2a\u8fed\u4ee3\u5668\u53ef\u4ee5\u6279\u91cf\u8fd4\u56de\u6570\u636e\u3002</p>"},{"location":"1.0/notes_autograd/","title":"\u81ea\u52a8\u6c42\u5bfc\u673a\u5236","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d \u6821\u9a8c\uff1aAlexJakin</p> <p>\u672c\u8bf4\u660e\u5c06\u6982\u8ff0autograd(\u81ea\u52a8\u6c42\u5bfc\uff09\u5982\u4f55\u5de5\u4f5c\u5e76\u8bb0\u5f55\u6bcf\u4e00\u6b65\u64cd\u4f5c\u3002\u4e86\u89e3\u8fd9\u4e9b\u5e76\u4e0d\u662f\u7edd\u5bf9\u5fc5\u8981\u7684\uff0c\u4f46\u6211\u4eec\u5efa\u8bae\u60a8\u719f\u6089\u5b83\uff0c\u56e0\u4e3a\u5b83\u5c06\u5e2e\u52a9\u4f60\u7f16\u5199\u66f4\u9ad8\u6548\uff0c\u66f4\u6e05\u6670\u7684\u7a0b\u5e8f\uff0c\u5e76\u53ef\u4ee5\u5e2e\u52a9\u60a8\u8fdb\u884c\u8c03\u8bd5\u3002  </p>"},{"location":"1.0/notes_autograd/#_2","title":"\u53cd\u5411\u6392\u9664\u5b50\u56fe","text":"<p>\u6bcf\u4e2a\u5f20\u91cf\u90fd\u6709\u4e00\u4e2a\u6807\u5fd7\uff1a<code>requires_grad</code>\uff0c\u5141\u8bb8\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u7ec6\u81f4\u5730\u6392\u9664\u5b50\u56fe\uff0c\u5e76\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\u3002    </p>"},{"location":"1.0/notes_autograd/#requires_grad","title":"<code>requires_grad</code>","text":"<p>\u53ea\u8981\u6709\u5355\u4e2a\u8f93\u5165\u8fdb\u884c\u68af\u5ea6\u8ba1\u7b97\u64cd\u4f5c\uff0c\u5219\u5176\u8f93\u51fa\u4e5f\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u3002\u76f8\u53cd\uff0c\u53ea\u6709\u5f53\u6240\u6709\u8f93\u5165\u90fd\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u65f6\uff0c\u8f93\u51fa\u624d\u4e0d\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u3002\u5982\u679c\u5176\u4e2d\u6240\u6709\u7684\u5f20\u91cf\u90fd\u4e0d\u9700\u8981\u8fdb\u884c\u68af\u5ea6\u8ba1\u7b97\uff0c\u540e\u5411\u8ba1\u7b97\u4e0d\u4f1a\u5728\u5b50\u56fe\u4e2d\u6267\u884c\u3002   </p> <pre><code>&gt;&gt;&gt; x = torch.randn(5, 5)  # requires_grad=False by default\n&gt;&gt;&gt; y = torch.randn(5, 5)  # requires_grad=False by default\n&gt;&gt;&gt; z = torch.randn((5, 5), requires_grad=True)\n&gt;&gt;&gt; a = x + y\n&gt;&gt;&gt; a.requires_grad\nFalse\n&gt;&gt;&gt; b = a + z\n&gt;&gt;&gt; b.requires_grad\nTrue\n\n</code></pre> <p>\u5f53\u4f60\u60f3\u8981\u51bb\u7ed3\u90e8\u5206\u6a21\u578b\u6216\u8005\u4e8b\u5148\u77e5\u9053\u4e0d\u4f1a\u4f7f\u7528\u67d0\u4e9b\u53c2\u6570\u7684\u68af\u5ea6\u65f6\uff0c\u8fd9\u4e2a<code>requires_grad</code>\u6807\u5fd7\u975e\u5e38\u6709\u7528\u3002\u4f8b\u5982\uff0c\u5982\u679c\u8981\u5fae\u8c03\u9884\u8bad\u7ec3\u7684CNN\uff0c\u53ea\u9700\u5728\u51bb\u7ed3\u7684\u57fa\u7840\u4e2d\u5207\u6362<code>requires_grad</code>\u6807\u5fd7\u5c31\u591f\u4e86\uff0c\u5e76\u4e14\u76f4\u5230\u8ba1\u7b97\u5230\u8fbe\u6700\u540e\u4e00\u5c42\uff0c\u624d\u4f1a\u4fdd\u5b58\u4e2d\u95f4\u7f13\u51b2\u533a\uff0c\uff0c\u5176\u4e2d\u4eff\u5c04\u53d8\u6362\u5c06\u4f7f\u7528\u6240\u9700\u8981\u68af\u5ea6\u7684\u6743\u91cd \uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u4e5f\u9700\u8981\u5b83\u4eec\u3002  </p> <pre><code>model = torchvision.models.resnet18(pretrained=True)\nfor param in model.parameters():\n    param.requires_grad = False\n# Replace the last fully-connected layer\n# Parameters of newly constructed modules have requires_grad=True by default\nmodel.fc = nn.Linear(512, 100)\n\n# Optimize only the classifier\noptimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n\n</code></pre>"},{"location":"1.0/notes_autograd/#_3","title":"\u81ea\u52a8\u6c42\u5bfc\u662f\u5982\u4f55\u8bb0\u5f55\u7f16\u7801\u5386\u53f2\u7684","text":"<p>\u81ea\u52a8\u6c42\u5bfc\u662f\u53cd\u5411\u81ea\u52a8\u5206\u5316\u7cfb\u7edf\u3002\u4ece\u6982\u5ff5\u4e0a\u8bb2\uff0c\u81ea\u52a8\u6c42\u5bfc\u4f1a\u8bb0\u5f55\u4e00\u4e2a\u56fe\u5f62\uff0c\u8bb0\u5f55\u5728\u6267\u884c\u64cd\u4f5c\u65f6\u521b\u5efa\u6570\u636e\u7684\u6240\u6709\u64cd\u4f5c\uff0c\u4e3a\u60a8\u63d0\u4f9b\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5176\u53f6\u5b50\u662f\u8f93\u5165\u5f20\u91cf\uff0c\u6839\u8282\u70b9\u662f\u8f93\u51fa\u5f20\u91cf\u3002\u901a\u8fc7\u4ece\u6839\u5230\u53f6\u8ddf\u8e2a\u6b64\u56fe\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u94fe\u6cd5\u5219\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6\u3002   </p> <p>\u5728\u5185\u90e8\uff0cautograd\u5c06\u6b64\u56fe\u8868\u793a\u4e3a<code>Function</code>\u5bf9\u8c61(\u5b9e\u9645\u8868\u8fbe\u5f0f\uff09\u7684\u56fe\u5f62\uff0c\u53ef\u4ee5\u7528\u6765\u8ba1\u7b97\u8bc4\u4f30\u56fe\u5f62\u7684\u7ed3\u679c\u3002 \u5f53\u8ba1\u7b97\u524d\u5411\u4f20\u64ad\u65f6\uff0c\u81ea\u52a8\u6c42\u5bfc\u540c\u65f6\u6267\u884c\u6240\u8bf7\u6c42\u7684\u8ba1\u7b97\u5e76\u5efa\u7acb\u8868\u793a\u8ba1\u7b97\u68af\u5ea6\u7684\u51fd\u6570\u7684\u56fe\u5f62(\u6bcf\u4e2a<code>torch.Tensor</code>\u7684<code>.grad_fn</code>\u5c5e\u6027\u662f\u8be5\u56fe\u7684\u5165\u53e3\u70b9\uff09\u3002\u5f53\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u65f6\uff0c\u6211\u4eec\u5728\u540e\u5411\u4f20\u64ad\u4e2d\u8bc4\u4f30\u8be5\u56fe\u4ee5\u8ba1\u7b97\u68af\u5ea6\u3002</p> <p>\u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u90fd\u4f1a\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u521b\u5efa\u56fe\u5f62\uff0c\u8fd9\u6b63\u662f\u5141\u8bb8\u4f7f\u7528\u4efb\u610fPython\u63a7\u5236\u6d41\u8bed\u53e5\u7684\u539f\u56e0\uff0c\u5b83\u53ef\u4ee5\u5728\u6bcf\u6b21\u8fed\u4ee3\u65f6\u66f4\u6539\u56fe\u5f62\u7684\u6574\u4f53\u5f62\u72b6\u548c\u5927\u5c0f\u3002 \u5728\u5f00\u59cb\u8bad\u7ec3\u4e4b\u524d\uff0c\u4e0d\u5fc5\u7f16\u7801\u6240\u6709\u53ef\u80fd\u7684\u8def\u5f84 - \u60a8\u8fd0\u884c\u7684\u662f\u60a8\u6240\u533a\u5206\u7684\u90e8\u5206\u3002  </p>"},{"location":"1.0/notes_autograd/#autogradin-place","title":"\u4f7f\u7528autograd\u8fdb\u884c<code>in-place</code>\u64cd\u4f5c","text":"<p>\u5728autograd\u4e2d\u652f\u6301<code>in-place</code>\u64cd\u4f5c\u662f\u4e00\u4ef6\u5f88\u96be\u7684\u4e8b\u60c5\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u4e0d\u9f13\u52b1\u4f7f\u7528\u5b83\u4eec\u3002Autograd\u79ef\u6781\u7684\u7f13\u51b2\u533a\u91ca\u653e\u548c\u91cd\u7528\u4f7f\u5176\u975e\u5e38\u9ad8\u6548\uff0c\u5b9e\u9645\u4e0a\u5728<code>in-place</code>\u64cd\u4f5c\u4f1a\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u60c5\u51b5\u4e5f\u975e\u5e38\u5c11\u3002\u9664\u975e\u5728\u5de8\u5927\u7684\u5185\u5b58\u538b\u529b\u4e0b\u8fd0\u884c\uff0c\u5426\u5219\u4f60\u53ef\u80fd\u6c38\u8fdc\u4e0d\u9700\u8981\u4f7f\u7528\u5b83\u4eec\u3002  </p> <p>\u9650\u5236<code>in-place</code>\u64cd\u4f5c\u9002\u7528\u6027\u7684\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u4e2a\uff1a  </p> <ol> <li>\u8fd9\u4e2a\u64cd\u4f5c\u53ef\u80fd\u4f1a\u8986\u76d6\u68af\u5ea6\u8ba1\u7b97\u6240\u9700\u7684\u503c\u3002  </li> <li>\u5b9e\u9645\u4e0a\uff0c\u6bcf\u4e2a<code>in-place</code>\u64cd\u4f5c\u9700\u8981\u91cd\u5199\u8ba1\u7b97\u56fe\u3002<code>out-of-place</code>\u7248\u672c\u53ea\u662f\u5206\u914d\u65b0\u5bf9\u8c61\u5e76\u4fdd\u7559\u5bf9\u65e7\u56fe\u7684\u5f15\u7528\uff0c\u800c<code>in-place</code>\u64cd\u4f5c\u5219\u9700\u8981\u5c06\u6240\u6709\u8f93\u5165\u7684creator\u66f4\u6539\u4e3a\u8868\u793a\u6b64\u64cd\u4f5c\u7684<code>Function</code>\u3002\u8fd9\u5c31\u6bd4\u8f83\u9ebb\u70e6\uff0c\u7279\u522b\u662f\u5982\u679c\u6709\u8bb8\u591a\u53d8\u91cf\u5f15\u7528\u540c\u4e00\u5b58\u50a8(\u4f8b\u5982\u901a\u8fc7\u7d22\u5f15\u6216\u8f6c\u7f6e\u521b\u5efa\u7684\uff09\uff0c\u5e76\u4e14\u5982\u679c\u88ab\u4fee\u6539\u8f93\u5165\u7684\u5b58\u50a8\u88ab\u4efb\u4f55\u5176\u4ed6\u5f20\u91cf\u5f15\u7528\uff0c\u8fd9\u6837\u7684\u8bdd\uff0c<code>in-place</code>\u51fd\u6570\u4f1a\u629b\u51fa\u9519\u8bef\u3002 </li> </ol>"},{"location":"1.0/notes_autograd/#in-place","title":"In-place\u6b63\u786e\u6027\u68c0\u67e5","text":"<p>\u6bcf\u4e00\u4e2a\u5f20\u91cf\u90fd\u6709\u4e00\u4e2a\u7248\u672c\u8ba1\u7b97\u5668\uff0c\u6bcf\u6b21\u5728\u4efb\u4f55\u64cd\u4f5c\u4e2d\u6807\u8bb0\u90fd\u4f1a\u9012\u589e\u3002 \u5f53<code>Function</code>\u4fdd\u5b58\u4efb\u4f55\u7528\u4e8e\u540e\u5411\u4f20\u64ad\u7684\u5f20\u91cf\u65f6\uff0c\u4e5f\u4f1a\u4fdd\u5b58\u5305\u542b\u5f20\u91cf\u7684\u7248\u672c\u8ba1\u6570\u5668\u3002\u4e00\u65e6\u8bbf\u95ee<code>self.saved_tensors</code>\u540e\uff0c\u5b83\u5c06\u88ab\u68c0\u67e5\uff0c\u5982\u679c\u5b83\u5927\u4e8e\u4fdd\u5b58\u7684\u503c\uff0c\u5219\u4f1a\u5f15\u53d1\u9519\u8bef\u3002\u8fd9\u53ef\u4ee5\u786e\u4fdd\u5982\u679c\u60a8\u4f7f\u7528<code>in-place</code>\u51fd\u6570\u800c\u6ca1\u6709\u770b\u5230\u4efb\u4f55\u9519\u8bef\uff0c\u5219\u53ef\u4ee5\u786e\u4fdd\u8ba1\u7b97\u51fa\u7684\u68af\u5ea6\u662f\u6b63\u786e\u7684\u3002</p>"},{"location":"1.0/notes_broadcasting/","title":"\u5e7f\u64ad\u8bed\u4e49","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d \u6821\u9a8c\uff1aAlexJakin</p> <p>\u8bb8\u8bb8\u591a\u591a\u7684PyTorch\u64cd\u4f5c\u90fd\u652f\u6301<code>NumPy Broadcasting Semantics</code>\u3002  </p> <p>\u7b80\u800c\u8a00\u4e4b\uff0c\u5982\u679cPyTorch\u64cd\u4f5c\u652f\u6301\u5e7f\u64ad\uff0c\u90a3\u4e48\u5b83\u7684Tensor\u53c2\u6570\u53ef\u4ee5\u81ea\u52a8\u6269\u5c55\u4e3a\u76f8\u540c\u7684\u7c7b\u578b\u5927\u5c0f(\u4e0d\u9700\u8981\u590d\u5236\u6570\u636e\uff09\u3002  </p>"},{"location":"1.0/notes_broadcasting/#_2","title":"\u4e00\u822c\u8bed\u4e49","text":"<p>\u5982\u679c\u9075\u5b88\u4ee5\u4e0b\u89c4\u5219\uff0c\u5219\u4e24\u4e2a\u5f20\u91cf\u662f\u201c\u53ef\u5e7f\u64ad\u7684\u201d\uff1a  </p> <ul> <li>\u6bcf\u4e2a\u5f20\u91cf\u81f3\u5c11\u6709\u4e00\u4e2a\u7ef4\u5ea6\uff1b</li> <li>\u904d\u5386\u5f20\u91cf\u7ef4\u5ea6\u5927\u5c0f\u65f6\uff0c\u4ece\u672b\u5c3e\u968f\u5f00\u59cb\u904d\u5386\uff0c\u4e24\u4e2a\u5f20\u91cf\u7684\u7ef4\u5ea6\u5927\u5c0f\u5fc5\u987b\u76f8\u7b49\uff0c\u5b83\u4eec\u5176\u4e2d\u4e00\u4e2a\u4e3a1\uff0c\u6216\u8005\u4e00\u4e2a\u4e0d\u5b58\u5728\u3002  </li> </ul> <p>\u4f8b\u5982\uff1a  </p> <pre><code>&gt;&gt;&gt; x=torch.empty(5,7,3)\n&gt;&gt;&gt; y=torch.empty(5,7,3)\n# \u76f8\u540c\u5f62\u72b6\u7684\u5f20\u91cf\u53ef\u4ee5\u88ab\u5e7f\u64ad(\u4e0a\u8ff0\u89c4\u5219\u603b\u662f\u6210\u7acb\u7684)\n\n&gt;&gt;&gt; x=torch.empty((0,))\n&gt;&gt;&gt; y=torch.empty(2,2)\n# x\u548cy\u4e0d\u80fd\u88ab\u5e7f\u64ad,\u56e0\u4e3ax\u6ca1\u6709\u7ef4\u5ea6\n\n# can line up trailing dimensions\n&gt;&gt;&gt; x=torch.empty(5,3,4,1)\n&gt;&gt;&gt; y=torch.empty(  3,1,1)\n# x\u548cy\u80fd\u591f\u5e7f\u64ad.\n# 1st trailing dimension: both have size 1\n# 2nd trailing dimension: y has size 1\n# 3rd trailing dimension: x size == y size\n# 4th trailing dimension: y dimension doesn't exist\n\n# \u4f46\u662f:\n&gt;&gt;&gt; x=torch.empty(5,2,4,1)\n&gt;&gt;&gt; y=torch.empty(  3,1,1)\n# x\u548cy\u4e0d\u80fd\u88ab\u5e7f\u64ad   (  )  \n\n</code></pre> <p>\u5982\u679cx,y\u4e24\u4e2a\u5f20\u91cf\u662f\u53ef\u4ee5\u5e7f\u64ad\u7684\uff0c\u5219\u901a\u8fc7\u8ba1\u7b97\u5f97\u5230\u7684\u5f20\u91cf\u5927\u5c0f\u9075\u5faa\u4ee5\u4e0b\u539f\u5219\uff1a   </p> <ul> <li>\u5982\u679cx\u548cy\u7684\u7ef4\u6570\u4e0d\u76f8\u7b49\uff0c\u5219\u5728\u7ef4\u5ea6\u8f83\u5c0f\u7684\u5f20\u91cf\u7684\u524d\u9762\u589e\u52a01\u4e2a\u7ef4\u5ea6\uff0c\u4f7f\u5b83\u4eec\u7684\u957f\u5ea6\u76f8\u7b49\u3002  </li> <li>\u7136\u540e,\u751f\u6210\u65b0\u5f20\u91cf\u7ef4\u5ea6\u7684\u5927\u5c0f\u662fx\u548cy\u5728\u6bcf\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u6700\u5927\u503c\u3002  </li> </ul> <p>\u4f8b\u5982\uff1a   </p> <pre><code># can line up trailing dimensions to make reading easier\n&gt;&gt;&gt; x=torch.empty(5,1,4,1)\n&gt;&gt;&gt; y=torch.empty(  3,1,1)\n&gt;&gt;&gt; (x+y).size()\ntorch.Size([5, 3, 4, 1])\n\n# but not necessary:\n&gt;&gt;&gt; x=torch.empty(1)\n&gt;&gt;&gt; y=torch.empty(3,1,7)\n&gt;&gt;&gt; (x+y).size()\ntorch.Size([3, 1, 7])\n\n&gt;&gt;&gt; x=torch.empty(5,2,4,1)\n&gt;&gt;&gt; y=torch.empty(3,1,1)\n&gt;&gt;&gt; (x+y).size()\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n</code></pre>"},{"location":"1.0/notes_broadcasting/#in-place","title":"In - place \u8bed\u4e49","text":"<p>\u4e00\u4e2a\u590d\u6742\u56e0\u7d20\u662fin-place\u64cd\u4f5c\u4e0d\u5141\u8bb8in-place\u5f20\u91cf\u50cf\u5e7f\u64ad\u90a3\u6837\u6539\u53d8\u5f62\u72b6\u3002  </p> <p>\u4f8b\u5982\uff1a  </p> <pre><code>&gt;&gt;&gt; x=torch.empty(5,3,4,1)\n&gt;&gt;&gt; y=torch.empty(3,1,1)\n&gt;&gt;&gt; (x.add_(y)).size()\ntorch.Size([5, 3, 4, 1])\n\n# but:\n&gt;&gt;&gt; x=torch.empty(1,3,1)\n&gt;&gt;&gt; y=torch.empty(3,1,7)\n&gt;&gt;&gt; (x.add_(y)).size()\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n\n</code></pre>"},{"location":"1.0/notes_broadcasting/#_3","title":"\u5411\u540e\u517c\u5bb9\u6027","text":"<p>PyTorch\u7684\u65e9\u671f\u7248\u672c\u5141\u8bb8\u67d0\u4e9b\u9010\u70b9\u51fd\u6570\u5728\u5177\u6709\u4e0d\u540c\u5f62\u72b6\u7684\u5f20\u91cf\u4e0a\u6267\u884c\uff0c\u53ea\u8981\u6bcf\u4e2a\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u6570\u91cf\u76f8\u7b49\u5373\u53ef\u3002 \u7136\u540e\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5f20\u91cf\u89c6\u4e3a1\u7ef4\u6765\u6267\u884c\u9010\u70b9\u8fd0\u7b97\u3002PyTorch\u73b0\u5728\u652f\u6301\u5e7f\u64ad\uff0c\u5e76\u4e14\u201c1\u7ef4\u201d\u9010\u70b9\u884c\u4e3a\u88ab\u8ba4\u4e3a\u5df2\u5f03\u7528\uff0c\u5e76\u4e14\u5728\u5f20\u91cf\u4e0d\u53ef\u5e7f\u64ad\u4f46\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\u7684\u60c5\u51b5\u4e0b\u5c06\u751f\u6210Python\u8b66\u544a\u3002  </p> <p>\u6ce8\u610f\uff0c\u5728\u4e24\u4e2a\u5f20\u91cf\u4e0d\u5177\u6709\u76f8\u540c\u5f62\u72b6\u4f46\u662f\u53ef\u5e7f\u64ad\u5e76\u4e14\u5177\u6709\u76f8\u540c\u6570\u91cf\u5143\u7d20\u7684\u60c5\u51b5\u4e0b\uff0c\u5e7f\u64ad\u7684\u5f15\u5165\u53ef\u80fd\u5bfc\u81f4\u5411\u540e\u4e0d\u517c\u5bb9\u3002\u4f8b\u5982\uff1a    </p> <pre><code>&gt;&gt;&gt; torch.add(torch.ones(4,1), torch.randn(4))\n\n</code></pre> <p>\u4ee5\u524d\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e00\u4e2atorch.Size([4,1]\uff09\u7684Tensor\uff0c\u4f46\u73b0\u5728\u4f1a\u4ea7\u751f\u4e00\u4e2atorch.Size([4,4]\uff09\u8fd9\u6837\u7684Tensor\u3002 \u4e3a\u4e86\u5e2e\u52a9\u8bc6\u522b\u4ee3\u7801\u4e2d\u53ef\u80fd\u5b58\u5728\u5e7f\u64ad\u5f15\u8d77\u7684\u5411\u540e\u4e0d\u517c\u5bb9\u6027\u7684\u60c5\u51b5\uff0c\u60a8\u53ef\u4ee5\u8bbe\u7f6e<code>torch.utils.backcompat.broadcast_warning.enabled</code> \u4e3a <code>True</code>\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751fpython\u8b66\u544a\u3002  </p> <pre><code>&gt;&gt;&gt; torch.utils.backcompat.broadcast_warning.enabled=True\n&gt;&gt;&gt; torch.add(torch.ones(4,1), torch.ones(4))\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n\n</code></pre>"},{"location":"1.0/notes_cuda/","title":"CUDA \u8bed\u4e49","text":"<p>\u8bd1\u8005\uff1a\u7247\u523b \u6821\u9a8c\uff1aAlexJakin</p> <p><code>torch.cuda</code> \u7528\u4e8e\u8bbe\u7f6e\u548c\u8fd0\u884c CUDA \u64cd\u4f5c\u3002\u5b83\u4f1a\u8ddf\u8e2a\u5f53\u524d\u9009\u5b9a\u7684GPU\uff0c\u5e76\u4e14\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u5728\u8be5\u8bbe\u5907\u4e0a\u521b\u5efa\u60a8\u5206\u914d\u7684\u6240\u6709 CUDA tensors\u3002\u53ef\u4ee5\u4f7f\u7528 <code>torch.cuda.device</code> \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u66f4\u6539\u6240\u9009\u8bbe\u5907\u3002</p> <p>\u4f46\u662f\uff0c\u4e00\u65e6\u5206\u914d\u4e86 tensor\uff0c\u5c31\u53ef\u4ee5\u5bf9\u5176\u8fdb\u884c\u64cd\u4f5c\u800c\u4e0d\u7ba1\u6240\u9009\u62e9\u7684\u8bbe\u5907\u5982\u4f55\uff0c\u7ed3\u679c\u5c06\u59cb\u7ec8\u4e0e tensor \u653e\u5728\u540c\u4e00\u8bbe\u5907\u4e0a\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4e0d\u5141\u8bb8\u8de8 GPU \u64cd\u4f5c\uff0c\u9664\u4e86 copy_() \u5177\u6709\u7c7b\u4f3c\u590d\u5236\u529f\u80fd\u7684\u5176\u4ed6\u65b9\u6cd5\uff0c\u4f8b\u5982 to() \u548c cuda()\u3002\u9664\u975e\u60a8\u542f\u7528\u70b9\u5bf9\u70b9\u5185\u5b58\u8bbf\u95ee\uff0c\u5426\u5219\u4efb\u4f55\u5c1d\u8bd5\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u4f20\u64ad\u7684 tensor \u4e0a\u542f\u52a8\u64cd\u4f5c\u90fd\u4f1a\u5f15\u53d1\u9519\u8bef\u3002</p> <p>\u4e0b\u9762\u6211\u4eec\u7528\u4e00\u4e2a\u5c0f\u4f8b\u5b50\u6765\u5c55\u793a:</p> <pre><code>cuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n</code></pre>"},{"location":"1.0/notes_cuda/#_1","title":"\u5f02\u6b65\u6267\u884c","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cGPU \u64cd\u4f5c\u662f\u5f02\u6b65\u7684\u3002\u5f53\u60a8\u8c03\u7528\u4f7f\u7528 GPU \u7684\u51fd\u6570\u65f6\uff0c\u64cd\u4f5c\u5c06\u6392\u5165\u7279\u5b9a\u8bbe\u5907\uff0c\u4f46\u4e0d\u4e00\u5b9a\u8981\u5728\u4ee5\u540e\u6267\u884c\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u5e76\u884c\u6267\u884c\u66f4\u591a\u8ba1\u7b97\uff0c\u5305\u62ec\u5728 CPU \u6216\u5176\u4ed6 GPU \u4e0a\u7684\u64cd\u4f5c\u3002</p> <p>\u901a\u5e38\uff0c\u5f02\u6b65\u8ba1\u7b97\u7684\u6548\u679c\u5bf9\u4e8e\u8c03\u7528\u8005\u662f\u4e0d\u53ef\u89c1\u7684\uff0c\u56e0\u4e3a (1) \u6bcf\u4e2a\u8bbe\u5907\u6309\u7167\u5b83\u4eec\u6392\u961f\u7684\u987a\u5e8f\u6267\u884c\u64cd\u4f5c\uff0c\u4ee5\u53ca (2) PyTorch \u5728 CPU \u548c GPU \u4e4b\u95f4\u6216\u4e24\u4e2a GPU \u4e4b\u95f4\u590d\u5236\u6570\u636e\u65f6\u81ea\u52a8\u6267\u884c\u5fc5\u8981\u7684\u540c\u6b65\u3002\u56e0\u6b64\uff0c\u8ba1\u7b97\u5c06\u5982\u540c\u6bcf\u4e2a\u64cd\u4f5c\u540c\u6b65\u6267\u884c\u4e00\u6837\u8fdb\u884c\u3002</p> <p>\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u5f3a\u5236\u8fdb\u884c\u540c\u6b65\u8ba1\u7b97 <code>CUDA_LAUNCH_BLOCKING=1</code>\u3002\u8fd9\u5728 GPU \u4e0a\u53d1\u751f\u9519\u8bef\u65f6\u975e\u5e38\u65b9\u4fbf\u3002(\u4f7f\u7528\u5f02\u6b65\u6267\u884c\u65f6\uff0c\u76f4\u5230\u5b9e\u9645\u6267\u884c\u64cd\u4f5c\u540e\u624d\u4f1a\u62a5\u544a\u6b64\u7c7b\u9519\u8bef\uff0c\u56e0\u6b64\u5806\u6808\u8ddf\u8e2a\u4e0d\u4f1a\u663e\u793a\u8bf7\u6c42\u7684\u4f4d\u7f6e\u3002\uff09</p> <p>\u5f02\u6b65\u8ba1\u7b97\u7684\u7ed3\u679c\u662f\u6ca1\u6709\u540c\u6b65\u7684\u65f6\u95f4\u6d4b\u91cf\u662f\u4e0d\u7cbe\u786e\u7684\u3002\u8981\u83b7\u5f97\u7cbe\u786e\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e94\u8be5\u5728\u6d4b\u91cf\u4e4b\u524d\u8c03\u7528<code>torch.cuda.synchronize()</code>\uff0c\u6216\u8005\u4f7f\u7528<code>torch.cuda.Event</code>\u8bb0\u5f55\u65f6\u95f4\u5982\u4e0b\uff1a</p> <pre><code>start_event = torch.cuda.Event(enable_timing=True)\nend_event = torch.cuda.Event(enable_timing=True)\nstart_event.record()\n\n# \u5728\u8fd9\u91cc\u6267\u884c\u4e00\u4e9b\u64cd\u4f5c\n\nend_event.record()\ntorch.cuda.synchronize()  # Wait for the events to be recorded!\nelapsed_time_ms = start_event.elapsed_time(end_event)\n</code></pre> <p>\u4f5c\u4e3a\u4e00\u4e2a\u4f8b\u5916\uff0c\u6709\u51e0\u4e2a\u51fd\u6570\uff0c\u4f8b\u5982 <code>to()</code> \u548c <code>copy_()</code> \u5141\u8bb8\u4e00\u4e2a\u663e\u5f0f <code>non_blocking</code> \u53c2\u6570\uff0c\u5b83\u5141\u8bb8\u8c03\u7528\u8005\u5728\u4e0d\u9700\u8981\u65f6\u7ed5\u8fc7\u540c\u6b65\u3002\u53e6\u4e00\u4e2a\u4f8b\u5916\u662f CUDA streams\uff0c\u5982\u4e0b\u6240\u8ff0\u3002</p>"},{"location":"1.0/notes_cuda/#cuda-streams","title":"CUDA streams","text":"<p>CUDA stream \u662f\u6267\u884c\u7684\u7ebf\u6027\u5e8f\u5217\u5c5e\u4e8e\u7279\u5b9a\u7684\u8bbe\u5907\u3002\u60a8\u901a\u5e38\u4e0d\u9700\u8981\u663e\u5f0f\u521b\u5efa\u4e00\u4e2a\uff1a\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u8bbe\u5907\u4f7f\u7528\u81ea\u5df1\u7684 \u201cdefault\u201d stream\u3002</p> <p>\u6bcf\u4e2a\u6d41\u5185\u7684\u64cd\u4f5c\u6309\u521b\u5efa\u987a\u5e8f\u8fdb\u884c\u5e8f\u5217\u5316\uff0c\u4f46\u4e0d\u540c\u6d41\u7684\u64cd\u4f5c\u53ef\u4ee5\u6309\u4efb\u4f55\u76f8\u5bf9\u987a\u5e8f\u540c\u65f6\u6267\u884c\uff0c\u9664\u975e\u4f7f\u7528\u663e\u5f0f\u540c\u6b65\u529f\u80fd(\u5982  <code>synchronize()</code> \u6216 <code>wait_stream()</code>)\u3002\u4f8b\u5982\uff0c\u4ee5\u4e0b\u4ee3\u7801\u4e0d\u6b63\u786e:</p> <pre><code>cuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\nwith torch.cuda.stream(s):\n    # sum() may start execution before normal_() finishes!\n    B = torch.sum(A)\n\n</code></pre> <p>\u5f53 \u201ccurrent stream\u201d \u662f default stream \u65f6\uff0cPyTorch \u5728\u6570\u636e\u79fb\u52a8\u65f6\u81ea\u52a8\u6267\u884c\u5fc5\u8981\u7684\u540c\u6b65\uff0c\u5982\u4e0a\u6240\u8ff0\u3002\u4f46\u662f\uff0c\u4f7f\u7528 non-default streams \u65f6\uff0c\u7528\u6237\u6709\u8d23\u4efb\u786e\u4fdd\u6b63\u786e\u540c\u6b65\u3002</p>"},{"location":"1.0/notes_cuda/#_2","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>PyTorch \u4f7f\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668\u6765\u52a0\u901f\u5185\u5b58\u5206\u914d\u3002\u8fd9\u5141\u8bb8\u5728\u6ca1\u6709\u8bbe\u5907\u540c\u6b65\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u91ca\u653e\u5185\u5b58\u3002\u4f46\u662f\uff0c\u5206\u914d\u5668\u7ba1\u7406\u7684\u672a\u4f7f\u7528\u5185\u5b58\u4ecd\u5c06\u663e\u793a\u4e3a\u4f7f\u7528 <code>nvidia-smi</code>\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>memory_allocated()</code> \u548c <code>max_memory_allocated()</code> \u76d1\u89c6\u5f20\u91cf\u5360\u7528\u7684\u5185\u5b58\uff0c\u5e76\u4f7f\u7528 <code>memory_cached()</code> \u548c <code>max_memory_cached()</code> \u76d1\u89c6\u7f13\u5b58\u5206\u914d\u5668\u7ba1\u7406\u7684\u5185\u5b58\u3002\u8c03\u7528 <code>empty_cache()</code> \u53ef\u4ee5\u4ece PyTorch \u91ca\u653e\u6240\u6709 unused \u7684\u7f13\u5b58\u5185\u5b58\uff0c\u4ee5\u4fbf\u5176\u4ed6 GPU \u5e94\u7528\u7a0b\u5e8f\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u3002\u4f46\u662f\uff0ctensor \u5360\u7528\u7684 GPU \u5185\u5b58\u4e0d\u4f1a\u88ab\u91ca\u653e\uff0c\u56e0\u6b64\u65e0\u6cd5\u589e\u52a0 PyTorch \u53ef\u7528\u7684 GPU \u5185\u5b58\u91cf\u3002</p>"},{"location":"1.0/notes_cuda/#_3","title":"\u6700\u4f73\u505a\u6cd5","text":""},{"location":"1.0/notes_cuda/#_4","title":"\u8bbe\u5907\u65e0\u5173\u7684\u4ee3\u7801","text":"<p>\u7531\u4e8e PyTorch \u7684\u7ed3\u6784\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u663e\u5f0f\u7f16\u5199\u8bbe\u5907\u65e0\u5173(CPU\u6216GPU\uff09\u4ee3\u7801; \u4e00\u4e2a\u4f8b\u5b50\u53ef\u80fd\u662f\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\u4f5c\u4e3a\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001\u3002</p> <p>\u7b2c\u4e00\u6b65\u662f\u786e\u5b9a\u662f\u5426\u5e94\u8be5\u4f7f\u7528GPU\u3002\u5e38\u89c1\u7684\u6a21\u5f0f\u662f\u4f7f\u7528Python\u7684 <code>argparse</code> \u6a21\u5757\u8bfb\u5165\u7528\u6237\u53c2\u6570\uff0c\u5e76\u6709\u4e00\u4e2a\u53ef\u7528\u4e8e\u7981\u7528 CUDA \u7684\u6807\u5fd7\uff0c\u5e76\u7ed3\u5408\u4f7f\u7528 <code>is_available()</code>\u3002\u5728\u4e0b\u6587\u4e2d\uff0c<code>args.device</code> \u7ed3\u679c <code>torch.device</code> \u53ef\u4ee5\u7528\u4e8e\u5c06 tensor \u79fb\u52a8\u5230 CPU \u6216 CUDA \u7684\u5bf9\u8c61\u3002</p> <pre><code>import argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='PyTorch Example')\nparser.add_argument('--disable-cuda', action='store_true',\n                    help='Disable CUDA')\nargs = parser.parse_args()\nargs.device = None\nif not args.disable_cuda and torch.cuda.is_available():\n    args.device = torch.device('cuda')\nelse:\n    args.device = torch.device('cpu')\n\n</code></pre> <p>\u73b0\u5728 <code>args.device</code> \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u5728\u6240\u9700\u7684\u8bbe\u5907\u4e0a\u521b\u5efa Tensor\u3002</p> <pre><code>x = torch.empty((8, 42), device=args.device)\nnet = Network().to(device=args.device)\n\n</code></pre> <p>\u8fd9\u53ef\u4ee5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u7528\u4e8e\u4ea7\u751f\u4e0e\u8bbe\u5907\u65e0\u5173\u7684\u4ee3\u7801\u3002\u4ee5\u4e0b\u662f\u4f7f\u7528 dataloader \u65f6\u7684\u793a\u4f8b:</p> <pre><code>cuda0 = torch.device('cuda:0')  # CUDA GPU 0\nfor i, x in enumerate(train_loader):\n    x = x.to(cuda0)\n\n</code></pre> <p>\u5728\u7cfb\u7edf\u4e0a\u4f7f\u7528\u591a\u4e2a GPU \u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>CUDA_VISIBLE_DEVICES</code> \u73af\u5883\u6807\u5fd7\u6765\u7ba1\u7406 PyTorch \u53ef\u7528\u7684 GPU\u3002\u5982\u4e0a\u6240\u8ff0\uff0c\u8981\u624b\u52a8\u63a7\u5236\u521b\u5efa\u5f20\u91cf\u7684GPU\uff0c\u6700\u4f73\u505a\u6cd5\u662f\u4f7f\u7528 <code>torch.cuda.device</code> \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> <pre><code>print(\"Outside device is 0\")  # On device 0 (default in most scenarios)\nwith torch.cuda.device(1):\n    print(\"Inside device is 1\")  # On device 1\nprint(\"Outside device is still 0\")  # On device 0\n\n</code></pre> <p>\u5982\u679c\u4f60\u6709\u4e00\u4e2a tensor \u5e76\u4e14\u60f3\u5728\u540c\u4e00\u4e2a\u8bbe\u5907\u4e0a\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u7c7b\u578b\u7684\u65b0 tensor\uff0c\u90a3\u4e48\u4f60\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a <code>torch.Tensor.new_*</code> \u65b9\u6cd5(\u53c2\u89c1\u53c2\u8003\u8d44\u6599 <code>torch.Tensor</code>\uff09\u3002\u867d\u7136\u524d\u9762\u63d0\u5230\u7684 <code>torch.*</code> factory \u51fd\u6570 (Creation Ops)\u4f9d\u8d56\u4e8e\u5f53\u524d GPU \u4e0a\u4e0b\u6587\u548c\u60a8\u4f20\u5165\u7684\u5c5e\u6027\u53c2\u6570\uff0c\u4f46 <code>torch.Tensor.new_*</code> \u65b9\u6cd5\u4f1a\u4fdd\u7559\u8bbe\u5907\u548c tensor \u7684\u5176\u4ed6\u5c5e\u6027\u3002</p> <p>\u5728\u521b\u5efa\u5728\u524d\u5411\u4f20\u9012\u671f\u95f4\u9700\u8981\u5728\u5185\u90e8\u521b\u5efa\u65b0 tensor \u7684\u6a21\u5757\u65f6\uff0c\u8fd9\u662f\u5efa\u8bae\u7684\u505a\u6cd5\u3002</p> <pre><code>cuda = torch.device('cuda')\nx_cpu = torch.empty(2)\nx_gpu = torch.empty(2, device=cuda)\nx_cpu_long = torch.empty(2, dtype=torch.int64)\n\ny_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\nprint(y_cpu)\n\n    tensor([[ 0.3000,  0.3000],\n            [ 0.3000,  0.3000],\n            [ 0.3000,  0.3000]])\n\ny_gpu = x_gpu.new_full([3, 2], fill_value=-5)\nprint(y_gpu)\n\n    tensor([[-5.0000, -5.0000],\n            [-5.0000, -5.0000],\n            [-5.0000, -5.0000]], device='cuda:0')\n\ny_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\nprint(y_cpu_long)\n\n    tensor([[ 1,  2,  3]])\n\n</code></pre> <p>\u5982\u679c\u4f60\u60f3\u521b\u5efa\u4e00\u4e2a\u4e0e\u53e6\u4e00\u4e2a tensor \u76f8\u540c\u7c7b\u578b\u548c\u5927\u5c0f\u7684 tensor\uff0c\u5e76\u7528\u4e00\u4e2a\u6216\u96f6\u586b\u5145\u5b83\uff0c<code>ones_like()</code> \u6216 <code>zeros_like()</code> \u4f5c\u4e3a\u65b9\u4fbf\u7684\u8f85\u52a9\u51fd\u6570(\u4e5f\u4fdd\u7559 Tensor \u7684 <code>torch.device</code> \u548c <code>torch.dtype</code>  )\u63d0\u4f9b\u3002</p> <pre><code>x_cpu = torch.empty(2, 3)\nx_gpu = torch.empty(2, 3)\n\ny_cpu = torch.ones_like(x_cpu)\ny_gpu = torch.zeros_like(x_gpu)\n\n</code></pre>"},{"location":"1.0/notes_cuda/#_5","title":"\u4f7f\u7528\u56fa\u5b9a\u5185\u5b58\u7f13\u51b2\u533a","text":"<p>\u5f53\u6e90\u81ea\u56fa\u5b9a(\u9875\u9762\u9501\u5b9a\uff09\u5185\u5b58\u65f6\uff0c\u4e3b\u673a\u5230 GPU \u526f\u672c\u7684\u901f\u5ea6\u8981\u5feb\u5f97\u591a\u3002CPU tensor \u548c\u5b58\u50a8\u5668\u516c\u5f00\u4e00\u79cd <code>pin_memory()</code> \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de\u5bf9\u8c61\u7684\u526f\u672c\uff0c\u6570\u636e\u653e\u5728\u56fa\u5b9a\u533a\u57df\u4e2d\u3002</p> <p>\u6b64\u5916\uff0c\u4e00\u65e6\u60a8\u56fa\u5b9a\u5f20\u91cf\u6216\u5b58\u50a8\uff0c\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528\u5f02\u6b65GPU\u526f\u672c\u3002\u53ea\u9700\u5c06\u4e00\u4e2a\u989d\u5916\u7684 <code>non_blocking=True</code> \u53c2\u6570\u4f20\u9012\u7ed9\u4e00\u4e2a <code>cuda()</code> \u8c03\u7528\u3002\u8fd9\u53ef\u4ee5\u7528\u4e8e\u901a\u8fc7\u8ba1\u7b97\u91cd\u53e0\u6570\u636e\u4f20\u8f93\u3002</p> <p>\u60a8\u53ef\u4ee5  <code>DataLoader</code> \u901a\u8fc7\u4f20\u9012 <code>pin_memory=True</code> \u7ed9\u6784\u9020\u51fd\u6570\u4f7f\u8fd4\u56de\u6279\u5904\u7406\u653e\u7f6e\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d\u3002</p>"},{"location":"1.0/notes_cuda/#nndataparallel","title":"\u4f7f\u7528 nn.DataParallel \u800c\u4e0d\u662f\u5e76\u884c\u5904\u7406","text":"<p>\u6d89\u53ca\u6279\u91cf\u8f93\u5165\u548c\u591a\u4e2a GPU \u7684\u5927\u591a\u6570\u7528\u4f8b\u5e94\u9ed8\u8ba4 <code>DataParallel</code> \u4f7f\u7528\u591a\u4e2aGPU\u3002\u5373\u4f7f\u4f7f\u7528GIL\uff0c\u5355\u4e2a Python \u8fdb\u7a0b\u4e5f\u53ef\u4ee5\u4f7f\u591a\u4e2a GPU \u9971\u548c\u3002</p> <p>\u4ece\u7248\u672c 0.1.9 \u5f00\u59cb\uff0c\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5927\u91cf GPUs (8+)\u3002\u4f46\u662f\uff0c\u8fd9\u662f\u4e00\u4e2a\u6b63\u5728\u79ef\u6781\u5f00\u53d1\u7684\u5df2\u77e5\u95ee\u9898\u3002\u4e00\u5982\u65e2\u5f80\uff0c\u6d4b\u8bd5\u60a8\u7684\u7528\u4f8b\u3002</p> <p>\u4f7f\u7528\u5e26\u6709 <code>multiprocessing</code> \u7684 CUDA \u6a21\u578b\u6709\u4e00\u4e9b\u91cd\u8981\u7684\u6ce8\u610f\u4e8b\u9879 ; \u9664\u975e\u6ce8\u610f\u5b8c\u5168\u6ee1\u8db3\u6570\u636e\u5904\u7406\u8981\u6c42\uff0c\u5426\u5219\u60a8\u7684\u7a0b\u5e8f\u53ef\u80fd\u4f1a\u6709\u4e0d\u6b63\u786e\u6216\u672a\u5b9a\u4e49\u7684\u884c\u4e3a\u3002</p>"},{"location":"1.0/notes_extending/","title":"\u6269\u5c55PyTorch","text":"<p>\u8bd1\u8005\uff1aPEGASUS1993</p> <p>\u672c\u7ae0\u4e2d\uff0c\u5c06\u8981\u4ecb\u7ecd\u4f7f\u7528\u6211\u4eec\u7684C\u5e93\u5982\u4f55\u6269\u5c55<code>torch.nn</code>\uff0c<code>torch.autograd</code>\u548c\u7f16\u5199\u81ea\u5b9a\u4e49\u7684<code>C</code>\u6269\u5c55\u5de5\u5177\u3002</p>"},{"location":"1.0/notes_extending/#torchautograd","title":"\u6269\u5c55torch.autograd","text":"<p>\u6dfb\u52a0\u64cd\u4f5c<code>autograd</code>\u9700\u8981<code>Function</code>\u4e3a\u6bcf\u4e2a\u64cd\u4f5c\u5b9e\u73b0\u4e00\u4e2a\u65b0\u7684\u5b50\u7c7b\u3002\u56de\u60f3\u4e00\u4e0b\uff0c<code>Function</code>\u4f7f\u7528<code>autograd</code>\u6765\u8ba1\u7b97\u7ed3\u679c\u548c\u68af\u5ea6\uff0c\u5e76\u5bf9\u64cd\u4f5c\u5386\u53f2\u8fdb\u884c\u7f16\u7801\u3002\u6bcf\u4e2a\u65b0\u529f\u80fd\u90fd\u9700\u8981\u60a8\u5b9e\u73b0\u4e24\u79cd\u65b9\u6cd5\uff1a</p> <ul> <li> <p><code>forward()</code> - \u6267\u884c\u64cd\u4f5c\u7684\u4ee3\u7801\u3002\u5982\u679c\u60a8\u6307\u5b9a\u4e86\u9ed8\u8ba4\u503c\uff0c\u5219\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u4f7f\u7528\u4efb\u610f\u53c2\u6570\uff0c\u5176\u4e2d\u4e00\u4e9b\u53c2\u6570\u53ef\u9009\u3002\u8fd9\u91cc\u652f\u6301\u5404\u79cd<code>Python</code>\u5bf9\u8c61\u3002<code>Variable</code>\u53c2\u6570\u5728\u8c03\u7528\u4e4b\u524d\u4f1a\u88ab\u8f6c\u6362<code>Tensor</code>\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u4f7f\u7528\u60c5\u51b5\u5c06\u5728<code>graph</code>\u4e2d\u6ce8\u518c\u3002\u8bf7\u6ce8\u610f\uff0c\u6b64\u903b\u8f91\u4e0d\u4f1a\u904d\u5386<code>lists</code>/<code>dicts</code>/\u548c\u5176\u4ed6\u4efb\u4f55\u6570\u636e\u7684\u7ed3\u6784\uff0c\u5e76\u4e14\u53ea\u8003\u8651\u88ab\u76f4\u63a5\u8c03\u7528\u7684<code>Variables</code>\u53c2\u6570\u3002\u5982\u679c\u6709\u591a\u4e2a\u8f93\u51fa\u4f60\u53ef\u4ee5\u8fd4\u56de\u5355\u4e2a<code>Tensor</code>\u6216<code>Tensor</code>\u683c\u5f0f\u7684\u5143\u7ec4\u3002\u53e6\u5916\uff0c\u8bf7\u53c2\u9605<code>Function</code>\u6587\u6863\u67e5\u627e\u53ea\u80fd\u88ab<code>forward()</code>\u8c03\u7528\u7684\u6709\u7528\u65b9\u6cd5\u7684\u8bf4\u660e\u3002</p> </li> <li> <p><code>backward()</code> - \u8ba1\u7b97\u68af\u5ea6\u7684\u516c\u5f0f. \u5b83\u5c06\u88ab\u8d4b\u4e88\u4e0e\u8f93\u51fa\u4e00\u6837\u591a\u7684<code>Variable</code>\u53c2\u6570, \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u5bf9\u5e94\u68af\u5ea6\u7684\u8f93\u51fa. \u5b83\u5e94\u8be5\u8fd4\u56de\u4e0e\u8f93\u5165\u4e00\u6837\u591a\u7684<code>Variable</code>, \u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8868\u793a\u90fd\u5305\u542b\u5176\u76f8\u5e94\u8f93\u5165\u7684\u68af\u5ea6. \u5982\u679c\u8f93\u5165\u4e0d\u9700\u8981\u8ba1\u7b97\u68af\u5ea6 (\u8bf7\u53c2\u9605<code>needs_input_grad</code>\u5c5e\u6027),\u6216\u8005\u662f\u975e<code>Variable</code>\u5bf9\u8c61,\u5219\u53ef\u8fd4\u56de<code>None</code>\u7c7b.\u6b64\u5916,\u5982\u679c\u4f60\u5728<code>forward()</code>\u65b9\u6cd5\u4e2d\u6709\u53ef\u9009\u7684\u53c2\u6570,\u5219\u53ef\u4ee5\u8fd4\u56de\u6bd4\u8f93\u5165\u66f4\u591a\u7684\u68af\u5ea6,\u53ea\u8981\u5b83\u4eec\u90fd\u662f<code>None</code>\u7c7b\u578b\u5373\u53ef.</p> </li> </ul> <p>\u4f60\u53ef\u4ee5\u4ece\u4e0b\u9762\u7684\u4ee3\u7801\u770b\u5230<code>torch.nn</code>\u6a21\u5757\u7684<code>Linear</code>\u51fd\u6570, \u4ee5\u53ca\u6ce8\u89e3</p> <pre><code># Inherit from Function\nclass Linear(Function):\n\n    # bias is an optional argument\n    def forward(self, input, weight, bias=None):\n        self.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = self.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if self.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if self.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and self.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0).squeeze(0)\n\n        return grad_input, grad_weight, grad_bias\n</code></pre> <p>\u73b0\u5728\uff0c\u4e3a\u4e86\u66f4\u65b9\u4fbf\u4f7f\u7528\u8fd9\u4e9b\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u63a8\u8350\u4f7f\u7528<code>apply</code>\u65b9\u6cd5\uff1a</p> <pre><code>linear = LinearFunction.apply\n</code></pre> <p>\u6211\u4eec\u4e0b\u9762\u7ed9\u51fa\u4e00\u4e2a\u7531\u975e\u53d8\u91cf\u53c2\u6570\u8fdb\u884c\u53c2\u6570\u5316\u7684\u51fd\u6570\u7684\u4f8b\u5b50:</p> <pre><code>class MulConstant(Function):\n    @staticmethod\n    def forward(ctx, tensor, constant):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        ctx.constant = constant\n        return tensor * constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n</code></pre> <ul> <li>\u6ce8\u610f \u5411\u540e\u8f93\u5165\uff0c\u5373grad_output\uff0c\u4e5f\u53ef\u4ee5\u662f\u8ddf\u8e2a\u5386\u53f2\u7684\u5f20\u91cf\u3002\u56e0\u6b64\uff0c\u5982\u679c\u4f7f\u7528\u53ef\u5fae\u8fd0\u7b97\u6765\u5b9e\u73b0\u5411\u540e\u8fd0\u7b97(\u4f8b\u5982\uff0c\u8c03\u7528\u53e6\u4e00\u4e2a\u81ea\u5b9a\u4e49\u51fd\u6570\uff09\uff0c\u5219\u66f4\u9ad8\u9636\u5bfc\u6570\u5c06\u8d77\u4f5c\u7528\u3002</li> </ul> <p>\u4f60\u53ef\u80fd\u60f3\u68c0\u6d4b\u4f60\u521a\u521a\u5b9e\u73b0\u7684<code>backward</code>\u65b9\u6cd5\u662f\u5426\u6b63\u786e\u7684\u8ba1\u7b97\u4e86\u68af\u5ea6\u3002\u4f60\u53ef\u4ee5\u4f7f\u7528\u5c0f\u7684\u6709\u9650\u5dee\u5206\u6cd5(<code>Finite Difference</code>)\u8fdb\u884c\u6570\u503c\u4f30\u8ba1\u3002</p> <pre><code>from torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (Variable(torch.randn(20,20).double(), requires_grad=True), Variable(torch.randn(30,20).double(), requires_grad=True),)\ntest = gradcheck(Linear.apply, input, eps=1e-6, atol=1e-4)\nprint(test)\n</code></pre> <p>\u6709\u5173\u6709\u9650\u5dee\u5206\u68af\u5ea6\u6bd4\u8f83\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6570\u503c\u68af\u5ea6\u68c0\u67e5\u3002</p>"},{"location":"1.0/notes_extending/#torchnn","title":"\u6269\u5c55 torch.nn","text":"<p><code>nn</code>\u6a21\u5757\u5305\u542b\u4e24\u79cd\u63a5\u53e3 - <code>modules</code>\u548c\u4ed6\u4eec\u7684\u529f\u80fd\u7248\u672c\u3002\u4f60\u53ef\u4ee5\u7528\u4e24\u79cd\u65b9\u6cd5\u6269\u5c55\u5b83,\u4f46\u662f\u6211\u4eec\u5efa\u8bae\uff0c\u5728\u6269\u5c55<code>layer</code>\u7684\u65f6\u5019\u4f7f\u7528<code>modules</code>\uff0c \u56e0\u4e3a<code>modules</code>\u4fdd\u5b58\u7740\u53c2\u6570\u548c<code>buffer</code>\u3002\u5982\u679c\u4f7f\u7528\u65e0\u53c2\u6570\u64cd\u4f5c\u7684\u8bdd\uff0c\u90a3\u4e48\u5efa\u8bae\u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570\uff0c\u6c60\u5316\u7b49\u51fd\u6570\u3002</p> <p>\u5728\u4e0a\u9762\u7684\u7ae0\u8282\u4e2d,\u6dfb\u52a0\u64cd\u4f5c\u7684\u529f\u80fd\u7248\u672c\u5df2\u7ecf\u4ecb\u7ecd\u8fc7\u4e86\u3002</p>"},{"location":"1.0/notes_extending/#module","title":"\u589e\u52a0\u4e00\u4e2a<code>Module</code>\u3002","text":"<p>\u7531\u4e8e<code>nn</code>\u5927\u91cf\u4f7f\u7528<code>autograd</code>\u3002\u6240\u4ee5\uff0c \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684Module\u7c7b\u9700\u8981\u5b9e\u73b0\u4e00\u4e2a<code>Function</code>\u7c7b, \u5b83\u4f1a\u6267\u884c\u5bf9\u5e94\u7684\u64cd\u4f5c\u5e76\u4e14\u8ba1\u7b97\u68af\u5ea6\u3002\u6211\u4eec\u53ea\u9700\u8981\u5f88\u5c11\u7684\u4ee3\u7801\u5c31\u53ef\u4ee5\u5b9e\u73b0\u4e0a\u9762<code>Linear</code>\u6a21\u5757\u7684\u529f\u80fd\u3002\u73b0\u5728\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0\u4e24\u4e2a\u51fd\u6570\uff1a</p> <ul> <li><code>__init__ (optional)</code> - \u63a5\u6536<code>kernel sizes</code>\u5185\u6838\u5927\u5c0f\uff0c\u7279\u5f81\u6570\u91cf\u7b49\u53c2\u6570\uff0c\u5e76\u521d\u59cb\u5316<code>parameters</code>\u53c2\u6570\u548c<code>buffers</code>\u7f13\u51b2\u533a\u3002</li> <li><code>forward()</code> - \u5b9e\u4f8b\u5316<code>Function</code>\u5e76\u4f7f\u7528\u5b83\u6765\u6267\u884c\u64cd\u4f5c\u3002\u5b83\u4e0e\u4e0a\u9762\u663e\u793a\u7684<code>functional wrapper</code>\u975e\u5e38\u76f8\u4f3c\u3002</li> </ul> <p>\u4e0b\u9762\u662f\u5b9e\u73b0<code>Linear</code>\u6a21\u5757\u7684\u65b9\u5f0f\uff1a</p> <pre><code>class Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super(Linear, self).__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Variable, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        self.weight.data.uniform_(-0.1, 0.1)\n        if bias is not None:\n            self.bias.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'in_features={}, out_features={}, bias={}'.format(\n            self.in_features, self.out_features, self.bias is not None\n        )\n</code></pre>"},{"location":"1.0/notes_extending/#c","title":"\u7f16\u5199\u81ea\u5b9a\u4e49\u7684C++\u6269\u5c55","text":"<p>\u6709\u5173\u8be6\u7ec6\u8bf4\u660e\u548c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605\u6b64PyTorch\u6559\u7a0b\u3002 \u6587\u6863\u53ef\u5728torch.utils.cpp_extension.\u83b7\u5f97\u3002</p>"},{"location":"1.0/notes_extending/#c_1","title":"\u7f16\u5199\u81ea\u5b9a\u4e49\u7684C\u6269\u5c55","text":"<p>\u53ef\u7528\u793a\u4f8b\u53ef\u4ee5\u5728\u8fd9\u4e2aGithub\u4ed3\u5e93\u91cc\u9762\u67e5\u770b\u53c2\u8003\u3002</p>"},{"location":"1.0/notes_faq/","title":"\u5e38\u89c1\u95ee\u9898\u89e3\u7b54","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d</p>"},{"location":"1.0/notes_faq/#cuda-runtime-error2-out-of-memory","title":"\u6211\u7684\u6a21\u578b\u62a5\u544a\u201ccuda runtime error(2): out of memory\u201d","text":"<p>\u6b63\u5982\u9519\u8bef\u6d88\u606f\u6240\u793a\uff0c\u60a8\u7684GPU\u663e\u5b58\u5df2\u8017\u5c3d\u3002\u7531\u4e8e\u7ecf\u5e38\u5728PyTorch\u4e2d\u5904\u7406\u5927\u91cf\u6570\u636e\uff0c\u56e0\u6b64\u5c0f\u9519\u8bef\u4f1a\u8fc5\u901f\u5bfc\u81f4\u7a0b\u5e8f\u8017\u5c3d\u6240\u6709GPU\u8d44\u6e90; \u5e78\u8fd0\u7684\u662f\uff0c\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u7684\u4fee\u590d\u901a\u5e38\u5f88\u7b80\u5355\u3002\u8fd9\u91cc\u6709\u4e00\u4e9b\u5e38\u89c1\u70b9\u9700\u8981\u68c0\u67e5\uff1a  </p> <p>\u4e0d\u8981\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u79ef\u7d2f\u5386\u53f2\u8bb0\u5f55\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6d89\u53ca\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u7684\u53d8\u91cf\u5c06\u4fdd\u7559\u5386\u53f2\u8bb0\u5f55\u3002\u8fd9\u610f\u5473\u7740\u60a8\u5e94\u8be5\u907f\u514d\u5728\u8ba1\u7b97\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u53d8\u91cf\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u53d8\u91cf\u5c06\u8d85\u51fa\u60a8\u7684\u8bad\u7ec3\u5faa\u73af\uff0c\u4f8b\u5982\uff0c\u5728\u8ddf\u8e2a\u7edf\u8ba1\u6570\u636e\u65f6\u3002\u76f8\u53cd\uff0c\u60a8\u5e94\u8be5\u5206\u79bb\u53d8\u91cf\u6216\u8bbf\u95ee\u5176\u57fa\u7840\u6570\u636e\u3002  </p> <p>\u6709\u65f6\uff0c\u5f53\u53ef\u5fae\u5206\u53d8\u91cf\u53d1\u751f\u65f6\uff0c\u5b83\u53ef\u80fd\u662f\u4e0d\u660e\u663e\u7684\u3002\u8003\u8651\u4ee5\u4e0b\u8bad\u7ec3\u5faa\u73af(\u4ece\u6e90\u4ee3\u7801\u4e2d\u5220\u9664\uff09\uff1a  </p> <pre><code>total_loss = 0\nfor i in range(10000):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss\n\n</code></pre> <p>\u5728\u8fd9\u91cc\uff0ctotal_loss\u5728\u60a8\u7684\u8bad\u7ec3\u5faa\u73af\u4e2d\u7d2f\u79ef\u5386\u53f2\u8bb0\u5f55\uff0c\u56e0\u4e3a\u4e22\u5931\u662f\u5177\u6709\u81ea\u52a8\u8bb0\u5f55\u5386\u53f2\u7684\u53ef\u5fae\u5206\u53d8\u91cf\u3002 \u60a8\u53ef\u4ee5\u901a\u8fc7\u7f16\u5199total_loss + = float(loss\uff09\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002  </p> <p>\u6b64\u95ee\u9898\u7684\u5176\u4ed6\u5b9e\u4f8b\uff1a1\u3002  </p> <p>\u4e0d\u8981\u6293\u4f4f\u4f60\u4e0d\u9700\u8981\u7684\u5f20\u91cf\u6216\u53d8\u91cf\u3002 \u5982\u679c\u5c06\u5f20\u91cf\u6216\u53d8\u91cf\u5206\u914d\u7ed9\u672c\u5730\uff0c\u5219\u5728\u672c\u5730\u8d85\u51fa\u8303\u56f4\u4e4b\u524d\uff0cPython\u4e0d\u4f1a\u89e3\u9664\u5206\u914d\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528<code>del x</code>\u91ca\u653e\u6b64\u5f15\u7528\u3002 \u540c\u6837\uff0c\u5982\u679c\u5c06\u5f20\u91cf\u6216\u5411\u91cf\u5206\u914d\u7ed9\u5bf9\u8c61\u7684\u6210\u5458\u53d8\u91cf\uff0c\u5219\u5728\u5bf9\u8c61\u8d85\u51fa\u8303\u56f4\u4e4b\u524d\u4e0d\u4f1a\u91ca\u653e\u3002\u5982\u679c\u60a8\u6ca1\u6709\u4fdd\u7559\u4e0d\u9700\u8981\u7684\u4e34\u65f6\u5de5\u5177\uff0c\u60a8\u5c06\u83b7\u5f97\u6700\u4f73\u7684\u5185\u5b58\u4f7f\u7528\u91cf\u3002  </p> <p>\u672c\u5730\u89c4\u6a21\u5927\u5c0f\u53ef\u80fd\u6bd4\u60a8\u9884\u671f\u7684\u8981\u5927\u3002 \u4f8b\u5982\uff1a  </p> <pre><code>for i in range(5):\n    intermediate = f(input[i])\n    result += g(intermediate)\noutput = h(result)\nreturn output\n\n</code></pre> <p>\u5728\u8fd9\u91cc\uff0c\u5373\u4f7f\u5728\u6267\u884ch\u65f6\uff0c\u4e2d\u95f4\u53d8\u91cf\u4ecd\u7136\u5b58\u5728\uff0c\u56e0\u4e3a\u5b83\u7684\u8303\u56f4\u8d85\u51fa\u4e86\u5faa\u73af\u7684\u672b\u5c3e\u3002\u8981\u63d0\u524d\u91ca\u653e\u5b83\uff0c\u4f60\u5e94\u8be5\u5728\u5b8c\u6210\u5b83\u65f6\u4f7f\u7528del\u3002  </p> <p>\u4e0d\u8981\u5728\u592a\u5927\u7684\u5e8f\u5217\u4e0a\u8fd0\u884cRNN\u3002 \u901a\u8fc7RNN\u53cd\u5411\u4f20\u64ad\u6240\u9700\u7684\u5b58\u50a8\u91cf\u4e0eRNN\u7684\u957f\u5ea6\u6210\u7ebf\u6027\u5173\u7cfb; \u56e0\u6b64\uff0c\u5982\u679c\u60a8\u5c1d\u8bd5\u5411RNN\u63d0\u4f9b\u8fc7\u957f\u7684\u5e8f\u5217\uff0c\u5219\u4f1a\u8017\u5c3d\u5185\u5b58\u3002</p> <p>\u8fd9\u79cd\u73b0\u8c61\u7684\u6280\u672f\u672f\u8bed\u662f\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u53cd\u5411\u4f20\u64ad\uff0c\u5e76\u4e14\u6709\u5f88\u591a\u5173\u4e8e\u5982\u4f55\u5b9e\u73b0\u622a\u65adBPTT\u7684\u53c2\u8003\uff0c\u5305\u62ec\u5728\u5355\u8bcd\u8bed\u8a00\u6a21\u578b\u793a\u4f8b\u4e2d; \u622a\u65ad\u7531\u91cd\u65b0\u6253\u5305\u529f\u80fd\u5904\u7406\uff0c\u5982\u672c\u8bba\u575b\u5e16\u5b50\u4e2d\u6240\u8ff0\u3002</p> <p>\u4e0d\u8981\u4f7f\u7528\u592a\u5927\u7684\u7ebf\u6027\u56fe\u5c42\u3002 \u7ebf\u6027\u5c42nn.Linear(m\uff0cn\uff09\u4f7f\u7528O(nm)\u5b58\u50a8\u5668\uff1a\u4e5f\u5c31\u662f\u8bf4\uff0c\u6743\u91cd\u7684\u5b58\u50a8\u5668\u9700\u6c42\u4e0e\u7279\u5f81\u7684\u6570\u91cf\u6210\u6bd4\u4f8b\u3002 \u4ee5\u8fd9\u79cd\u65b9\u5f0f\u5f88\u5bb9\u6613\u5360\u7528\u4f60\u7684\u5b58\u50a8(\u5e76\u4e14\u8bb0\u4f4f\uff0c\u4f60\u5c06\u81f3\u5c11\u9700\u8981\u4e24\u500d\u5b58\u50a8\u6743\u503c\u7684\u5185\u5b58\u91cf\uff0c\u56e0\u4e3a\u4f60\u8fd8\u9700\u8981\u5b58\u50a8\u68af\u5ea6\u3002\uff09</p>"},{"location":"1.0/notes_faq/#my-gpu-memory-isnt-freed-properly","title":"My GPU memory isn't freed properly","text":"<p>PyTorch\u4f7f\u7528\u7f13\u5b58\u5185\u5b58\u5206\u914d\u5668\u6765\u52a0\u901f\u5185\u5b58\u5206\u914d\u3002 \u56e0\u6b64\uff0c<code>nvidia-smi</code>\u4e2d\u663e\u793a\u7684\u503c\u901a\u5e38\u4e0d\u4f1a\u53cd\u6620\u771f\u5b9e\u7684\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002 \u6709\u5173GPU\u5185\u5b58\u7ba1\u7406\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5185\u5b58\u7ba1\u7406 \u3002</p> <p>\u5982\u679c\u5728Python\u9000\u51fa\u540e\u4f60\u7684GPU\u5185\u5b58\u4ecd\u65e7\u6ca1\u6709\u88ab\u91ca\u653e\uff0c\u90a3\u4e48\u5f88\u53ef\u80fd\u662f\u4e00\u4e9bPython\u5b50\u8fdb\u7a0b\u4ecd\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7<code>ps -elf |grep python</code>\u627e\u5230\u5b83\u4eec\u5e76\u7528<code>kill -9 [pid]</code>\u624b\u52a8\u7ed3\u675f\u8fd9\u4e9b\u8fdb\u7a0b\u3002  </p>"},{"location":"1.0/notes_faq/#my-data-loader-workers-return-identical-random-numbers","title":"My data loader workers return identical random numbers","text":"<p>\u60a8\u53ef\u80fd\u6b63\u5728\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u5176\u4ed6\u5e93\u6765\u751f\u6210\u968f\u673a\u6570\u3002 \u4f8b\u5982\uff0c\u5f53\u901a\u8fc7<code>fork</code>\u542f\u52a8\u5de5\u4f5c\u7a0b\u5e8f\u5b50\u8fdb\u7a0b\u65f6\uff0cNumPy\u7684RNG\u4f1a\u91cd\u590d\u3002\u6709\u5173\u5982\u4f55\u4f7f\u7528<code>worker_init_fn</code>\u9009\u9879\u5728\u5de5\u4f5c\u7a0b\u5e8f\u4e2d\u6b63\u786e\u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\u7684\u6587\u6863\uff0c\u8bf7\u53c2\u9605torch.utils.data.DataLoader\u6587\u6863\u3002  </p>"},{"location":"1.0/notes_faq/#my-recurrent-network-doesnt-work-with-data-parallelism","title":"My recurrent network doesn't work with data parallelism","text":"<p>\u5728\u5177\u6709<code>DataParallel</code>\u6216<code>data_parallel()</code>\u7684\u6a21\u5757\u4e2d\u4f7f\u7528<code>pack sequence -&gt; recurrent network -&gt; unpack sequence</code>\u6a21\u5f0f\u65f6\u6709\u4e00\u4e2a\u975e\u5e38\u5fae\u5999\u7684\u5730\u65b9\u3002\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u7684<code>forward()</code>\u7684\u8f93\u5165\u53ea\u4f1a\u662f\u6574\u4e2a\u8f93\u5165\u7684\u4e00\u90e8\u5206\u3002\u7531\u4e8e\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u89e3\u5305\u64cd\u4f5c<code>torch.nn.utils.rnn.pad_packed_sequence()</code>\u4ec5\u586b\u5145\u5230\u5176\u6240\u89c1\u7684\u6700\u957f\u8f93\u5165\uff0c\u5373\u8be5\u7279\u5b9a\u8bbe\u5907\u4e0a\u7684\u6700\u957f\u8f93\u5165\uff0c\u6240\u4ee5\u5728\u5c06\u7ed3\u679c\u6536\u96c6\u5728\u4e00\u8d77\u65f6\u4f1a\u53d1\u751f\u5c3a\u5bf8\u7684\u4e0d\u5339\u914d\u3002\u56e0\u6b64\uff0c\u60a8\u53ef\u4ee5\u5229\u7528<code>pad_packed_sequence()</code>\u7684 <code>total_length</code>\u53c2\u6570\u6765\u786e\u4fdd<code>forward()</code>\u8c03\u7528\u8fd4\u56de\u76f8\u540c\u957f\u5ea6\u7684\u5e8f\u5217\u3002\u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u5199\uff1a</p> <pre><code>from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass MyModule(nn.Module):\n    #  ... __init__, \u4ee5\u53ca\u5176\u4ed6\u8bbf\u6c42\n\n    # padding_input \u7684\u5f62\u72b6\u662f[B x T x *](batch_first \u6a21\u5f0f\uff09\uff0c\u5305\u542b\u6309\u957f\u5ea6\u6392\u5e8f\u7684\u5e8f\u5217\n    # B \u662f\u6279\u91cf\u5927\u5c0f\n    # T \u662f\u6700\u5927\u5e8f\u5217\u957f\u5ea6\n    def forward(self, padded_input, input_lengths):\n        total_length = padded_input.size(1)  # get the max sequence length\n        packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                            batch_first=True)\n        packed_output, _ = self.my_lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                        total_length=total_length)\n        return output\n\nm = MyModule().cuda()\ndp_m = nn.DataParallel(m)\n\n</code></pre> <p>\u53e6\u5916\uff0c\u5728\u6279\u91cf\u7684\u7ef4\u5ea6\u4e3adim 1(\u5373 batch_first = False )\u65f6\u9700\u8981\u6ce8\u610f\u6570\u636e\u7684\u5e76\u884c\u6027\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cpack_padded_sequence \u51fd\u6570\u7684\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570 padding_input \u7ef4\u5ea6\u5c06\u662f [T x B x *] \uff0c\u5e76\u4e14\u5e94\u8be5\u6cbfdim 1 (\u7b2c1\u8f74\uff09\u5206\u6563\uff0c\u4f46\u7b2c\u4e8c\u4e2a\u53c2\u6570 input_lengths \u7684\u7ef4\u5ea6\u4e3a [B]\uff0c\u5e94\u8be5\u6cbfdim 0 (\u7b2c0\u8f74\uff09\u5206\u6563\u3002\u9700\u8981\u989d\u5916\u7684\u4ee3\u7801\u6765\u64cd\u7eb5\u5f20\u91cf\u7684\u7ef4\u5ea6\u3002</p>"},{"location":"1.0/notes_multiprocessing/","title":"\u591a\u8fdb\u7a0b\u6700\u4f73\u5b9e\u8df5","text":"<p>\u8bd1\u8005\uff1acvley</p> <p><code>torch.multiprocessing</code> \u662f Python \u7684 <code>multiprocessing</code> \u7684\u76f4\u63a5\u66ff\u4ee3\u6a21\u5757\u3002\u5b83\u652f\u6301\u5b8c\u5168\u76f8\u540c\u7684\u64cd\u4f5c\uff0c\u4f46\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u8fd9\u6837\u6240\u6709\u7684\u5f20\u91cf\u5c31\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a <code>multiprocessing.Queue</code> \u8fdb\u884c\u4f20\u9012\uff0c\u5c06\u6570\u636e\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u5e76\u53ea\u5c06\u53e5\u67c4\u4f20\u9012\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u3002</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4e00\u4e2a <code>Tensor</code> \u4f20\u9012\u5230\u53e6\u4e00\u4e2a\u8fdb\u7a0b\u65f6\uff0c<code>Tensor</code> \u7684\u6570\u636e\u662f\u5171\u4eab\u7684\u3002\u5982\u679c <code>torch.Tensor.grad</code> \u4e0d\u662f <code>None</code>, \u4e5f\u4f1a\u88ab\u5171\u4eab\u3002\u5728\u4e00\u4e2a\u6ca1\u6709 <code>torch.Tensor.grad</code> \u57df\u7684 <code>Tensor</code> \u88ab\u9001\u5230\u5176\u4ed6\u8fdb\u7a0b\u65f6\uff0c\u4e00\u4e2a\u6807\u51c6\u7684\u8fdb\u7a0b\u4e13\u7528\u7684 <code>.grad</code> <code>Tensor</code> \u4f1a\u88ab\u521b\u5efa\uff0c\u800c\u5b83\u5728\u6240\u6709\u7684\u8fdb\u7a0b\u4e2d\u4e0d\u4f1a\u81ea\u52a8\u88ab\u5171\u4eab\uff0c\u4e0e <code>Tensor</code> \u6570\u636e\u7684\u5171\u4eab\u65b9\u5f0f\u4e0d\u540c\u3002</p> <p>\u8fd9\u5c31\u5141\u8bb8\u5b9e\u73b0\u5404\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6bd4\u5982 Hogwild\u3001A3C\uff0c\u6216\u8005\u5176\u4ed6\u90a3\u4e9b\u9700\u8981\u5f02\u6b65\u64cd\u4f5c\u7684\u65b9\u6cd5\u3002</p>"},{"location":"1.0/notes_multiprocessing/#cuda","title":"\u5171\u4eab CUDA \u5f20\u91cf","text":"<p>\u8fdb\u7a0b\u95f4\u5171\u4eab CUDA \u5f20\u91cf\u4ec5\u652f\u6301 Python 3\uff0c\u4f7f\u7528\u7684\u662f <code>spawn</code> \u6216\u8005 <code>forkserver</code> \u542f\u52a8\u65b9\u6cd5\u3002Python 2 \u4e2d\u7684 <code>multiprocessing</code> \u4ec5\u4f7f\u7528 <code>fork</code> \u6765\u521b\u5efa\u5b50\u8fdb\u7a0b\uff0c\u800c CUDA \u8fd0\u884c\u65f6\u4e0d\u652f\u6301\u8be5\u65b9\u6cd5\u3002</p> <p>\u8b66\u544a</p> <p>CUDA API \u9700\u8981\u5206\u914d\u7ed9\u5176\u4ed6\u8fdb\u7a0b\u7684\u663e\u5b58\u5728\u5b83\u4eec\u8fd8\u5728\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u4e00\u76f4\u6709\u6548\u3002\u4f60\u9700\u8981\u4ed4\u7ec6\u786e\u4fdd\u5171\u4eab\u7684 CUDA \u5f20\u91cf\u82e5\u975e\u5fc5\u987b\uff0c\u4e0d\u4f1a\u8d85\u51fa\u4f7f\u7528\u8303\u56f4\u3002\u8fd9\u5bf9\u4e8e\u5171\u4eab\u6a21\u578b\u53c2\u6570\u4e0d\u4f1a\u662f\u4e00\u4e2a\u95ee\u9898\uff0c\u4f46\u4f20\u9012\u5176\u4ed6\u7c7b\u578b\u7684\u6570\u636e\u65f6\u9700\u8981\u8c28\u614e\u3002\u6ce8\u610f\u8be5\u9650\u5236\u5e76\u4e0d\u9002\u7528\u4e8e\u5171\u4eab CPU \u5185\u5b58\u3002</p> <p>\u4e5f\u53ef\u4ee5\u53c2\u8003\uff1a\u4f7f\u7528 nn.DataParallel \u66ff\u4ee3 multiprocessing</p>"},{"location":"1.0/notes_multiprocessing/#_2","title":"\u6700\u4f73\u5b9e\u8df5\u548c\u63d0\u793a","text":""},{"location":"1.0/notes_multiprocessing/#_3","title":"\u907f\u514d\u548c\u5904\u7406\u6b7b\u9501","text":"<p>\u5f53\u521b\u5efa\u4e00\u4e2a\u65b0\u8fdb\u7a0b\u65f6\uff0c\u5f88\u591a\u60c5\u51b5\u4f1a\u53d1\u751f\uff0c\u6700\u5e38\u89c1\u7684\u5c31\u662f\u540e\u53f0\u7ebf\u7a0b\u95f4\u7684\u6b7b\u9501\u3002\u5982\u679c\u4efb\u4f55\u4e00\u4e2a\u7ebf\u7a0b\u6709\u9501\u7684\u72b6\u6001\u6216\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u6a21\u5757\uff0c\u7136\u540e\u8c03\u7528\u4e86<code>fork</code>\uff0c\u5b50\u8fdb\u7a0b\u5f88\u6709\u53ef\u80fd\u5904\u4e8e\u4e2d\u65ad\u72b6\u6001\uff0c\u5e76\u4ee5\u53e6\u5916\u7684\u65b9\u5f0f\u6b7b\u9501\u6216\u8005\u5931\u8d25\u3002\u6ce8\u610f\u5373\u4f7f\u4f60\u6ca1\u8fd9\u4e48\u505a\uff0cPython \u5185\u5efa\u7684\u5e93\u4e5f\u6709\u53ef\u80fd\u8fd9\u4e48\u505a\u2014\u2014\u65e0\u9700\u820d\u8fd1\u6c42\u8fdc\uff0c<code>multiprocessing</code>\u5373\u662f\u5982\u6b64\u3002<code>multiprocessing.Queue</code> \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u7c7b\uff0c\u53ef\u4ee5\u521b\u5efa\u591a\u4e2a\u7ebf\u7a0b\u7528\u4e8e\u4e32\u884c\u3001\u53d1\u9001\u548c\u63a5\u6536\u5bf9\u8c61\uff0c\u5b83\u4eec\u4e5f\u4f1a\u51fa\u73b0\u524d\u9762\u63d0\u5230\u7684\u95ee\u9898\u3002\u5982\u679c\u4f60\u53d1\u73b0\u81ea\u5df1\u9047\u5230\u4e86\u8fd9\u79cd\u60c5\u51b5\uff0c\u5c1d\u8bd5\u4f7f\u7528 <code>multiprocessing.queues.SimpleQueue</code>\uff0c\u5b83\u4e0d\u4f1a\u4f7f\u7528\u989d\u5916\u7684\u7ebf\u7a0b\u3002</p> <p>\u6211\u4eec\u5728\u5c3d\u6700\u5927\u52aa\u529b\u4e3a\u4f60\u5316\u7e41\u4e3a\u7b80\uff0c\u786e\u4fdd\u4e0d\u4f1a\u53d1\u751f\u6b7b\u9501\u7684\u60c5\u51b5\uff0c\u4f46\u6709\u65f6\u4e5f\u4f1a\u51fa\u73b0\u5931\u63a7\u7684\u60c5\u51b5\u3002\u5982\u679c\u4f60\u9047\u5230\u4efb\u4f55\u6682\u65f6\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u5728\u8bba\u575b\u4e0a\u6c42\u52a9\uff0c\u6211\u4eec\u5c06\u4f1a\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u4fee\u590d\u3002</p>"},{"location":"1.0/notes_multiprocessing/#queue","title":"\u901a\u8fc7 Queue \u4f20\u9012\u91cd\u7528\u7f13\u5b58","text":"<p>\u8bb0\u4f4f\u6bcf\u6b21\u5c06\u4e00\u4e2a <code>Tensor</code> \u653e\u8fdb\u4e00\u4e2a <code>multiprocessing.Queue</code> \u65f6\uff0c\u5b83\u5c31\u4f1a\u88ab\u79fb\u52a8\u5230\u5171\u4eab\u5185\u5b58\u4e2d\u3002\u5982\u679c\u5b83\u5df2\u7ecf\u88ab\u5171\u4eab\uff0c\u90a3\u5c06\u4e0d\u4f1a\u6709\u64cd\u4f5c\uff0c\u5426\u5219\u5c06\u4f1a\u89e6\u53d1\u4e00\u6b21\u989d\u5916\u7684\u5185\u5b58\u62f7\u8d1d\uff0c\u800c\u8fd9\u5c06\u4f1a\u62d6\u6162\u6574\u4e2a\u8fdb\u7a0b\u3002\u5373\u4f7f\u4f60\u6709\u4e00\u4e2a\u8fdb\u7a0b\u6c60\u628a\u6570\u636e\u53d1\u9001\u5230\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u5e76\u628a\u7f13\u5b58\u9001\u56de\u6765\u2014\u2014\u8fd9\u8fd1\u4e4e\u4e8e\u65e0\u64cd\u4f5c\uff0c\u5728\u53d1\u9001\u4e0b\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\u65f6\u907f\u514d\u62f7\u8d1d\u3002</p>"},{"location":"1.0/notes_multiprocessing/#hogwild","title":"\u5f02\u6b65\u591a\u8fdb\u7a0b\u8bad\u7ec3(\u5982Hogwild\uff09","text":"<p>\u4f7f\u7528 <code>torch.multiprocessing</code>\uff0c\u53ef\u4ee5\u5f02\u6b65\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\uff0c\u53c2\u6570\u8981\u4e48\u4e00\u76f4\u5171\u4eab\uff0c\u8981\u4e48\u5468\u671f\u6027\u540c\u6b65\u3002\u5728\u7b2c\u4e00\u4e2a\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5efa\u8bae\u4f20\u9012\u6574\u4e2a\u6a21\u578b\u7684\u5bf9\u8c61\uff0c\u800c\u5bf9\u4e8e\u540e\u4e00\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u5c06\u4ee5\u4ec5\u4f20\u9012 <code>state_dict()</code>\u3002</p> <p>\u6211\u4eec\u5efa\u8bae\u4f7f\u7528 <code>multiprocessing.Queue</code>\u5728\u8fdb\u7a0b\u95f4\u4f20\u9012 PyTorch \u5bf9\u8c61\u3002\u5f53\u4f7f\u7528<code>fork</code>\u547d\u4ee4\u65f6\uff0c\u53ef\u4ee5\u8fdb\u884c\u8bf8\u5982\u7ee7\u627f\u5171\u4eab\u5185\u5b58\u4e2d\u7684\u5f20\u91cf\u548c\u5b58\u50a8\u7684\u64cd\u4f5c\uff0c\u7136\u800c\u8fd9\u4e2a\u64cd\u4f5c\u5bb9\u6613\u4ea7\u751f\u95ee\u9898\uff0c\u5e94\u8be5\u5c0f\u5fc3\u4f7f\u7528\uff0c\u4ec5\u5efa\u8bae\u9ad8\u7ea7\u7528\u6237\u4f7f\u7528\u3002Queue\uff0c\u5c3d\u7ba1\u6709\u65f6\u4e0d\u662f\u4e00\u4e2a\u90a3\u4e48\u4f18\u96c5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u6240\u6709\u7684\u60c5\u51b5\u4e0b\u90fd\u53ef\u4ee5\u5408\u7406\u4f7f\u7528\u3002</p> <p>\u8b66\u544a</p> <p>\u4f60\u5e94\u8be5\u6ce8\u610f\u90a3\u4e9b\u4e0d\u5728<code>if __name__ == '__main__'</code>\u4e2d\u7684\u5168\u5c40\u58f0\u660e\u3002\u5982\u679c\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e0d\u662f<code>fork</code>\u7684\u7cfb\u7edf\u8c03\u7528\uff0c\u5b83\u4eec\u5c06\u4f1a\u5728\u6240\u6709\u5b50\u8fdb\u7a0b\u4e2d\u6267\u884c\u3002</p>"},{"location":"1.0/notes_multiprocessing/#hogwild_1","title":"Hogwild","text":"<p>\u5728\u793a\u4f8b\u4ed3\u5e93\u4e2d\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a\u5177\u4f53\u7684Hogwild\u5b9e\u73b0\uff0c\u4f46\u9664\u4e86\u5b8c\u6574\u7684\u4ee3\u7801\u7ed3\u6784\u4e4b\u5916\uff0c\u4e0b\u9762\u4e5f\u6709\u4e00\u4e2a\u7b80\u5316\u7684\u4f8b\u5b50\uff1a</p> <pre><code>import torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n</code></pre>"},{"location":"1.0/notes_randomness/","title":"\u518d\u751f\u6027","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <p>PyTorch\u7248\u672c\uff0c\u5355\u4e2a\u63d0\u4ea4\u6216\u4e0d\u540c\u5e73\u53f0\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u5168\u53ef\u91cd\u73b0\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u4f7f\u7528\u76f8\u540c\u7684\u79cd\u5b50\uff0c\u4e5f\u4e0d\u9700\u8981\u5728CPU\u548cGPU\u6267\u884c\u4e4b\u95f4\u91cd\u73b0\u7ed3\u679c\u3002</p> <p>\u4f46\u662f\uff0c\u4e3a\u4e86\u5728\u4e00\u4e2a\u7279\u5b9a\u5e73\u53f0\u548cPyTorch\u7248\u672c\u4e0a\u5bf9\u60a8\u7684\u7279\u5b9a\u95ee\u9898\u8fdb\u884c\u8ba1\u7b97\u786e\u5b9a\uff0c\u9700\u8981\u91c7\u53d6\u51e0\u4e2a\u6b65\u9aa4\u3002</p> <p>PyTorch\u4e2d\u6d89\u53ca\u4e24\u4e2a\u4f2a\u968f\u673a\u6570\u751f\u6210\u5668\uff0c\u60a8\u9700\u8981\u624b\u52a8\u64ad\u79cd\u4ee5\u4f7f\u8fd0\u884c\u53ef\u91cd\u73b0\u3002\u6b64\u5916\uff0c\u60a8\u5e94\u8be5\u786e\u4fdd\u60a8\u7684\u4ee3\u7801\u4f9d\u8d56\u4e8e\u4f7f\u7528\u968f\u673a\u6570\u7684\u6240\u6709\u5176\u4ed6\u5e93\u4e5f\u4f7f\u7528\u56fa\u5b9a\u79cd\u5b50\u3002</p>"},{"location":"1.0/notes_randomness/#pytorch","title":"PyTorch","text":"<p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4e3a\u6240\u6709\u8bbe\u5907(CPU\u548cCUDA\uff09\u64ad\u79cdRNG\uff1a</p> <pre><code>import torch\ntorch.manual_seed(0)\n\n</code></pre> <p>\u6709\u4e00\u4e9bPyTorch\u51fd\u6570\u4f7f\u7528CUDA\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u53ef\u80fd\u662f\u975e\u786e\u5b9a\u6027\u7684\u6765\u6e90\u3002\u4e00\u7c7b\u8fd9\u6837\u7684CUDA\u51fd\u6570\u662f\u539f\u5b50\u64cd\u4f5c\uff0c\u7279\u522b\u662f<code>atomicAdd</code>\uff0c\u5176\u4e2d\u5bf9\u4e8e\u76f8\u540c\u503c\u7684\u5e76\u884c\u52a0\u6cd5\u7684\u987a\u5e8f\u662f\u672a\u786e\u5b9a\u7684\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6d6e\u70b9\u53d8\u91cf\uff0c\u662f\u7ed3\u679c\u4e2d\u7684\u53d8\u5316\u6e90\u3002\u5728\u524d\u5411\u4e2d\u4f7f\u7528<code>atomicAdd</code>\u7684PyTorch\u51fd\u6570\u5305\u62ec\uff0c\u3002</p> <p>\u8bb8\u591a\u64cd\u4f5c\u5177\u6709\u5411\u540e\u4f7f\u7528<code>atomicAdd</code>\uff0c\u7279\u522b\u662f\u8bb8\u591a\u5f62\u5f0f\u7684\u6c60\uff0c\u586b\u5145\u548c\u91c7\u6837\u3002\u76ee\u524d\u6ca1\u6709\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u907f\u514d\u8fd9\u4e9b\u529f\u80fd\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u3002</p>"},{"location":"1.0/notes_randomness/#cudnn","title":"CuDNN","text":"<p>\u5728CuDNN\u540e\u7aef\u8fd0\u884c\u65f6\uff0c\u5fc5\u987b\u8bbe\u7f6e\u53e6\u5916\u4e24\u4e2a\u9009\u9879\uff1a</p> <pre><code>torch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n</code></pre> <p>\u8b66\u544a</p> <p>\u786e\u5b9a\u6027\u6a21\u5f0f\u53ef\u80fd\u4f1a\u5bf9\u6027\u80fd\u4ea7\u751f\u5f71\u54cd\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u60a8\u7684\u578b\u53f7\u3002</p>"},{"location":"1.0/notes_randomness/#numpy","title":"NumPy\u7684","text":"<p>\u5982\u679c\u60a8\u6216\u60a8\u4f7f\u7528\u7684\u4efb\u4f55\u5e93\u4f9d\u8d56\u4e8eNumpy\uff0c\u60a8\u4e5f\u5e94\u8be5\u4e3aNumpy RNG\u64ad\u79cd\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b8c\u6210\uff1a</p> <pre><code>import numpy as np\nnp.random.seed(0)\n\n</code></pre>"},{"location":"1.0/notes_serialization/","title":"\u5e8f\u5217\u5316\u7684\u76f8\u5173\u8bed\u4e49","text":"<p>\u8bd1\u8005\uff1ayuange250</p>"},{"location":"1.0/notes_serialization/#_2","title":"\u6700\u4f73\u65b9\u6848","text":""},{"location":"1.0/notes_serialization/#_3","title":"\u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5","text":"<p>Pytorch\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u7528\u4e8e\u5e8f\u5217\u5316\u548c\u4fdd\u5b58\u4e00\u4e2a\u6a21\u578b\u3002</p> <p>\u7b2c\u4e00\u79cd\u53ea\u5b58\u53d6\u6a21\u578b\u7684\u53c2\u6570(\u66f4\u4e3a\u63a8\u8350\uff09\uff1a \u4fdd\u5b58\u53c2\u6570\uff1a</p> <pre><code>torch.save(the_model.state_dict(), PATH)\n\n</code></pre> <p>\u8bfb\u53d6\u53c2\u6570\uff1a</p> <pre><code>the_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n\n</code></pre> <p>\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u5219\u5c06\u6574\u4e2a\u6a21\u578b\u90fd\u4fdd\u5b58\u4e0b\u6765\uff1a</p> <pre><code>torch.save(the_model, PATH)\n\n</code></pre> <p>\u8bfb\u53d6\u7684\u65f6\u5019\u4e5f\u662f\u8bfb\u53d6\u6574\u4e2a\u6a21\u578b\uff1a</p> <pre><code>the_model = torch.load(PATH)\n\n</code></pre> <p>\u5728\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u4e2d, \u7531\u4e8e\u7279\u5b9a\u7684\u5e8f\u5217\u5316\u7684\u6570\u636e\u4e0e\u5176\u7279\u5b9a\u7684\u7c7b\u522b(class)\u76f8\u7ed1\u5b9a\uff0c\u5e76\u4e14\u5728\u5e8f\u5217\u5316\u7684\u65f6\u5019\u4f7f\u7528\u4e86\u56fa\u5b9a\u7684\u76ee\u5f55\u7ed3\u6784\uff0c\u6240\u4ee5\u5728\u5f88\u591a\u60c5\u51b5\u4e0b\uff0c\u5982\u5728\u5176\u4ed6\u7684\u4e00\u4e9b\u9879\u76ee\u4e2d\u4f7f\u7528\uff0c\u6216\u8005\u4ee3\u7801\u8fdb\u884c\u4e86\u8f83\u5927\u7684\u91cd\u6784\u7684\u65f6\u5019\uff0c\u5f88\u5bb9\u6613\u51fa\u73b0\u95ee\u9898\u3002</p>"},{"location":"1.0/notes_windows/","title":"Windows FAQ","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d</p>"},{"location":"1.0/notes_windows/#_1","title":"\u4ece\u6e90\u7801\u4e2d\u6784\u5efa","text":""},{"location":"1.0/notes_windows/#_2","title":"\u5305\u542b\u53ef\u9009\u7ec4\u4ef6","text":"<p>Windows PyTorch\u6709\u4e24\u4e2a\u53d7\u652f\u6301\u7684\u7ec4\u4ef6\uff1aMKL\u548cMAGMA\u3002 \u4ee5\u4e0b\u662f\u4f7f\u7528\u5b83\u4eec\u6784\u5efa\u7684\u6b65\u9aa4\u3002  </p> <pre><code>REM Make sure you have 7z and curl installed.\n\nREM Download MKL files\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2018.2.185.7z -k -O\n7z x -aoa mkl_2018.2.185.7z -omkl\n\nREM Download MAGMA files\nREM cuda90/cuda92/cuda100 is also available in the following line.\nset CUDA_PREFIX=cuda80\ncurl -k https://s3.amazonaws.com/ossci-windows/magma_2.4.0_%CUDA_PREFIX%_release.7z -o magma.7z\n7z x -aoa magma.7z -omagma\n\nREM Setting essential environment variables\nset \"CMAKE_INCLUDE_PATH=%cd%\\\\mkl\\\\include\"\nset \"LIB=%cd%\\\\mkl\\\\lib;%LIB%\"\nset \"MAGMA_HOME=%cd%\\\\magma\"\n\n</code></pre>"},{"location":"1.0/notes_windows/#windowscuda","title":"\u4e3aWindows\u6784\u5efa\u52a0\u901fCUDA","text":"<p>Visual Studio\u5f53\u524d\u4e0d\u652f\u6301\u5e76\u884c\u81ea\u5b9a\u4e49\u4efb\u52a1\u3002 \u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528Ninja\u6765\u5e76\u884c\u5316CUDA\u6784\u5efa\u4efb\u52a1\u3002 \u53ea\u9700\u952e\u5165\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u4f7f\u7528\u5b83\u3002 </p> <pre><code>REM Let's install ninja first.\npip install ninja\n\nREM Set it as the cmake generator\nset CMAKE_GENERATOR=Ninja  \n</code></pre>"},{"location":"1.0/notes_windows/#_3","title":"\u811a\u672c\u4e00\u952e\u5b89\u88c5","text":"<p>\u4f60\u53ef\u4ee5\u53c2\u8003\u8fd9\u4e9b\u811a\u672c\u3002\u5b83\u4f1a\u7ed9\u4f60\u6307\u5bfc\u65b9\u5411\u3002  </p>"},{"location":"1.0/notes_windows/#_4","title":"\u6269\u5c55","text":""},{"location":"1.0/notes_windows/#cfei","title":"CFEI\u6269\u5c55","text":"<p>\u5bf9CFFI\u6269\u5c55\u7684\u652f\u6301\u662f\u975e\u5e38\u8bd5\u9a8c\u6027\u7684\u3002\u5728Windows\u4e0b\u542f\u7528\u5b83\u901a\u5e38\u6709\u4e24\u4e2a\u6b65\u9aa4\u3002</p> <p>\u9996\u5148\uff0c\u5728Extension\u5bf9\u8c61\u4e2d\u6307\u5b9a\u5176\u4ed6\u5e93\u4ee5\u4f7f\u5176\u5728Windows\u4e0a\u6784\u5efa\u3002   </p> <pre><code>ffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_compile_args=[\"-std=c99\"],\n    libraries=['ATen', '_C'] # Append cuda libaries when necessary, like cudart\n)\n\n</code></pre> <p>\u5176\u6b21\uff0c\u8fd9\u662f\u201c\u7531<code>extern THCState *state</code>\u72b6\u6001\u5f15\u8d77\u7684\u672a\u89e3\u51b3\u7684\u5916\u90e8\u7b26\u53f7\u72b6\u6001\u201d\u7684\u5de5\u4f5c\u573a\u6240;</p> <p>\u5c06\u6e90\u4ee3\u7801\u4eceC\u66f4\u6539\u4e3aC ++\u3002 \u4e0b\u9762\u5217\u51fa\u4e86\u4e00\u4e2a\u4f8b\u5b50\u3002 </p> <pre><code>#include &lt;THC/THC.h&gt;\n#include &lt;ATen/ATen.h&gt;\n\nTHCState *state = at::globalContext().thc_state;\n\nextern \"C\" int my_lib_add_forward_cuda(THCudaTensor *input1, THCudaTensor *input2,\n                                        THCudaTensor *output)\n{\n    if (!THCudaTensor_isSameSizeAs(state, input1, input2))\n    return 0;\n    THCudaTensor_resizeAs(state, output, input1);\n    THCudaTensor_cadd(state, output, input1, 1.0, input2);\n    return 1;\n}\n\nextern \"C\" int my_lib_add_backward_cuda(THCudaTensor *grad_output, THCudaTensor *grad_input)\n{\n    THCudaTensor_resizeAs(state, grad_input, grad_output);\n    THCudaTensor_fill(state, grad_input, 1);\n    return 1;\n}\n\n</code></pre>"},{"location":"1.0/notes_windows/#c","title":"C++\u6269\u5c55","text":"<p>\u4e0e\u524d\u4e00\u79cd\u7c7b\u578b\u76f8\u6bd4\uff0c\u8fd9\u79cd\u7c7b\u578b\u7684\u6269\u5c55\u5177\u6709\u66f4\u597d\u7684\u652f\u6301\u3002\u4e0d\u8fc7\u5b83\u4ecd\u7136\u9700\u8981\u4e00\u4e9b\u624b\u52a8\u914d\u7f6e\u3002\u9996\u5148\uff0c\u6253\u5f00VS 2017\u7684x86_x64\u4ea4\u53c9\u5de5\u5177\u547d\u4ee4\u63d0\u793a\u7b26\u3002\u7136\u540e\uff0c\u5728\u5176\u4e2d\u6253\u5f00Git-Bash\u3002\u5b83\u901a\u5e38\u4f4d\u4e8eC\uff1a\\Program Files\\Git\\git-bash.exe\u4e2d\u3002\u6700\u540e\uff0c\u60a8\u53ef\u4ee5\u5f00\u59cb\u7f16\u8bd1\u8fc7\u7a0b\u3002  </p>"},{"location":"1.0/notes_windows/#_5","title":"\u5b89\u88c5","text":""},{"location":"1.0/notes_windows/#win32","title":"\u5728Win32 \u627e\u4e0d\u5230\u5b89\u88c5\u5305","text":"<pre><code>Solving environment: failed\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n- pytorch\n\nCurrent channels:\n- https://conda.anaconda.org/pytorch/win-32\n- https://conda.anaconda.org/pytorch/noarch\n- https://repo.continuum.io/pkgs/main/win-32\n- https://repo.continuum.io/pkgs/main/noarch\n- https://repo.continuum.io/pkgs/free/win-32\n- https://repo.continuum.io/pkgs/free/noarch\n- https://repo.continuum.io/pkgs/r/win-32\n- https://repo.continuum.io/pkgs/r/noarch\n- https://repo.continuum.io/pkgs/pro/win-32\n- https://repo.continuum.io/pkgs/pro/noarch\n- https://repo.continuum.io/pkgs/msys2/win-32\n- https://repo.continuum.io/pkgs/msys2/noarch\n\n</code></pre> <p>Pytorch\u4e0d\u80fd\u572832\u4f4d\u7cfb\u7edf\u4e2d\u5de5\u4f5c\u8fd0\u884c\u3002\u8bf7\u5b89\u88c5\u4f7f\u752864\u4f4d\u7684Windows\u548cPython\u3002  </p>"},{"location":"1.0/notes_windows/#_6","title":"\u5bfc\u5165\u9519\u8bef","text":"<pre><code>from torch._C import *\n\nImportError: DLL load failed: The specified module could not be found.\n</code></pre> <p>\u95ee\u9898\u662f\u7531\u57fa\u672c\u6587\u4ef6\u4e22\u5931\u5bfc\u81f4\u7684\u3002\u5b9e\u9645\u4e0a\uff0c\u9664\u4e86VC2017\u53ef\u518d\u53d1\u884c\u7ec4\u4ef6\u548c\u4e00\u4e9bmkl\u5e93\u4e4b\u5916\uff0c\u6211\u4eec\u51e0\u4e4e\u5305\u542b\u4e86PyTorch\u5bf9conda\u5305\u6240\u9700\u7684\u6240\u6709\u57fa\u672c\u6587\u4ef6\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u952e\u5165\u4ee5\u4e0b\u547d\u4ee4\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002</p> <pre><code>conda install -c peterjc123 vc vs2017_runtime\nconda install mkl_fft intel_openmp numpy mkl\n</code></pre> <p>\u81f3\u4e8ewheel\u5305(\u8f6e\u5b50)\uff0c\u7531\u4e8e\u6211\u4eec\u6ca1\u6709\u5305\u542b\u4e00\u4e9b\u5e93\u548cVS2017\u53ef\u518d\u53d1\u884c\u6587\u4ef6\uff0c\u8bf7\u624b\u52a8\u5b89\u88c5\u5b83\u4eec\u3002\u53ef\u4ee5\u4e0b\u8f7dVS 2017\u53ef\u518d\u53d1\u884c\u5b89\u88c5\u7a0b\u5e8f\u3002\u4f60\u8fd8\u5e94\u8be5\u6ce8\u610f\u4f60\u7684Numpy\u7684\u5b89\u88c5\u3002 \u786e\u4fdd\u5b83\u4f7f\u7528MKL\u800c\u4e0d\u662fOpenBLAS\u7248\u672c\u7684\u3002\u60a8\u53ef\u4ee5\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u3002  </p> <pre><code>pip install numpy mkl intel-openmp mkl_fft\n</code></pre> <p>\u53e6\u5916\u4e00\u79cd\u53ef\u80fd\u662f\u4f60\u5b89\u88c5\u4e86GPU\u7248\u672c\u7684Pytorch\u4f46\u662f\u7535\u8111\u4e2d\u5e76\u6ca1\u6709NVIDIA\u7684\u663e\u5361\u3002\u78b0\u5230\u8fd9\u79cd\u60c5\u51b5\uff0c\u5c31\u628aGPU\u7248\u672c\u7684Pytorch\u6362\u6210CPU\u7248\u672c\u7684\u5c31\u597d\u4e86\u3002  </p> <pre><code>from torch._C import *\n\nImportError: DLL load failed: The operating system cannot run %1.\n</code></pre> <p>\u8fd9\u5b9e\u9645\u4e0a\u662fAnaconda\u7684\u4e0a\u6e38\u95ee\u9898\u3002\u4f7f\u7528conda-forge\u901a\u9053\u521d\u59cb\u5316\u73af\u5883\u65f6,\u5c06\u51fa\u73b0\u6b64\u95ee\u9898\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u6b64\u547d\u4ee4\u4fee\u590dintel-openmp\u5e93\u3002  </p>"},{"location":"1.0/notes_windows/#_7","title":"\u4f7f\u7528(\u5e76\u884c\u5904\u7406\uff09","text":""},{"location":"1.0/notes_windows/#if","title":"\u65e0if\u8bed\u53e5\u4fdd\u62a4\u7684\u591a\u8fdb\u7a0b\u5904\u7406\u9519\u8bef","text":"<pre><code>RuntimeError:\n    An attempt has been made to start a new process before the\n    current process has finished its bootstrapping phase.\n\n   This probably means that you are not using fork to start your\n   child processes and you have forgotten to use the proper idiom\n   in the main module:\n\n       if __name__ == '__main__':\n           freeze_support()\n           ...\n\n   The \"freeze_support()\" line can be omitted if the program\n   is not going to be frozen to produce an executable.\n\n</code></pre> <p>\u5728Windows\u4e0a\u5b9e\u73b0<code>\u591a\u8fdb\u7a0b\u5904\u7406</code>\u662f\u4e0d\u540c\u7684\uff0c\u5b83\u4f7f\u7528\u7684\u662fspawn\u800c\u4e0d\u662ffork\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u5fc5\u987b\u4f7f\u7528if\u5b50\u53e5\u5305\u88c5\u4ee3\u7801\uff0c\u4ee5\u9632\u6b62\u4ee3\u7801\u6267\u884c\u591a\u6b21\u3002\u5c06\u60a8\u7684\u4ee3\u7801\u91cd\u6784\u4e3a\u4ee5\u4e0b\u7ed3\u6784\u3002 </p> <pre><code>import torch\n\ndef main()\n    for i, data in enumerate(dataloader):\n        # do something here\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"1.0/notes_windows/#_8","title":"\u591a\u8fdb\u7a0b\u5904\u7406\u9519\u8bef\u201c\u574f\u9053\u201d","text":"<pre><code>ForkingPickler(file, protocol).dump(obj)\n\nBrokenPipeError: [Errno 32] Broken pipe\n</code></pre> <p>\u5f53\u5728\u7236\u8fdb\u7a0b\u5b8c\u6210\u53d1\u9001\u6570\u636e\u4e4b\u524d\u5b50\u8fdb\u7a0b\u7ed3\u675f\u65f6\uff0c\u4f1a\u53d1\u751f\u6b64\u95ee\u9898\u3002\u60a8\u7684\u4ee3\u7801\u53ef\u80fd\u6709\u95ee\u9898\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5c06DataLoader\u7684num_worker\u51cf\u5c11\u4e3a\u96f6\u6765\u8c03\u8bd5\u4ee3\u7801\uff0c\u5e76\u67e5\u770b\u95ee\u9898\u662f\u5426\u4ecd\u7136\u5b58\u5728\u3002  </p>"},{"location":"1.0/notes_windows/#_9","title":"\u591a\u8fdb\u7a0b\u5904\u7406\u9519\u8bef\u201c\u9a71\u52a8\u7a0b\u5e8f\u5173\u95ed\u201d","text":"<pre><code>Couldn't open shared file mapping: &lt;torch_14808_1591070686&gt;, error code: &lt;1455&gt; at torch\\lib\\TH\\THAllocator.c:154\n\n[windows] driver shut down\n</code></pre> <p>\u8bf7\u66f4\u65b0\u60a8\u7684\u663e\u5361\u9a71\u52a8\u7a0b\u5e8f\u3002\u5982\u679c\u8fd9\u79cd\u60c5\u51b5\u6301\u7eed\u5b58\u5728\uff0c\u5219\u53ef\u80fd\u662f\u60a8\u7684\u663e\u5361\u592a\u65e7\u6216\u6240\u9700\u8981\u7684\u8ba1\u7b97\u80fd\u529b\u5bf9\u60a8\u7684\u663e\u5361\u8d1f\u62c5\u592a\u91cd\u3002\u8bf7\u6839\u636e\u8fd9\u7bc7\u6587\u7ae0\u66f4\u65b0TDR\u8bbe\u7f6e\u3002</p>"},{"location":"1.0/notes_windows/#cuda-ipc","title":"CUDA IPC\u64cd\u4f5c","text":"<pre><code>THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n</code></pre> <p>Windows\u4e0d\u652f\u6301\u5b83\u4eec\u3002\u5728CUDA\u5f20\u91cf\u4e0a\u8fdb\u884c\u5e76\u884c\u5904\u7406\u8fd9\u6837\u7684\u4e8b\u60c5\u65e0\u6cd5\u6210\u529f\uff0c\u6709\u4e24\u79cd\u9009\u62e9:  </p> <p>1.\u4e0d\u8981\u4f7f\u7528\u5e76\u884c\u5904\u7406\u3002\u5c06Data Loader\u7684num_worker\u8bbe\u7f6e\u4e3a\u96f6\u3002  </p> <p>2.\u91c7\u7528\u5171\u4eabCPU\u5f20\u91cf\u65b9\u6cd5\u3002\u786e\u4fdd\u60a8\u7684\u81ea\u5b9a\u4e49<code>DataSet</code>\u8fd4\u56deCPU\u5f20\u91cf\u3002</p>"},{"location":"1.0/numpy_extensions_tutorial/","title":"\u7528 numpy \u548c scipy \u521b\u5efa\u6269\u5c55","text":"<p>\u8bd1\u8005\uff1acangyunye</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005: Adam Paszke</p> <p>\u4fee\u8ba2\u8005: Adam Dziedzic</p> <p>\u5728\u8fd9\u4e2a\u6559\u7a0b\u91cc\uff0c\u6211\u4eec\u8981\u5b8c\u6210\u4e24\u4e2a\u4efb\u52a1:</p> <ol> <li> <p>\u521b\u5efa\u4e00\u4e2a\u65e0\u53c2\u795e\u7ecf\u7f51\u7edc\u5c42\u3002</p> <p>\u8fd9\u91cc\u9700\u8981\u8c03\u7528numpy\u4f5c\u4e3a\u5b9e\u73b0\u7684\u4e00\u90e8\u5206\u3002</p> </li> <li> <p>\u521b\u5efa\u4e00\u4e2a\u6743\u91cd\u81ea\u4e3b\u4f18\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u3002</p> <p>\u8fd9\u91cc\u9700\u8981\u8c03\u7528Scipy\u4f5c\u4e3a\u5b9e\u73b0\u7684\u4e00\u90e8\u5206\u3002</p> </li> </ol> <pre><code>import torch\nfrom torch.autograd import Function\n\n</code></pre>"},{"location":"1.0/numpy_extensions_tutorial/#_1","title":"\u65e0\u53c2\u6570\u795e\u7ecf\u7f51\u7edc\u5c42\u793a\u4f8b","text":"<p>\u8fd9\u4e00\u5c42\u5e76\u6ca1\u6709\u7279\u610f\u505a\u4ec0\u4e48\u4efb\u4f55\u6709\u7528\u7684\u4e8b\u6216\u8005\u53bb\u8fdb\u884c\u6570\u5b66\u4e0a\u7684\u4fee\u6b63\u3002</p> <p>\u5b83\u53ea\u662f\u88ab\u6070\u5f53\u7684\u547d\u540d\u4e3aBadFFTFunction</p> <p>\u672c\u5c42\u7684\u5b9e\u73b0\u65b9\u5f0f</p> <pre><code>from numpy.fft import rfft2, irfft2\n\nclass BadFFTFunction(Function):\n\n    def forward(self, input):\n        numpy_input = input.detach().numpy()\n        result = abs(rfft2(numpy_input))\n        return input.new(result)\n\n    def backward(self, grad_output):\n        numpy_go = grad_output.numpy()\n        result = irfft2(numpy_go)\n        return grad_output.new(result)\n\n# \u7531\u4e8e\u672c\u5c42\u6ca1\u6709\u4efb\u4f55\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u7684\u58f0\u660e\u4e3a\u4e00\u4e2a\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u5f53\u505a nn.Module \u7c7b\n\ndef incorrect_fft(input):\n    return BadFFTFunction()(input)\n\n</code></pre> <p>\u521b\u5efa\u65e0\u53c2\u6570\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u793a\u4f8b\u65b9\u6cd5:</p> <pre><code>input = torch.randn(8, 8, requires_grad=True)\nresult = incorrect_fft(input)\nprint(result)\nresult.backward(torch.randn(result.size()))\nprint(input)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>tensor([[2.2488e-03, 5.1309e+00, 6.4310e+00, 6.0649e+00, 8.1197e+00],\n        [3.4379e+00, 1.5772e+00, 1.0834e+01, 5.2234e+00, 1.0509e+01],\n        [2.6480e+00, 1.2934e+01, 9.1619e+00, 1.6011e+01, 9.7914e+00],\n        [4.0796e+00, 8.6867e+00, 8.8971e+00, 1.0232e+01, 5.7227e+00],\n        [1.8085e+01, 5.4060e+00, 5.2141e+00, 3.5451e+00, 5.1584e+00],\n        [4.0796e+00, 8.2662e+00, 1.1570e+01, 8.7164e+00, 5.7227e+00],\n        [2.6480e+00, 4.5982e+00, 1.1056e+00, 8.8158e+00, 9.7914e+00],\n        [3.4379e+00, 6.2059e+00, 5.9354e+00, 3.1194e+00, 1.0509e+01]],\n       grad_fn=&lt;BadFFTFunction&gt;)\ntensor([[-0.6461,  0.3270, -1.2190, -0.5480, -1.7273, -0.7326,  0.6294, -0.2311],\n        [ 0.4305,  1.7503, -0.2914, -0.4237,  0.5441,  1.6597, -0.5645, -0.7901],\n        [ 0.4248, -2.5986, -0.9257, -0.8651, -0.1673,  1.5749, -1.1857,  1.2867],\n        [-0.5180,  2.3175, -1.9279,  1.2128,  0.7789,  0.0385, -1.1871,  0.3431],\n        [ 0.6934,  1.0216, -0.7450,  0.0463, -1.5447, -1.5220,  0.9389, -0.5811],\n        [ 1.9286, -1.0957,  0.6878, -0.5469, -0.5505,  0.5088,  0.8965,  0.4874],\n        [-0.2699,  0.3370,  0.3749, -0.3639, -0.0599,  0.8904,  0.1679, -1.8218],\n        [-0.2963,  0.2246,  0.6617,  1.2258,  0.1530,  0.3114,  0.4568,  0.6181]],\n       requires_grad=True)\n\n</code></pre>"},{"location":"1.0/numpy_extensions_tutorial/#_2","title":"\u53c2\u6570\u5316\u793a\u4f8b","text":"<p>\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u6587\u732e\u4e2d\uff0c\u8fd9\u4e00\u5c42\u88ab\u610f\u5916\u7684\u79f0\u4f5c\u5377\u79ef<code>convolution</code>\uff0c\u5c3d\u7ba1\u5b9e\u9645\u64cd\u4f5c\u662f\u4ea4\u53c9-\u5173\u8054\u6027<code>cross-correlation</code> (\u552f\u4e00\u7684\u533a\u522b\u662f\u8fc7\u6ee4\u5668<code>filter</code>\u662f\u4e3a\u4e86\u5377\u79ef\u800c\u7ffb\u8f6c\uff0c\u800c\u4e0d\u662f\u4e3a\u4e86\u4ea4\u53c9\u5173\u8054)\u3002</p> <p>\u672c\u5c42\u7684\u53ef\u81ea\u4f18\u5316\u6743\u91cd\u7684\u5b9e\u73b0\uff0c\u4f9d\u8d56\u4e8e\u4ea4\u53c9-\u5173\u8054<code>cross-correlation</code> \u4e00\u4e2a\u8868\u793a\u6743\u91cd\u7684\u8fc7\u6ee4\u5668filter (kernel)\u3002</p> <p>\u5411\u540e\u4f20\u64ad\u7684\u51fd\u6570<code>backward</code>\u8ba1\u7b97\u7684\u662f\u8f93\u5165\u6570\u636e\u7684\u68af\u5ea6\u4ee5\u53ca\u8fc7\u6ee4\u5668\u7684\u68af\u5ea6\u3002</p> <pre><code>from numpy import flip\nimport numpy as np\nfrom scipy.signal import convolve2d, correlate2d\nfrom torch.nn.modules.module import Module\nfrom torch.nn.parameter import Parameter\n\nclass ScipyConv2dFunction(Function):\n    @staticmethod\n    def forward(ctx, input, filter, bias):\n        # detach so we can cast to NumPy\n        input, filter, bias = input.detach(), filter.detach(), bias.detach()\n        result = correlate2d(input.numpy(), filter.numpy(), mode='valid')\n        result += bias.numpy()\n        ctx.save_for_backward(input, filter, bias)\n        return torch.as_tensor(result, dtype=input.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_output = grad_output.detach()\n        input, filter, bias = ctx.saved_tensors\n        grad_output = grad_output.numpy()\n        grad_bias = np.sum(grad_output, keepdims=True)\n        grad_input = convolve2d(grad_output, filter.numpy(), mode='full')\n        # \u4e0a\u4e00\u884c\u53ef\u4ee5\u7b49\u6548\u8868\u793a\u4e3a:\n        # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')\n        grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')\n        return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)\n\nclass ScipyConv2d(Module):\n    def __init__(self, filter_width, filter_height):\n        super(ScipyConv2d, self).__init__()\n        self.filter = Parameter(torch.randn(filter_width, filter_height))\n        self.bias = Parameter(torch.randn(1, 1))\n\n    def forward(self, input):\n        return ScipyConv2dFunction.apply(input, self.filter, self.bias)\n\n</code></pre> <p>\u793a\u4f8b:</p> <pre><code>module = ScipyConv2d(3, 3)\nprint(\"Filter and bias: \", list(module.parameters()))\ninput = torch.randn(10, 10, requires_grad=True)\noutput = module(input)\nprint(\"Output from the convolution: \", output)\noutput.backward(torch.randn(8, 8))\nprint(\"Gradient for the input map: \", input.grad)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Filter and bias:  [Parameter containing:\ntensor([[-0.8330,  0.3568,  1.3209],\n        [-0.5273, -0.9138, -1.0039],\n        [-1.1179,  1.3722,  1.5137]], requires_grad=True), Parameter containing:\ntensor([[0.1973]], requires_grad=True)]\nOutput from the convolution:  tensor([[-0.7304, -3.5437,  2.4701,  1.0625, -1.8347,  3.3246,  2.5547, -1.1341],\n        [-5.0441, -7.1261,  2.8344,  2.5797, -2.4117, -1.4123, -0.2520, -3.1231],\n        [ 1.2296, -0.7957,  1.9413,  1.5257,  0.2727,  6.2466,  2.3363,  2.1833],\n        [-2.6944, -3.3933,  2.3844,  0.2523, -2.0322, -3.1275, -0.2472,  1.5382],\n        [ 3.6807, -1.1985, -3.9278,  0.8025,  3.3435,  6.6806,  1.1656,  1.3711],\n        [-1.7426,  1.3875,  8.2674, -0.8234, -4.7534,  3.0932,  1.3048,  2.1184],\n        [ 0.2095,  1.3225,  0.9022,  3.3324,  0.8768, -5.3459, -1.0970, -4.5304],\n        [ 2.1688, -1.7967, -0.5568, -9.3585,  0.3259,  5.4264,  2.8449,  6.8120]],\n       grad_fn=&lt;ScipyConv2dFunctionBackward&gt;)\nGradient for the input map:  tensor([[ 7.7001e-01, -2.6786e-02, -1.0917e+00, -4.1148e-01,  2.2833e-01,\n         -1.7494e+00, -1.4960e+00,  2.3307e-01,  2.2004e+00,  3.1210e+00],\n        [ 7.0960e-02,  1.8954e+00,  2.0912e+00, -1.3058e+00, -6.1822e-02,\n          3.8630e+00, -5.1720e-01, -6.9586e+00, -2.5478e+00, -1.4459e+00],\n        [ 9.3677e-01, -7.5248e-01,  3.0795e-03, -2.1788e+00, -2.6326e+00,\n         -3.4089e+00,  2.2524e-01,  4.7127e+00,  3.7717e+00,  2.0393e+00],\n        [-2.0010e+00,  2.7616e+00,  4.0060e+00, -2.0298e+00,  1.6074e+00,\n          2.3062e+00, -5.4927e+00, -5.3029e+00,  3.5081e+00,  4.5952e+00],\n        [ 3.4492e-01, -2.3043e+00, -1.5235e+00, -3.3520e+00, -1.3291e-01,\n          1.4629e+00,  1.9298e+00,  4.5369e-01, -1.5986e+00, -2.3851e+00],\n        [-2.3929e+00,  5.3965e+00,  5.1353e+00, -1.0269e+00,  2.1031e+00,\n         -6.2344e+00, -3.6539e+00, -1.7951e+00, -5.6712e-01,  8.6987e-01],\n        [ 1.1006e-01, -1.5961e+00,  1.2179e+00,  3.4799e-01, -7.1710e-01,\n          2.5705e+00,  4.5020e-01,  3.8066e+00,  4.8558e+00,  2.1423e+00],\n        [-9.9457e-01,  1.5614e+00,  1.3985e+00,  3.6700e+00, -1.9708e+00,\n         -2.4845e+00,  2.5387e+00, -1.2250e+00, -4.6877e+00, -3.3492e+00],\n        [-4.5289e-01,  2.4210e+00,  3.3681e+00, -2.7785e+00,  1.5472e+00,\n         -5.0358e-01, -9.7416e-01,  1.1032e+00,  2.0812e-01,  8.2830e-01],\n        [ 1.1052e+00, -2.5233e+00,  2.0461e+00,  1.1886e-01, -4.8352e+00,\n          2.4197e-01, -1.5177e-01, -6.9245e-01, -1.8357e+00, -1.5302e+00]])\n\n</code></pre> <p>\u68af\u5ea6\u68c0\u67e5:</p> <pre><code>from torch.autograd.gradcheck import gradcheck\n\nmoduleConv = ScipyConv2d(3, 3)\n\ninput = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]\ntest = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)\nprint(\"Are the gradients correct: \", test)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Are the gradients correct:  True\n\n</code></pre>"},{"location":"1.0/onnx/","title":"torch.onnx","text":"<p>\u8bd1\u8005\uff1aguobaoyo</p>"},{"location":"1.0/onnx/#pytorchcaffe2alexnet","title":"\u793a\u4f8b:\u4ecePytorch\u5230Caffe2\u7684\u7aef\u5bf9\u7aefAlexNet\u6a21\u578b","text":"<p>\u8fd9\u91cc\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u811a\u672c\u7a0b\u5e8f,\u5b83\u5c06\u4e00\u4e2a\u5728 torchvision \u4e2d\u5df2\u7ecf\u5b9a\u4e49\u7684\u9884\u8bad\u7ec3 AlexNet \u6a21\u578b\u5bfc\u51fa\u5230 ONNX \u683c\u5f0f. \u5b83\u4f1a\u8fd0\u884c\u4e00\u6b21,\u7136\u540e\u628a\u6a21\u578b\u4fdd\u5b58\u81f3 <code>alexnet.onnx</code>:</p> <pre><code>import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# \u53ef\u4ee5\u6839\u636e\u6a21\u5757\u56fe\u5f62\u7684\u6570\u503c\u8bbe\u7f6e\u8f93\u5165\u8f93\u51fa\u7684\u663e\u793a\u540d\u79f0\u3002\u8fd9\u4e9b\u8bbe\u7f6e\u4e0d\u4f1a\u6539\u53d8\u6b64\u56fe\u5f62\u7684\u8bed\u4e49\u3002\u53ea\u662f\u4f1a\u53d8\u5f97\u66f4\u52a0\u53ef\u8bfb\u4e86\u3002\n#\u8be5\u7f51\u7edc\u7684\u8f93\u5165\u5305\u542b\u4e86\u8f93\u5165\u7684\u6241\u5e73\u8868(flat list)\u3002\u4e5f\u5c31\u662f\u8bf4\u4f20\u5165forward()\u91cc\u9762\u7684\u503c\uff0c\u5176\u540e\u662f\u6241\u5e73\u8868\u7684\u53c2\u6570\u3002\u4f60\u53ef\u4ee5\u6307\u5b9a\u4e00\u90e8\u5206\u540d\u5b57\uff0c\u4f8b\u5982\u6307\u5b9a\u4e00\u4e2a\u6bd4\u8be5\u6a21\u5757\u8f93\u5165\u6570\u91cf\u66f4\u5c11\u7684\u8868\uff0c\u968f\u540e\u6211\u4eec\u4f1a\u4ece\u4e00\u5f00\u59cb\u5c31\u8bbe\u5b9a\u540d\u5b57\u3002\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n</code></pre> <p>\u5f97\u5230\u7684 <code>alexnet.onnx</code> \u662f\u4e00\u4e2a protobuf \u4e8c\u503c\u6587\u4ef6, \u5b83\u5305\u542b\u6240\u5bfc\u51fa\u6a21\u578b ( \u8fd9\u91cc\u662f AlexNet )\u4e2d\u7f51\u7edc\u67b6\u6784\u548c\u7f51\u7edc\u53c2\u6570. \u5173\u952e\u53c2\u6570 <code>verbose=True</code> \u4f1a\u4f7f\u5bfc\u51fa\u8fc7\u7a0b\u4e2d\u6253\u5370\u51fa\u7684\u7f51\u7edc\u66f4\u53ef\u8bfb:</p> <pre><code>#\u8fd9\u4e9b\u662f\u7f51\u7edc\u7684\u8f93\u5165\u548c\u53c2\u6570\uff0c\u5305\u542b\u4e86\u6211\u4eec\u4e4b\u524d\u8bbe\u5b9a\u7684\u540d\u79f0\u3002\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- \u4e3a\u4e86\u7b80\u4ecb\u53ef\u4ee5\u7701\u7565 ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # \u6bcf\u4e2a\u58f0\u660e\u90fd\u5305\u542b\u4e86\u4e00\u4e9b\u8f93\u51fa\u5f20\u91cf\u4ee5\u53ca\u4ed6\u4eec\u7684\u7c7b\u578b\uff0c\u4ee5\u53ca\u5373\u5c06\u8fd0\u884c\u7684\u64cd\u4f5c\u7b26(\u5e76\u4e14\u5305\u542b\u5b83\u7684\u5c5e\u6027\uff0c\u4f8b\u5982\u6838\u90e8\u5206\uff0c\u6b65\u957f\u7b49\u7b49\uff09\u5b83\u7684\u8f93\u5165\u5f20\u91cf(%actual_input_1, %learned_0, %learned_1\uff09\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- \u4e3a\u4e86\u7b80\u6d01\u53ef\u4ee5\u7701\u7565 ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  #\u52a8\u6001\u610f\u5473\u7740\u5b83\u7684\u5f62\u72b6\u662f\u672a\u77e5\u7684\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u6211\u4eec\u7684\u6267\u884c\u64cd\u4f5c\u6216\u8005\u5176\u5f62\u72b6\u5927\u5c0f\u662f\u5426\u786e\u5b9e\u4e3a\u52a8\u6001\u7684\u800c\u53d7\u5230\u4e86\u9650\u5236\u3002(\u8fd9\u4e00\u70b9\u6211\u4eec\u60f3\u5728\u5c06\u6765\u7684\u7248\u672c\u4e2d\u4fee\u590d\uff09\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- \u4e3a\u4e86\u7b80\u6d01\u53ef\u4ee5\u7701\u7565 ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}\n</code></pre> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528 onnx \u5e93\u9a8c\u8bc1 protobuf, \u5e76\u4e14\u7528 conda \u5b89\u88c5 <code>onnx</code></p> <pre><code>conda install -c conda-forge onnx\n\n</code></pre> <p>\u7136\u540e\u8fd0\u884c:</p> <pre><code>import onnx\n\n# \u8f7d\u5165onnx\u6a21\u5757\nmodel = onnx.load(\"alexnet.onnx\")\n\n#\u68c0\u67e5IR\u662f\u5426\u826f\u597d\nonnx.checker.check_model(model)\n\n#\u8f93\u51fa\u4e00\u4e2a\u56fe\u5f62\u7684\u53ef\u8bfb\u8868\u793a\u65b9\u5f0f\nonnx.helper.printable_graph(model.graph)\n\n</code></pre> <p>\u4e3a\u4e86\u80fd\u591f\u4f7f\u7528 caffe2 \u8fd0\u884c\u811a\u672c\uff0c\u4f60\u9700\u8981\u5b89\u88c5 Caffe2. \u5982\u679c\u4f60\u4e4b\u524d\u6ca1\u6709\u5b89\u88c5,\u8bf7\u53c2\u7167 \u5b89\u88c5\u6307\u5357\u3002 \u4e00\u65e6\u8fd9\u4e9b\u5b89\u88c5\u5b8c\u6210,\u4f60\u5c31\u53ef\u4ee5\u5728\u540e\u53f0\u4f7f\u7528 Caffe2 :</p> <pre><code># ...\u63a5\u7740\u4e0a\u9762\u7684\u7ee7\u7eed\nimport onnx_caffe2.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") #\u6216\u8005 \"CPU\"\n#\u540e\u53f0\u8fd0\u884cCaffe2\uff1a\n#rep.predict_net\u662f\u8be5\u7f51\u7edc\u7684Caffe2 protobuf\n#rep.workspace\u662f\u8be5\u7f51\u7edc\u7684Caffe2 workspace\n#(\u8be6\u89c1\u7c7b\u201connx_caffe2.backend.Workspace\u201d\uff09\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n#\u4e3a\u4e86\u591a\u8f93\u5165\u5730\u8fd0\u884c\u8be5\u7f51\u7edc\uff0c\u5e94\u8be5\u4f20\u9012\u5143\u7ec4\u800c\u4e0d\u662f\u4e00\u4e2a\u5355\u5143\u683c\u3002\nprint(outputs[0])\n\n</code></pre> <p>\u4e4b\u540e,\u6211\u4eec\u8fd8\u4f1a\u63d0\u4f9b\u5176\u5b83\u6846\u67b6\u7684\u540e\u7aef\u652f\u6301.</p>"},{"location":"1.0/onnx/#_1","title":"\u5c40\u9650","text":"<ul> <li>ONNX \u5bfc\u51fa\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f68\u8ff9\u7684\u5bfc\u51fa\u5668\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u6267\u884c\u65f6\u9700\u8981\u8fd0\u884c\u4e00\u6b21\u6a21\u578b\uff0c\u7136\u540e\u5bfc\u51fa\u5b9e\u9645\u53c2\u4e0e\u8fd0\u7b97\u7684\u8fd0\u7b97\u7b26\u3002\u8fd9\u4e5f\u610f\u5473\u7740\uff0c\u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u52a8\u6001\u7684\uff0c\u4f8b\u5982\uff0c\u6539\u53d8\u4e00\u4e9b\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7684\u64cd\u4f5c\uff0c\u8fd9\u65f6\u7684\u5bfc\u51fa\u7ed3\u679c\u662f\u4e0d\u51c6\u786e\u7684\u3002\u540c\u6837\uff0c\u4e00\u4e2a\u8f68\u8ff9\u53ef\u80fd\u53ea\u5bf9\u4e00\u4e2a\u5177\u4f53\u7684\u8f93\u5165\u5c3a\u5bf8\u6709\u6548 (\u8fd9\u5c31\u662f\u6211\u4eec\u5728\u8f68\u8ff9\u4e2d\u9700\u8981\u6709\u660e\u786e\u7684\u8f93\u5165\u7684\u539f\u56e0\u4e4b\u4e00\u3002) \u6211\u4eec\u5efa\u8bae\u68c0\u67e5\u6a21\u578b\u7684\u8f68\u8ff9\uff0c\u786e\u4fdd\u88ab\u8ffd\u8e2a\u7684\u8fd0\u7b97\u7b26\u662f\u5408\u7406\u7684\u3002</li> <li>Pytorch\u548cCaffe2\u4e2d\u7684\u4e00\u4e9b\u8fd0\u7b97\u7b26\u7ecf\u5e38\u6709\u7740\u6570\u503c\u4e0a\u7684\u5dee\u5f02.\u6839\u636e\u6a21\u578b\u7684\u7ed3\u6784,\u8fd9\u4e9b\u5dee\u5f02\u53ef\u80fd\u662f\u5fae\u5c0f\u7684,\u4f46\u5b83\u4eec\u4f1a\u5728\u8868\u73b0\u4e0a\u4ea7\u751f\u5f88\u5927\u7684\u5dee\u522b (\u5c24\u5176\u662f\u5bf9\u4e8e\u672a\u8bad\u7ec3\u7684\u6a21\u578b\u3002)\u4e4b\u540e\uff0c\u4e3a\u4e86\u5e2e\u52a9\u4f60\u5728\u51c6\u786e\u5ea6\u8981\u6c42\u5f88\u9ad8\u7684\u60c5\u51b5\u4e2d\uff0c\u80fd\u591f\u8f7b\u677e\u5730\u907f\u514d\u8fd9\u4e9b\u5dee\u5f02\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u8ba1\u5212\u8ba9Caffe2\u80fd\u591f\u76f4\u63a5\u8c03\u7528Torch\u7684\u8fd0\u7b97\u7b26.</li> </ul>"},{"location":"1.0/onnx/#_2","title":"\u652f\u6301\u7684\u8fd0\u7b97\u7b26","text":"<p>\u4ee5\u4e0b\u662f\u5df2\u7ecf\u88ab\u652f\u6301\u7684\u8fd0\u7b97\u7b26:</p> <ul> <li>add (\u4e0d\u652f\u6301\u975e\u96f6\u03b1)</li> <li>sub (\u4e0d\u652f\u6301\u975e\u96f6\u03b1)</li> <li>mul</li> <li>div</li> <li>cat</li> <li>mm</li> <li>addmm</li> <li>neg</li> <li>sqrt</li> <li>tanh</li> <li>sigmoid</li> <li>mean</li> <li>sum</li> <li>prod</li> <li>t</li> <li>expand (\u53ea\u6709\u5728\u6269\u5c55onnx\u64cd\u4f5c\u7b26\u4e4b\u524d\u53ef\u4ee5\u4f7f\u7528\uff0c\u4f8b\u5982add)</li> <li>transpose</li> <li>view</li> <li>split</li> <li>squeeze</li> <li>prelu (\u4e0d\u652f\u6301\u8f93\u5165\u901a\u9053\u4e4b\u95f4\u7684\u5355\u91cd\u5171\u4eab)</li> <li>threshold (\u4e0d\u652f\u6301\u975e\u96f6\u503c\u9608\u503c/\u975e\u96f6\u503c)</li> <li>leaky_relu</li> <li>glu</li> <li>softmax (\u53ea\u652f\u6301dim=-1)</li> <li>avg_pool2d (\u4e0d\u652f\u6301ceil_mode)</li> <li>log_softmax</li> <li>unfold (\u4e3aATen-Caffe2\u96c6\u6210\u4f5c\u5b9e\u9a8c\u652f\u6491)</li> <li>elu</li> <li>concat</li> <li>abs</li> <li>index_select</li> <li>pow</li> <li>clamp</li> <li>max</li> <li>min</li> <li>eq</li> <li>gt</li> <li>lt</li> <li>ge</li> <li>le</li> <li>exp</li> <li>sin</li> <li>cos</li> <li>tan</li> <li>asin</li> <li>acos</li> <li>atan</li> <li>permute</li> <li>Conv</li> <li>BatchNorm</li> <li>MaxPool1d (\u4e0d\u652f\u6301ceil_mode)</li> <li>MaxPool2d (\u4e0d\u652f\u6301ceil_mode)</li> <li>MaxPool3d (\u4e0d\u652f\u6301ceil_mode)</li> <li>Embedding (\u4e0d\u652f\u6301\u53ef\u9009\u53c2\u6570)</li> <li>RNN</li> <li>ConstantPadNd</li> <li>Dropout</li> <li>FeatureDropout (\u4e0d\u652f\u6301\u8bad\u7ec3\u6a21\u5f0f)</li> <li>Index (\u652f\u6301\u5e38\u91cf\u6574\u6570\u548c\u5143\u7ec4\u7d22\u5f15)</li> </ul> <p>\u4e0a\u9762\u7684\u8fd0\u7b97\u7b26\u8db3\u591f\u5bfc\u51fa\u4e0b\u9762\u7684\u6a21\u578b:</p> <ul> <li>AlexNet</li> <li>DCGAN</li> <li>DenseNet</li> <li>Inception (\u6ce8\u610f:\u8be5\u6a21\u578b\u5bf9\u64cd\u4f5c\u7b26\u5341\u5206\u654f\u611f)</li> <li>ResNet</li> <li>SuperResolution</li> <li>VGG</li> <li>word_language_model</li> </ul> <p>\u4e3a\u64cd\u4f5c\u7b26\u589e\u52a0\u5bfc\u51fa\u652f\u6301\u662f\u4e00\u79cd \u63d0\u524d\u7684\u7528\u6cd5\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5f00\u53d1\u8005\u9700\u8981\u638c\u63e1PyTorch\u7684\u6e90\u4ee3\u7801\u3002\u8bf7\u6309\u7167\u8fd9\u4e2a\u7f51\u5740\u94fe\u63a5 \u53bb\u4e0b\u8f7dPyTorch\u3002\u5982\u679c\u60a8\u60f3\u8981\u7684\u8fd0\u7b97\u7b26\u5df2\u7ecf\u5728ONNX\u6807\u51c6\u5316\u4e86\uff0c\u90a3\u4e48\u652f\u6301\u5bf9\u5bfc\u51fa\u6b64\u7c7b\u8fd0\u7b97\u7b26\u7684\u64cd\u4f5c(\u4e3a\u8fd0\u7b97\u7b26\u6dfb\u52a0\u7b26\u53f7\u51fd\u6570\uff09\u5c31\u5f88\u5bb9\u6613\u4e86\u3002\u4e3a\u4e86\u786e\u8ba4\u8fd0\u7b97\u7b26\u662f\u5426\u5df2\u7ecf\u88ab\u6807\u51c6\u5316\uff0c\u8bf7\u68c0\u67e5ONNX \u64cd\u4f5c\u7b26\u5217\u8868.\u5982\u679c\u8fd9\u4e2a\u64cd\u4f5c\u7b26\u662fATen\u64cd\u4f5c\u7b26\uff0c\u8fd9\u5c31\u610f\u5473\u7740\u4f60\u53ef\u4ee5\u5728 <code>torch/csrc/autograd/generated/VariableType.h</code>\u627e\u5230\u5b83\u7684\u5b9a\u4e49\u3002(\u5728PyTorch\u5b89\u88c5\u6587\u4ef6\u5217\u8868\u7684\u5408\u6210\u7801\u4e2d\u53ef\u89c1)\uff0c\u4f60\u5e94\u8be5\u5728 <code>torch/onnx/symbolic.py</code>\u91cc\u9762\u52a0\u4e0a\u7b26\u53f7\u5e76\u4e14\u9075\u5faa\u4e0b\u9762\u7684\u6307\u4ee4\uff1a *   \u5728 torch/onnx/symbolic.py\u91cc\u9762\u5b9a\u4e49\u7b26\u53f7\u3002\u786e\u4fdd\u8be5\u529f\u80fd\u4e0e\u5728ATen\u64cd\u4f5c\u7b26\u5728<code>VariableType.h</code>\u7684\u529f\u80fd\u76f8\u540c\u3002 *   \u7b2c\u4e00\u4e2a\u53c2\u6570\u603b\u662fONNX\u56fe\u5f62\u53c2\u6570\uff0c\u53c2\u6570\u7684\u540d\u5b57\u5fc5\u987b\u4e0e <code>VariableType.h</code>\u91cc\u7684\u5339\u914d\uff0c\u56e0\u4e3a\u8c03\u5ea6\u662f\u4f9d\u8d56\u4e8e\u5173\u952e\u5b57\u53c2\u6570\u5b8c\u6210\u7684\u3002 *   \u53c2\u6570\u6392\u5e8f\u4e0d\u9700\u8981\u4e25\u683c\u4e0e<code>VariableType.h</code>\u5339\u914d\uff0c\u9996\u5148\u7684\u5f20\u91cf\u4e00\u5b9a\u662f\u8f93\u5165\u7684\u5f20\u91cf\uff0c\u7136\u540e\u662f\u975e\u5f20\u91cf\u53c2\u6570\u3002 *   \u5728\u7b26\u53f7\u529f\u80fd\u91cc\uff0c\u5982\u679c\u64cd\u4f5c\u7b26\u5df2\u7ecf\u5728ONNX\u6807\u51c6\u5316\u4e86\uff0c\u6211\u4eec\u53ea\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4ee3\u7801\u53bb\u8868\u793a\u5728\u56fe\u5f62\u91cc\u9762\u7684ONNX\u64cd\u4f5c\u7b26\u3002 *   \u5982\u679c\u8f93\u5165\u53c2\u6570\u662f\u4e00\u4e2a\u5f20\u91cf\uff0c\u4f46\u662fONNX\u9700\u8981\u7684\u662f\u4e00\u4e2a\u6807\u91cf\u5f62\u5f0f\u7684\u8f93\u5165\uff0c\u6211\u4eec\u9700\u8981\u505a\u4e2a\u8f6c\u5316\u3002<code>_scalar</code>\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5c06\u4e00\u4e2a\u5f20\u91cf\u8f6c\u5316\u4e3a\u4e00\u4e2apython\u6807\u91cf\uff0c\u5e76\u4e14<code>_if_scalar_type_as</code>\u51fd\u6570\u53ef\u4ee5\u5c06python\u6807\u91cf\u8f6c\u5316\u4e3aPyTorch\u5f20\u91cf\u3002</p> <p>\u5982\u679c\u64cd\u4f5c\u7b26\u662f\u4e00\u4e2a\u975eATen\u64cd\u4f5c\u7b26\uff0c\u90a3\u4e48\u7b26\u53f7\u529f\u80fd\u9700\u8981\u52a0\u5728\u76f8\u5e94\u7684PyTorch\u51fd\u6570\u7c7b\u4e2d\u3002\u8bf7\u9605\u8bfb\u4e0b\u9762\u7684\u6307\u793a\uff1a</p> <ul> <li>\u5728\u76f8\u5e94\u7684\u51fd\u6570\u7c7b\u4e2d\u521b\u5efa\u4e00\u4e2a\u7b26\u53f7\u51fd\u6570\u547d\u540d\u4e3a<code>symbolic</code>\u3002</li> <li>\u7b2c\u4e00\u4e2a\u53c2\u6570\u603b\u662f\u5bfc\u51faONNX\u56fe\u5f62\u53c2\u6570\u3002</li> <li>\u53c2\u6570\u7684\u540d\u5b57\u9664\u4e86\u7b2c\u4e00\u4e2a\u5fc5\u987b\u4e0e<code>\u524d\u9762\u7684\u5f62\u5f0f</code>\u4e25\u683c\u5339\u914d\u3002</li> <li>\u8f93\u51fa\u5143\u7ec4\u5927\u5c0f\u5fc5\u987b\u4e0e<code>\u524d\u9762\u7684\u5f62\u5f0f</code>\u4e25\u683c\u5339\u914d\u3002</li> <li>\u5728\u7b26\u53f7\u529f\u80fd\u4e2d\uff0c\u5982\u679c\u64cd\u4f5c\u7b26\u5df2\u7ecf\u5728ONNX\u6807\u51c6\u5316\u4e86\uff0c\u6211\u4eec\u53ea\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4ee3\u7801\u53bb\u8868\u793a\u5728\u56fe\u5f62\u91cc\u9762\u7684ONNX\u64cd\u4f5c\u7b26\u3002</li> </ul> <p>\u7b26\u53f7\u529f\u80fd\u5e94\u8be5\u5728Python\u91cc\u9762\u914d\u7f6e\u597d\u3002\u6240\u6709\u7684\u8fd9\u4e9b\u4e0ePython\u65b9\u6cd5\u76f8\u5173\u7684\u529f\u80fd\u90fd\u901a\u8fc7C++-Python\u7ed1\u5b9a\u914d\u7f6e\u597d\uff0c\u4e14\u4e0a\u8005\u63d0\u4f9b\u7684\u754c\u9762\u76f4\u89c2\u5730\u663e\u793a\u5982\u4e0b\uff1a</p> <pre><code>def operator/symbolic(g, *inputs):\n  \"\"\"\n \u4fee\u6539\u56fe\u50cf(\u4f8b\u5982\u4f7f\u7528 \"op\")\uff0c\u52a0\u4e0a\u4ee3\u8868\u8fd9\u4e2aPyTorch\u529f\u80fd\u7684ONNX\u64cd\u4f5c\u7b26\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u6307\u5b9a\u7684ONNX\u8f93\u51fa\u503c\u6216\u8005\u5143\u7ec4\u503c\uff0c\u8fd9\u4e9b\u503c\u4e0e\u6700\u5f00\u59cbPyTorch\u8fd4\u56de\u7684\u81ea\u52a8\u6c42\u5bfc\u529f\u80fd\u76f8\u5173(\u6216\u8005\u5982\u679cONNX\u4e0d\u652f\u6301\u8f93\u51fa\uff0c\u5219\u8fd4\u56denone\u3002 ).\n\n\u53c2\u6570\uff1a\n g (\u56fe\u5f62)\uff1a\u5199\u5165\u56fe\u5f62\u7684ONNX\u8868\u793a\u65b9\u6cd5\u3002\n inputs (\u503c...)\uff1a\u8be5\u503c\u7684\u5217\u8868\u8868\u793a\u5305\u542b\u8fd9\u4e2a\u529f\u80fd\u7684\u8f93\u5165\u7684\u53ef\u53d8\u56e0\u7d20\u3002\n \"\"\"\n\nclass Value(object):\n  \"\"\"\u4ee3\u8868\u4e00\u4e2a\u5728ONNX\u91cc\u8ba1\u7b97\u7684\u4e2d\u95f4\u5f20\u91cf\u3002\"\"\"\n  def type(self):\n    \"\"\"\u8fd4\u56de\u503c\u7684\u7c7b\u578b\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"\u8fd4\u56de\u4ee3\u8868\u8fd9\u4e2a\u5f20\u91cf\u5927\u5c0f\u5f62\u72b6\u7684\u6574\u6570\u5143\u7ec4\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n \u95ef\u5c06\u4e00\u4e2aONNX\u64cd\u4f5c\u7b26'opname'\uff0c\u5c06'args'\u4f5c\u4e3a\u8f93\u5165\u548c\u5c5e\u6027'kwargs'\u5e76\u4e14\u5c06\u5b83\u4f5c\u4e3a\u5f53\u524d\u56fe\u5f62\u7684\u8282\u70b9\uff0c\u8fd4\u56de\u4ee3\u8868\u8fd9\u4e2a\u64cd\u4f5c\u7b26\u7684\u5355\u4e00\u8f93\u51fa\u503c(\u8be6\u89c1`outputs`\u591a\u5173\u952e\u53c2\u6570\u8fd4\u56de\u8282\u70b9)\u3002\n\n \u64cd\u4f5c\u7b26\u7684\u8bbe\u7f6e\u548c\u4ed6\u4eec\u8f93\u5165\u5c5e\u6027\u8be6\u60c5\u8bf7\u89c1 https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n \u53c2\u6570\uff1a\n opname (\u5b57\u7b26\u4e32)\uff1aONNX\u64cd\u4f5c\u7b26\u7684\u540d\u5b57\uff0c\u4f8b\u5982`Abs`\u6216\u8005`Add`\u3002\n args (\u503c...)\uff1a\u8be5\u64cd\u4f5c\u7b26\u7684\u8f93\u5165\u7ecf\u5e38\u88ab\u4f5c\u4e3a`symbolic`\u5b9a\u4e49\u53c2\u6570\u8f93\u5165\u3002\n kwargs\uff1a\u8be5ONNX\u64cd\u4f5c\u7b26\u7684\u5c5e\u6027\u952e\u540d\u6839\u636e\u4ee5\u4e0b\u7ea6\u5b9a\uff1a`alpha_f` \u4ee3\u8868\u7740`alpha`\u5177\u6709`f`\u7684\u5c5e\u6027\u3002\u6709\u6548\u7684\u7c7b\u578b\u8bf4\u660e\u7b26\u662f\n  `f`(float\uff09\uff0c`i`(int\uff09\uff0c`s`(string\uff09\u6216`t`(Tensor\uff09\u3002\u4f7f\u7528float\u7c7b\u578b\u6307\u5b9a\u7684\u5c5e\u6027\u63a5\u53d7\u5355\u4e2afloat\u6216float\u5217\u8868(\u4f8b\u5982\uff0c\u5bf9\u4e8e\u5e26\u6709\u6574\u6570\u5217\u8868\u7684`dims`\u5c5e\u6027\uff0c\u4f60\u53ef\u4ee5\u79f0\u5176\u4e3a'dims_i`\uff09\u3002\n outputs (\u8bc1\u4e66\uff0c\u53ef\u9009)\uff1a\u8fd9\u4e2a\u8fd0\u7b97\u7b26\u8fd4\u56de\u7684\u8f93\u51fa\u53c2\u6570\u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5047\u5b9a\u8fd0\u7b97\u7b26\u8fd4\u56de\u5355\u4e2a\u8f93\u51fa\u3002\n \u5982\u679c`\u8f93\u51fa`\u4e0d\u6b62\u4e00\u4e2a\uff0c\u8fd9\u4e2a\u529f\u80fd\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u8f93\u51fa\u503c\u7684\u5143\u7ec4\uff0c\u4ee3\u8868\u7740\u6bcf\u4e2aONNX\u64cd\u4f5c\u7b26\u7684\u8f93\u51fa\u7684\u4f4d\u7f6e\u3002\n \"\"\"\n\n</code></pre> <p>ONNX\u7684\u56fe\u5f62C++\u5b9a\u4e49\u8be6\u60c5\u8bf7\u89c1<code>torch/csrc/jit/ir.h</code>\u3002</p> <p>\u8fd9\u662f\u4e00\u4e2a\u5904\u7406<code>elu</code>\u64cd\u4f5c\u7b26\u7f3a\u5c11\u7b26\u53f7\u51fd\u6570\u7684\u4f8b\u5b50\u3002\u6211\u4eec\u5c1d\u8bd5\u5bfc\u51fa\u6a21\u578b\u5e76\u67e5\u770b\u9519\u8bef\u6d88\u606f\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>UserWarning: ONNX export failed on elu because torch.onnx.symbolic.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu\n\n</code></pre> <p>\u5bfc\u51fa\u5931\u8d25\uff0c\u56e0\u4e3aPyTorch\u4e0d\u652f\u6301\u5bfc\u51fa<code>elu</code>\u64cd\u4f5c\u7b26\u3002 \u6211\u4eec\u53d1\u73b0<code>virtual Tensor elu(const Tensor\uff06input\uff0cScalar alpha\uff0cbool inplace\uff09const override;```VariableType.h</code>\u3002 \u8fd9\u610f\u5473\u7740<code>elu</code>\u662f\u4e00\u4e2aATen\u64cd\u4f5c\u7b26\u3002 \u6211\u4eec\u53ef\u4ee5\u53c2\u8003ONNX\u64cd\u4f5c\u8fd0\u7b97\u7b26\u5217\u8868\uff0c\u5e76\u4e14\u786e\u8ba4 <code>Elu</code> \u5728ONNX\u4e2d\u5df2\u7ecf\u88ab\u6807\u51c6\u5316\u3002\u6211\u4eec\u5c06\u4ee5\u4e0b\u884c\u6dfb\u52a0\u5230<code>symbolic.py</code>\uff1a</p> <pre><code>def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))\n\n</code></pre> <p>\u73b0\u5728PyTorch\u80fd\u591f\u5bfc\u51fa<code>elu</code>\u64cd\u4f5c\u7b26\uff1a</p> <p>\u5728\u4e0b\u9762\u7684\u94fe\u63a5\u4e2d\u6709\u66f4\u591a\u7684\u4f8b\u5b50\uff1a symbolic.py, tensor.py, padding.py.</p> <p>\u7528\u4e8e\u6307\u5b9a\u8fd0\u7b97\u7b26\u5b9a\u4e49\u7684\u63a5\u53e3\u662f\u5b9e\u9a8c\u6027\u7684; \u559c\u6b22\u5c1d\u8bd5\u7684\u7528\u6237\u5e94\u8be5\u6ce8\u610f\uff0cAPI\u53ef\u80fd\u4f1a\u5728\u672a\u6765\u7684\u754c\u9762\u4e2d\u53d1\u751f\u53d8\u5316\u3002</p>"},{"location":"1.0/onnx/#_3","title":"\u529f\u80fd\u51fd\u6570","text":"<pre><code>torch.onnx.export(*args, **kwargs)\n</code></pre>"},{"location":"1.0/optim/","title":"torch.optim","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <p>\u662f\u4e00\u4e2a\u5b9e\u73b0\u5404\u79cd\u4f18\u5316\u7b97\u6cd5\u7684\u5305\u3002\u5df2\u7ecf\u652f\u6301\u6700\u5e38\u7528\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u754c\u9762\u8db3\u591f\u901a\u7528\uff0c\u56e0\u6b64\u5c06\u6765\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u3002</p>"},{"location":"1.0/optim/#_1","title":"\u5982\u4f55\u4f7f\u7528\u4f18\u5316\u5668","text":"<p>\u8981\u4f7f\u7528\uff0c\u60a8\u5fc5\u987b\u6784\u9020\u4e00\u4e2a\u4f18\u5316\u5668\u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c06\u4fdd\u6301\u5f53\u524d\u72b6\u6001\u5e76\u5c06\u6839\u636e\u8ba1\u7b97\u7684\u6e10\u53d8\u66f4\u65b0\u53c2\u6570\u3002</p>"},{"location":"1.0/optim/#_2","title":"\u6784\u5efa\u5b83","text":"<p>\u8981\u6784\u9020\u4e00\u4e2a\u4f60\u5fc5\u987b\u7ed9\u5b83\u4e00\u4e2a\u5305\u542b\u53c2\u6570\u7684\u8fed\u4ee3(\u6240\u6709\u5e94\u8be5\u662f<code>Variable</code> s\uff09\u6765\u4f18\u5316\u3002\u7136\u540e\uff0c\u60a8\u53ef\u4ee5\u6307\u5b9a\u7279\u5b9a\u4e8e\u4f18\u5316\u7a0b\u5e8f\u7684\u9009\u9879\uff0c\u4f8b\u5982\u5b66\u4e60\u7387\uff0c\u91cd\u91cf\u8870\u51cf\u7b49\u3002</p> <p>\u6ce8\u610f</p> <p>\u5982\u679c\u60a8\u9700\u8981\u901a\u8fc7<code>.cuda()</code>\u5c06\u6a21\u578b\u79fb\u52a8\u5230GPU\uff0c\u8bf7\u5728\u4e3a\u5176\u6784\u5efa\u4f18\u5316\u5668\u4e4b\u524d\u6267\u884c\u6b64\u64cd\u4f5c\u3002 <code>.cuda()</code>\u4e4b\u540e\u7684\u6a21\u578b\u53c2\u6570\u4e0e\u8c03\u7528\u4e4b\u524d\u7684\u53c2\u6570\u4e0d\u540c\u3002</p> <p>\u901a\u5e38\uff0c\u5728\u6784\u9020\u548c\u4f7f\u7528\u4f18\u5316\u7a0b\u5e8f\u65f6\uff0c\u5e94\u786e\u4fdd\u4f18\u5316\u53c2\u6570\u4f4d\u4e8e\u4e00\u81f4\u7684\u4f4d\u7f6e\u3002</p> <p>\u4f8b\uff1a</p> <pre><code>optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr = 0.0001)\n\n</code></pre>"},{"location":"1.0/optim/#_3","title":"\u6bcf\u4e2a\u53c2\u6570\u9009\u9879","text":"<p>s\u8fd8\u652f\u6301\u6307\u5b9a\u6bcf\u4e2a\u53c2\u6570\u9009\u9879\u3002\u8981\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u4e0d\u8981\u4f20\u9012\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684<code>Variable</code>\uff0c\u800c\u662f\u4f20\u9012\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684s\u3002\u5b83\u4eec\u4e2d\u7684\u6bcf\u4e00\u4e2a\u90fd\u5c06\u5b9a\u4e49\u4e00\u4e2a\u5355\u72ec\u7684\u53c2\u6570\u7ec4\uff0c\u5e76\u4e14\u5e94\u5305\u542b<code>params</code>\u952e\uff0c\u5176\u4e2d\u5305\u542b\u5c5e\u4e8e\u5b83\u7684\u53c2\u6570\u5217\u8868\u3002\u5176\u4ed6\u952e\u5e94\u4e0e\u4f18\u5316\u7a0b\u5e8f\u63a5\u53d7\u7684\u5173\u952e\u5b57\u53c2\u6570\u5339\u914d\uff0c\u5e76\u5c06\u7528\u4f5c\u6b64\u7ec4\u7684\u4f18\u5316\u9009\u9879\u3002</p> <p>Note</p> <p>\u60a8\u4ecd\u7136\u53ef\u4ee5\u5c06\u9009\u9879\u4f5c\u4e3a\u5173\u952e\u5b57\u53c2\u6570\u4f20\u9012\u3002\u5b83\u4eec\u5c06\u5728\u672a\u8986\u76d6\u5b83\u4eec\u7684\u7ec4\u4e2d\u7528\u4f5c\u9ed8\u8ba4\u503c\u3002\u5f53\u60a8\u53ea\u60f3\u6539\u53d8\u5355\u4e2a\u9009\u9879\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u7ec4\u4e4b\u95f4\u7684\u6240\u6709\u5176\u4ed6\u9009\u9879\u4fdd\u6301\u4e00\u81f4\u65f6\uff0c\u8fd9\u975e\u5e38\u6709\u7528\u3002</p> <p>\u4f8b\u5982\uff0c\u5f53\u60f3\u8981\u6307\u5b9a\u6bcf\u5c42\u5b66\u4e60\u901f\u7387\u65f6\uff0c\u8fd9\u975e\u5e38\u6709\u7528\uff1a</p> <pre><code>optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)\n\n</code></pre> <p>\u8fd9\u610f\u5473\u7740<code>model.base</code>\u7684\u53c2\u6570\u5c06\u4f7f\u7528<code>1e-2</code>\u7684\u9ed8\u8ba4\u5b66\u4e60\u901f\u7387\uff0c<code>model.classifier</code>\u7684\u53c2\u6570\u5c06\u4f7f\u7528<code>1e-3</code>\u7684\u5b66\u4e60\u901f\u7387\uff0c<code>0.9</code>\u7684\u52a8\u91cf\u5c06\u7528\u4e8e\u6240\u6709\u53c2\u6570</p>"},{"location":"1.0/optim/#_4","title":"\u91c7\u53d6\u4f18\u5316\u6b65\u9aa4","text":"<p>\u6240\u6709\u4f18\u5316\u5668\u90fd\u5b9e\u73b0\u4e86\u4e00\u4e2a\u66f4\u65b0\u53c2\u6570\u7684\u65b9\u6cd5\u3002\u5b83\u53ef\u4ee5\u4ee5\u4e24\u79cd\u65b9\u5f0f\u4f7f\u7528\uff1a</p>"},{"location":"1.0/optim/#optimizerstep","title":"<code>optimizer.step()</code>","text":"<p>\u8fd9\u662f\u5927\u591a\u6570\u4f18\u5316\u5668\u652f\u6301\u7684\u7b80\u5316\u7248\u672c\u3002\u4e00\u65e6\u4f7f\u7528\u4f8b\u5982\u8ba1\u7b97\u68af\u5ea6\uff0c\u5c31\u53ef\u4ee5\u8c03\u7528\u8be5\u51fd\u6570\u3002 <code>backward()</code>\u3002</p> <p>Example:</p> <pre><code>for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n</code></pre>"},{"location":"1.0/optim/#optimizerstepclosure","title":"<code>optimizer.step(closure)</code>","text":"<p>\u4e00\u4e9b\u4f18\u5316\u7b97\u6cd5\uff0c\u4f8b\u5982Conjugate Gradient\u548cLBFGS\u9700\u8981\u591a\u6b21\u91cd\u65b0\u8bc4\u4f30\u51fd\u6570\uff0c\u56e0\u6b64\u60a8\u5fc5\u987b\u4f20\u5165\u4e00\u4e2a\u5141\u8bb8\u5b83\u4eec\u91cd\u65b0\u8ba1\u7b97\u6a21\u578b\u7684\u95ed\u5305\u3002\u95ed\u5408\u5e94\u6e05\u9664\u68af\u5ea6\uff0c\u8ba1\u7b97\u635f\u5931\u5e76\u8fd4\u56de\u3002</p> <p>Example:</p> <pre><code>for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n\n</code></pre>"},{"location":"1.0/optim/#_5","title":"\u7b97\u6cd5","text":"<pre><code>class torch.optim.Optimizer(params, defaults)\n</code></pre> <p>\u6240\u6709\u4f18\u5316\u5668\u7684\u57fa\u7c7b\u3002</p> <p>\u8b66\u544a</p> <p>\u9700\u8981\u5c06\u53c2\u6570\u6307\u5b9a\u4e3a\u5177\u6709\u5728\u8fd0\u884c\u4e4b\u95f4\u4e00\u81f4\u7684\u786e\u5b9a\u6027\u6392\u5e8f\u7684\u96c6\u5408\u3002\u4e0d\u6ee1\u8db3\u8fd9\u4e9b\u5c5e\u6027\u7684\u5bf9\u8c61\u7684\u793a\u4f8b\u662f\u5b57\u5178\u503c\u7684\u96c6\u5408\u548c\u8fed\u4ee3\u5668\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>params  (iterable ) - s\u6216s\u7684\u53ef\u8fed\u4ee3\u3002\u6307\u5b9a\u5e94\u4f18\u5316\u7684\u5f20\u91cf\u3002</li> <li>\u9ed8\u8ba4\u503c - (dict\uff09\uff1a\u5305\u542b\u4f18\u5316\u9009\u9879\u9ed8\u8ba4\u503c\u7684dict(\u5f53\u53c2\u6570\u7ec4\u672a\u6307\u5b9a\u5b83\u4eec\u65f6\u4f7f\u7528\uff09\u3002</li> </ul> <pre><code>add_param_group(param_group)\n</code></pre> <p>\u5c06\u53c2\u6570\u7ec4\u6dfb\u52a0\u5230s <code>param_groups</code>\u3002</p> <p>\u5f53\u5fae\u8c03\u9884\u5148\u8bad\u7ec3\u7684\u7f51\u7edc\u65f6\uff0c\u8fd9\u53ef\u4ee5\u662f\u6709\u7528\u7684\uff0c\u56e0\u4e3a\u51bb\u7ed3\u5c42\u53ef\u4ee5\u88ab\u8bad\u7ec3\u5e76\u4e14\u88ab\u6dfb\u52a0\u5230\u8bad\u7ec3\u8fdb\u5c55\u4e2d\u3002</p> <p>Parameters:</p> <ul> <li>param_group (\uff09 - \u6307\u5b9a\u5e94\u8be5\u4e0e\u7ec4\u4e00\u8d77\u4f18\u5316\u7684\u5f20\u91cf</li> <li>\u4f18\u5316\u9009\u9879\u3002 (\u7279\u5f02\u6027\uff09 -</li> </ul> <pre><code>load_state_dict(state_dict)\n</code></pre> <p>\u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001\u3002</p> \u53c2\u6570\uff1a state_dict (\uff09 - \u4f18\u5316\u5668\u72b6\u6001\u3002\u5e94\u8be5\u662f\u4ece\u8c03\u7528\u8fd4\u56de\u7684\u5bf9\u8c61\u3002 <pre><code>state_dict()\n</code></pre> <p>\u4ee5...\u683c\u5f0f\u8fd4\u56de\u4f18\u5316\u7a0b\u5e8f\u7684\u72b6\u6001\u3002</p> <p>\u5b83\u5305\u542b\u4e24\u4e2a\u6761\u76ee\uff1a</p> <ul> <li> <p><code>state - a dict holding current optimization state. Its content</code></p> <p>\u4f18\u5316\u5668\u7c7b\u4e4b\u95f4\u6709\u6240\u4e0d\u540c\u3002</p> </li> <li> <p>param_groups - \u5305\u542b\u6240\u6709\u53c2\u6570\u7ec4\u7684dict</p> </li> </ul> <pre><code>step(closure)\n</code></pre> <p>\u6267\u884c\u5355\u4e2a\u4f18\u5316\u6b65\u9aa4(\u53c2\u6570\u66f4\u65b0\uff09\u3002</p> Parameters: \u95ed\u5305(\u53ef\u8c03\u7528\uff09 - \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u95ed\u5305\u3002\u5927\u591a\u6570\u4f18\u5316\u5668\u90fd\u662f\u53ef\u9009\u7684\u3002 <pre><code>zero_grad()\n</code></pre> <p>\u6e05\u9664\u6240\u6709\u4f18\u5316s\u7684\u6e10\u53d8\u3002</p> <pre><code>class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n</code></pre> <p>\u5b9e\u73b0Adadelta\u7b97\u6cd5\u3002</p> <p>\u5df2\u5728 ADADELTA\u4e2d\u63d0\u51fa\uff1a\u81ea\u9002\u5e94\u5b66\u4e60\u901f\u7387\u65b9\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>rho (\uff0c \u53ef\u9009\uff09 - \u7528\u4e8e\u8ba1\u7b97\u5e73\u65b9\u68af\u5ea6\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570(\u9ed8\u8ba4\u503c\uff1a0.9\uff09</li> <li>eps (\uff0c \u53ef\u9009\uff09 - \u672f\u8bed\u52a0\u5165\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c\uff1a1e-6\uff09</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5728\u5e94\u7528\u4e8e\u53c2\u6570\u4e4b\u524d\u7f29\u653e\u589e\u91cf\u7684\u7cfb\u6570(\u9ed8\u8ba4\u503c\uff1a1.0\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>\u6267\u884c\u5355\u4e2a\u4f18\u5316\u6b65\u9aa4\u3002</p> Parameters: \u5173\u95ed(\u53ef\u8c03\u7528 \uff0c \u53ef\u9009\uff09 - \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u95ed\u5305\u3002 <pre><code>class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n</code></pre> <p>\u5b9e\u73b0Adagrad\u7b97\u6cd5\u3002</p> <p>\u5df2\u7ecf\u5728\u81ea\u9002\u5e94\u5b50\u68af\u5ea6\u65b9\u6cd5\u4e2d\u63d0\u51fa\u4e86\u5728\u7ebf\u5b66\u4e60\u548c\u968f\u673a\u4f18\u5316\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-2\uff09</li> <li>lr_decay (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387\u8870\u51cf(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n</code></pre> <p>\u5b9e\u73b0Adam\u7b97\u6cd5\u3002</p> <p>\u5df2\u5728 Adam\u4e2d\u63d0\u51fa\uff1a\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-3\uff09</li> <li>beta (\u5143\u7ec4 _ [\uff0c ] __\uff0c _\u4efb\u9009\uff09 - \u7528\u4e8e\u8ba1\u7b97\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570\u6e10\u53d8\u53ca\u5176\u65b9\u5f62(\u9ed8\u8ba4\u503c\uff1a(0.9,0.999\uff09\uff09</li> <li>eps (\uff0c \u53ef\u9009\uff09 - \u672f\u8bed\u52a0\u5165\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c\uff1a1e-8\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>amsgrad (\u5e03\u5c14 \uff0c \u53ef\u9009\uff09 - \u662f\u5426\u4f7f\u7528\u8be5\u7b97\u6cd5\u7684AMSGrad\u53d8\u4f53\u5173\u4e8e\u4e9a\u5f53\u53ca\u5176\u540e\u7684\u6536\u655b(\u9ed8\u8ba4\u503c\uff1aFalse\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n</code></pre> <p>\u5b9e\u73b0\u9002\u7528\u4e8e\u7a00\u758f\u5f20\u91cf\u7684\u61d2\u60f0\u7248Adam\u7b97\u6cd5\u3002</p> <p>\u5728\u6b64\u53d8\u4f53\u4e2d\uff0c\u53ea\u6709\u6e10\u53d8\u4e2d\u663e\u793a\u7684\u65f6\u523b\u624d\u4f1a\u66f4\u65b0\uff0c\u5e76\u4e14\u53ea\u6709\u6e10\u53d8\u7684\u90a3\u4e9b\u90e8\u5206\u624d\u4f1a\u5e94\u7528\u4e8e\u53c2\u6570\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-3\uff09</li> <li>beta (\u5143\u7ec4 _ [\uff0c ] __\uff0c _\u4efb\u9009\uff09 - \u7528\u4e8e\u8ba1\u7b97\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570\u6e10\u53d8\u53ca\u5176\u65b9\u5f62(\u9ed8\u8ba4\u503c\uff1a(0.9,0.999\uff09\uff09</li> <li>eps (\uff0c \u53ef\u9009\uff09 - \u672f\u8bed\u52a0\u5165\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c\uff1a1e-8\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n</code></pre> <p>\u5b9e\u73b0Adamax\u7b97\u6cd5(\u57fa\u4e8e\u65e0\u7a77\u5927\u89c4\u8303\u7684Adam\u7684\u53d8\u4f53\uff09\u3002</p> <p>It has been proposed in Adam: A Method for Stochastic Optimization.</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a2e-3\uff09</li> <li>beta (\u5143\u7ec4 _ [\uff0c ] __\uff0c _\u4efb\u9009\uff09 - \u7528\u4e8e\u8ba1\u7b97\u8fd0\u884c\u5e73\u5747\u503c\u7684\u7cfb\u6570\u6e10\u53d8\u548c\u5b83\u7684\u6b63\u65b9\u5f62</li> <li>eps (\uff0c \u53ef\u9009\uff09 - \u672f\u8bed\u52a0\u5165\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c\uff1a1e-8\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n</code></pre> <p>\u5b9e\u73b0\u5e73\u5747\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002</p> <p>\u5df2\u7ecf\u5728\u4e2d\u901a\u8fc7\u5e73\u5747\u6765\u52a0\u901f\u968f\u673a\u8fd1\u4f3c\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-2\uff09</li> <li>lambd (\uff0c \u53ef\u9009\uff09 - \u8870\u53d8\u671f\u9650(\u9ed8\u8ba4\u503c\uff1a1e-4\uff09</li> <li>alpha (\uff0c \u53ef\u9009\uff09 - eta\u66f4\u65b0\u7684\u6743\u529b(\u9ed8\u8ba4\u503c\uff1a0.75\uff09</li> <li>t0 (\uff0c \u53ef\u9009\uff09 - \u5f00\u59cb\u5e73\u5747\u7684\u70b9(\u9ed8\u8ba4\u503c\uff1a1e6\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n</code></pre> <p>\u5b9e\u73b0L-BFGS\u7b97\u6cd5\u3002</p> <p>Warning</p> <p>\u6b64\u4f18\u5316\u5668\u4e0d\u652f\u6301\u6bcf\u4e2a\u53c2\u6570\u9009\u9879\u548c\u53c2\u6570\u7ec4(\u53ea\u80fd\u6709\u4e00\u4e2a\uff09\u3002</p> <p>Warning</p> <p>\u73b0\u5728\u6240\u6709\u53c2\u6570\u90fd\u5fc5\u987b\u5728\u4e00\u53f0\u8bbe\u5907\u4e0a\u3002\u8fd9\u5c06\u5728\u672a\u6765\u5f97\u5230\u6539\u5584\u3002</p> <p>Note</p> <p>\u8fd9\u662f\u4e00\u4e2a\u5185\u5b58\u5bc6\u96c6\u578b\u4f18\u5316\u5668(\u5b83\u9700\u8981\u989d\u5916\u7684<code>param_bytes * (history_size + 1)</code>\u5b57\u8282\uff09\u3002\u5982\u679c\u5b83\u4e0d\u9002\u5408\u5185\u5b58\u5c1d\u8bd5\u51cf\u5c11\u5386\u53f2\u8bb0\u5f55\u5927\u5c0f\uff0c\u6216\u4f7f\u7528\u4e0d\u540c\u7684\u7b97\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>lr (\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1\uff09</li> <li>max_iter (\uff09 - \u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u7684\u6700\u5927\u8fed\u4ee3\u6b21\u6570(\u9ed8\u8ba4\u503c\uff1a20\uff09</li> <li>max_eval (\uff09 - \u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u7684\u6700\u5927\u51fd\u6570\u8bc4\u4f30\u6570(\u9ed8\u8ba4\u503c\uff1amax_iter * 1.25\uff09\u3002</li> <li>tolerance_grad (\uff09 - \u4e00\u9636\u6700\u4f18\u6027\u7684\u7ec8\u6b62\u5bb9\u5dee(\u9ed8\u8ba4\u503c\uff1a1e-5\uff09\u3002</li> <li>tolerance_change (\uff09 - \u529f\u80fd\u503c/\u53c2\u6570\u66f4\u6539\u7684\u7ec8\u6b62\u5bb9\u5dee(\u9ed8\u8ba4\u503c\uff1a1e-9\uff09\u3002</li> <li>history_size (\uff09 - \u66f4\u65b0\u5386\u53f2\u8bb0\u5f55\u5927\u5c0f(\u9ed8\u8ba4\u503c\uff1a100\uff09\u3002</li> </ul> <pre><code>step(closure)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: \u95ed\u5305(\u53ef\u8c03\u7528\uff09 - \u4e00\u4e2a\u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u95ed\u5305\u3002 <pre><code>class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n</code></pre> <p>\u5b9e\u73b0RMSprop\u7b97\u6cd5\u3002</p> <p>G. Hinton\u5728\u4ed6\u7684\u8bfe\u7a0b\u4e2d\u63d0\u51fa\u7684\u5efa\u8bae\u3002</p> <p>\u4e2d\u5fc3\u7248\u672c\u9996\u5148\u51fa\u73b0\u5728\u751f\u6210\u5177\u6709\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u7684\u5e8f\u5217\u4e2d\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-2\uff09</li> <li>\u52a8\u91cf(\uff0c \u53ef\u9009\uff09 - \u52a8\u91cf\u56e0\u5b50(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>alpha (\uff0c \u53ef\u9009\uff09 - \u5e73\u6ed1\u5e38\u6570(\u9ed8\u8ba4\u503c\uff1a0.99\uff09</li> <li>eps (\uff0c \u53ef\u9009\uff09 - \u672f\u8bed\u52a0\u5165\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027(\u9ed8\u8ba4\u503c\uff1a1e-8\uff09</li> <li>\u5c45\u4e2d(\uff0c \u53ef\u9009\uff09 - \u5982\u679c<code>True</code>\u8ba1\u7b97\u5c45\u4e2d\u7684RMSProp\uff0c\u5219\u901a\u8fc7\u4f30\u8ba1\u5176\u65b9\u5dee\u5bf9\u68af\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n</code></pre> <p>\u5b9e\u73b0\u5f39\u6027\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff0c \u53ef\u9009\uff09 - \u5b66\u4e60\u7387(\u9ed8\u8ba4\u503c\uff1a1e-2\uff09</li> <li>etas  (Tuple _ [\uff0c ] __\uff0c _\u4efb\u9009\uff09 - \u5bf9(etaminus\uff0cetaplis\uff09 \uff0c\u8fd9\u662f\u4e58\u6cd5\u589e\u52a0\u548c\u51cf\u5c11\u56e0\u5b50(\u9ed8\u8ba4\u503c\uff1a(0.5,1.2\uff09\uff09</li> <li>step_sizes  (Tuple _ [\uff0c ] __\uff0c _\u4efb\u9009\uff09 - \u4e00\u5bf9\u6700\u5c0f\u548c\u6700\u5927\u5141\u8bb8\u6b65\u957f(\u9ed8\u8ba4\u503c\uff1a(1e-6,50\uff09\uff09</li> </ul> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss. <pre><code>class torch.optim.SGD(params, lr=&lt;required parameter&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n</code></pre> <p>\u5b9e\u73b0\u968f\u673a\u68af\u5ea6\u4e0b\u964d(\u53ef\u9009\u62e9\u5e26\u52a8\u91cf\uff09\u3002</p> <p>Nesterov\u52a8\u91cf\u662f\u57fa\u4e8e\u5173\u4e8e\u521d\u59cb\u5316\u548c\u52a8\u91cf\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u7684\u516c\u5f0f\u3002</p> <p>Parameters:</p> <ul> <li>params  (iterable ) - \u53ef\u8fed\u4ee3\u53c2\u6570\u4ee5\u4f18\u5316\u6216\u51b3\u5b9a\u53c2\u6570\u7ec4</li> <li>lr (\uff09 - \u5b66\u4e60\u7387</li> <li>\u52a8\u91cf(\uff0c \u53ef\u9009\uff09 - \u52a8\u91cf\u56e0\u5b50(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>weight_decay (\uff0c \u53ef\u9009\uff09 - \u4f53\u91cd\u8870\u51cf(L2\u60e9\u7f5a\uff09(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>\u963b\u5c3c(\uff0c \u53ef\u9009\uff09 - \u6291\u5236\u52a8\u91cf(\u9ed8\u8ba4\u503c\uff1a0\uff09</li> <li>nesterov (\uff0c \u53ef\u9009\uff09 - \u542f\u7528Nesterov\u52a8\u91cf(\u9ed8\u8ba4\u503c\uff1aFalse\uff09</li> </ul> <p>\u4f8b</p> <pre><code>&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n&gt;&gt;&gt; optimizer.zero_grad()\n&gt;&gt;&gt; loss_fn(model(input), target).backward()\n&gt;&gt;&gt; optimizer.step()\n\n</code></pre> <p>Note</p> <p>\u4f7f\u7528Momentum / Nesterov\u5b9e\u65bdSGD\u4e0eSutskever\u7b49\u6709\u6240\u4e0d\u540c\u3002\u4eba\u3002\u548c\u5176\u4ed6\u4e00\u4e9b\u6846\u67b6\u4e2d\u7684\u5b9e\u73b0\u3002</p> <p>\u8003\u8651\u5230Momentum\u7684\u5177\u4f53\u60c5\u51b5\uff0c\u66f4\u65b0\u53ef\u4ee5\u5199\u6210</p> <p>\u5176\u4e2dp\uff0cg\uff0cv\u5206\u522b\u8868\u793a\u53c2\u6570\uff0c\u68af\u5ea6\uff0c\u901f\u5ea6\u548c\u52a8\u91cf\u3002</p> <p>\u8fd9\u4e0eSutskever\u7b49\u4eba\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u4eba\u3002\u548c\u5176\u4ed6\u91c7\u7528\u8868\u683c\u66f4\u65b0\u7684\u6846\u67b6</p> <p>Nesterov\u7248\u672c\u7ecf\u8fc7\u7c7b\u4f3c\u4fee\u6539\u3002</p> <pre><code>step(closure=None)\n</code></pre> <p>Performs a single optimization step.</p> Parameters: closure (callable__, optional) \u2013 A closure that reevaluates the model and returns the loss."},{"location":"1.0/optim/#_6","title":"\u5982\u4f55\u8c03\u6574\u5b66\u4e60\u7387","text":"<p><code>torch.optim.lr_scheduler</code>\u63d0\u4f9b\u4e86\u51e0\u79cd\u6839\u636e\u65f6\u671f\u6570\u8c03\u6574\u5b66\u4e60\u7387\u7684\u65b9\u6cd5\u3002\u5141\u8bb8\u57fa\u4e8e\u4e00\u4e9b\u9a8c\u8bc1\u6d4b\u91cf\u6765\u964d\u4f4e\u52a8\u6001\u5b66\u4e60\u901f\u7387\u3002</p> <pre><code>class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n</code></pre> <p>\u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\u8bbe\u7f6e\u4e3a\u7ed9\u5b9a\u51fd\u6570\u7684\u521d\u59cblr\u500d\u3002\u5f53last_epoch = -1\u65f6\uff0c\u5c06\u521d\u59cblr\u8bbe\u7f6e\u4e3alr\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>lr_lambda (\u51fd\u6570 \u6216\uff09 - \u4e00\u4e2a\u51fd\u6570\uff0c\u5b83\u8ba1\u7b97\u7ed9\u5b9a\u6574\u6570\u53c2\u6570\u65f6\u671f\u7684\u4e58\u6cd5\u56e0\u5b50\uff0c\u6216\u8fd9\u4e9b\u51fd\u6570\u7684\u5217\u8868\uff0c\u4f18\u5316\u5668\u4e2d\u6bcf\u4e2a\u7ec4\u4e00\u4e2a.param_groups\u3002</li> <li>last_epoch (\uff09 - \u6700\u540e\u4e00\u4e2a\u7eaa\u5143\u7684\u7d22\u5f15\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; # Assuming optimizer has two groups.\n&gt;&gt;&gt; lambda1 = lambda epoch: epoch // 30\n&gt;&gt;&gt; lambda2 = lambda epoch: 0.95 ** epoch\n&gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n&gt;&gt;&gt; for epoch in range(100):\n&gt;&gt;&gt;     scheduler.step()\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     validate(...)\n\n</code></pre> <pre><code>load_state_dict(state_dict)\n</code></pre> <p>\u52a0\u8f7d\u8c03\u5ea6\u7a0b\u5e8f\u72b6\u6001\u3002</p> Parameters: state_dict (\uff09 - \u8c03\u5ea6\u7a0b\u5e8f\u72b6\u6001\u3002\u5e94\u8be5\u662f\u4ece\u8c03\u7528\u8fd4\u56de\u7684\u5bf9\u8c61\u3002 <pre><code>state_dict()\n</code></pre> <p>\u5c06\u8c03\u5ea6\u7a0b\u5e8f\u7684\u72b6\u6001\u4f5c\u4e3aa\u8fd4\u56de\u3002</p> <p>\u5b83\u5305\u542b\u81ea\u6211\u4e2d\u6bcf\u4e2a\u53d8\u91cf\u7684\u6761\u76ee\u3002 dict \u4e0d\u662f\u4f18\u5316\u5668\u3002\u5b66\u4e60\u7387lambda\u51fd\u6570\u53ea\u6709\u5728\u5b83\u4eec\u662f\u53ef\u8c03\u7528\u5bf9\u8c61\u65f6\u624d\u4f1a\u88ab\u4fdd\u5b58\uff0c\u800c\u4e0d\u662f\u5b83\u4eec\u662f\u51fd\u6570\u6216lambdas\u3002</p> <pre><code>class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n</code></pre> <p>\u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\u8bbe\u7f6e\u4e3a\u6bcf\u4e2astep_size epochs\u7531gamma\u8870\u51cf\u7684\u521d\u59cblr\u3002\u5f53last_epoch = -1\u65f6\uff0c\u5c06\u521d\u59cblr\u8bbe\u7f6e\u4e3alr\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>step_size (\uff09 - \u5b66\u4e60\u7387\u8870\u51cf\u7684\u65f6\u671f\u3002</li> <li>gamma (\uff09 - \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50\u3002\u9ed8\u8ba4\u503c\uff1a0.1\u3002</li> <li>last_epoch (\uff09 - \u6700\u540e\u4e00\u4e2a\u7eaa\u5143\u7684\u7d22\u5f15\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups\n&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30\n&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 60\n&gt;&gt;&gt; # lr = 0.0005   if 60 &lt;= epoch &lt; 90\n&gt;&gt;&gt; # ...\n&gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n&gt;&gt;&gt; for epoch in range(100):\n&gt;&gt;&gt;     scheduler.step()\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     validate(...)\n\n</code></pre> <pre><code>class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)\n</code></pre> <p>\u4e00\u65e6\u7eaa\u5143\u6570\u8fbe\u5230\u5176\u4e2d\u4e00\u4e2a\u91cc\u7a0b\u7891\uff0c\u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\u8bbe\u7f6e\u4e3a\u7531\u4f3d\u739b\u8870\u51cf\u7684\u521d\u59cblr\u3002\u5f53last_epoch = -1\u65f6\uff0c\u5c06\u521d\u59cblr\u8bbe\u7f6e\u4e3alr\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>\u91cc\u7a0b\u7891(\uff09 - \u65f6\u4ee3\u6307\u6570\u5217\u8868\u3002\u5fc5\u987b\u589e\u52a0\u3002</li> <li>gamma (\uff09 - \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50\u3002\u9ed8\u8ba4\u503c\uff1a0.1\u3002</li> <li>last_epoch (\uff09 - \u6700\u540e\u4e00\u4e2a\u7eaa\u5143\u7684\u7d22\u5f15\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups\n&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30\n&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80\n&gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80\n&gt;&gt;&gt; scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n&gt;&gt;&gt; for epoch in range(100):\n&gt;&gt;&gt;     scheduler.step()\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     validate(...)\n\n</code></pre> <pre><code>class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)\n</code></pre> <p>\u5c06\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a\u6bcf\u4e2a\u65f6\u671f\u7531\u4f3d\u739b\u8870\u51cf\u7684\u521d\u59cblr\u3002\u5f53last_epoch = -1\u65f6\uff0c\u5c06\u521d\u59cblr\u8bbe\u7f6e\u4e3alr\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>gamma (\uff09 - \u5b66\u4e60\u7387\u8870\u51cf\u7684\u4e58\u6cd5\u56e0\u5b50\u3002</li> <li>last_epoch (\uff09 - \u6700\u540e\u4e00\u4e2a\u7eaa\u5143\u7684\u7d22\u5f15\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <pre><code>class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)\n</code></pre> <p>\u4f7f\u7528\u4f59\u5f26\u9000\u706b\u8ba1\u5212\u8bbe\u7f6e\u6bcf\u4e2a\u53c2\u6570\u7ec4\u7684\u5b66\u4e60\u901f\u7387\uff0c\u5176\u4e2d\u8bbe\u7f6e\u4e3a\u521d\u59cblr\uff0c\u5e76\u4e14\u662f\u81eaSGDR\u4e0a\u6b21\u91cd\u542f\u4ee5\u6765\u7684\u7eaa\u5143\u6570\uff1a</p> <p>\u5f53last_epoch = -1\u65f6\uff0c\u5c06\u521d\u59cblr\u8bbe\u7f6e\u4e3alr\u3002</p> <p>\u5df2\u5728 SGDR\u4e2d\u63d0\u51fa\uff1a\u5177\u6709\u6696\u542f\u52a8\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4ec5\u5b9e\u73b0SGDR\u7684\u4f59\u5f26\u9000\u706b\u90e8\u5206\uff0c\u800c\u4e0d\u662f\u91cd\u542f\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>T_max (\uff09 - \u6700\u5927\u8fed\u4ee3\u6b21\u6570\u3002</li> <li>eta_min (\uff09 - \u6700\u4f4e\u5b66\u4e60\u7387\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>last_epoch (\uff09 - \u6700\u540e\u4e00\u4e2a\u7eaa\u5143\u7684\u7d22\u5f15\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <pre><code>class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n</code></pre> <p>\u5f53\u6307\u6807\u505c\u6b62\u6539\u8fdb\u65f6\u964d\u4f4e\u5b66\u4e60\u7387\u3002\u4e00\u65e6\u5b66\u4e60\u505c\u6ede\uff0c\u6a21\u578b\u901a\u5e38\u4f1a\u5c06\u5b66\u4e60\u7387\u964d\u4f4e2-10\u500d\u3002\u8be5\u8c03\u5ea6\u7a0b\u5e8f\u8bfb\u53d6\u5ea6\u91cf\u6570\u91cf\uff0c\u5982\u679c\u201c\u8010\u5fc3\u201d\u6570\u91cf\u7684\u65f6\u671f\u6ca1\u6709\u770b\u5230\u6539\u5584\uff0c\u5219\u5b66\u4e60\u901f\u7387\u964d\u4f4e\u3002</p> <p>Parameters:</p> <ul> <li>\u4f18\u5316\u5668(\uff09 - \u5305\u88c5\u4f18\u5316\u5668\u3002</li> <li>\u6a21\u5f0f(\uff09 - <code>min</code>\uff0c<code>max</code>\u4e4b\u4e00\u3002\u5728<code>min</code>\u6a21\u5f0f\u4e0b\uff0c\u5f53\u76d1\u63a7\u91cf\u505c\u6b62\u4e0b\u964d\u65f6\uff0clr\u5c06\u51cf\u5c11;\u5728<code>max</code>\u6a21\u5f0f\u4e0b\uff0c\u5f53\u76d1\u63a7\u91cf\u505c\u6b62\u589e\u52a0\u65f6\uff0c\u5b83\u5c06\u51cf\u5c11\u3002\u9ed8\u8ba4\u503c\uff1a'min'\u3002</li> <li>factor (\uff09 - \u5b66\u4e60\u7387\u964d\u4f4e\u7684\u56e0\u7d20\u3002 new_lr = lr * factor\u3002\u9ed8\u8ba4\u503c\uff1a0.1\u3002</li> <li>\u8010\u5fc3(\uff09 - \u6ca1\u6709\u6539\u5584\u7684\u65f6\u671f\u6570\uff0c\u4e4b\u540e\u5b66\u4e60\u7387\u4f1a\u964d\u4f4e\u3002\u4f8b\u5982\uff0c\u5982\u679c<code>patience = 2</code>\uff0c\u90a3\u4e48\u6211\u4eec\u5c06\u5ffd\u7565\u6ca1\u6709\u6539\u8fdb\u7684\u524d2\u4e2a\u65f6\u671f\uff0c\u5e76\u4e14\u5982\u679c\u635f\u5931\u4ecd\u7136\u6ca1\u6709\u6539\u5584\u90a3\u4e48\u5c06\u4ec5\u5728\u7b2c3\u4e2a\u65f6\u671f\u4e4b\u540e\u51cf\u5c11LR\u3002\u9ed8\u8ba4\u503c\uff1a10\u3002</li> <li>verbose (\uff09 - \u5982\u679c<code>True</code>\uff0c\u6bcf\u6b21\u66f4\u65b0\u90fd\u4f1a\u5411stdout\u8f93\u51fa\u4e00\u6761\u6d88\u606f\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> <li>\u9608\u503c(\uff09 - \u6d4b\u91cf\u65b0\u6700\u4f73\u503c\u7684\u9608\u503c\uff0c\u4ec5\u5173\u6ce8\u91cd\u5927\u53d8\u5316\u3002\u9ed8\u8ba4\u503c\uff1a1e-4\u3002</li> <li>threshold_mode (\uff09 - <code>rel</code>\uff0c<code>abs</code>\u4e4b\u4e00\u3002\u5728<code>rel</code>\u6a21\u5f0f\u4e0b\uff0cdynamic_threshold ='max'\u6a21\u5f0f\u4e0b\u7684\u6700\u4f73(1 +\u9608\u503c\uff09\u6216<code>min</code>\u6a21\u5f0f\u4e0b\u7684\u6700\u4f73(1 - \u9608\u503c\uff09\u3002\u5728<code>abs</code>\u6a21\u5f0f\u4e0b\uff0cdynamic_threshold = <code>max</code>\u6a21\u5f0f\u4e0b\u7684\u6700\u4f73+\u9608\u503c\u6216<code>min</code>\u6a21\u5f0f\u4e0b\u7684\u6700\u4f73\u9608\u503c\u3002\u9ed8\u8ba4\u503c\uff1a'rel'\u3002</li> <li>\u51b7\u5374\u65f6\u95f4(\uff09 - \u5728\u51cf\u5c11lr\u4e4b\u540e\u6062\u590d\u6b63\u5e38\u64cd\u4f5c\u4e4b\u524d\u8981\u7b49\u5f85\u7684\u65f6\u671f\u6570\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>min_lr (\u6216\uff09 - \u6807\u91cf\u6216\u6807\u91cf\u5217\u8868\u3002\u6240\u6709\u53c2\u6570\u7ec4\u6216\u6bcf\u7ec4\u7684\u5b66\u4e60\u7387\u7684\u4e0b\u9650\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>eps (\uff09 - \u5e94\u7528\u4e8elr\u7684\u6700\u5c0f\u8870\u51cf\u3002\u5982\u679c\u65b0\u65e7lr\u4e4b\u95f4\u7684\u5dee\u5f02\u5c0f\u4e8eeps\uff0c\u5219\u5ffd\u7565\u66f4\u65b0\u3002\u9ed8\u8ba4\u503c\uff1a1e-8\u3002</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')\n&gt;&gt;&gt; for epoch in range(10):\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     val_loss = validate(...)\n&gt;&gt;&gt;     # Note that step should be called after validate()\n&gt;&gt;&gt;     scheduler.step(val_loss)\n\n</code></pre>"},{"location":"1.0/pytorch_with_examples/","title":"\u7528\u4f8b\u5b50\u5b66\u4e60 PyTorch","text":"<p>\u8bd1\u8005\uff1abat67</p> <p>\u6700\u65b0\u7248\u4f1a\u5728\u8bd1\u8005\u4ed3\u5e93\u9996\u5148\u540c\u6b65\u3002</p> <p>\u4f5c\u8005\uff1aJustin Johnson</p> <p>\u8fd9\u4e2a\u6559\u7a0b\u901a\u8fc7\u81ea\u6d3d\u7684\u793a\u4f8b\u4ecb\u7ecd\u4e86PyTorch\u7684\u57fa\u672c\u6982\u5ff5\u3002</p> <p>PyTorch\u4e3b\u8981\u662f\u63d0\u4f9b\u4e86\u4e24\u4e2a\u6838\u5fc3\u7684\u529f\u80fd\u7279\u6027\uff1a</p> <ul> <li>\u4e00\u4e2a\u7c7b\u4f3c\u4e8enumpy\u7684n\u7ef4\u5f20\u91cf\uff0c\u4f46\u662f\u53ef\u4ee5\u5728GPU\u4e0a\u8fd0\u884c</li> <li>\u642d\u5efa\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\u7684\u81ea\u52a8\u5fae\u5206/\u6c42\u5bfc\u673a\u5236</li> </ul> <p>\u6211\u4eec\u5c06\u4f7f\u7528\u5168\u8fde\u63a5\u7684ReLU\u7f51\u7edc\u4f5c\u4e3a\u8fd0\u884c\u793a\u4f8b\u3002\u8be5\u7f51\u7edc\u5c06\u6709\u4e00\u4e2a\u5355\u4e00\u7684\u9690\u85cf\u5c42\uff0c\u5e76\u5c06\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u7f51\u7edc\u8f93\u51fa\u548c\u771f\u6b63\u7ed3\u679c\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff0c\u6765\u62df\u5408\u968f\u673a\u751f\u6210\u7684\u6570\u636e\u3002</p>"},{"location":"1.0/pytorch_with_examples/#_1","title":"\u76ee\u5f55","text":"<ul> <li>\u7528\u4f8b\u5b50\u5b66\u4e60 PyTorch</li> <li>\u76ee\u5f55</li> <li>\u5f20\u91cf<ul> <li>Warm-up\uff1aNumPy</li> <li>PyTorch\uff1a\u5f20\u91cf</li> </ul> </li> <li>\u81ea\u52a8\u6c42\u5bfc<ul> <li>PyTorch\uff1a\u5f20\u91cf\u548c\u81ea\u52a8\u6c42\u5bfc</li> <li>PyTorch\uff1a\u5b9a\u4e49\u65b0\u7684\u81ea\u52a8\u6c42\u5bfc\u51fd\u6570</li> <li>TensorFlow\uff1a\u9759\u6001\u56fe</li> </ul> </li> <li><code>nn</code>\u6a21\u5757<ul> <li>PyTorch\uff1a<code>nn</code></li> <li>PyTorch\uff1a<code>optim</code></li> <li>PyTorch\uff1a\u81ea\u5b9a\u4e49<code>nn</code>\u6a21\u5757</li> <li>PyTorch\uff1a\u63a7\u5236\u6d41\u548c\u6743\u91cd\u5171\u4eab</li> </ul> </li> <li>Examples<ul> <li>Tensors</li> <li>Autograd</li> <li><code>nn</code> module</li> </ul> </li> </ul>"},{"location":"1.0/pytorch_with_examples/#_2","title":"\u5f20\u91cf","text":""},{"location":"1.0/pytorch_with_examples/#warm-upnumpy","title":"Warm-up\uff1aNumPy","text":"<p>\u5728\u4ecb\u7ecdPyTorch\u4e4b\u524d\uff0c\u6211\u4eec\u5c06\u9996\u5148\u4f7f\u7528NumPy\u5b9e\u73b0\u7f51\u7edc\u3002</p> <p>NumPy\u63d0\u4f9b\u4e86\u4e00\u4e2an\u7ef4\u6570\u7ec4\u5bf9\u8c61\u548c\u8bb8\u591a\u7528\u4e8e\u64cd\u4f5c\u8fd9\u4e9b\u6570\u7ec4\u7684\u51fd\u6570\u3002NumPy\u662f\u7528\u4e8e\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u6846\u67b6\uff1b\u5b83\u5bf9\u8ba1\u7b97\u56fe\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u68af\u5ea6\u4e00\u65e0\u6240\u77e5\u3002\u7136\u800c\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4f7f\u7528NumPy\uff0c\u624b\u52a8\u5b9e\u73b0\u7f51\u7edc\u7684\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\uff0c\u6765\u62df\u5408\u968f\u673a\u6570\u636e\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_numpy.py\nimport numpy as np\n\n# N\u662f\u6279\u5927\u5c0f\uff1bD_in\u662f\u8f93\u5165\u7ef4\u5ea6\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6  \nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\nx = np.random.randn(N, D_in)\ny = np.random.randn(N, D_out)\n\n# \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\nw1 = np.random.randn(D_in, H)\nw2 = np.random.randn(H, D_out)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \u524d\u5411\u4f20\u64ad\uff1a\u8ba1\u7b97\u9884\u6d4b\u503cy\n    h = x.dot(w1)\n    h_relu = np.maximum(h, 0)\n    y_pred = h_relu.dot(w2)\n\n    # \u8ba1\u7b97\u5e76\u663e\u793aloss(\u635f\u5931\uff09\n    loss = np.square(y_pred - y).sum()\n    print(t, loss)\n\n    # \u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97w1\u3001w2\u5bf9loss\u7684\u68af\u5ea6\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.T.dot(grad_y_pred)\n    grad_h_relu = grad_y_pred.dot(w2.T)\n    grad_h = grad_h_relu.copy()\n    grad_h[h &lt; 0] = 0\n    grad_w1 = x.T.dot(grad_h)\n\n    # \u66f4\u65b0\u6743\u91cd\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#pytorch_1","title":"PyTorch\uff1a\u5f20\u91cf","text":"<p>NumPy\u662f\u4e00\u4e2a\u5f88\u68d2\u7684\u6846\u67b6\uff0c\u4f46\u662f\u5b83\u4e0d\u652f\u6301GPU\u4ee5\u52a0\u901f\u8fd0\u7b97\u3002\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0cGPU\u5e38\u5e38\u63d0\u4f9b50\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u6240\u4ee5NumPy\u4e0d\u80fd\u6ee1\u8db3\u5f53\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u9700\u6c42\u3002 </p> <p>\u6211\u4eec\u5148\u4ecb\u7ecdPyTorch\u6700\u57fa\u7840\u7684\u6982\u5ff5\uff1a\u5f20\u91cf(Tensor\uff09\u3002\u903b\u8f91\u4e0a\uff0cPyTorch\u7684tensor\u548cNumPy array\u662f\u4e00\u6837\u7684\uff1atensor\u662f\u4e00\u4e2an\u7ef4\u6570\u7ec4\uff0cPyTorch\u63d0\u4f9b\u4e86\u5f88\u591a\u51fd\u6570\u64cd\u4f5c\u8fd9\u4e9btensor\u3002\u4efb\u4f55\u5e0c\u671b\u4f7f\u7528NumPy\u6267\u884c\u7684\u8ba1\u7b97\u4e5f\u53ef\u4ee5\u4f7f\u7528PyTorch\u7684tensor\u6765\u5b8c\u6210\uff1b\u53ef\u4ee5\u8ba4\u4e3a\u5b83\u4eec\u662f\u79d1\u5b66\u8ba1\u7b97\u7684\u901a\u7528\u5de5\u5177\u3002</p> <p>\u548cNumPy\u4e0d\u540c\u7684\u662f\uff0cPyTorch\u53ef\u4ee5\u5229\u7528GPU\u52a0\u901f\u3002\u8981\u5728GPU\u4e0a\u8fd0\u884cPyTorch\u5f20\u91cf\uff0c\u5728\u6784\u9020\u5f20\u91cf\u4f7f\u7528<code>device</code>\u53c2\u6570\u628atensor\u5efa\u7acb\u5728GPU\u4e0a\u3002</p> <p>\u8fd9\u91cc\u6211\u4eec\u5229\u7528PyTorch\u7684tensor\u5728\u968f\u673a\u6570\u636e\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u4e24\u5c42\u7684\u7f51\u7edc\u3002\u548c\u524d\u9762NumPy\u7684\u4f8b\u5b50\u7c7b\u4f3c\uff0c\u6211\u4eec\u4f7f\u7528PyTorch\u7684tensor\uff0c\u624b\u52a8\u5728\u7f51\u7edc\u4e2d\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\uff1a </p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_tensor.py\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n\n# N\u662f\u6279\u5927\u5c0f\uff1b D_in \u662f\u8f93\u5165\u7ef4\u5ea6\uff1b\n# H \u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1b D_out \u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\nw1 = torch.randn(D_in, H, device=device)\nw2 = torch.randn(H, D_out, device=device)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \u524d\u5411\u4f20\u64ad\uff1a\u8ba1\u7b97\u9884\u6d4b\u503cy\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n\n    # \u8ba1\u7b97\u5e76\u8f93\u51faloss\uff1bloss\u662f\u5b58\u50a8\u5728PyTorch\u7684tensor\u4e2d\u7684\u6807\u91cf\uff0c\u7ef4\u5ea6\u662f()(\u96f6\u7ef4\u6807\u91cf\uff09\uff1b\n    # \u6211\u4eec\u4f7f\u7528loss.item()\u5f97\u5230tensor\u4e2d\u7684\u7eafpython\u6570\u503c\u3002\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n\n    # \u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97w1\u3001w2\u5bf9loss\u7684\u68af\u5ea6\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h &lt; 0] = 0\n    grad_w1 = x.t().mm(grad_h)\n\n    # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\n    w1 -= learning_rate * grad_w1\n    w2 -= learning_rate * grad_w2\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#_3","title":"\u81ea\u52a8\u6c42\u5bfc","text":""},{"location":"1.0/pytorch_with_examples/#pytorch_2","title":"PyTorch\uff1a\u5f20\u91cf\u548c\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u91cc\uff0c\u9700\u8981\u6211\u4eec\u624b\u52a8\u5b9e\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u3002\u5bf9\u4e8e\u7b80\u5355\u7684\u4e24\u5c42\u7f51\u7edc\uff0c\u624b\u52a8\u5b9e\u73b0\u524d\u5411\u3001\u540e\u5411\u4f20\u64ad\u4e0d\u662f\u4ec0\u4e48\u96be\u4e8b\uff0c\u4f46\u662f\u5bf9\u4e8e\u5927\u578b\u7684\u590d\u6742\u7f51\u7edc\u5c31\u6bd4\u8f83\u9ebb\u70e6\u4e86\u3002 </p> <p>\u5e86\u5e78\u7684\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u81ea\u52a8\u5fae\u5206\u6765\u81ea\u52a8\u5b8c\u6210\u795e\u7ecf\u7f51\u7edc\u4e2d\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u3002PyTorch\u4e2dautograd\u5305\u63d0\u4f9b\u7684\u6b63\u662f\u8fd9\u4e2a\u529f\u80fd\u3002\u5f53\u4f7f\u7528autograd\u65f6\uff0c\u7f51\u7edc\u524d\u5411\u4f20\u64ad\u5c06\u5b9a\u4e49\u4e00\u4e2a\u8ba1\u7b97\u56fe\uff1b\u56fe\u4e2d\u7684\u8282\u70b9\u662ftensor\uff0c\u8fb9\u662f\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u662f\u8f93\u51fatensor\u5230\u8f93\u5165tensor\u7684\u6620\u5c04\u3002\u8fd9\u5f20\u8ba1\u7b97\u56fe\u4f7f\u5f97\u5728\u7f51\u7edc\u4e2d\u53cd\u5411\u4f20\u64ad\u65f6\u68af\u5ea6\u7684\u8ba1\u7b97\u5341\u5206\u7b80\u5355\u3002 </p> <p>\u8fd9\u542c\u8d77\u6765\u590d\u6742\uff0c\u4f46\u662f\u5b9e\u9645\u64cd\u4f5c\u5f88\u7b80\u5355\u3002\u5982\u679c\u6211\u4eec\u60f3\u8ba1\u7b97\u67d0\u4e9b\u7684tensor\u7684\u68af\u5ea6\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5728\u5efa\u7acb\u8fd9\u4e2atensor\u65f6\u52a0\u5165\u8fd9\u4e48\u4e00\u53e5\uff1a<code>requires_grad=True</code>\u3002\u8fd9\u4e2atensor\u4e0a\u7684\u4efb\u4f55PyTorch\u7684\u64cd\u4f5c\u90fd\u5c06\u6784\u9020\u4e00\u4e2a\u8ba1\u7b97\u56fe\uff0c\u4ece\u800c\u5141\u8bb8\u6211\u4eec\u7a0d\u540e\u5728\u56fe\u4e2d\u6267\u884c\u53cd\u5411\u4f20\u64ad\u3002\u5982\u679c\u8fd9\u4e2atensor<code>x</code>\u7684<code>requires_grad=True</code>\uff0c\u90a3\u4e48\u53cd\u5411\u4f20\u64ad\u4e4b\u540e<code>x.grad</code>\u5c06\u4f1a\u662f\u53e6\u4e00\u4e2a\u5f20\u91cf\uff0c\u5176\u4e3a<code>x</code>\u5173\u4e8e\u67d0\u4e2a\u6807\u91cf\u503c\u7684\u68af\u5ea6\u3002</p> <p>\u6709\u65f6\u53ef\u80fd\u5e0c\u671b\u9632\u6b62PyTorch\u5728<code>requires_grad=True</code>\u7684\u5f20\u91cf\u6267\u884c\u67d0\u4e9b\u64cd\u4f5c\u65f6\u6784\u5efa\u8ba1\u7b97\u56fe\uff1b\u4f8b\u5982\uff0c\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u4e0d\u5e0c\u671b\u901a\u8fc7\u6743\u91cd\u66f4\u65b0\u6b65\u9aa4\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528<code>torch.no_grad()</code>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u6765\u9632\u6b62\u6784\u9020\u8ba1\u7b97\u56fe\u3002</p> <p>\u4e0b\u9762\u6211\u4eec\u4f7f\u7528PyTorch\u7684Tensors\u548cautograd\u6765\u5b9e\u73b0\u6211\u4eec\u7684\u4e24\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff1b\u6211\u4eec\u4e0d\u518d\u9700\u8981\u624b\u52a8\u6267\u884c\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_autograd.py\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n\n# N\u662f\u6279\u5927\u5c0f\uff1bD_in\u662f\u8f93\u5165\u7ef4\u5ea6\uff1b\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6  \nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u6570\u636e\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \u4ea7\u751f\u968f\u673a\u6743\u91cdtensor\uff0c\u5c06requires_grad\u8bbe\u7f6e\u4e3aTrue\u610f\u5473\u7740\u6211\u4eec\u5e0c\u671b\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u5019\u8ba1\u7b97\u8fd9\u4e9b\u503c\u7684\u68af\u5ea6\nw1 = torch.randn(D_in, H, device=device, requires_grad=True)\nw2 = torch.randn(H, D_out, device=device, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n\n    # \u524d\u5411\u4f20\u64ad\uff1a\u4f7f\u7528tensor\u7684\u64cd\u4f5c\u8ba1\u7b97\u9884\u6d4b\u503cy\u3002\n    # \u7531\u4e8ew1\u548cw2\u6709requires_grad=True\uff0c\u6d89\u53ca\u8fd9\u4e9b\u5f20\u91cf\u7684\u64cd\u4f5c\u5c06\u8ba9PyTorch\u6784\u5efa\u8ba1\u7b97\u56fe\uff0c\n    # \u4ece\u800c\u5141\u8bb8\u81ea\u52a8\u8ba1\u7b97\u68af\u5ea6\u3002\u7531\u4e8e\u6211\u4eec\u4e0d\u518d\u624b\u5de5\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u4fdd\u7559\u4e2d\u95f4\u503c\u7684\u5f15\u7528\u3002\n    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n\n    # \u8ba1\u7b97\u5e76\u8f93\u51faloss\uff0closs\u662f\u4e00\u4e2a\u5f62\u72b6\u4e3a()\u7684\u5f20\u91cf\uff0closs.item()\u662f\u8fd9\u4e2a\u5f20\u91cf\u5bf9\u5e94\u7684python\u6570\u503c\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n\n    # \u4f7f\u7528autograd\u8ba1\u7b97\u53cd\u5411\u4f20\u64ad\u3002\u8fd9\u4e2a\u8c03\u7528\u5c06\u8ba1\u7b97loss\u5bf9\u6240\u6709requires_grad=True\u7684tensor\u7684\u68af\u5ea6\u3002\n    # \u8fd9\u6b21\u8c03\u7528\u540e\uff0cw1.grad\u548cw2.grad\u5c06\u5206\u522b\u662floss\u5bf9w1\u548cw2\u7684\u68af\u5ea6\u5f20\u91cf\u3002\n    loss.backward()\n\n\n    # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\u3002\u5bf9\u4e8e\u8fd9\u4e00\u6b65\uff0c\u6211\u4eec\u53ea\u60f3\u5bf9w1\u548cw2\u7684\u503c\u8fdb\u884c\u539f\u5730\u6539\u53d8\uff1b\u4e0d\u60f3\u4e3a\u66f4\u65b0\u9636\u6bb5\u6784\u5efa\u8ba1\u7b97\u56fe\uff0c\n    # \u6240\u4ee5\u6211\u4eec\u4f7f\u7528torch.no_grad()\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u9632\u6b62PyTorch\u4e3a\u66f4\u65b0\u6784\u5efa\u8ba1\u7b97\u56fe\n    with torch.no_grad():\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n\n        # \u53cd\u5411\u4f20\u64ad\u4e4b\u540e\u624b\u52a8\u7f6e\u96f6\u68af\u5ea6\n        w1.grad.zero_()\n        w2.grad.zero_()\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#pytorch_3","title":"PyTorch\uff1a\u5b9a\u4e49\u65b0\u7684\u81ea\u52a8\u6c42\u5bfc\u51fd\u6570","text":"<p>\u5728\u5e95\u5c42\uff0c\u6bcf\u4e00\u4e2a\u539f\u59cb\u7684\u81ea\u52a8\u6c42\u5bfc\u8fd0\u7b97\u5b9e\u9645\u4e0a\u662f\u4e24\u4e2a\u5728Tensor\u4e0a\u8fd0\u884c\u7684\u51fd\u6570\u3002\u5176\u4e2d\uff0cforward\u51fd\u6570\u8ba1\u7b97\u4ece\u8f93\u5165Tensors\u83b7\u5f97\u7684\u8f93\u51faTensors\u3002\u800cbackward\u51fd\u6570\u63a5\u6536\u8f93\u51faTensors\u5bf9\u4e8e\u67d0\u4e2a\u6807\u91cf\u503c\u7684\u68af\u5ea6\uff0c\u5e76\u4e14\u8ba1\u7b97\u8f93\u5165Tensors\u76f8\u5bf9\u4e8e\u8be5\u76f8\u540c\u6807\u91cf\u503c\u7684\u68af\u5ea6\u3002 </p> <p>\u5728PyTorch\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u901a\u8fc7\u5b9a\u4e49<code>torch.autograd.Function</code>\u7684\u5b50\u7c7b\u5e76\u5b9e\u73b0<code>forward</code>\u548c<code>backward</code>\u51fd\u6570\uff0c\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u81ea\u52a8\u6c42\u5bfc\u8fd0\u7b97\u3002\u4e4b\u540e\u6211\u4eec\u5c31\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u65b0\u7684\u81ea\u52a8\u68af\u5ea6\u8fd0\u7b97\u7b26\u4e86\u3002\u7136\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6784\u9020\u4e00\u4e2a\u5b9e\u4f8b\u5e76\u50cf\u8c03\u7528\u51fd\u6570\u4e00\u6837\uff0c\u4f20\u5165\u5305\u542b\u8f93\u5165\u6570\u636e\u7684tensor\u8c03\u7528\u5b83\uff0c\u8fd9\u6837\u6765\u4f7f\u7528\u65b0\u7684\u81ea\u52a8\u6c42\u5bfc\u8fd0\u7b97\u3002</p> <p>\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u81ea\u5b9a\u4e49\u4e00\u4e2a\u81ea\u52a8\u6c42\u5bfc\u51fd\u6570\u6765\u5c55\u793aReLU\u7684\u975e\u7ebf\u6027\u3002\u5e76\u7528\u5b83\u5b9e\u73b0\u6211\u4eec\u7684\u4e24\u5c42\u7f51\u7edc\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_custom_function.py\nimport torch\n\nclass MyReLU(torch.autograd.Function):\n    \"\"\"\n    \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5efa\u7acbtorch.autograd\u7684\u5b50\u7c7b\u6765\u5b9e\u73b0\u6211\u4eec\u81ea\u5b9a\u4e49\u7684autograd\u51fd\u6570\uff0c\n    \u5e76\u5b8c\u6210\u5f20\u91cf\u7684\u6b63\u5411\u548c\u53cd\u5411\u4f20\u64ad\u3002\n    \"\"\"\n    @staticmethod\n    def forward(ctx, x):\n        \"\"\"\n        \u5728\u6b63\u5411\u4f20\u64ad\u4e2d\uff0c\u6211\u4eec\u63a5\u6536\u5230\u4e00\u4e2a\u4e0a\u4e0b\u6587\u5bf9\u8c61\u548c\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u7684\u5f20\u91cf\uff1b\n        \u6211\u4eec\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u8f93\u51fa\u7684\u5f20\u91cf\uff0c\n        \u5e76\u4e14\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e0a\u4e0b\u6587\u5bf9\u8c61\u6765\u7f13\u5b58\u5bf9\u8c61\uff0c\u4ee5\u4fbf\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u4f7f\u7528\u3002\n        \"\"\"\n        ctx.save_for_backward(x)\n        return x.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        \u5728\u53cd\u5411\u4f20\u64ad\u4e2d\uff0c\u6211\u4eec\u63a5\u6536\u5230\u4e0a\u4e0b\u6587\u5bf9\u8c61\u548c\u4e00\u4e2a\u5f20\u91cf\uff0c\n        \u5176\u5305\u542b\u4e86\u76f8\u5bf9\u4e8e\u6b63\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u8f93\u51fa\u7684\u635f\u5931\u7684\u68af\u5ea6\u3002\n        \u6211\u4eec\u53ef\u4ee5\u4ece\u4e0a\u4e0b\u6587\u5bf9\u8c61\u4e2d\u68c0\u7d22\u7f13\u5b58\u7684\u6570\u636e\uff0c\n        \u5e76\u4e14\u5fc5\u987b\u8ba1\u7b97\u5e76\u8fd4\u56de\u4e0e\u6b63\u5411\u4f20\u64ad\u7684\u8f93\u5165\u76f8\u5173\u7684\u635f\u5931\u7684\u68af\u5ea6\u3002\n        \"\"\"\n        x, = ctx.saved_tensors\n        grad_x = grad_output.clone()\n        grad_x[x &lt; 0] = 0\n        return grad_x\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# N\u662f\u6279\u5927\u5c0f\uff1b D_in \u662f\u8f93\u5165\u7ef4\u5ea6\uff1b\n# H \u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1b D_out \u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u968f\u673a\u5f20\u91cf\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n# \u4ea7\u751f\u968f\u673a\u6743\u91cd\u7684\u5f20\u91cf\nw1 = torch.randn(D_in, H, device=device, requires_grad=True)\nw2 = torch.randn(H, D_out, device=device, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # \u6b63\u5411\u4f20\u64ad\uff1a\u4f7f\u7528\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u6765\u8ba1\u7b97\u8f93\u51fa\u503cy\uff1b\n    # \u6211\u4eec\u901a\u8fc7\u8c03\u7528 MyReLU.apply \u51fd\u6570\u6765\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684ReLU\n    y_pred = MyReLU.apply(x.mm(w1)).mm(w2)\n\n    # \u8ba1\u7b97\u5e76\u8f93\u51faloss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.item())\n\n    # \u4f7f\u7528autograd\u8ba1\u7b97\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u3002\n    loss.backward()\n\n    with torch.no_grad():\n        # \u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\n        w1 -= learning_rate * w1.grad\n        w2 -= learning_rate * w2.grad\n\n        # \u5728\u53cd\u5411\u4f20\u64ad\u4e4b\u540e\u624b\u52a8\u6e05\u96f6\u68af\u5ea6\n        w1.grad.zero_()\n        w2.grad.zero_()\n\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#tensorflow","title":"TensorFlow\uff1a\u9759\u6001\u56fe","text":"<p>PyTorch\u81ea\u52a8\u6c42\u5bfc\u770b\u8d77\u6765\u975e\u5e38\u50cfTensorFlow\uff1a\u8fd9\u4e24\u4e2a\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u90fd\u5b9a\u4e49\u8ba1\u7b97\u56fe\uff0c\u4f7f\u7528\u81ea\u52a8\u5fae\u5206\u6765\u8ba1\u7b97\u68af\u5ea6\u3002\u4e24\u8005\u6700\u5927\u7684\u4e0d\u540c\u5c31\u662fTensorFlow\u7684\u8ba1\u7b97\u56fe\u662f\u9759\u6001\u7684\uff0c\u800cPyTorch\u4f7f\u7528\u52a8\u6001\u7684\u8ba1\u7b97\u56fe\u3002 </p> <p>\u5728TensorFlow\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u8ba1\u7b97\u56fe\u4e00\u6b21\uff0c\u7136\u540e\u91cd\u590d\u6267\u884c\u8fd9\u4e2a\u76f8\u540c\u7684\u56fe\uff0c\u53ef\u80fd\u4f1a\u63d0\u4f9b\u4e0d\u540c\u7684\u8f93\u5165\u6570\u636e\u3002\u800c\u5728PyTorch\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u524d\u5411\u901a\u9053\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u56fe\u3002 </p> <p>\u9759\u6001\u56fe\u7684\u597d\u5904\u5728\u4e8e\u4f60\u53ef\u4ee5\u9884\u5148\u5bf9\u56fe\u8fdb\u884c\u4f18\u5316\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a\u6846\u67b6\u53ef\u80fd\u8981\u878d\u5408\u4e00\u4e9b\u56fe\u7684\u8fd0\u7b97\u6765\u63d0\u5347\u6548\u7387\uff0c\u6216\u8005\u4ea7\u751f\u4e00\u4e2a\u7b56\u7565\u6765\u5c06\u56fe\u5206\u5e03\u5230\u591a\u4e2aGPU\u6216\u673a\u5668\u4e0a\u3002\u5982\u679c\u91cd\u590d\u4f7f\u7528\u76f8\u540c\u7684\u56fe\uff0c\u90a3\u4e48\u5728\u91cd\u590d\u8fd0\u884c\u540c\u4e00\u4e2a\u56fe\u65f6\uff0c\uff0c\u524d\u671f\u6f5c\u5728\u7684\u4ee3\u4ef7\u9ad8\u6602\u7684\u9884\u5148\u4f18\u5316\u7684\u6d88\u8017\u5c31\u4f1a\u88ab\u5206\u644a\u5f00\u3002</p> <p>\u9759\u6001\u56fe\u548c\u52a8\u6001\u56fe\u7684\u4e00\u4e2a\u533a\u522b\u662f\u63a7\u5236\u6d41\u3002\u5bf9\u4e8e\u4e00\u4e9b\u6a21\u578b\uff0c\u6211\u4eec\u5e0c\u671b\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u6267\u884c\u4e0d\u540c\u7684\u8ba1\u7b97\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u5bf9\u4e8e\u6bcf\u4e2a\u6570\u636e\u70b9\u6267\u884c\u4e0d\u540c\u7684\u65f6\u95f4\u6b65\u6570\uff0c\u8fd9\u4e2a\u5c55\u5f00(unrolling\uff09\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5faa\u73af\u6765\u5b9e\u73b0\u3002\u5bf9\u4e8e\u4e00\u4e2a\u9759\u6001\u56fe\uff0c\u5faa\u73af\u7ed3\u6784\u8981\u4f5c\u4e3a\u56fe\u7684\u4e00\u90e8\u5206\u3002\u56e0\u6b64\uff0cTensorFlow\u63d0\u4f9b\u4e86\u8fd0\u7b97\u7b26(\u4f8b\u5982<code>tf.scan</code>\uff09\u6765\u628a\u5faa\u73af\u5d4c\u5165\u5230\u56fe\u5f53\u4e2d\u3002\u5bf9\u4e8e\u52a8\u6001\u56fe\u6765\u8bf4\uff0c\u60c5\u51b5\u66f4\u52a0\u7b80\u5355\uff1a\u65e2\u7136\u6211\u4eec\u4e3a\u6bcf\u4e2a\u4f8b\u5b50\u5373\u65f6\u521b\u5efa\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684\u547d\u4ee4\u5f0f\u63a7\u5236\u6d41\u6765\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6267\u884c\u4e0d\u540c\u7684\u8ba1\u7b97\u3002 </p> <p>\u4e3a\u4e86\u4e0e\u4e0a\u9762\u7684PyTorch\u81ea\u52a8\u68af\u5ea6\u5b9e\u4f8b\u505a\u5bf9\u6bd4\uff0c\u6211\u4eec\u4f7f\u7528TensorFlow\u6765\u62df\u5408\u4e00\u4e2a\u7b80\u5355\u76842\u5c42\u7f51\u7edc\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 tf_two_layer_net.py\nimport tensorflow as tf\nimport numpy as np\n\n# \u9996\u5148\u6211\u4eec\u5efa\u7acb\u8ba1\u7b97\u56fe(computational graph\uff09\n\n# N\u662f\u6279\u5927\u5c0f\uff1bD\u662f\u8f93\u5165\u7ef4\u5ea6\uff1b\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6\u3002\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4e3a\u8f93\u5165\u548c\u76ee\u6807\u6570\u636e\u521b\u5efaplaceholder\uff1b\n# \u5f53\u6267\u884c\u8ba1\u7b97\u56fe\u65f6\uff0c\u4ed6\u4eec\u5c06\u4f1a\u88ab\u771f\u5b9e\u7684\u6570\u636e\u586b\u5145\nx = tf.placeholder(tf.float32, shape=(None, D_in))\ny = tf.placeholder(tf.float32, shape=(None, D_out))\n\n# \u4e3a\u6743\u91cd\u521b\u5efaVariable\u5e76\u7528\u968f\u673a\u6570\u636e\u521d\u59cb\u5316\n# TensorFlow\u7684Variable\u5728\u6267\u884c\u8ba1\u7b97\u56fe\u65f6\u4e0d\u4f1a\u6539\u53d8\nw1 = tf.Variable(tf.random_normal((D_in, H)))\nw2 = tf.Variable(tf.random_normal((H, D_out)))\n\n# \u524d\u5411\u4f20\u64ad\uff1a\u4f7f\u7528TensorFlow\u7684\u5f20\u91cf\u8fd0\u7b97\u8ba1\u7b97\u9884\u6d4b\u503cy\u3002\n# \u6ce8\u610f\u8fd9\u6bb5\u4ee3\u7801\u5b9e\u9645\u4e0a\u4e0d\u6267\u884c\u4efb\u4f55\u6570\u503c\u8fd0\u7b97\uff1b\n# \u5b83\u53ea\u662f\u5efa\u7acb\u4e86\u6211\u4eec\u7a0d\u540e\u5c06\u6267\u884c\u7684\u8ba1\u7b97\u56fe\u3002\nh = tf.matmul(x, w1)\nh_relu = tf.maximum(h, tf.zeros(1))\ny_pred = tf.matmul(h_relu, w2)\n\n# \u4f7f\u7528TensorFlow\u7684\u5f20\u91cf\u8fd0\u7b97\u635f\u5931(loss\uff09\nloss = tf.reduce_sum((y - y_pred) ** 2.0)\n\n# \u8ba1\u7b97loss\u5bf9\u4e8ew1\u548cw2\u7684\u5bfc\u6570\ngrad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n\n# \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\u3002\u4e3a\u4e86\u5b9e\u9645\u66f4\u65b0\u6743\u91cd\uff0c\u6211\u4eec\u9700\u8981\u5728\u6267\u884c\u8ba1\u7b97\u56fe\u65f6\u8ba1\u7b97new_w1\u548cnew_w2\u3002\n# \u6ce8\u610f\uff0c\u5728TensorFlow\u4e2d\uff0c\u66f4\u65b0\u6743\u91cd\u503c\u7684\u884c\u4e3a\u662f\u8ba1\u7b97\u56fe\u7684\u4e00\u90e8\u5206;\n# \u4f46\u5728PyTorch\u4e2d\uff0c\u8fd9\u53d1\u751f\u5728\u8ba1\u7b97\u56fe\u5f62\u4e4b\u5916\u3002\nlearning_rate = 1e-6\nnew_w1 = w1.assign(w1 - learning_rate * grad_w1)\nnew_w2 = w2.assign(w2 - learning_rate * grad_w2)\n\n# \u73b0\u5728\u6211\u4eec\u642d\u5efa\u597d\u4e86\u8ba1\u7b97\u56fe\uff0c\u6240\u4ee5\u6211\u4eec\u5f00\u59cb\u4e00\u4e2aTensorFlow\u7684\u4f1a\u8bdd(session\uff09\u6765\u5b9e\u9645\u6267\u884c\u8ba1\u7b97\u56fe\u3002\nwith tf.Session() as sess:\n\n    # \u8fd0\u884c\u4e00\u6b21\u8ba1\u7b97\u56fe\u6765\u521d\u59cb\u5316Variable w1\u548cw2\n    sess.run(tf.global_variables_initializer())\n\n    # \u521b\u5efanumpy\u6570\u7ec4\u6765\u5b58\u50a8\u8f93\u5165x\u548c\u76ee\u6807y\u7684\u5b9e\u9645\u6570\u636e\n    x_value = np.random.randn(N, D_in)\n    y_value = np.random.randn(N, D_out)\n\n    for _ in range(500):\n        # \u591a\u6b21\u8fd0\u884c\u8ba1\u7b97\u56fe\u3002\u6bcf\u6b21\u6267\u884c\u65f6\uff0c\u6211\u4eec\u90fd\u7528feed_dict\u53c2\u6570\uff0c\n        # \u5c06x_value\u7ed1\u5b9a\u5230x\uff0c\u5c06y_value\u7ed1\u5b9a\u5230y\uff0c\n        # \u6bcf\u6b21\u6267\u884c\u56fe\u5f62\u65f6\u6211\u4eec\u90fd\u8981\u8ba1\u7b97\u635f\u5931\u3001new_w1\u548cnew_w2\uff1b\n        # \u8fd9\u4e9b\u5f20\u91cf\u7684\u503c\u4ee5numpy\u6570\u7ec4\u7684\u5f62\u5f0f\u8fd4\u56de\u3002\n        loss_value, _, _ = sess.run([loss, new_w1, new_w2], \n                                    feed_dict={x: x_value, y: y_value})\n        print(loss_value)\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#nn","title":"<code>nn</code>\u6a21\u5757","text":""},{"location":"1.0/pytorch_with_examples/#pytorchnn","title":"PyTorch\uff1a<code>nn</code>","text":"<p>\u8ba1\u7b97\u56fe\u548cautograd\u662f\u5341\u5206\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5b9a\u4e49\u590d\u6742\u7684\u64cd\u4f5c\u5e76\u81ea\u52a8\u6c42\u5bfc\uff1b\u7136\u800c\u5bf9\u4e8e\u5927\u89c4\u6a21\u7684\u7f51\u7edc\uff0cautograd\u592a\u8fc7\u4e8e\u5e95\u5c42\u3002</p> <p>\u5728\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u6211\u4eec\u7ecf\u5e38\u8003\u8651\u5c06\u8ba1\u7b97\u5b89\u6392\u6210\u5c42\uff0c\u5176\u4e2d\u4e00\u4e9b\u5177\u6709\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u5b83\u4eec\u5c06\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u4f18\u5316\u3002</p> <p>TensorFlow\u91cc\uff0c\u6709\u7c7b\u4f3cKeras\uff0cTensorFlow-Slim\u548cTFLearn\u8fd9\u79cd\u5c01\u88c5\u4e86\u5e95\u5c42\u8ba1\u7b97\u56fe\u7684\u9ad8\u5ea6\u62bd\u8c61\u7684\u63a5\u53e3\uff0c\u8fd9\u4f7f\u5f97\u6784\u5efa\u7f51\u7edc\u5341\u5206\u65b9\u4fbf\u3002 </p> <p>\u5728PyTorch\u4e2d\uff0c\u5305<code>nn</code>\u5b8c\u6210\u4e86\u540c\u6837\u7684\u529f\u80fd\u3002<code>nn</code>\u5305\u4e2d\u5b9a\u4e49\u4e00\u7ec4\u5927\u81f4\u7b49\u4ef7\u4e8e\u5c42\u7684\u6a21\u5757\u3002\u4e00\u4e2a\u6a21\u5757\u63a5\u53d7\u8f93\u5165\u7684tesnor\uff0c\u8ba1\u7b97\u8f93\u51fa\u7684tensor\uff0c\u800c\u4e14\u8fd8\u4fdd\u5b58\u4e86\u4e00\u4e9b\u5185\u90e8\u72b6\u6001\u6bd4\u5982\u9700\u8981\u5b66\u4e60\u7684tensor\u7684\u53c2\u6570\u7b49\u3002<code>nn</code>\u5305\u4e2d\u4e5f\u5b9a\u4e49\u4e86\u4e00\u7ec4\u635f\u5931\u51fd\u6570(loss functions\uff09\uff0c\u7528\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002 </p> <p>\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u7528<code>nn</code>\u5305\u5b9e\u73b0\u4e24\u5c42\u7684\u7f51\u7edc\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_nn.py\nimport torch\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# N\u662f\u6279\u5927\u5c0f\uff1bD\u662f\u8f93\u5165\u7ef4\u5ea6\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u8f93\u5165\u548c\u8f93\u51fa\u968f\u673a\u5f20\u91cf\nx = torch.randn(N, D_in, device=device)\ny = torch.randn(N, D_out, device=device)\n\n\n# \u4f7f\u7528nn\u5305\u5c06\u6211\u4eec\u7684\u6a21\u578b\u5b9a\u4e49\u4e3a\u4e00\u7cfb\u5217\u7684\u5c42\u3002\n# nn.Sequential\u662f\u5305\u542b\u5176\u4ed6\u6a21\u5757\u7684\u6a21\u5757\uff0c\u5e76\u6309\u987a\u5e8f\u5e94\u7528\u8fd9\u4e9b\u6a21\u5757\u6765\u4ea7\u751f\u5176\u8f93\u51fa\u3002\n# \u6bcf\u4e2a\u7ebf\u6027\u6a21\u5757\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u4ece\u8f93\u5165\u8ba1\u7b97\u8f93\u51fa\uff0c\u5e76\u4fdd\u5b58\u5176\u5185\u90e8\u7684\u6743\u91cd\u548c\u504f\u5dee\u5f20\u91cf\u3002\n# \u5728\u6784\u9020\u6a21\u578b\u4e4b\u540e\uff0c\u6211\u4eec\u4f7f\u7528.to()\u65b9\u6cd5\u5c06\u5176\u79fb\u52a8\u5230\u6240\u9700\u7684\u8bbe\u5907\u3002\nmodel = torch.nn.Sequential(\n            torch.nn.Linear(D_in, H),\n            torch.nn.ReLU(),\n            torch.nn.Linear(H, D_out),\n        ).to(device)\n\n\n# nn\u5305\u8fd8\u5305\u542b\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\uff1b\n# \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u5e73\u5747\u5e73\u65b9\u8bef\u5dee(MSE)\u4f5c\u4e3a\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570\u3002\n# \u8bbe\u7f6ereduction='sum'\uff0c\u8868\u793a\u6211\u4eec\u8ba1\u7b97\u7684\u662f\u5e73\u65b9\u8bef\u5dee\u7684\u201c\u548c\u201d\uff0c\u800c\u4e0d\u662f\u5e73\u5747\u503c;\n# \u8fd9\u662f\u4e3a\u4e86\u4e0e\u524d\u9762\u6211\u4eec\u624b\u5de5\u8ba1\u7b97\u635f\u5931\u7684\u4f8b\u5b50\u4fdd\u6301\u4e00\u81f4\uff0c\n# \u4f46\u662f\u5728\u5b9e\u8df5\u4e2d\uff0c\u901a\u8fc7\u8bbe\u7f6ereduction='elementwise_mean'\u6765\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u4f5c\u4e3a\u635f\u5931\u66f4\u4e3a\u5e38\u89c1\u3002\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-4\nfor t in range(500):\n\n    # \u524d\u5411\u4f20\u64ad\uff1a\u901a\u8fc7\u5411\u6a21\u578b\u4f20\u5165x\u8ba1\u7b97\u9884\u6d4b\u7684y\u3002\n    # \u6a21\u5757\u5bf9\u8c61\u91cd\u8f7d\u4e86__call__\u8fd0\u7b97\u7b26\uff0c\u6240\u4ee5\u53ef\u4ee5\u50cf\u51fd\u6570\u90a3\u6837\u8c03\u7528\u5b83\u4eec\u3002\n    # \u8fd9\u4e48\u505a\u76f8\u5f53\u4e8e\u5411\u6a21\u5757\u4f20\u5165\u4e86\u4e00\u4e2a\u5f20\u91cf\uff0c\u7136\u540e\u5b83\u8fd4\u56de\u4e86\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\u3002\n    y_pred = model(x)\n\n    # \u8ba1\u7b97\u5e76\u6253\u5370\u635f\u5931\u3002\u6211\u4eec\u4f20\u9012\u5305\u542by\u7684\u9884\u6d4b\u503c\u548c\u771f\u5b9e\u503c\u7684\u5f20\u91cf\uff0c\u635f\u5931\u51fd\u6570\u8fd4\u56de\u5305\u542b\u635f\u5931\u7684\u5f20\u91cf\u3002\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \u53cd\u5411\u4f20\u64ad\u4e4b\u524d\u6e05\u96f6\u68af\u5ea6\n    model.zero_grad()\n\n    # \u53cd\u5411\u4f20\u64ad\uff1a\u8ba1\u7b97\u6a21\u578b\u7684\u635f\u5931\u5bf9\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u5bfc\u6570(\u68af\u5ea6\uff09\u3002\n    # \u5728\u5185\u90e8\uff0c\u6bcf\u4e2a\u6a21\u5757\u7684\u53c2\u6570\u5b58\u50a8\u5728requires_grad=True\u7684\u5f20\u91cf\u4e2d\uff0c\n    # \u56e0\u6b64\u8fd9\u4e2a\u8c03\u7528\u5c06\u8ba1\u7b97\u6a21\u578b\u4e2d\u6240\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u68af\u5ea6\u3002\n    loss.backward()\n\n    # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u6743\u91cd\u3002\n    # \u6bcf\u4e2a\u53c2\u6570\u90fd\u662f\u5f20\u91cf\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u50cf\u6211\u4eec\u4ee5\u524d\u90a3\u6837\u53ef\u4ee5\u5f97\u5230\u5b83\u7684\u6570\u503c\u548c\u68af\u5ea6\n    with torch.no_grad():\n        for param in model.parameters():\n            param.data -= learning_rate * param.grad\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#pytorchoptim","title":"PyTorch\uff1a<code>optim</code>","text":"<p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u5df2\u7ecf\u901a\u8fc7\u624b\u52a8\u6539\u53d8\u5305\u542b\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u5f20\u91cf\u6765\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd\u3002\u5bf9\u4e8e\u968f\u673a\u68af\u5ea6\u4e0b\u964d(SGD/stochastic gradient descent)\u7b49\u7b80\u5355\u7684\u4f18\u5316\u7b97\u6cd5\u6765\u8bf4\uff0c\u8fd9\u4e0d\u662f\u4e00\u4e2a\u5f88\u5927\u7684\u8d1f\u62c5\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u7ecf\u5e38\u4f7f\u7528AdaGrad\u3001RMSProp\u3001Adam\u7b49\u66f4\u590d\u6742\u7684\u4f18\u5316\u5668\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_optim.py\nimport torch\n\n# N\u662f\u6279\u5927\u5c0f\uff1bD\u662f\u8f93\u5165\u7ef4\u5ea6\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u968f\u673a\u8f93\u5165\u548c\u8f93\u51fa\u5f20\u91cf\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \u4f7f\u7528nn\u5305\u5b9a\u4e49\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570\nmodel = torch.nn.Sequential(\n          torch.nn.Linear(D_in, H),\n          torch.nn.ReLU(),\n          torch.nn.Linear(H, D_out),\n        )\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# \u4f7f\u7528optim\u5305\u5b9a\u4e49\u4f18\u5316\u5668(Optimizer\uff09\u3002Optimizer\u5c06\u4f1a\u4e3a\u6211\u4eec\u66f4\u65b0\u6a21\u578b\u7684\u6743\u91cd\u3002\n# \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528Adam\u4f18\u5316\u65b9\u6cd5\uff1boptim\u5305\u8fd8\u5305\u542b\u4e86\u8bb8\u591a\u522b\u7684\u4f18\u5316\u7b97\u6cd5\u3002\n# Adam\u6784\u9020\u51fd\u6570\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u544a\u8bc9\u4f18\u5316\u5668\u5e94\u8be5\u66f4\u65b0\u54ea\u4e9b\u5f20\u91cf\u3002\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nfor t in range(500):\n\n    # \u524d\u5411\u4f20\u64ad\uff1a\u901a\u8fc7\u50cf\u6a21\u578b\u8f93\u5165x\u8ba1\u7b97\u9884\u6d4b\u7684y\n    y_pred = model(x)\n\n    # \u8ba1\u7b97\u5e76\u6253\u5370loss\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \u5728\u53cd\u5411\u4f20\u64ad\u4e4b\u524d\uff0c\u4f7f\u7528optimizer\u5c06\u5b83\u8981\u66f4\u65b0\u7684\u6240\u6709\u5f20\u91cf\u7684\u68af\u5ea6\u6e05\u96f6(\u8fd9\u4e9b\u5f20\u91cf\u662f\u6a21\u578b\u53ef\u5b66\u4e60\u7684\u6743\u91cd)\n    optimizer.zero_grad()\n\n    # \u53cd\u5411\u4f20\u64ad\uff1a\u6839\u636e\u6a21\u578b\u7684\u53c2\u6570\u8ba1\u7b97loss\u7684\u68af\u5ea6\n    loss.backward()\n\n    # \u8c03\u7528Optimizer\u7684step\u51fd\u6570\u4f7f\u5b83\u6240\u6709\u53c2\u6570\u66f4\u65b0\n    optimizer.step()\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#pytorchnn_1","title":"PyTorch\uff1a\u81ea\u5b9a\u4e49<code>nn</code>\u6a21\u5757","text":"<p>\u6709\u65f6\u5019\u9700\u8981\u6307\u5b9a\u6bd4\u73b0\u6709\u6a21\u5757\u5e8f\u5217\u66f4\u590d\u6742\u7684\u6a21\u578b\uff1b\u5bf9\u4e8e\u8fd9\u4e9b\u60c5\u51b5\uff0c\u53ef\u4ee5\u901a\u8fc7\u7ee7\u627f<code>nn.Module</code>\u5e76\u5b9a\u4e49<code>forward</code>\u51fd\u6570\uff0c\u8fd9\u4e2a<code>forward</code>\u51fd\u6570\u53ef\u4ee5\u4f7f\u7528\u5176\u4ed6\u6a21\u5757\u6216\u8005\u5176\u4ed6\u7684\u81ea\u52a8\u6c42\u5bfc\u8fd0\u7b97\u6765\u63a5\u6536\u8f93\u5165tensor\uff0c\u4ea7\u751f\u8f93\u51fatensor\u3002 </p> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u7528\u81ea\u5b9a\u4e49Module\u7684\u5b50\u7c7b\u6784\u5efa\u4e24\u5c42\u7f51\u7edc\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 two_layer_net_module.py\nimport torch\n\nclass TwoLayerNet(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        \"\"\"\n        \u5728\u6784\u9020\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u5b9e\u4f8b\u5316\u4e86\u4e24\u4e2ann.Linear\u6a21\u5757\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u6210\u5458\u53d8\u91cf\u3002\n        \"\"\"\n        super(TwoLayerNet, self).__init__()\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        \"\"\"\n        \u5728\u524d\u5411\u4f20\u64ad\u7684\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u63a5\u6536\u4e00\u4e2a\u8f93\u5165\u7684\u5f20\u91cf\uff0c\u4e5f\u5fc5\u987b\u8fd4\u56de\u4e00\u4e2a\u8f93\u51fa\u5f20\u91cf\u3002\n        \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6784\u9020\u51fd\u6570\u4e2d\u5b9a\u4e49\u7684\u6a21\u5757\u4ee5\u53ca\u5f20\u91cf\u4e0a\u7684\u4efb\u610f\u7684(\u53ef\u5fae\u5206\u7684\uff09\u64cd\u4f5c\u3002\n        \"\"\"\n        h_relu = self.linear1(x).clamp(min=0)\n        y_pred = self.linear2(h_relu)\n        return y_pred\n\n# N\u662f\u6279\u5927\u5c0f\uff1b D_in \u662f\u8f93\u5165\u7ef4\u5ea6\uff1b\n# H \u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1b D_out \u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u8f93\u5165\u548c\u8f93\u51fa\u7684\u968f\u673a\u5f20\u91cf\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \u901a\u8fc7\u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u5efa\u6211\u4eec\u7684\u6a21\u578b\u3002\nmodel = TwoLayerNet(D_in, H, D_out)\n\n# \u6784\u9020\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\u3002\n# SGD\u6784\u9020\u51fd\u6570\u4e2d\u5bf9model.parameters()\u7684\u8c03\u7528\uff0c\n# \u5c06\u5305\u542b\u6a21\u578b\u7684\u4e00\u90e8\u5206\uff0c\u5373\u4e24\u4e2ann.Linear\u6a21\u5757\u7684\u53ef\u5b66\u4e60\u53c2\u6570\u3002\nloss_fn = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\nfor t in range(500):\n    # \u524d\u5411\u4f20\u64ad\uff1a\u901a\u8fc7\u5411\u6a21\u578b\u4f20\u9012x\u8ba1\u7b97\u9884\u6d4b\u503cy\n    y_pred = model(x)\n\n    #\u8ba1\u7b97\u5e76\u8f93\u51faloss\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # \u6e05\u96f6\u68af\u5ea6\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u66f4\u65b0\u6743\u91cd\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#pytorch_4","title":"PyTorch\uff1a\u63a7\u5236\u6d41\u548c\u6743\u91cd\u5171\u4eab","text":"<p>\u4f5c\u4e3a\u52a8\u6001\u56fe\u548c\u6743\u91cd\u5171\u4eab\u7684\u4e00\u4e2a\u4f8b\u5b50\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u975e\u5e38\u5947\u602a\u7684\u6a21\u578b\uff1a\u4e00\u4e2a\u5168\u8fde\u63a5\u7684ReLU\u7f51\u7edc\uff0c\u5728\u6bcf\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u65f6\uff0c\u5b83\u7684\u9690\u85cf\u5c42\u7684\u5c42\u6570\u4e3a\u968f\u673a1\u52304\u4e4b\u95f4\u7684\u6570\uff0c\u8fd9\u6837\u53ef\u4ee5\u591a\u6b21\u91cd\u7528\u76f8\u540c\u7684\u6743\u91cd\u6765\u8ba1\u7b97\u3002</p> <p>\u56e0\u4e3a\u8fd9\u4e2a\u6a21\u578b\u53ef\u4ee5\u4f7f\u7528\u666e\u901a\u7684Python\u6d41\u63a7\u5236\u6765\u5b9e\u73b0\u5faa\u73af\uff0c\u5e76\u4e14\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5728\u5b9a\u4e49\u8f6c\u53d1\u65f6\u591a\u6b21\u91cd\u7528\u540c\u4e00\u4e2a\u6a21\u5757\u6765\u5b9e\u73b0\u6700\u5185\u5c42\u4e4b\u95f4\u7684\u6743\u91cd\u5171\u4eab\u3002</p> <p>\u6211\u4eec\u5229\u7528Mudule\u7684\u5b50\u7c7b\u5f88\u5bb9\u6613\u5b9e\u73b0\u8fd9\u4e2a\u6a21\u578b\uff1a</p> <pre><code># \u53ef\u8fd0\u884c\u4ee3\u7801\u89c1\u672c\u6587\u4ef6\u5939\u4e2d\u7684 dynamic_net.py\nimport random\nimport torch\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        \"\"\"\n        \u5728\u6784\u9020\u51fd\u6570\u4e2d\uff0c\u6211\u4eec\u6784\u9020\u4e86\u4e09\u4e2ann.Linear\u5b9e\u4f8b\uff0c\u5b83\u4eec\u5c06\u5728\u524d\u5411\u4f20\u64ad\u65f6\u88ab\u4f7f\u7528\u3002\n        \"\"\"\n        super(DynamicNet, self).__init__()\n        self.input_linear = torch.nn.Linear(D_in, H)\n        self.middle_linear = torch.nn.Linear(H, H)\n        self.output_linear = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        \"\"\"\n        \u5bf9\u4e8e\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\uff0c\u6211\u4eec\u968f\u673a\u9009\u62e90\u30011\u30012\u30013\uff0c\n        \u5e76\u91cd\u7528\u4e86\u591a\u6b21\u8ba1\u7b97\u9690\u85cf\u5c42\u7684middle_linear\u6a21\u5757\u3002\n        \u7531\u4e8e\u6bcf\u4e2a\u524d\u5411\u4f20\u64ad\u6784\u5efa\u4e00\u4e2a\u52a8\u6001\u8ba1\u7b97\u56fe\uff0c\n        \u6211\u4eec\u53ef\u4ee5\u5728\u5b9a\u4e49\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u65f6\u4f7f\u7528\u5e38\u89c4Python\u63a7\u5236\u6d41\u8fd0\u7b97\u7b26\uff0c\u5982\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5\u3002\n        \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u8fd8\u770b\u5230\uff0c\u5728\u5b9a\u4e49\u8ba1\u7b97\u56fe\u5f62\u65f6\u591a\u6b21\u91cd\u7528\u540c\u4e00\u4e2a\u6a21\u5757\u662f\u5b8c\u5168\u5b89\u5168\u7684\u3002\n        \u8fd9\u662fLua Torch\u7684\u4e00\u5927\u6539\u8fdb\uff0c\u56e0\u4e3aLua Torch\u4e2d\u6bcf\u4e2a\u6a21\u5757\u53ea\u80fd\u4f7f\u7528\u4e00\u6b21\u3002\n        \"\"\"\n        h_relu = self.input_linear(x).clamp(min=0)\n        for _ in range(random.randint(0, 3)):\n            h_relu = self.middle_linear(h_relu).clamp(min=0)\n        y_pred = self.output_linear(h_relu)\n        return y_pred\n\n\n# N\u662f\u6279\u5927\u5c0f\uff1bD\u662f\u8f93\u5165\u7ef4\u5ea6\n# H\u662f\u9690\u85cf\u5c42\u7ef4\u5ea6\uff1bD_out\u662f\u8f93\u51fa\u7ef4\u5ea6\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# \u4ea7\u751f\u8f93\u5165\u548c\u8f93\u51fa\u968f\u673a\u5f20\u91cf\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# \u5b9e\u4f8b\u5316\u4e0a\u9762\u5b9a\u4e49\u7684\u7c7b\u6765\u6784\u9020\u6211\u4eec\u7684\u6a21\u578b\nmodel = DynamicNet(D_in, H, D_out)\n\n# \u6784\u9020\u6211\u4eec\u7684\u635f\u5931\u51fd\u6570(loss function\uff09\u548c\u4f18\u5316\u5668(Optimizer\uff09\u3002\n# \u7528\u5e73\u51e1\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fd9\u4e2a\u5947\u602a\u7684\u6a21\u578b\u662f\u56f0\u96be\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u4f7f\u7528\u4e86momentum\u65b9\u6cd5\u3002\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\nfor t in range(500):\n\n    # \u524d\u5411\u4f20\u64ad\uff1a\u901a\u8fc7\u5411\u6a21\u578b\u4f20\u5165x\u8ba1\u7b97\u9884\u6d4b\u7684y\u3002\n    y_pred = model(x)\n\n    # \u8ba1\u7b97\u5e76\u6253\u5370\u635f\u5931\n    loss = criterion(y_pred, y)\n    print(t, loss.item())\n\n    # \u6e05\u96f6\u68af\u5ea6\uff0c\u53cd\u5411\u4f20\u64ad\uff0c\u66f4\u65b0\u6743\u91cd \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"1.0/pytorch_with_examples/#examples","title":"Examples","text":"<p>You can browse the above examples here.</p>"},{"location":"1.0/pytorch_with_examples/#tensors","title":"Tensors","text":"<p>Warm-up: numpy</p> <p></p> <p>PyTorch: Tensors</p>"},{"location":"1.0/pytorch_with_examples/#autograd","title":"Autograd","text":"<p>PyTorch: Tensors and autograd</p> <p></p> <p>PyTorch: Defining New autograd Functions</p> <p></p> <p>TensorFlow: Static Graphs</p>"},{"location":"1.0/pytorch_with_examples/#nn-module","title":"<code>nn</code> module","text":"<p>PyTorch: nn</p> <p></p> <p>PyTorch: optim</p> <p></p> <p>PyTorch: Custom nn Modules</p> <p></p> <p>PyTorch: Control Flow + Weight Sharing</p>"},{"location":"1.0/reinforcement_q_learning/","title":"\u5f3a\u5316\u5b66\u4e60 (DQN) \u6559\u7a0b","text":"<p>\u8bd1\u8005\uff1a\u5e73\u6de1\u7684\u5929 </p> <p>\u4f5c\u8005: Adam Paszke</p> <p>\u672c\u6559\u7a0b\u5c06\u5c55\u793a\u5982\u4f55\u4f7f\u7528 PyTorch \u5728OpenAI Gym\u7684\u4efb\u52a1\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6df1\u5ea6Q\u5b66\u4e60 (DQN) \u667a\u80fd\u70b9\u3002</p> <p>\u4efb\u52a1</p> <p>\u667a\u80fd\u70b9\u9700\u8981\u51b3\u5b9a\u4e24\u79cd\u52a8\u4f5c\uff1a\u5411\u5de6\u6216\u5411\u53f3\u6765\u4f7f\u5176\u4e0a\u7684\u6746\u4fdd\u6301\u76f4\u7acb\u3002\u4f60\u53ef\u4ee5\u5728 Gym website \u627e\u5230\u4e00\u4e2a\u6709\u5404\u79cd\u7b97\u6cd5\u548c\u53ef\u89c6\u5316\u7684\u5b98\u65b9\u6392\u884c\u699c\u3002</p> <p></p> <p>\u5f53\u667a\u80fd\u70b9\u89c2\u5bdf\u73af\u5883\u7684\u5f53\u524d\u72b6\u6001\u5e76\u9009\u62e9\u52a8\u4f5c\u65f6\uff0c\u73af\u5883\u5c06\u8f6c\u6362\u4e3a\u65b0\u72b6\u6001\uff0c\u5e76\u8fd4\u56de\u6307\u793a\u52a8\u4f5c\u7ed3\u679c\u7684\u5956\u52b1\u3002\u5728\u8fd9\u9879\u4efb\u52a1\u4e2d\uff0c\u6bcf\u589e\u52a0\u4e00\u4e2a\u65f6\u95f4\u6b65\uff0c\u5956\u52b1+1\uff0c\u5982\u679c\u6746\u5b50\u6389\u5f97\u592a\u8fdc\u6216\u5927\u8f66\u79fb\u52a8\u8ddd\u79bb\u4e2d\u5fc3\u8d85\u8fc72.4\u4e2a\u5355\u4f4d\uff0c\u73af\u5883\u5c31\u4f1a\u7ec8\u6b62\u3002\u8fd9\u610f\u5473\u7740\u66f4\u597d\u7684\u6267\u884c\u573a\u666f\u5c06\u6301\u7eed\u66f4\u957f\u7684\u65f6\u95f4\uff0c\u79ef\u7d2f\u66f4\u5927\u7684\u56de\u62a5\u3002</p> <p>Cartpole\u4efb\u52a1\u7684\u8bbe\u8ba1\u4e3a\u667a\u80fd\u70b9\u8f93\u5165\u4ee3\u8868\u73af\u5883\u72b6\u6001(\u4f4d\u7f6e\u3001\u901f\u5ea6\u7b49\uff09\u76844\u4e2a\u5b9e\u9645\u503c\u3002\u7136\u800c\uff0c\u795e\u7ecf\u7f51\u7edc\u5b8c\u5168\u53ef\u4ee5\u901a\u8fc7\u89c2\u5bdf\u573a\u666f\u6765\u89e3\u51b3\u8fd9\u4e2a\u4efb\u52a1\uff0c\u6240\u4ee5\u6211\u4eec\u5c06\u4f7f\u7528\u4ee5\u8f66\u4e3a\u4e2d\u5fc3\u7684\u4e00\u5757\u5c4f\u5e55\u4f5c\u4e3a\u8f93\u5165\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u65e0\u6cd5\u76f4\u63a5\u4e0e\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u7684\u7ed3\u679c\u76f8\u6bd4\u2014\u2014\u6211\u4eec\u7684\u4efb\u52a1\u66f4\u8270\u5de8\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u4f1a\u51cf\u6162\u8bad\u7ec3\u901f\u5ea6\uff0c\u56e0\u4e3a\u6211\u4eec\u5fc5\u987b\u6e32\u67d3\u6240\u6709\u5e27\u3002</p> <p>\u4e25\u683c\u5730\u8bf4\uff0c\u6211\u4eec\u5c06\u4ee5\u5f53\u524d\u5e27\u548c\u524d\u4e00\u4e2a\u5e27\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5448\u73b0\u72b6\u6001\u3002\u8fd9\u5c06\u5141\u8bb8\u4ee3\u7406\u4ece\u4e00\u5f20\u56fe\u50cf\u4e2d\u8003\u8651\u6746\u5b50\u7684\u901f\u5ea6\u3002</p> <p>\u5305</p> <p>\u9996\u5148\u4f60\u9700\u8981\u5bfc\u5165\u5fc5\u987b\u7684\u5305\u3002\u6211\u4eec\u9700\u8981 gym \u4f5c\u4e3a\u73af\u5883 (\u4f7f\u7528 <code>pip install gym</code> \u5b89\u88c5). \u6211\u4eec\u4e5f\u9700\u8981 PyTorch \u7684\u5982\u4e0b\u529f\u80fd:</p> <ul> <li>\u795e\u7ecf\u7f51\u7edc (<code>torch.nn</code>)</li> <li>\u4f18\u5316 (<code>torch.optim</code>)</li> <li>\u81ea\u52a8\u5fae\u5206 (<code>torch.autograd</code>)</li> <li>\u89c6\u89c9\u4efb\u52a1 (<code>torchvision</code> - a separate package).</li> </ul> <pre><code>import gym\nimport math\nimport random\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom itertools import count\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nenv = gym.make('CartPole-v0').unwrapped\n\n# \u5efa\u7acb matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# \u5982\u679c\u4f7f\u7528gpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre>"},{"location":"1.0/reinforcement_q_learning/#_1","title":"\u56de\u653e\u5185\u5b58","text":"<p>\u6211\u4eec\u5c06\u4f7f\u7528\u7ecf\u9a8c\u56de\u653e\u5185\u5b58\u6765\u8bad\u7ec3DQN\u3002\u5b83\u5b58\u50a8\u667a\u80fd\u70b9\u89c2\u5bdf\u5230\u7684\u8f6c\u6362\uff0c\u5141\u8bb8\u6211\u4eec\u7a0d\u540e\u91cd\u7528\u6b64\u6570\u636e\u3002\u901a\u8fc7\u4ece\u4e2d\u968f\u673a\u62bd\u6837\uff0c\u7ec4\u6210\u6279\u5bf9\u8c61\u7684\u8f6c\u6362\u5c06\u88ab\u53d6\u6d88\u76f8\u5173\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u5927\u5927\u7a33\u5b9a\u548c\u6539\u8fdb\u4e86DQN\u8bad\u7ec3\u8fc7\u7a0b\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e24\u4e2a\u7c7b\u522b\uff1a</p> <ul> <li><code>Transition</code> - \u4e00\u4e2a\u547d\u540d\u7684\u5143\u7ec4\uff0c\u8868\u793a\u6211\u4eec\u73af\u5883\u4e2d\u7684\u5355\u4e2a\u8f6c\u6362\u3002\u5b83\u57fa\u672c\u4e0a\u5c06(\u72b6\u6001\u3001\u52a8\u4f5c\uff09\u5bf9\u6620\u5c04\u5230\u5b83\u4eec\u7684(\u4e0b\u4e00\u4e2a\u72b6\u6001\u3001\u5956\u52b1\uff09\u7ed3\u679c\uff0c\u72b6\u6001\u662f\u5c4f\u5e55\u5dee\u5206\u56fe\u50cf\uff0c\u5982\u540e\u9762\u6240\u8ff0\u3002</li> <li><code>ReplayMemory</code> - \u4e00\u4e2a\u6709\u754c\u5927\u5c0f\u7684\u5faa\u73af\u7f13\u51b2\u533a\uff0c\u7528\u4e8e\u4fdd\u5b58\u6700\u8fd1\u89c2\u5bdf\u5230\u7684\u8f6c\u6362\u3002\u5b83\u8fd8\u5b9e\u73b0\u4e86\u4e00\u4e2a<code>.sample(\uff09</code>\u65b9\u6cd5\uff0c\u7528\u4e8e\u9009\u62e9\u4e00\u6279\u968f\u673a\u8f6c\u6362\u8fdb\u884c\u8bad\u7ec3\u3002</li> </ul> <pre><code>Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        \"\"\"\u4fdd\u5b58\u53d8\u6362\"\"\"\n        if len(self.memory) &lt; self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n</code></pre> <p>\u73b0\u5728\u6211\u4eec\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u6a21\u578b\u3002\u4f46\u9996\u5148\u6765\u5feb\u901f\u4e86\u89e3\u4e00\u4e0bDQN\u3002</p>"},{"location":"1.0/reinforcement_q_learning/#dqn_1","title":"DQN \u7b97\u6cd5","text":"<p>\u6211\u4eec\u7684\u73af\u5883\u662f\u786e\u5b9a\u7684\uff0c\u6240\u4ee5\u8fd9\u91cc\u63d0\u51fa\u7684\u6240\u6709\u65b9\u7a0b\u4e5f\u90fd\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u6587\u732e\u4e2d\uff0c\u5b83\u4eec\u8fd8\u5305\u542b\u5bf9\u73af\u5883\u4e2d\u968f\u673a\u8f6c\u6362\u7684\u671f\u671b\u3002</p> <p>\u6211\u4eec\u7684\u76ee\u6807\u662f\u5236\u5b9a\u4e00\u9879\u7b56\u7565\uff0c\u8bd5\u56fe\u6700\u5927\u5316\u6298\u6263\u3001\u7d2f\u79ef\u5956\u52b1 \\(\\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\)\\)\uff0c\u5176\u4e2d \\(\\(R_{t_0}\\)\\) \u4e5f\u88ab\u8ba4\u4e3a\u662f\u8fd4\u56de\u503c\u3002\\(\\(\\gamma\\)\\) \u5e94\u8be5\u662f\u4ecb\u4e8e \\(\\(0\\)\\) \u548c \\(\\(1\\)\\) \u4e4b\u95f4\u7684\u5e38\u91cf\uff0c\u4ee5\u786e\u4fdd\u548c\u6536\u655b\u3002\u5b83\u4f7f\u6765\u81ea\u4e0d\u786e\u5b9a\u7684\u9065\u8fdc\u672a\u6765\u7684\u56de\u62a5\u5bf9\u6211\u4eec\u7684\u4ee3\u7406\u6765\u8bf4\u6bd4\u5b83\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u76f8\u5f53\u6709\u4fe1\u5fc3\u7684\u56de\u62a5\u66f4\u4e0d\u91cd\u8981\u3002</p> <p>Q-Learning\u80cc\u540e\u7684\u4e3b\u8981\u601d\u60f3\u662f\uff0c\u5982\u679c\u6211\u4eec\u6709\u4e00\u4e2a\u51fd\u6570 \\(\\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\)\\), \u5219\u5982\u679c\u6211\u4eec\u5728\u7279\u5b9a\u7684\u72b6\u6001\u4e0b\u91c7\u53d6\u884c\u52a8\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u6784\u5efa\u4e00\u4e2a\u6700\u5927\u5316\u56de\u62a5\u7684\u7b56\u7565\uff1a</p> \\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\] <p>\u7136\u800c\uff0c\u6211\u4eec\u5e76\u4e0d\u4e86\u89e3\u4e16\u754c\u7684\u4e00\u5207\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u8bbf\u95ee \\(\\(Q^*\\)\\)\u3002\u4f46\u662f\uff0c\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u662f\u901a\u7528\u7684\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5730\u521b\u5efa\u4e00\u4e2a\u5e76\u8bad\u7ec3\u5b83\u7c7b\u4f3c\u4e8e \\(\\(Q^*\\)\\)\u3002</p> <p>\u5bf9\u4e8e\u6211\u4eec\u7684\u8bad\u7ec3\u66f4\u65b0\u89c4\u5219\uff0c\u6211\u4eec\u5c06\u5047\u8bbe\u67d0\u4e9b\u7b56\u7565\u7684\u6bcf\u4e2a \\(\\(Q\\)\\) \u51fd\u6570\u90fd\u9075\u5faaBellman\u65b9\u7a0b\uff1a \\(\\(Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\)\\)</p> <p>\u7b49\u5f0f\u4e24\u8fb9\u7684\u5dee\u5f02\u88ab\u79f0\u4e3a\u65f6\u95f4\u5dee\u8bef\u5dee\uff0c\u5373 \\(\\(\\delta\\)\\):</p> \\[\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\] <p>\u4e3a\u4e86\u5c3d\u91cf\u51cf\u5c11\u8fd9\u4e2a\u9519\u8bef\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 Huber loss\u3002Huber\u635f\u5931\u5728\u8bef\u5dee\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e3a\u5747\u65b9\u8bef\u5dee\uff0c\u4f46\u5728\u8bef\u5dee\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e3a\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u2014\u2014\u8fd9\u4f7f\u5f97\u5f53\u5bf9 \\(\\(Q\\)\\) \u7684\u4f30\u8ba1\u566a\u97f3\u5f88\u5927\u65f6\uff0c\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\u66f4\u5f3a\u3002\u6211\u4eec\u901a\u8fc7\u4ece\u91cd\u653e\u5185\u5b58\u4e2d\u53d6\u6837\u7684\u4e00\u6279\u8f6c\u6362\u6765\u8ba1\u7b97 \\(\\(B\\)\\)\uff1a</p> \\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\] <p>\\( \\begin{split} \\text{where}  \\quad \\mathcal{L}(\\delta) = \\begin{cases} \\frac{1}{2}{\\delta^2} &amp; \\text{for } |\\delta| \\le 1, \\ |\\delta| - \\frac{1}{2} &amp; \\text{otherwise.}  \\end{cases} \\end{split} \\)</p>"},{"location":"1.0/reinforcement_q_learning/#q-","title":"Q-\u7f51\u7edc","text":"<p>\u6211\u4eec\u7684\u6a21\u578b\u662f\u4e00\u4e2a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u53ef\u4ee5\u5904\u7406\u5f53\u524d\u548c\u4ee5\u524d\u7684\u5e27\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5b83\u6709\u4e24\u4e2a\u8f93\u51fa\uff0c\u5206\u522b\u8868\u793a\\(\\(Q(s, \\mathrm{left})\\)\\) \u548c \\(\\(Q(s, \\mathrm{right})\\)\\)(\u5176\u4e2d \\(\\(s\\)\\)\u662f\u7f51\u7edc\u7684\u8f93\u5165\uff09\u3002\u5b9e\u9645\u4e0a\uff0c\u7f51\u7edc\u6b63\u8bd5\u56fe\u9884\u6d4b\u5728\u7ed9\u5b9a\u7535\u6d41\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u91c7\u53d6\u6bcf\u9879\u884c\u52a8\u7684\u9884\u671f\u56de\u62a5\u3002</p> <pre><code>class DQN(nn.Module):\n\n    def __init__(self, h, w):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.bn3 = nn.BatchNorm2d(32)\n\n        # \u7ebf\u6027\u8f93\u5165\u8fde\u63a5\u7684\u6570\u91cf\u53d6\u51b3\u4e8econv2d\u5c42\u7684\u8f93\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u8ba1\u7b97\u8f93\u5165\u56fe\u50cf\u7684\u5927\u5c0f\u3002\n        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n            return (size - (kernel_size - 1) - 1) // stride  + 1\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n        linear_input_size = convw * convh * 32\n        self.head = nn.Linear(linear_input_size, 2) # 448 \u6216\u8005 512\n\n    # \u4f7f\u7528\u4e00\u4e2a\u5143\u7d20\u8c03\u7528\u4ee5\u786e\u5b9a\u4e0b\u4e00\u4e2a\u64cd\u4f5c\uff0c\u6216\u5728\u4f18\u5316\u671f\u95f4\u8c03\u7528\u6279\u5904\u7406\u3002\u8fd4\u56de\u5f20\u91cf\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))\n\n</code></pre>"},{"location":"1.0/reinforcement_q_learning/#_2","title":"\u83b7\u53d6\u8f93\u5165","text":"<p>\u4e0b\u9762\u7684\u4ee3\u7801\u662f\u7528\u4e8e\u4ece\u73af\u5883\u4e2d\u63d0\u53d6\u548c\u5904\u7406\u6e32\u67d3\u56fe\u50cf\u7684\u5b9e\u7528\u7a0b\u5e8f\u3002\u5b83\u4f7f\u7528\u4e86<code>torchvision</code> \u5305\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u7ec4\u5408\u56fe\u50cf\u8f6c\u6362\u3002\u8fd0\u884c\u5355\u5143\u540e\uff0c\u5b83\u5c06\u663e\u793a\u5b83\u63d0\u53d6\u7684\u793a\u4f8b\u5e27\u3002</p> <pre><code>resize = T.Compose([T.ToPILImage(),\n                    T.Resize(40, interpolation=Image.CUBIC),\n                    T.ToTensor()])\n\ndef get_cart_location(screen_width):\n    world_width = env.x_threshold * 2\n    scale = screen_width / world_width\n    return int(env.state[0] * scale + screen_width / 2.0)  # \u8f66\u5b50\u7684\u4e2d\u5fc3\n\ndef get_screen():\n    # \u8fd4\u56de gym \u9700\u8981\u7684400x600x3 \u56fe\u7247, \u4f46\u6709\u65f6\u4f1a\u66f4\u5927\uff0c\u5982800x1200x3. \u5c06\u5176\u8f6c\u6362\u4e3atorch (CHW).\n    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n    # \u8f66\u5b50\u5728\u4e0b\u534a\u90e8\u5206\uff0c\u56e0\u6b64\u8bf7\u5265\u53bb\u5c4f\u5e55\u7684\u9876\u90e8\u548c\u5e95\u90e8\u3002\n    _, screen_height, screen_width = screen.shape\n    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n    view_width = int(screen_width * 0.6)\n    cart_location = get_cart_location(screen_width)\n    if cart_location &lt; view_width // 2:\n        slice_range = slice(view_width)\n    elif cart_location &gt; (screen_width - view_width // 2):\n        slice_range = slice(-view_width, None)\n    else:\n        slice_range = slice(cart_location - view_width // 2,\n                            cart_location + view_width // 2)\n    # \u53bb\u6389\u8fb9\u7f18\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u4ee5\u8f66\u4e3a\u4e2d\u5fc3\u7684\u6b63\u65b9\u5f62\u56fe\u50cf\u3002\n    screen = screen[:, :, slice_range]\n    # \u8f6c\u5316\u4e3a float, \u91cd\u65b0\u88c1\u526a, \u8f6c\u5316\u4e3a torch \u5f20\u91cf(\u8fd9\u5e76\u4e0d\u9700\u8981\u62f7\u8d1d)\n    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n    screen = torch.from_numpy(screen)\n    # \u91cd\u65b0\u88c1\u526a,\u52a0\u5165\u6279\u7ef4\u5ea6 (BCHW)\n    return resize(screen).unsqueeze(0).to(device)\n\nenv.reset()\nplt.figure()\nplt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n           interpolation='none')\nplt.title('Example extracted screen')\nplt.show()\n\n</code></pre>"},{"location":"1.0/reinforcement_q_learning/#_3","title":"\u8bad\u7ec3","text":""},{"location":"1.0/reinforcement_q_learning/#_4","title":"\u8d85\u53c2\u6570\u548c\u914d\u7f6e","text":"<p>\u6b64\u5355\u5143\u5b9e\u4f8b\u5316\u6a21\u578b\u53ca\u5176\u4f18\u5316\u5668\uff0c\u5e76\u5b9a\u4e49\u4e00\u4e9b\u5b9e\u7528\u7a0b\u5e8f\uff1a</p> <ul> <li><code>select_action</code> - \u5c06\u6839\u636e\u8fed\u4ee3\u6b21\u6570\u8d2a\u5a6a\u7b56\u7565\u9009\u62e9\u4e00\u4e2a\u884c\u52a8\u3002\u7b80\u5355\u5730\u8bf4\uff0c\u6211\u4eec\u6709\u65f6\u4f1a\u4f7f\u7528\u6211\u4eec\u7684\u6a21\u578b\u6765\u9009\u62e9\u52a8\u4f5c\uff0c\u6709\u65f6\u6211\u4eec\u53ea\u4f1a\u5bf9\u5176\u4e2d\u4e00\u4e2a\u8fdb\u884c\u7edf\u4e00\u7684\u91c7\u6837\u3002\u9009\u62e9\u968f\u673a\u52a8\u4f5c\u7684\u6982\u7387\u5c06\u4ece <code>EPS_START</code> \u5f00\u59cb\u5e76\u4ee5\u6307\u6570\u5f62\u5f0f\u5411 <code>EPS_END</code>\u8870\u51cf\u3002 <code>EPS_DECAY</code> \u63a7\u5236\u8870\u51cf\u901f\u7387\u3002</li> <li><code>plot_durations</code> - \u4e00\u4e2a\u5e2e\u52a9\u7ed8\u5236\u8fed\u4ee3\u6b21\u6570\u6301\u7eed\u65f6\u95f4\uff0c\u4ee5\u53ca\u8fc7\u53bb100\u8fed\u4ee3\u6b21\u6570\u7684\u5e73\u5747\u503c(\u5b98\u65b9\u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u5ea6\u91cf\uff09\u3002\u8fed\u4ee3\u6b21\u6570\u5c06\u5728\u5305\u542b\u4e3b\u8bad\u7ec3\u5faa\u73af\u7684\u5355\u5143\u4e0b\u65b9\uff0c\u5e76\u5728\u6bcf\u8fed\u4ee3\u4e4b\u540e\u66f4\u65b0\u3002</li> </ul> <pre><code>BATCH_SIZE = 128\nGAMMA = 0.999\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 200\nTARGET_UPDATE = 10\n\n#\u83b7\u53d6\u5c4f\u5e55\u5927\u5c0f\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u4eceai-gym\u8fd4\u56de\u7684\u5f62\u72b6\u6b63\u786e\u521d\u59cb\u5316\u5c42\u3002\u8fd9\u4e00\u70b9\u4e0a\u7684\u5178\u578b\u5c3a\u5bf8\u63a5\u8fd13x40x90\uff0c\u8fd9\u662f\u5728get_screen(\uff09\u4e2d\u6291\u5236\u548c\u7f29\u5c0f\u7684\u6e32\u67d3\u7f13\u51b2\u533a\u7684\u7ed3\u679c\u3002\ninit_screen = get_screen()\n_, _, screen_height, screen_width = init_screen.shape\n\npolicy_net = DQN(screen_height, screen_width).to(device)\ntarget_net = DQN(screen_height, screen_width).to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\n\noptimizer = optim.RMSprop(policy_net.parameters())\nmemory = ReplayMemory(10000)\n\nsteps_done = 0\n\ndef select_action(state):\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample &gt; eps_threshold:\n        with torch.no_grad():\n            # t.max(1\uff09\u5c06\u4e3a\u6bcf\u884c\u7684\u5217\u8fd4\u56de\u6700\u5927\u503c\u3002max result\u7684\u7b2c\u4e8c\u5217\u662f\u627e\u5230max\u5143\u7d20\u7684\u7d22\u5f15\uff0c\u56e0\u6b64\u6211\u4eec\u9009\u62e9\u9884\u671f\u56de\u62a5\u8f83\u5927\u7684\u64cd\u4f5c\u3002\n            return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n\nepisode_durations = []\n\ndef plot_durations():\n    plt.figure(2)\n    plt.clf()\n    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n    plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # \u5e73\u5747 100 \u6b21\u8fed\u4ee3\u753b\u4e00\u6b21\n    if len(durations_t) &gt;= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # \u6682\u5b9a\u4e00\u4f1a\u7b49\u5f85\u5c4f\u5e55\u66f4\u65b0\n    if is_ipython:\n        display.clear_output(wait=True)\n        display.display(plt.gcf())\n\n</code></pre>"},{"location":"1.0/reinforcement_q_learning/#_5","title":"\u8bad\u7ec3\u5faa\u73af","text":"<p>\u6700\u540e\uff0c\u8bad\u7ec3\u6211\u4eec\u6a21\u578b\u7684\u4ee3\u7801\u3002</p> <p>\u5728\u8fd9\u91cc\uff0c\u60a8\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a<code>optimize_model</code>\u51fd\u6570\uff0c\u5b83\u6267\u884c\u4f18\u5316\u7684\u4e00\u4e2a\u6b65\u9aa4\u3002\u5b83\u9996\u5148\u5bf9\u4e00\u6279\u6570\u636e\u8fdb\u884c\u91c7\u6837\uff0c\u5c06\u6240\u6709\u5f20\u91cf\u8fde\u63a5\u6210\u4e00\u4e2a\u5f20\u91cf\uff0c\u8ba1\u7b97\u51fa\\(\\(Q(s_t, a_t)\\)\\) \u548c \\(\\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\)\\)\uff0c\u5e76\u5c06\u5b83\u4eec\u7ec4\u5408\u6210\u6211\u4eec\u7684\u635f\u5931\u3002\u6839\u636e\u5b9a\u4e49\uff0c\u5982\u679c \\(\\(s\\)\\)\u662f\u7ed3\u675f\u72b6\u6001\uff0c\u6211\u4eec\u8bbe\u7f6e \\(\\(V(s) = 0\\)\\)\u3002\u6211\u4eec\u8fd8\u4f7f\u7528\u76ee\u6807\u7f51\u7edc\u6765\u8ba1\u7b97\\(\\(V(s_{t+1})\\)\\)`\u4ee5\u589e\u52a0\u7a33\u5b9a\u6027\u3002\u76ee\u6807\u7f51\u7edc\u7684\u6743\u91cd\u5927\u90e8\u5206\u65f6\u95f4\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u6bcf\u9694\u4e00\u6bb5\u65f6\u95f4\u5c31\u4f1a\u66f4\u65b0\u4e00\u6b21\u7b56\u7565\u7f51\u7edc\u7684\u6743\u91cd\u3002\u8fd9\u901a\u5e38\u662f\u4e00\u7ec4\u6b65\u9aa4\uff0c\u4f46\u4e3a\u4e86\u7b80\u5355\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u8fed\u4ee3\u6b21\u6570\u3002</p> <pre><code>def optimize_model():\n    if len(memory) &lt; BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # \u8f6c\u7f6e\u6279\u6837\u672c(\u6709\u5173\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605https://stackoverflow.com/a/19343/3343043\uff09\u3002\u8fd9\u4f1a\u5c06\u8f6c\u6362\u7684\u6279\u5904\u7406\u6570\u7ec4\u8f6c\u6362\u4e3a\u6279\u5904\u7406\u6570\u7ec4\u7684\u8f6c\u6362\u3002\n    batch = Transition(*zip(*transitions))\n\n    # \u8ba1\u7b97\u975e\u6700\u7ec8\u72b6\u6001\u7684\u63a9\u7801\u5e76\u8fde\u63a5\u6279\u5904\u7406\u5143\u7d20(\u6700\u7ec8\u72b6\u6001\u5c06\u662f\u6a21\u62df\u7ed3\u675f\u540e\u7684\u72b6\u6001\uff09\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.uint8)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    # \u8ba1\u7b97Q(s_t, a)-\u6a21\u578b\u8ba1\u7b97 Q(s_t)\uff0c\u7136\u540e\u9009\u62e9\u6240\u91c7\u53d6\u884c\u52a8\u7684\u5217\u3002\u8fd9\u4e9b\u662f\u6839\u636e\u7b56\u7565\u7f51\u7edc\u5bf9\u6bcf\u4e2a\u6279\u5904\u7406\u72b6\u6001\u6240\u91c7\u53d6\u7684\u64cd\u4f5c\u3002\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # \u8ba1\u7b97\u4e0b\u4e00\u4e2a\u72b6\u6001\u7684V(s_{t+1})\u3002\u975e\u6700\u7ec8\u72b6\u6001\u4e0b\u4e00\u4e2a\u72b6\u6001\u7684\u9884\u671f\u64cd\u4f5c\u503c\u662f\u57fa\u4e8e\u201c\u65e7\u201d\u76ee\u6807\u7f51\u7edc\u8ba1\u7b97\u7684\uff1b\u9009\u62e9max(1)[0]\u7684\u6700\u4f73\u5956\u52b1\u3002\u8fd9\u662f\u57fa\u4e8e\u63a9\u7801\u5408\u5e76\u7684\uff0c\u8fd9\u6837\u5f53\u72b6\u6001\u4e3a\u6700\u7ec8\u72b6\u6001\u65f6\uff0c\u6211\u4eec\u5c06\u83b7\u5f97\u9884\u671f\u72b6\u6001\u503c\u62160\u3002\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n    # \u8ba1\u7b97\u671f\u671b Q \u503c\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # \u8ba1\u7b97 Huber \u635f\u5931\n    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # \u4f18\u5316\u6a21\u578b\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()\n\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u627e\u5230\u4e3b\u8bad\u7ec3\u5faa\u73af\u3002\u5f00\u59cb\u65f6\uff0c\u6211\u4eec\u91cd\u7f6e\u73af\u5883\u5e76\u521d\u59cb\u5316<code>state</code>\u5f20\u91cf\u3002\u7136\u540e\uff0c\u6211\u4eec\u5bf9\u4e00\u4e2a\u64cd\u4f5c\u8fdb\u884c\u91c7\u6837\uff0c\u6267\u884c\u5b83\uff0c\u89c2\u5bdf\u4e0b\u4e00\u4e2a\u5c4f\u5e55\u548c\u5956\u52b1(\u603b\u662f1\uff09\uff0c\u5e76\u5bf9\u6211\u4eec\u7684\u6a21\u578b\u8fdb\u884c\u4e00\u6b21\u4f18\u5316\u3002\u5f53\u63d2\u66f2\u7ed3\u675f(\u6211\u4eec\u7684\u6a21\u578b\u5931\u8d25\uff09\u65f6\uff0c\u6211\u4eec\u91cd\u65b0\u542f\u52a8\u5faa\u73af\u3002</p> <p><code>num_episodes</code>\u8bbe\u7f6e\u5f97\u5f88\u5c0f\u3002\u4f60\u53ef\u4ee5\u4e0b\u8f7d\u5e76\u8fd0\u884c\u66f4\u591a\u7684epsiodes\uff0c\u6bd4\u5982300+\u6765\u8fdb\u884c\u6709\u610f\u4e49\u7684\u6301\u7eed\u65f6\u95f4\u6539\u8fdb\u3002</p> <pre><code>num_episodes = 50\nfor i_episode in range(num_episodes):\n    # \u521d\u59cb\u5316\u73af\u5883\u548c\u72b6\u6001\n    env.reset()\n    last_screen = get_screen()\n    current_screen = get_screen()\n    state = current_screen - last_screen\n    for t in count():\n        # \u9009\u62e9\u5e76\u6267\u884c\u52a8\u4f5c\n        action = select_action(state)\n        _, reward, done, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n\n        # \u89c2\u5bdf\u65b0\u72b6\u6001\n        last_screen = current_screen\n        current_screen = get_screen()\n        if not done:\n            next_state = current_screen - last_screen\n        else:\n            next_state = None\n\n        # \u5728\u5185\u5b58\u4e2d\u50a8\u5b58\u5f53\u524d\u53c2\u6570\n        memory.push(state, action, next_state, reward)\n\n        # \u8fdb\u5165\u4e0b\u4e00\u72b6\u6001\n        state = next_state\n\n        # \u8bb0\u6027\u4e00\u6b65\u4f18\u5316 (\u5728\u76ee\u6807\u7f51\u7edc)\n        optimize_model()\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n    #\u66f4\u65b0\u76ee\u6807\u7f51\u7edc, \u590d\u5236\u5728 DQN \u4e2d\u7684\u6240\u6709\u6743\u91cd\u504f\u5dee\n    if i_episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())\n\nprint('Complete')\nenv.render()\nenv.close()\nplt.ioff()\nplt.show()\n\n</code></pre> <p>\u4e0b\u9762\u662f\u4e00\u4e2a\u56fe\u8868\uff0c\u5b83\u8bf4\u660e\u4e86\u6574\u4e2a\u7ed3\u679c\u6570\u636e\u6d41\u3002</p> <p></p> <p>\u52a8\u4f5c\u53ef\u4ee5\u662f\u968f\u673a\u9009\u62e9\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u57fa\u4e8e\u4e00\u4e2a\u7b56\u7565\uff0c\u4ecegym\u73af\u5883\u4e2d\u83b7\u53d6\u4e0b\u4e00\u6b65\u7684\u6837\u672c\u3002\u6211\u4eec\u5c06\u7ed3\u679c\u8bb0\u5f55\u5728\u56de\u653e\u5185\u5b58\u4e2d\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u8fd0\u884c\u4f18\u5316\u6b65\u9aa4\u3002\u4f18\u5316\u4ece\u91cd\u653e\u5185\u5b58\u4e2d\u968f\u673a\u62bd\u53d6\u4e00\u6279\u6765\u8bad\u7ec3\u65b0\u7b56\u7565\u3002\u201c\u65e7\u7684\u201d\u76ee\u6807\u7f51\u4e5f\u7528\u4e8e\u4f18\u5316\u8ba1\u7b97\u9884\u671f\u7684Q\u503c\uff1b\u5b83\u5076\u5c14\u4f1a\u66f4\u65b0\u4ee5\u4fdd\u6301\u5176\u6700\u65b0\u3002</p>"},{"location":"1.0/saving_loading_models/","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b","text":"<p>\u8bd1\u8005 bruce1408</p> <p>\u4f5c\u8005: Matthew Inkawhich</p> <p>\u672c\u6587\u63d0\u4f9b\u6709\u5173Pytorch\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d\u7684\u5404\u79cd\u7528\u4f8b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u60a8\u53ef\u4ee5\u968f\u610f\u9605\u8bfb\u6574\u4e2a\u6587\u6863\uff0c\u6216\u8005\u53ea\u662f\u8df3\u8f6c\u5230\u6240\u9700\u7528\u4f8b\u7684\u4ee3\u7801\u90e8\u5206\u3002</p> <p>\u5f53\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\u65f6\uff0c\u6709\u4e09\u4e2a\u6838\u5fc3\u529f\u80fd\u9700\u8981\u719f\u6089\uff1a</p> <ol> <li>torch.save: \u5c06\u5e8f\u5217\u5316\u5bf9\u8c61\u4fdd\u5b58\u5230\u78c1\u76d8\u3002 \u6b64\u51fd\u6570\u4f7f\u7528 Python \u7684pickle\u6a21\u5757\u8fdb\u884c\u5e8f\u5217\u5316\u3002\u4f7f\u7528\u6b64\u51fd\u6570\u53ef\u4ee5\u4fdd\u5b58\u5982\u6a21\u578b\u3001tensor\u3001\u5b57\u5178\u7b49\u5404\u79cd\u5bf9\u8c61\u3002</li> <li>torch.load: \u4f7f\u7528 pickle\u7684 unpickling \u529f\u80fd\u5c06pickle\u5bf9\u8c61\u6587\u4ef6\u53cd\u5e8f\u5217\u5316\u5230\u5185\u5b58\u3002 \u6b64\u529f\u80fd\u8fd8\u53ef\u4ee5\u6709\u52a9\u4e8e\u8bbe\u5907\u52a0\u8f7d\u6570\u636e(\u8be6\u89c1 Saving &amp; Loading Model Across Devices).</li> <li>torch.nn.Module.load_state_dict: \u4f7f\u7528\u53cd\u5e8f\u5217\u5316\u51fd\u6570 state_dict \u6765\u52a0\u8f7d\u6a21\u578b\u7684\u53c2\u6570\u5b57\u5178\u3002\u66f4\u591a\u6709\u5173 state_dict \u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003What is a state_dict?.</li> </ol> <p>\u5185\u5bb9:</p> <ul> <li>\u4ec0\u4e48\u662f<code>\u72b6\u6001\u5b57\u5178</code>?</li> <li>\u4fdd\u5b58\u548c\u52a0\u8f7d\u63a8\u65ad\u6a21\u578b</li> <li>\u4fdd\u5b58 \u548c \u52a0\u8f7d Checkpoint</li> <li>\u5728\u4e00\u4e2a\u6587\u4ef6\u4e2d\u4fdd\u5b58\u591a\u4e2a\u6a21\u578b</li> <li>\u4f7f\u7528\u5728\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u4e0b\u7684\u70ed\u542f\u52a8\u6a21\u5f0f</li> <li>Saving &amp; Loading Model Across Devices</li> </ul>"},{"location":"1.0/saving_loading_models/#_2","title":"\u4ec0\u4e48\u662f <code>\u72b6\u6001\u5b57\u5178</code>?","text":"<p>\u5728Pytorch\u4e2d\uff0c<code>torch.nn.Module</code> \u6a21\u578b\u7684\u53ef\u5b66\u4e60\u53c2\u6570(\u5373\u6743\u91cd\u548c\u504f\u5dee)\u5305\u542b\u5728\u6a21\u578b\u7684 parameters \u4e2d\uff0c(\u4f7f\u7528<code>model.parameters()</code>\u53ef\u4ee5\u8fdb\u884c\u8bbf\u95ee)\u3002 state_dict \u4ec5\u4ec5\u662fpython\u5b57\u5178\u5bf9\u8c61\uff0c\u5b83\u5c06\u6bcf\u4e00\u5c42\u6620\u5c04\u5230\u5176\u53c2\u6570\u5f20\u91cf\u3002\u6ce8\u610f\uff0c\u53ea\u6709\u5177\u6709\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u5c42(\u5982\u5377\u79ef\u5c42\u3001\u7ebf\u6027\u5c42\u7b49)\u7684\u6a21\u578b\u624d\u5177\u6709 state_dict \u8fd9\u4e00\u9879\u3002\u4f18\u5316\u76ee\u6807 <code>torch.optim</code> \u4e5f\u6709 state_dict \u5c5e\u6027\uff0c\u5b83\u5305\u542b\u6709\u5173\u4f18\u5316\u5668\u7684\u72b6\u6001\u4fe1\u606f\uff0c\u4ee5\u53ca\u4f7f\u7528\u7684\u8d85\u53c2\u6570\u3002</p> <p>\u56e0\u4e3a state_dict \u7684\u5bf9\u8c61\u662fpython\u5b57\u5178\uff0c\u6240\u4ee5\u4ed6\u4eec\u53ef\u4ee5\u5f88\u5bb9\u6613\u7684\u4fdd\u5b58\u3001\u66f4\u65b0\u3001\u66f4\u6539\u548c\u6062\u590d\uff0c\u4e3aPytorch\u6a21\u578b\u548c\u4f18\u5316\u5668\u6dfb\u52a0\u4e86\u5927\u91cf\u6a21\u5757\u3002</p>"},{"location":"1.0/saving_loading_models/#_3","title":"\u793a\u4f8b:","text":"<p>\u8ba9\u6211\u4eec\u4ece \u7b80\u5355\u6a21\u578b\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u4e2d\u4e86\u89e3\u4e00\u4e0b state_dict \u7684\u4f7f\u7528\u3002</p> <pre><code># Define model\nclass TheModelClass(nn.Module):\n    def __init__(self):\n        super(TheModelClass, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Initialize model\nmodel = TheModelClass()\n\n# Initialize optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n# Print optimizer's state_dict\nprint(\"Optimizer's state_dict:\")\nfor var_name in optimizer.state_dict():\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Model's state_dict:\nconv1.weight     torch.Size([6, 3, 5, 5])\nconv1.bias   torch.Size([6])\nconv2.weight     torch.Size([16, 6, 5, 5])\nconv2.bias   torch.Size([16])\nfc1.weight   torch.Size([120, 400])\nfc1.bias     torch.Size([120])\nfc2.weight   torch.Size([84, 120])\nfc2.bias     torch.Size([84])\nfc3.weight   torch.Size([10, 84])\nfc3.bias     torch.Size([10])\n\nOptimizer's state_dict:\nstate    {}\nparam_groups     [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}]\n\n</code></pre>"},{"location":"1.0/saving_loading_models/#_4","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u63a8\u65ad\u6a21\u578b","text":""},{"location":"1.0/saving_loading_models/#state_dict","title":"\u4fdd\u5b58/\u52a0\u8f7d <code>state_dict</code> (\u63a8\u8350\u4f7f\u7528)","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model.state_dict(), PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code>model = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\n\n</code></pre> <p>\u5f53\u4fdd\u5b58\u597d\u6a21\u578b\u7528\u6765\u63a8\u65ad\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u4fdd\u5b58\u6a21\u578b\u5b66\u4e60\u5230\u7684\u53c2\u6570\uff0c\u4f7f\u7528 <code>torch.save()</code> \u51fd\u6570\u6765\u4fdd\u5b58\u6a21\u578b state_dict ,\u5b83\u4f1a\u7ed9\u6a21\u578b\u6062\u590d\u63d0\u4f9b\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u8981\u63a8\u8350\u5b83\u6765\u4fdd\u5b58\u7684\u539f\u56e0\u3002</p> <p>\u5728 Pytorch \u4e2d\u6700\u5e38\u89c1\u7684\u6a21\u578b\u4fdd\u5b58\u4f7f\u7528 '.pt' \u6216\u8005\u662f '.pth' \u4f5c\u4e3a\u6a21\u578b\u6587\u4ef6\u6269\u5c55\u540d\u3002</p> <p>\u8bf7\u8bb0\u4f4f\uff0c\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u52a1\u5fc5\u8c03\u7528 <code>model.eval()</code> \u53bb\u8bbe\u7f6e dropout \u548c batch normalization \u5c42\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u3002\u5982\u679c\u4e0d\u8fd9\u4e48\u505a\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u63a8\u65ad\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002</p> <p>\u6ce8\u610f</p> <p>\u8bf7\u6ce8\u610f <code>load_state_dict()</code> \u51fd\u6570\u53ea\u63a5\u53d7\u5b57\u5178\u5bf9\u8c61\uff0c\u800c\u4e0d\u662f\u4fdd\u5b58\u5bf9\u8c61\u7684\u8def\u5f84\u3002\u8fd9\u5c31\u610f\u5473\u7740\u5728\u4f60\u4f20\u7ed9 <code>load_state_dict()</code> \u51fd\u6570\u4e4b\u524d\uff0c\u4f60\u5fc5\u987b\u53cd\u5e8f\u5217\u5316\u4f60\u4fdd\u5b58\u7684 state_dict\u3002\u4f8b\u5982\uff0c\u4f60\u65e0\u6cd5\u901a\u8fc7 <code>model.load_state_dict(PATH)</code>\u6765\u52a0\u8f7d\u6a21\u578b\u3002</p>"},{"location":"1.0/saving_loading_models/#_5","title":"\u4fdd\u5b58/\u52a0\u8f7d\u5b8c\u6574\u6a21\u578b","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model, PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code># Model class must be defined somewhere\nmodel = torch.load(PATH)\nmodel.eval()\n\n</code></pre> <p>\u6b64\u90e8\u5206\u4fdd\u5b58/\u52a0\u8f7d\u8fc7\u7a0b\u4f7f\u7528\u6700\u76f4\u89c2\u7684\u8bed\u6cd5\u5e76\u6d89\u53ca\u6700\u5c11\u91cf\u7684\u4ee3\u7801\u3002\u4ee5Pythonpickle\u6a21\u5757\u7684\u65b9\u5f0f\u6765\u4fdd\u5b58\u6a21\u578b\u3002\u8fd9\u79cd\u65b9\u6cd5\u7684\u7f3a\u70b9\u662f\u5e8f\u5217\u5316\u6570\u636e\u53d7\u9650\u4e8e\u67d0\u79cd\u7279\u6b8a\u7684\u7c7b\u800c\u4e14\u9700\u8981\u786e\u5207\u7684\u5b57\u5178\u7ed3\u6784\u3002\u8fd9\u662f\u56e0\u4e3apickle\u65e0\u6cd5\u4fdd\u5b58\u6a21\u578b\u7c7b\u672c\u8eab\u3002\u76f8\u53cd\uff0c\u5b83\u4fdd\u5b58\u5305\u542b\u7c7b\u7684\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8be5\u6587\u4ef6\u5728\u52a0\u8f7d\u65f6\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u5f53\u5728\u5176\u4ed6\u9879\u76ee\u4f7f\u7528\u6216\u8005\u91cd\u6784\u4e4b\u540e\uff0c\u60a8\u7684\u4ee3\u7801\u53ef\u80fd\u4f1a\u4ee5\u5404\u79cd\u65b9\u5f0f\u4e2d\u65ad\u3002</p> <p>\u5728 Pytorch \u4e2d\u6700\u5e38\u89c1\u7684\u6a21\u578b\u4fdd\u5b58\u4f7f\u7528 '.pt' \u6216\u8005\u662f '.pth' \u4f5c\u4e3a\u6a21\u578b\u6587\u4ef6\u6269\u5c55\u540d\u3002</p> <p>\u8bf7\u8bb0\u4f4f\uff0c\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u52a1\u5fc5\u8c03\u7528 <code>model.eval()</code> \u53bb\u8bbe\u7f6e dropout \u548c batch normalization \u5c42\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u3002\u5982\u679c\u4e0d\u8fd9\u4e48\u505a\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u63a8\u65ad\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002</p>"},{"location":"1.0/saving_loading_models/#checkpoint","title":"\u4fdd\u5b58 \u548c \u52a0\u8f7d Checkpoint \u7528\u4e8e\u63a8\u7406/\u7ee7\u7eed\u8bad\u7ec3","text":""},{"location":"1.0/saving_loading_models/#_6","title":"\u4fdd\u5b58:","text":"<pre><code>torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            ...\n            }, PATH)\n\n</code></pre>"},{"location":"1.0/saving_loading_models/#_7","title":"\u52a0\u8f7d:","text":"<pre><code>model = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.eval()\n# - or -\nmodel.train()\n\n</code></pre> <p>\u5f53\u4fdd\u5b58\u6210 checkpoint \u7684\u65f6\u5019\uff0c\u53ef\u7528\u4e8e\u63a8\u7406\u6216\u8005\u662f\u6062\u590d\u8bad\u7ec3\uff0c\u60a8\u4fdd\u5b58\u7684\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u7684 state_dict \u3002 \u4fdd\u5b58\u4f18\u5316\u5668\u7684 state_dict \u4e5f\u5f88\u91cd\u8981, \u56e0\u4e3a\u5b83\u5305\u542b\u4f5c\u4e3a\u6a21\u578b\u8bad\u7ec3\u66f4\u65b0\u7684\u7f13\u51b2\u533a\u548c\u53c2\u6570\u3002\u4f60\u4e5f\u8bb8\u60f3\u4fdd\u5b58\u5176\u4ed6\u9879\u76ee\uff0c\u6bd4\u5982\u6700\u65b0\u8bb0\u5f55\u7684\u8bad\u7ec3\u635f\u5931\uff0c\u5916\u90e8\u7684 <code>torch.nn.Embedding</code> \u5c42\u7b49\u7b49\u3002</p> <p>\u8981\u4fdd\u5b58\u591a\u4e2a\u7ec4\u4ef6\uff0c\u8bf7\u5728\u5b57\u5178\u4e2d\u7ec4\u7ec7\u5b83\u4eec\u5e76\u4f7f\u7528 <code>torch.save()</code> \u6765\u5e8f\u5217\u5316\u5b57\u5178\u3002 Pytorch \u4e2d\u5e38\u89c1\u7684\u4fdd\u5b58checkpoint \u662f\u4f7f\u7528 <code>.tar</code> \u6587\u4ef6\u6269\u5c55\u540d\u3002</p> <p>\u8981\u52a0\u8f7d\u9879\u76ee\uff0c\u9996\u5148\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668\uff0c\u7136\u540e\u4f7f\u7528 <code>torch.load()</code> \u6765\u52a0\u8f7d\u672c\u5730\u5b57\u5178\u3002 \u8fd9\u91cc\uff0c\u60a8\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u901a\u8fc7\u7b80\u5355\u67e5\u8be2\u5b57\u5178\u6765\u8bbf\u95ee\u60a8\u6240\u4fdd\u5b58\u7684\u9879\u76ee\u3002</p> <p>\u8bf7\u8bb0\u4f4f\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u52a1\u5fc5\u8c03\u7528 <code>model.eval()</code> \u53bb\u8bbe\u7f6e dropout \u548c batch normalization \u4e3a\u8bc4\u4f30\u3002\u5982\u679c\u4e0d\u8fd9\u6837\u505a\uff0c\u6709\u53ef\u80fd\u5f97\u5230\u4e0d\u4e00\u81f4\u7684\u63a8\u65ad\u7ed3\u679c\u3002\u5982\u679c\u4f60\u60f3\u8981\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u8c03\u7528 <code>model.train()</code> \u4ee5\u786e\u4fdd\u8fd9\u4e9b\u5c42\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\u3002</p>"},{"location":"1.0/saving_loading_models/#_8","title":"\u5728\u4e00\u4e2a\u6587\u4ef6\u4e2d\u4fdd\u5b58\u591a\u4e2a\u6a21\u578b","text":""},{"location":"1.0/saving_loading_models/#_9","title":"\u4fdd\u5b58:","text":"<pre><code>torch.save({\n            'modelA_state_dict': modelA.state_dict(),\n            'modelB_state_dict': modelB.state_dict(),\n            'optimizerA_state_dict': optimizerA.state_dict(),\n            'optimizerB_state_dict': optimizerB.state_dict(),\n            ...\n            }, PATH)\n\n</code></pre>"},{"location":"1.0/saving_loading_models/#_10","title":"\u52a0\u8f7d:","text":"<pre><code>modelA = TheModelAClass(*args, **kwargs)\nmodelB = TheModelBClass(*args, **kwargs)\noptimizerA = TheOptimizerAClass(*args, **kwargs)\noptimizerB = TheOptimizerBClass(*args, **kwargs)\n\ncheckpoint = torch.load(PATH)\nmodelA.load_state_dict(checkpoint['modelA_state_dict'])\nmodelB.load_state_dict(checkpoint['modelB_state_dict'])\noptimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\noptimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n\nmodelA.eval()\nmodelB.eval()\n# - or -\nmodelA.train()\nmodelB.train()\n\n</code></pre> <p>\u5f53\u4fdd\u5b58\u4e00\u4e2a\u6a21\u578b\u7531\u591a\u4e2a <code>torch.nn.Modules</code>\u7ec4\u6210\u65f6\uff0c\u4f8b\u5982GAN(\u5bf9\u6297\u751f\u6210\u7f51\u7edc), sequence-to-sequence (\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b), \u6216\u8005\u662f\u591a\u4e2a\u6a21\u578b\u878d\u5408, \u60a8\u53ef\u4ee5\u91c7\u7528\u4e0e\u4fdd\u5b58\u5e38\u89c4\u68c0\u67e5\u70b9\u76f8\u540c\u7684\u65b9\u6cd5\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u4fdd\u5b58\u6bcf\u4e2a\u6a21\u578b\u7684 state_dict \u7684\u5b57\u5178\u548c\u76f8\u5bf9\u5e94\u7684\u4f18\u5316\u5668\u3002\u5982\u524d\u6240\u8ff0\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u5730\u5c06\u5b83\u4eec\u9644\u52a0\u5230\u5b57\u5178\u7684\u65b9\u5f0f\u6765\u4fdd\u5b58\u4efb\u4f55\u5176\u4ed6\u9879\u76ee\uff0c\u8fd9\u6837\u6709\u52a9\u4e8e\u60a8\u6062\u590d\u8bad\u7ec3\u3002</p> <p>Pytorch \u4e2d\u5e38\u89c1\u7684\u4fdd\u5b58checkpoint \u662f\u4f7f\u7528 <code>.tar</code> \u6587\u4ef6\u6269\u5c55\u540d\u3002</p> <p>\u8981\u52a0\u8f7d\u9879\u76ee\uff0c\u9996\u5148\u9700\u8981\u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668\uff0c\u7136\u540e\u4f7f\u7528 <code>torch.load()</code> \u6765\u52a0\u8f7d\u672c\u5730\u5b57\u5178\u3002 \u8fd9\u91cc\uff0c\u60a8\u53ef\u4ee5\u975e\u5e38\u5bb9\u6613\u7684\u901a\u8fc7\u7b80\u5355\u67e5\u8be2\u5b57\u5178\u6765\u8bbf\u95ee\u60a8\u6240\u4fdd\u5b58\u7684\u9879\u76ee\u3002</p> <p>\u8bf7\u8bb0\u4f4f\u5728\u8fd0\u884c\u63a8\u7406\u4e4b\u524d\uff0c\u52a1\u5fc5\u8c03\u7528 <code>model.eval()</code> \u53bb\u8bbe\u7f6e dropout \u548c batch normalization \u4e3a\u8bc4\u4f30\u3002\u5982\u679c\u4e0d\u8fd9\u6837\u505a\uff0c\u6709\u53ef\u80fd\u5f97\u5230\u4e0d\u4e00\u81f4\u7684\u63a8\u65ad\u7ed3\u679c\u3002\u5982\u679c\u4f60\u60f3\u8981\u6062\u590d\u8bad\u7ec3\uff0c\u8bf7\u8c03\u7528 <code>model.train()</code> \u4ee5\u786e\u4fdd\u8fd9\u4e9b\u5c42\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\u3002</p>"},{"location":"1.0/saving_loading_models/#_11","title":"\u4f7f\u7528\u5728\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u4e0b\u7684\u70ed\u542f\u52a8\u6a21\u5f0f","text":""},{"location":"1.0/saving_loading_models/#_12","title":"\u4fdd\u5b58:","text":"<pre><code>torch.save(modelA.state_dict(), PATH)\n\n</code></pre>"},{"location":"1.0/saving_loading_models/#_13","title":"\u52a0\u8f7d:","text":"<pre><code>modelB = TheModelBClass(*args, **kwargs)\nmodelB.load_state_dict(torch.load(PATH), strict=False)\n\n</code></pre> <p>\u5728\u8fc1\u79fb\u5b66\u4e60\u6216\u8bad\u7ec3\u65b0\u7684\u590d\u6742\u6a21\u578b\u65f6\uff0c \u90e8\u5206\u52a0\u8f7d\u6a21\u578b\u6216\u52a0\u8f7d\u90e8\u5206\u6a21\u578b\u662f\u5e38\u89c1\u7684\u60c5\u51b5\u3002\u5229\u7528\u8bad\u7ec3\u597d\u7684\u53c2\u6570\uff0c\u6709\u52a9\u4e8e\u70ed\u542f\u52a8\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5e0c\u671b\u5e2e\u52a9\u60a8\u7684\u6a21\u578b\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u66f4\u5feb\u5730\u6536\u655b</p> <p>\u65e0\u8bba\u662f\u4ece\u7f3a\u5c11\u67d0\u4e9b\u952e\u7684 state_dict \u52a0\u8f7d\u8fd8\u662f\u4ece\u952e\u6570\u591a\u4e8e\u52a0\u8f7d\u6a21\u578b\u7684 state_dict , \u60a8\u53ef\u4ee5\u901a\u8fc7\u5728<code>load_state_dict()</code>\u51fd\u6570\u4e2d\u5c06<code>strict</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a False \u6765\u5ffd\u7565\u975e\u5339\u914d\u952e\u7684\u51fd\u6570\u3002</p> <p>\u5982\u679c\u8981\u5c06\u53c2\u6570\u4ece\u4e00\u4e2a\u5c42\u52a0\u8f7d\u5230\u53e6\u4e00\u4e2a\u5c42\uff0c\u4f46\u662f\u67d0\u4e9b\u952e\u4e0d\u5339\u914d\uff0c\u4e3b\u8981\u4fee\u6539\u6b63\u5728\u52a0\u8f7d\u7684 state_dict \u4e2d\u7684\u53c2\u6570\u952e\u7684\u540d\u79f0\u4ee5\u5339\u914d\u8981\u5728\u52a0\u8f7d\u5230\u6a21\u578b\u4e2d\u7684\u952e\u5373\u53ef\u3002</p>"},{"location":"1.0/saving_loading_models/#_14","title":"\u901a\u8fc7\u8bbe\u5907\u4fdd\u5b58/\u52a0\u8f7d\u6a21\u578b","text":""},{"location":"1.0/saving_loading_models/#gpu-cpu","title":"\u4fdd\u5b58\u5230 GPU, \u52a0\u8f7d\u5230 CPU","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model.state_dict(), PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code>device = torch.device('cpu')\nmodel = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load(PATH, map_location=device))\n\n</code></pre> <p>\u5f53\u4eceCPU\u4e0a\u52a0\u8f7d\u6a21\u578b\u5728GPU\u4e0a\u8bad\u7ec3\u65f6, \u5c06 <code>torch.device('cpu')</code> \u4f20\u9012\u7ed9 <code>torch.load()</code> \u51fd\u6570\u4e2d\u7684 <code>map_location</code>\u53c2\u6570.\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528<code>map_location</code> \u53c2\u6570\u5c06\u5f20\u91cf\u4e0b\u7684\u5b58\u50a8\u5668\u52a8\u6001\u7684\u91cd\u65b0\u6620\u5c04\u5230CPU\u8bbe\u5907\u3002</p>"},{"location":"1.0/saving_loading_models/#gpu-gpu","title":"\u4fdd\u5b58\u5230 GPU, \u52a0\u8f7d\u5230 GPU","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model.state_dict(), PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code>device = torch.device(\"cuda\")\nmodel = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load(PATH))\nmodel.to(device)\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n\n</code></pre> <p>\u5f53\u5728GPU\u4e0a\u8bad\u7ec3\u5e76\u628a\u6a21\u578b\u4fdd\u5b58\u5728GPU\uff0c\u53ea\u9700\u8981\u4f7f\u7528 <code>model.to(torch.device('cuda'))</code>\uff0c\u5c06\u521d\u59cb\u5316\u7684 <code>model</code> \u8f6c\u6362\u4e3aCUDA\u4f18\u5316\u6a21\u578b\u3002\u53e6\u5916\uff0c\u8bf7\u52a1\u5fc5\u5728\u6240\u6709\u6a21\u578b\u8f93\u5165\u4e0a\u4f7f\u7528 <code>.to(torch.device('cuda'))</code> \u51fd\u6570\u6765\u4e3a\u6a21\u578b\u51c6\u5907\u6570\u636e\u3002\u8bf7\u6ce8\u610f\uff0c\u8c03\u7528 <code>my_tensor.to(device)</code> \u4f1a\u5728GPU\u4e0a\u8fd4\u56de<code>my_tensor</code> \u7684\u526f\u672c\u3002\u56e0\u6b64\uff0c\u8bf7\u8bb0\u4f4f\u624b\u52a8\u8986\u76d6\u5f20\u91cf\uff1a<code>my_tensor= my_tensor.to(torch.device('cuda'))</code>\u3002</p>"},{"location":"1.0/saving_loading_models/#cpu-gpu","title":"\u4fdd\u5b58\u5230 CPU, \u52a0\u8f7d\u5230 GPU","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model.state_dict(), PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code>device = torch.device(\"cuda\")\nmodel = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\nmodel.to(device)\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\n\n</code></pre> <p>\u5728CPU\u4e0a\u8bad\u7ec3\u597d\u5e76\u4fdd\u5b58\u7684\u6a21\u578b\u52a0\u8f7d\u5230GPU\u65f6\uff0c \u5c06<code>torch.load()</code> \u51fd\u6570\u4e2d\u7684 <code>map_location</code> \u53c2\u6570\u8bbe\u7f6e\u4e3a cuda:device_id\u3002\u8fd9\u4f1a\u5c06\u6a21\u578b\u52a0\u8f7d\u5230\u6307\u5b9a\u7684GPU\u8bbe\u5907\u3002\u63a5\u4e0b\u6765\uff0c\u8bf7\u52a1\u5fc5\u8c03\u7528 <code>model.to(torch.device('cuda'))</code> \u5c06\u6a21\u578b\u7684\u53c2\u6570\u5f20\u91cf\u8f6c\u6362\u4e3a CUDA \u5f20\u91cf\u3002\u6700\u540e\uff0c\u786e\u4fdd\u5728\u6240\u6709\u6a21\u578b\u8f93\u5165\u4e0a\u4f7f\u7528 <code>.to(torch.device('cuda'))</code> \u51fd\u6570\u6765\u4e3aCUDA\u4f18\u5316\u6a21\u578b\u3002\u8bf7\u6ce8\u610f\uff0c \u8c03\u7528 <code>my_tensor.to(device)</code> \u4f1a\u5728GPU\u4e0a\u8fd4\u56de <code>my_tensor</code> \u7684\u65b0\u526f\u672c\u3002 \u5b83\u4e0d\u4f1a\u8986\u76d6 <code>my_tensor</code>\u3002\u56e0\u6b64\uff0c \u8bf7\u624b\u52a8\u8986\u76d6\u5f20\u91cf <code>my_tensor = my_tensor.to(torch.device('cuda'))</code>\u3002</p>"},{"location":"1.0/saving_loading_models/#torchnndataparallel","title":"\u4fdd\u5b58 <code>torch.nn.DataParallel</code> \u6a21\u578b","text":"<p>\u4fdd\u5b58:</p> <pre><code>torch.save(model.module.state_dict(), PATH)\n\n</code></pre> <p>\u52a0\u8f7d:</p> <pre><code># Load to whatever device you want\n\n</code></pre> <p><code>torch.nn.DataParallel</code> \u662f\u4e00\u4e2a\u6a21\u578b\u5c01\u88c5\uff0c\u652f\u6301\u5e76\u884cGPU\u4f7f\u7528\u3002\u8981\u4e00\u822c\u6027\u7684\u4fdd\u5b58 <code>DataParallel</code> \u6a21\u578b, \u8bf7\u4fdd\u5b58 <code>model.module.state_dict()</code>\u3002\u8fd9\u6837\uff0c\u60a8\u5c31\u53ef\u4ee5\u975e\u5e38\u7075\u6d3b\u5730\u4ee5\u4efb\u4f55\u65b9\u5f0f\u52a0\u8f7d\u6a21\u578b\u5230\u60a8\u60f3\u8981\u7684\u8bbe\u5907\u4e2d\u3002</p>"},{"location":"1.0/seq2seq_translation_tutorial/","title":"\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684 seq2seq \u795e\u7ecf\u7f51\u7edc\u7ffb\u8bd1","text":"<p>\u8bd1\u8005\uff1amengfu188</p> <p>\u6821\u5bf9\u8005\uff1aFontTian</p> <p>\u4f5c\u8005: Sean Robertson</p> <p>\u5728\u8fd9\u4e2a\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u5c06\u7f16\u5199\u4e00\u4e2a\u628a\u6cd5\u8bed\u7ffb\u8bd1\u6210\u82f1\u8bed\u7684\u795e\u7ecf\u7f51\u7edc\u3002</p> <pre><code>[KEY: &gt; input, = target, &lt; output]\n\n&gt; il est en train de peindre un tableau .\n= he is painting a picture .\n&lt; he is painting a picture .\n\n&gt; pourquoi ne pas essayer ce vin delicieux ?\n= why not try that delicious wine ?\n&lt; why not try that delicious wine ?\n\n&gt; elle n est pas poete mais romanciere .\n= she is not a poet but a novelist .\n&lt; she not not a poet but a novelist .\n\n&gt; vous etes trop maigre .\n= you re too skinny .\n&lt; you re all alone .\n\n</code></pre> <p>\u2026 \u53d6\u5f97\u4e86\u4e0d\u540c\u7a0b\u5ea6\u7684\u6210\u529f</p> <p>\u8fd9\u662f\u901a\u8fc7seq2seq\u7f51\u7edc\u6765\u8fdb\u884c\u5b9e\u73b0\u7684\uff0c\u5728\u8fd9\u4e2a\u7f51\u7edc\u4e2d\u4f7f\u7528\u4e24\u4e2a\u9012\u5f52\u7684\u795e\u7ecf\u7f51\u7edc(\u7f16\u7801\u5668\u7f51\u7edc\u548c\u89e3\u7801\u5668\u7f51\u7edc\uff09\u4e00\u8d77\u5de5\u4f5c\u4f7f\u5f97\u4e00\u6bb5\u5e8f\u5217\u53d8\u6210\u53e6\u4e00\u6bb5\u5e8f\u5217\u3002 \u7f16\u7801\u5668\u7f51\u7edc\u5c06\u8f93\u5165\u5e8f\u5217\u53d8\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u89e3\u7801\u5668\u7f51\u7edc\u5c06\u8be5\u5411\u91cf\u5c55\u5f00\u4e3a\u65b0\u7684\u5e8f\u5217\u3002</p> <p></p> <p>\u6211\u4eec\u5c06\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u8fd9\u4e2a\u6a21\u578b\uff0c\u5b83\u53ef\u4ee5\u8ba9\u89e3\u7801\u5668\u5b66\u4f1a\u96c6\u4e2d\u5728\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u8303\u56f4\u4e2d\u3002</p> <p>\u63a8\u8350\u9605\u8bfb\uff1a</p> <p>\u6211\u5047\u8bbe\u4f60\u81f3\u5c11\u5df2\u7ecf\u4e86\u89e3Python\uff0c\u5b89\u88c5\u4e86PyTorch\uff0c\u5e76\u4e14\u4e86\u89e3\u4ec0\u4e48\u662f\u5f20\u91cf\uff1a</p> <ul> <li>https://pytorch.org/ PyTorch\u5b89\u88c5\u8bf4\u660e</li> <li>PyTorch \u6df1\u5ea6\u5b66\u4e60: 60 \u5206\u949f\u6781\u901f\u5165\u95e8\u6559\u7a0b \u5f00\u59cb\u4f7f\u7528PyTorch</li> <li>\u8ddf\u7740\u4f8b\u5b50\u5b66\u4e60PyTorch \u66f4\u52a0\u5e7f\u6cdb\u800c\u6df1\u5165\u7684\u4e86\u89e3PyTorch</li> <li>PyTorch for Former Torch Users \u5982\u679c\u4f60\u662fLua Torch\u7528\u6237</li> </ul> <p>\u8fd9\u4e9b\u5185\u5bb9\u6709\u5229\u4e8e\u4e86\u89e3seq2seq\u7f51\u7edc\u53ca\u5176\u5de5\u4f5c\u673a\u5236\uff1a</p> <ul> <li>\u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a</li> <li>\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60</li> <li>\u901a\u8fc7\u5171\u540c\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1</li> <li>\u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b</li> </ul> <p>\u4f60\u8fd8\u53ef\u4ee5\u627e\u5230\u4ee5\u524d\u7c7b\u4f3c\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6559\u7a0b\uff0c\u5982\u7528\u5b57\u7b26\u96c6RNN\u5206\u7c7b\u540d\u79f0 \u548c [\u7528\u5b57\u7b26\u96c6RNN\u751f\u6210\u540d\u79f0]](char_rnn_generation_tutorial.html)\uff0c\u5b66\u4e60\u8fd9\u4e9b\u6982\u5ff5\u6bd4\u8f83\u6709\u5e2e\u52a9\u3002</p> <p>\u66f4\u591a\u5185\u5bb9\u8bf7\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a</p> <ul> <li>\u7528RNN\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6765\u5b66\u4e60\u7528\u4e8e\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1\u7684\u77ed\u8bed\u8868\u793a</li> <li>\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cseq2seq\u5b66\u4e60</li> <li>\u901a\u8fc7\u5171\u540c\u5b66\u4e60\u5bf9\u9f50\u548c\u7ffb\u8bd1\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1</li> <li>\u795e\u7ecf\u4f1a\u8bdd\u6a21\u578b</li> </ul> <p>\u9700\u8981\u5982\u4e0b\uff1a</p> <pre><code>from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_1","title":"\u52a0\u8f7d\u6570\u636e\u6587\u4ef6","text":"<p>\u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u662f\u4e00\u7ec4\u6570\u4ee5\u5343\u8ba1\u7684\u82f1\u8bed\u5230\u6cd5\u8bed\u7684\u7ffb\u8bd1\u7528\u4f8b</p> <p>\u8fd9\u4e2a\u95ee\u9898\u5728 Open Data Stack Exchange \u4e0a \u70b9\u6211\u6253\u5f00\u7ffb\u8bd1\u7f51\u5740 https://tatoeba.org/ \u8fd9\u4e2a\u7f51\u7ad9\u7684\u4e0b\u8f7d\u5730\u5740 https://tatoeba.org/eng/downloads - \u66f4\u68d2\u7684\u662f\uff0c\u6709\u4eba\u5c06\u8fd9\u4e9b\u8bed\u8a00\u5207\u5206\u6210\u5355\u4e2a\u6587\u4ef6: https://www.manythings.org/anki/</p> <p>\u7531\u4e8e\u7ffb\u8bd1\u6587\u4ef6\u592a\u5927\u800c\u4e0d\u80fd\u653e\u5230repo\u4e2d\uff0c\u8bf7\u7ee7\u7eed\u5f80\u4e0b\u524d\u4e0b\u8f7d\u6570\u636e\u5230 <code>data/eng-fra.txt</code> \u3002\u8be5\u6587\u4ef6\u662f\u4e00\u4e2a\u4f7f\u7528\u5236\u8868\u7b26\u5206\u5272\u7684\u7ffb\u8bd1\u5217\u8868:</p> <pre><code>I am cold.    J'ai froid.\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u4ece \u8fd9\u91cc \u4e0b\u8f7d\u6570\u636e\u548c\u89e3\u538b\u5230\u76f8\u5173\u7684\u8def\u5f84.</p> <p>\u4e0echaracter-level RNN\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u5b57\u7b26\u7f16\u7801\u7c7b\u4f3c,\u6211\u4eec\u5c06\u7528\u8bed\u8a00\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd \u4f5c\u4e3a\u72ec\u70ed\u5411\u91cf,\u6216\u8005\u9664\u4e86\u5355\u4e2a\u5355\u8bcd\u4e4b\u5916(\u5728\u5355\u8bcd\u7684\u7d22\u5f15\u5904)\u7684\u5927\u7684\u96f6\u5411\u91cf. \u76f8\u8f83\u4e8e\u53ef\u80fd \u5b58\u5728\u4e8e\u4e00\u79cd\u8bed\u8a00\u4e2d\u4ec5\u6709\u5341\u4e2a\u5b57\u7b26\u76f8\u6bd4,\u591a\u6570\u90fd\u662f\u6709\u5927\u91cf\u7684\u5b57,\u56e0\u6b64\u7f16\u7801\u5411\u91cf\u5f88\u5927. \u7136\u800c,\u6211\u4eec\u4f1a\u6b3a\u9a97\u6027\u7684\u505a\u4e00\u4e9b\u6570\u636e\u4fee\u526a,\u4fdd\u8bc1\u6bcf\u79cd\u8bed\u8a00\u53ea\u4f7f\u7528\u51e0\u5343\u5b57.</p> <p></p> <p>\u6211\u4eec\u4e4b\u540e\u9700\u8981\u5c06\u6bcf\u4e2a\u5355\u8bcd\u5bf9\u5e94\u552f\u4e00\u7684\u7d22\u5f15\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u548c\u76ee\u6807.\u4e3a\u4e86\u8ffd\u8e2a\u8fd9\u4e9b\u7d22\u5f15\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u5e2e\u52a9\u7c7b <code>Lang</code> \u7c7b\u4e2d\u6709 \u8bcd \u2192 \u7d22\u5f15 (<code>word2index</code>) \u548c \u7d22\u5f15 \u2192 \u8bcd(<code>index2word</code>) \u7684\u5b57\u5178, \u4ee5\u53ca\u6bcf\u4e2a\u8bcd<code>word2count</code> \u7528\u6765\u66ff\u6362\u7a00\u758f\u8bcd\u6c47\u3002</p> <pre><code>SOS_token = 0\nEOS_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\n</code></pre> <p>\u8fd9\u4e9b\u6587\u4ef6\u5168\u90e8\u91c7\u7528Unicode\u7f16\u7801\uff0c\u4e3a\u4e86\u7b80\u5316\u8d77\u89c1\uff0c\u6211\u4eec\u5c06Unicode\u5b57\u7b26\u8f6c\u6362\u6210ASCII\u7f16\u7801\u3001\u6240\u6709\u5185\u5bb9\u5c0f\u5199\u3001\u5e76\u4fee\u526a\u5927\u90e8\u5206\u6807\u70b9\u7b26\u53f7\u3002</p> <pre><code># \u611f\u8c22\u60a8\u5c06Unicode\u5b57\u7b26\u8f6c\u6362\u6210ASCII\n# https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# \u5c0f\u5199\uff0c\u4fee\u526a\u548c\u5220\u9664\u975e\u5b57\u7b26\u5b57\u7b26\n\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\n</code></pre> <p>\u6211\u4eec\u5c06\u6309\u884c\u5206\u5f00\u5e76\u5c06\u6bcf\u4e00\u884c\u5206\u6210\u4e24\u5217\u6765\u8bfb\u53d6\u6587\u4ef6\u3002\u8fd9\u4e9b\u6587\u4ef6\u90fd\u662f\u82f1\u8bed -&gt; \u5176\u4ed6\u8bed\u8a00\uff0c\u6240\u4ee5\u5982\u679c\u6211\u4eec\u60f3\u4ece\u5176\u4ed6\u8bed\u8a00\u7ffb\u8bd1 -&gt; \u82f1\u8bed\uff0c\u6dfb\u52a0<code>reverse</code>\u6807\u5fd7\u6765\u7ffb\u8f6c\u8bcd\u8bed\u5bf9\u3002</p> <pre><code>def readLangs(lang1, lang2, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n        read().strip().split('\\n')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\n</code></pre> <p>\u7b80\u77ed\u7684\u53e5\u5b50\u3002\u8fd9\u4e9b\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\u662f10\u4e2a\u5355\u8bcd(\u5305\u62ec\u6807\u70b9\u7b26\u53f7\uff09\uff0c\u540c\u65f6\u6211\u4eec\u5c06\u90a3\u4e9b\u7ffb\u8bd1\u4e3a\u201cI am\u201d\u6216\u201che is\u201d\u7b49\u5f62\u5f0f\u7684\u53e5\u5b50\u8fdb\u884c\u4e86\u4fee\u6539(\u8003\u8651\u5230\u4e4b\u524d\u6e05\u9664\u7684\u6807\u70b9\u7b26\u53f7\u2014\u2014'\uff09\u3002</p> <pre><code>MAX_LENGTH = 10\n\neng_prefixes = (\n    \"i am \", \"i m \",\n    \"he is\", \"he s \",\n    \"she is\", \"she s \",\n    \"you are\", \"you re \",\n    \"we are\", \"we re \",\n    \"they are\", \"they re \"\n)\n\ndef filterPair(p):\n    return len(p[0].split(' ')) &lt; MAX_LENGTH and \\\n        len(p[1].split(' ')) &lt; MAX_LENGTH and \\\n        p[1].startswith(eng_prefixes)\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\n</code></pre> <p>\u5b8c\u6574\u7684\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\uff1a</p> <ul> <li>\u6309\u884c\u8bfb\u53d6\u6587\u672c\u6587\u4ef6\uff0c\u5c06\u884c\u62c6\u5206\u6210\u5bf9</li> <li>\u89c4\u8303\u6587\u672c\uff0c\u6309\u957f\u5ea6\u548c\u5185\u5bb9\u8fc7\u6ee4</li> <li>\u4ece\u53e5\u5b50\u4e2d\u6210\u5bf9\u5217\u51fa\u5355\u8bcd\u5217\u8868</li> </ul> <pre><code>def prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\ninput_lang, output_lang, pairs = prepareData('eng', 'fra', True)\nprint(random.choice(pairs))\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Reading lines...\nRead 135842 sentence pairs\nTrimmed to 10599 sentence pairs\nCounting words...\nCounted words:\nfra 4345\neng 2803\n['ils ne sont pas encore chez eux .', 'they re not home yet .']\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#seq2seq_1","title":"Seq2Seq\u6a21\u578b","text":"<p>\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(RNN\uff09\u662f\u4e00\u79cd\u5bf9\u5e8f\u5217\u8fdb\u884c\u64cd\u4f5c\u5e76\u5229\u7528\u81ea\u5df1\u7684\u8f93\u51fa\u4f5c\u4e3a\u540e\u5e8f\u8f93\u5165\u7684\u7f51\u7edc</p> <p>\u5e8f\u5217\u5230\u5e8f\u5217\u7f51\u7edc(Sequence to Sequence network\uff09, \u4e5f\u53eb\u505a seq2seq \u7f51\u7edc, \u53c8\u6216\u8005\u662f \u7f16\u7801\u5668\u89e3\u7801\u5668\u7f51\u7edc(Encoder Decoder network\uff09, \u662f\u4e00\u4e2a\u7531\u4e24\u4e2a\u79f0\u4e3a\u7f16\u7801\u5668\u89e3\u7801\u5668\u7684RNN\u7ec4\u6210\u7684\u6a21\u578b\u3002\u7f16\u7801\u5668\u8bfb\u53d6\u8f93\u5165\u5e8f\u5217\u5e76\u8f93\u51fa\u4e00\u4e2a\u77e2\u91cf\uff0c\u89e3\u7801\u5668\u8bfb\u53d6\u8be5\u77e2\u91cf\u5e76\u4ea7\u751f\u8f93\u51fa\u5e8f\u5217\u3002</p> <p></p> <p>\u4e0e\u6bcf\u4e2a\u8f93\u5165\u5bf9\u5e94\u4e00\u4e2a\u8f93\u51fa\u7684\u5355\u4e2aRNN\u7684\u5e8f\u5217\u9884\u6d4b\u4e0d\u540c\uff0cseq2seq\u6a21\u578b\u5c06\u6211\u4eec\u4ece\u5e8f\u5217\u957f\u5ea6\u548c\u987a\u5e8f\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u8fd9\u4f7f\u5f97\u5b83\u66f4\u9002\u5408\u4e24\u79cd\u8bed\u8a00\u7684\u8f6c\u6362\u3002</p> <p>\u8003\u8651\u8fd9\u53e5\u8bdd\u201cJe ne suis pas le chat noir\u201d \u2192 \u201cI am not the black cat\u201d.\u867d\u7136\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u8f93\u5165\u8f93\u51fa\u5e8f\u5217\u53ef\u4ee5\u5bf9\u5355\u8bcd\u8fdb\u884c\u6bd4\u8f83\u76f4\u63a5\u7684\u7ffb\u8bd1\uff0c\u4f46\u662f\u5f88\u591a\u65f6\u5019\u5355\u8bcd\u7684\u987a\u5e8f\u5374\u7565\u6709\u4e0d\u540c\uff0c\u4f8b\u5982: \u201cchat noir\u201d \u548c \u201cblack cat\u201d\u3002\u7531\u4e8e \u201cne/pas\u201d\u7ed3\u6784, \u8f93\u5165\u7684\u53e5\u5b50\u4e2d\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u5355\u8bcd.\u3002\u56e0\u6b64\u76f4\u63a5\u4ece\u8f93\u5165\u8bcd\u7684\u5e8f\u5217\u4e2d\u76f4\u63a5\u751f\u6210\u6b63\u786e\u7684\u7ffb\u8bd1\u662f\u5f88\u56f0\u96be\u7684\u3002</p> <p>\u4f7f\u7528seq2seq\u6a21\u578b\u65f6\uff0c\u7f16\u7801\u5668\u4f1a\u521b\u5efa\u4e00\u4e2a\u5411\u91cf\uff0c\u5728\u7406\u60f3\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u8f93\u5165\u5e8f\u5217\u7684\u5b9e\u9645\u8bed\u4e49\u7f16\u7801\u4e3a\u5355\u4e2a\u5411\u91cf - \u5e8f\u5217\u7684\u4e00\u4e9bN\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5355\u4e2a\u70b9\u3002</p>"},{"location":"1.0/seq2seq_translation_tutorial/#_2","title":"\u7f16\u7801\u5668","text":"<p>seq2seq\u7f51\u7edc\u7684\u7f16\u7801\u5668\u662fRNN\uff0c\u5b83\u4e3a\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u6bcf\u4e2a\u5355\u8bcd\u8f93\u51fa\u4e00\u4e9b\u503c\u3002 \u5bf9\u4e8e\u6bcf\u4e2a\u8f93\u5165\u5355\u8bcd\uff0c\u7f16\u7801\u5668\u8f93\u51fa\u4e00\u4e2a\u5411\u91cf\u548c\u4e00\u4e2a\u9690\u72b6\u6001\uff0c\u5e76\u5c06\u8be5\u9690\u72b6\u6001\u7528\u4e8e\u4e0b\u4e00\u4e2a\u8f93\u5165\u7684\u5355\u8bcd\u3002</p> <p></p> <pre><code>class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_3","title":"\u89e3\u7801\u5668","text":"<p>\u89e3\u7801\u5668\u662f\u4e00\u4e2a\u63a5\u53d7\u7f16\u7801\u5668\u8f93\u51fa\u5411\u91cf\u5e76\u8f93\u51fa\u4e00\u7cfb\u5217\u5355\u8bcd\u4ee5\u521b\u5efa\u7ffb\u8bd1\u7684RNN\u3002</p>"},{"location":"1.0/seq2seq_translation_tutorial/#_4","title":"\u7b80\u5355\u7684\u7f16\u7801\u5668","text":"<p>\u5728\u6700\u7b80\u5355\u7684seq2seq\u89e3\u7801\u5668\u4e2d\uff0c\u6211\u4eec\u53ea\u4f7f\u7528\u7f16\u7801\u5668\u7684\u6700\u540e\u8f93\u51fa\u3002\u8fd9\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\u6709\u65f6\u79f0\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\u56e0\u4e3a\u5b83\u4ece\u6574\u4e2a\u5e8f\u5217\u4e2d\u7f16\u7801\u4e0a\u4e0b\u6587\u3002\u8be5\u4e0a\u4e0b\u6587\u5411\u91cf\u7528\u4f5c\u89e3\u7801\u5668\u7684\u521d\u59cb\u9690\u85cf\u72b6\u6001\u3002</p> <p>\u5728\u89e3\u7801\u7684\u6bcf\u4e00\u6b65,\u89e3\u7801\u5668\u90fd\u88ab\u8d4b\u4e88\u4e00\u4e2a\u8f93\u5165\u6307\u4ee4\u548c\u9690\u85cf\u72b6\u6001. \u521d\u59cb\u8f93\u5165\u6307\u4ee4\u5b57\u7b26\u4e32\u5f00\u59cb\u7684<code>&lt;SOS&gt;</code>\u6307\u4ee4,\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u662f\u4e0a\u4e0b\u6587\u5411\u91cf(\u7f16\u7801\u5668\u7684\u6700\u540e\u9690\u85cf\u72b6\u6001).</p> <p></p> <pre><code>class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)\n\n</code></pre> <p>\u6211\u4eec\u9f13\u52b1\u4f60\u8bad\u7ec3\u548c\u89c2\u5bdf\u8fd9\u4e2a\u6a21\u578b\u7684\u7ed3\u679c,\u4f46\u4e3a\u4e86\u8282\u7701\u7a7a\u95f4,\u6211\u4eec\u5c06\u76f4\u5165\u4e3b\u9898\u5f00\u59cb\u8bb2\u89e3\u6ce8\u610f\u529b\u673a\u5236.</p>"},{"location":"1.0/seq2seq_translation_tutorial/#_5","title":"\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u7801\u5668","text":"<p>\u5982\u679c\u4ec5\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u4f20\u9012\u4e0a\u4e0b\u6587\u5411\u91cf,\u5219\u8be5\u5355\u4e2a\u5411\u91cf\u627f\u62c5\u7f16\u7801\u6574\u4e2a\u53e5\u5b50\u7684\u8d1f\u62c5.</p> <p>\u6ce8\u610f\u529b\u673a\u5236\u5141\u8bb8\u89e3\u7801\u5668\u7f51\u7edc\u9488\u5bf9\u89e3\u7801\u5668\u81ea\u8eab\u8f93\u51fa\u7684\u6bcf\u4e00\u6b65\u201d\u805a\u7126\u201d\u7f16\u7801\u5668\u8f93\u51fa\u7684\u4e0d\u540c\u90e8\u5206. \u9996\u5148\u6211\u4eec\u8ba1\u7b97\u4e00\u7ec4\u6ce8\u610f\u529b\u6743\u91cd. \u8fd9\u4e9b\u5c06\u88ab\u4e58\u4ee5\u7f16\u7801\u5668\u8f93\u51fa\u77e2\u91cf\u83b7\u5f97\u52a0\u6743\u7684\u7ec4\u5408. \u7ed3\u679c(\u5728\u4ee3\u7801\u4e2d\u4e3a<code>attn_applied</code>) \u5e94\u8be5\u5305\u542b\u5173\u4e8e\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u90e8\u5206\u7684\u4fe1\u606f, \u4ece\u800c\u5e2e\u52a9\u89e3\u7801\u5668\u9009\u62e9\u6b63\u786e\u7684\u8f93\u51fa\u5355\u8bcd.</p> <p></p> <p>\u6ce8\u610f\u6743\u503c\u7684\u8ba1\u7b97\u662f\u7528\u53e6\u4e00\u4e2a\u524d\u9988\u5c42<code>attn</code>\u8fdb\u884c\u7684, \u5c06\u89e3\u7801\u5668\u7684\u8f93\u5165\u548c\u9690\u85cf\u5c42\u72b6\u6001\u4f5c\u4e3a\u8f93\u5165. \u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8f93\u5165\u5e8f\u5217(\u8bed\u53e5\uff09\u957f\u77ed\u4e0d\u4e00,\u4e3a\u4e86\u5b9e\u9645\u521b\u5efa\u548c\u8bad\u7ec3\u6b64\u5c42, \u6211\u4eec\u5fc5\u987b\u9009\u62e9\u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50(\u8f93\u5165\u957f\u5ea6,\u7528\u4e8e\u7f16\u7801\u5668\u8f93\u51fa),\u4ee5\u9002\u7528\u4e8e\u6b64\u5c42. \u6700\u5927\u957f\u5ea6\u7684\u53e5\u5b50\u5c06\u4f7f\u7528\u6240\u6709\u6ce8\u610f\u529b\u6743\u91cd,\u800c\u8f83\u77ed\u7684\u53e5\u5b50\u53ea\u4f7f\u7528\u524d\u51e0\u4e2a.</p> <p></p> <pre><code>class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u8fd8\u6709\u5176\u4ed6\u5f62\u5f0f\u7684\u6ce8\u610f\u529b\u901a\u8fc7\u4f7f\u7528\u76f8\u5bf9\u4f4d\u7f6e\u65b9\u6cd5\u6765\u89e3\u51b3\u957f\u5ea6\u9650\u5236. \u9605\u8bfb\u5173\u4e8e \u201clocal attention\u201d \u5728 \u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u9014\u5f84.</p>"},{"location":"1.0/seq2seq_translation_tutorial/#_6","title":"\u8bad\u7ec3","text":""},{"location":"1.0/seq2seq_translation_tutorial/#_7","title":"\u51c6\u5907\u8bad\u7ec3\u6570\u636e","text":"<p>\u4e3a\u4e86\u8bad\u7ec3,\u5bf9\u4e8e\u6bcf\u4e00\u5bf9\u6211\u4eec\u90fd\u9700\u8981\u8f93\u5165\u5f20\u91cf(\u8f93\u5165\u53e5\u5b50\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15)\u548c \u76ee\u6807\u5f20\u91cf(\u76ee\u6807\u8bed\u53e5\u4e2d\u7684\u8bcd\u7684\u7d22\u5f15). \u5728\u521b\u5efa\u8fd9\u4e9b\u5411\u91cf\u65f6,\u6211\u4eec\u4f1a\u5c06EOS\u6807\u8bb0\u6dfb\u52a0\u5230\u4e24\u4e2a\u5e8f\u5217\u4e2d\u3002</p> <pre><code>def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_8","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u4e3a\u4e86\u8bad\u7ec3\u6211\u4eec\u901a\u8fc7\u7f16\u7801\u5668\u8fd0\u884c\u8f93\u5165\u5e8f\u5217,\u5e76\u8ddf\u8e2a\u6bcf\u4e2a\u8f93\u51fa\u548c\u6700\u65b0\u7684\u9690\u85cf\u72b6\u6001.  \u7136\u540e\u89e3\u7801\u5668\u88ab\u8d4b\u4e88<code>&lt;SOS&gt;</code> \u6807\u5fd7\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u8f93\u5165, \u5e76\u5c06\u7f16\u7801\u5668\u7684\u6700\u540e\u4e00\u4e2a\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u5176\u7b2c\u4e00\u4e2a\u9690\u85cf\u72b6\u6001.</p> <p>\u201cTeacher forcing\u201d \u662f\u5c06\u5b9e\u9645\u76ee\u6807\u8f93\u51fa\u7528\u4f5c\u6bcf\u4e2a\u4e0b\u4e00\u4e2a\u8f93\u5165\u7684\u6982\u5ff5,\u800c\u4e0d\u662f\u5c06\u89e3\u7801\u5668\u7684 \u731c\u6d4b\u7528\u4f5c\u4e0b\u4e00\u4e2a\u8f93\u5165.\u4f7f\u7528\u201cTeacher forcing\u201d \u4f1a\u4f7f\u5176\u66f4\u5feb\u5730\u6536\u655b,\u4f46\u662f \u5f53\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u88ab\u5229\u7528\u65f6,\u5b83\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u6027..</p> <p>\u60a8\u53ef\u4ee5\u89c2\u5bdf\u201cTeacher forcing\u201d\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u8fd9\u4e9b\u7f51\u7edc\u4f7f\u7528\u8fde\u8d2f\u7684\u8bed\u6cd5\u9605\u8bfb\uff0c\u4f46\u8fdc\u79bb\u6b63\u786e\u7684\u7ffb\u8bd1 - \u76f4\u89c9\u4e0a\u5b83\u5df2\u7ecf\u5b66\u4f1a\u8868\u793a\u8f93\u51fa\u8bed\u6cd5\uff0c\u5e76\u4e14\u4e00\u65e6\u8001\u5e08\u544a\u8bc9\u5b83\u524d\u51e0\u4e2a\u5355\u8bcd\u5c31\u53ef\u4ee5\u201c\u63d0\u53d6\u201d\u610f\u4e49\uff0c\u4f46\u662f \u5b83\u6ca1\u6709\u6b63\u786e\u5730\u5b66\u4e60\u5982\u4f55\u4ece\u7ffb\u8bd1\u4e2d\u521b\u5efa\u53e5\u5b50\u3002</p> <p>\u7531\u4e8ePyTorch\u7684autograd\u7ed9\u6211\u4eec\u7684\u81ea\u7531,\u6211\u4eec\u53ef\u4ee5\u968f\u610f\u9009\u62e9\u4f7f\u7528\u201cTeacher forcing\u201d\u6216\u4e0d\u4f7f\u7528\u7b80\u5355\u7684if\u8bed\u53e5. \u8c03\u9ad8<code>teacher_forcing_ratio</code>\u6765\u66f4\u597d\u5730\u4f7f\u7528\u5b83.</p> <pre><code>teacher_forcing_ratio = 0.5\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: \u5c06\u76ee\u6807\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # \u4e0d\u9002\u7528 teacher forcing: \u4f7f\u7528\u81ea\u5df1\u7684\u9884\u6d4b\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u8f93\u5165\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length\n\n</code></pre> <p>\u8fd9\u662f\u4e00\u4e2a\u5e2e\u52a9\u51fd\u6570\uff0c\u7528\u4e8e\u5728\u7ed9\u5b9a\u5f53\u524d\u65f6\u95f4\u548c\u8fdb\u5ea6%\u7684\u60c5\u51b5\u4e0b\u6253\u5370\u7ecf\u8fc7\u7684\u65f6\u95f4\u548c\u4f30\u8ba1\u7684\u5269\u4f59\u65f6\u95f4\u3002</p> <pre><code>import time\nimport math\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n\n</code></pre> <p>\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u5982\u4e0b\u6240\u793a:</p> <ul> <li>\u542f\u52a8\u8ba1\u65f6\u5668</li> <li>\u521d\u59cb\u5316\u4f18\u5316\u5668\u548c\u51c6\u5219</li> <li>\u521b\u5efa\u4e00\u7ec4\u8bad\u7ec3\u961f</li> <li>\u4e3a\u8fdb\u884c\u7ed8\u56fe\u542f\u52a8\u7a7a\u635f\u5931\u6570\u7ec4</li> </ul> <p>\u4e4b\u540e\u6211\u4eec\u591a\u6b21\u8c03\u7528<code>train</code>\u51fd\u6570\uff0c\u5076\u5c14\u6253\u5370\u8fdb\u5ea6 (\u6837\u672c\u7684\u767e\u5206\u6bd4\uff0c\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u65f6\u95f4\uff0c\u72d9\u51fb\u7684\u65f6\u95f4) \u548c\u5e73\u5747\u635f\u5931</p> <pre><code>def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print('%s (%d  %d%%) %.4f' % (timeSince(start, iter / n_iters),\n                                         iter, iter / n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total / plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_9","title":"\u7ed8\u5236\u7ed3\u679c","text":"<p>\u4f7f\u7528matplotlib\u5b8c\u6210\u7ed8\u56fe\uff0c\u4f7f\u7528<code>plot_losses</code>\u4fdd\u5b58\u8bad\u7ec3\u65f6\u7684\u6570\u7ec4\u3002</p> <pre><code>import matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # \u8be5\u5b9a\u65f6\u5668\u7528\u4e8e\u5b9a\u65f6\u8bb0\u5f55\u65f6\u95f4\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_10","title":"\u8bc4\u4f30","text":"<p>\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u5927\u90e8\u5206\u76f8\u540c,\u4f46\u6ca1\u6709\u76ee\u6807,\u56e0\u6b64\u6211\u4eec\u53ea\u662f\u5c06\u89e3\u7801\u5668\u7684\u6bcf\u4e00\u6b65\u9884\u6d4b\u53cd\u9988\u7ed9\u5b83\u81ea\u8eab. \u6bcf\u5f53\u5b83\u9884\u6d4b\u5230\u4e00\u4e2a\u5355\u8bcd\u65f6,\u6211\u4eec\u5c31\u4f1a\u5c06\u5b83\u6dfb\u52a0\u5230\u8f93\u51fa\u5b57\u7b26\u4e32\u4e2d,\u5e76\u4e14\u5982\u679c\u5b83\u9884\u6d4b\u5230\u6211\u4eec\u5728\u90a3\u91cc\u505c\u6b62\u7684EOS\u6307\u4ee4. \u6211\u4eec\u8fd8\u5b58\u50a8\u89e3\u7801\u5668\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u4ee5\u4f9b\u7a0d\u540e\u663e\u793a.</p> <pre><code>def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('&lt;EOS&gt;')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]\n\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u4ece\u8bad\u7ec3\u96c6\u4e2d\u5bf9\u968f\u673a\u53e5\u5b50\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u6253\u5370\u51fa\u8f93\u5165\u3001\u76ee\u6807\u548c\u8f93\u51fa\uff0c\u4ece\u800c\u505a\u51fa\u4e00\u4e9b\u4e3b\u89c2\u7684\u8d28\u91cf\u5224\u65ad\uff1a</p> <pre><code>def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('&gt;', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('&lt;', output_sentence)\n        print('')\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_11","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30","text":"<p>\u6709\u4e86\u6240\u6709\u8fd9\u4e9b\u5e2e\u52a9\u51fd\u6570(\u5b83\u770b\u8d77\u6765\u50cf\u662f\u989d\u5916\u7684\u5de5\u4f5c\uff0c\u4f46\u5b83\u4f7f\u8fd0\u884c\u591a\u4e2a\u5b9e\u9a8c\u66f4\u5bb9\u6613)\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u53ef\u4ee5\u521d\u59cb\u5316\u4e00\u4e2a\u7f51\u7edc\u5e76\u5f00\u59cb\u8bad\u7ec3\u3002</p> <p>\u8bf7\u8bb0\u4f4f\u8f93\u5165\u53e5\u5b50\u88ab\u4e25\u91cd\u8fc7\u6ee4, \u5bf9\u4e8e\u8fd9\u4e2a\u5c0f\u6570\u636e\u96c6,\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5305\u542b256\u4e2a\u9690\u85cf\u8282\u70b9 \u548c\u5355\u4e2aGRU\u5c42\u7684\u76f8\u5bf9\u8f83\u5c0f\u7684\u7f51\u7edc.\u5728MacBook CPU\u4e0a\u7ea640\u5206\u949f\u540e,\u6211\u4eec\u4f1a\u5f97\u5230\u4e00\u4e9b\u5408\u7406\u7684\u7ed3\u679c.</p> <p>\u6ce8</p> <p>\u5982\u679c\u4f60\u8fd0\u884c\u8fd9\u4e2a\u7b14\u8bb0\u672c\uff0c\u4f60\u53ef\u4ee5\u8bad\u7ec3\uff0c\u4e2d\u65ad\u5185\u6838\uff0c\u8bc4\u4f30\uff0c\u5e76\u5728\u4ee5\u540e\u7ee7\u7eed\u8bad\u7ec3\u3002 \u6ce8\u91ca\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u521d\u59cb\u5316\u7684\u884c\u5e76\u518d\u6b21\u8fd0\u884c <code>trainIters</code> .</p> <pre><code>hidden_size = 256\nencoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\nattn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n\ntrainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>1m 47s (- 25m 8s) (5000 6%) 2.8641\n3m 30s (- 22m 45s) (10000 13%) 2.2666\n5m 15s (- 21m 1s) (15000 20%) 1.9537\n7m 0s (- 19m 17s) (20000 26%) 1.7170\n8m 46s (- 17m 32s) (25000 33%) 1.5182\n10m 31s (- 15m 46s) (30000 40%) 1.3280\n12m 15s (- 14m 0s) (35000 46%) 1.2137\n14m 1s (- 12m 16s) (40000 53%) 1.0843\n15m 48s (- 10m 32s) (45000 60%) 0.9847\n17m 34s (- 8m 47s) (50000 66%) 0.8515\n19m 20s (- 7m 2s) (55000 73%) 0.7940\n21m 6s (- 5m 16s) (60000 80%) 0.7189\n22m 53s (- 3m 31s) (65000 86%) 0.6490\n24m 41s (- 1m 45s) (70000 93%) 0.5954\n26m 26s (- 0m 0s) (75000 100%) 0.5257\n\n</code></pre> <pre><code>evaluateRandomly(encoder1, attn_decoder1)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>&gt; nous sommes contents que tu sois la .\n= we re glad you re here .\n&lt; we re glad you re here . &lt;EOS&gt;\n\n&gt; il est dependant a l heroine .\n= he is a heroin addict .\n&lt; he is in heroin heroin . &lt;EOS&gt;\n\n&gt; nous sommes les meilleurs .\n= we are the best .\n&lt; we are the best . &lt;EOS&gt;\n\n&gt; tu es puissant .\n= you re powerful .\n&lt; you re powerful . &lt;EOS&gt;\n\n&gt; j ai peur des chauves souris .\n= i m afraid of bats .\n&lt; i m afraid of bats . &lt;EOS&gt;\n\n&gt; tu es enseignant n est ce pas ?\n= you re a teacher right ?\n&lt; you re a teacher aren t you ? &lt;EOS&gt;\n\n&gt; je suis pret a tout faire pour toi .\n= i am ready to do anything for you .\n&lt; i am ready to do anything for you . &lt;EOS&gt;\n\n&gt; c est desormais un homme .\n= he s a man now .\n&lt; he is in an man . &lt;EOS&gt;\n\n&gt; elle est une mere tres avisee .\n= she s a very wise mother .\n&lt; she s a very wise mother . &lt;EOS&gt;\n\n&gt; je suis completement vanne .\n= i m completely exhausted .\n&lt; i m completely exhausted . &lt;EOS&gt;\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_12","title":"\u53ef\u89c6\u5316\u6ce8\u610f\u529b","text":"<p>\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e00\u4e2a\u6709\u7528\u7684\u7279\u6027\u662f\u5176\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002\u7531\u4e8e\u5b83\u7528\u4e8e\u52a0\u6743\u8f93\u5165\u5e8f\u5217\u7684\u7279\u5b9a\u7f16\u7801\u5668\u8f93\u51fa\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9aa4\u4e2d\uff0c\u67e5\u770b\u7f51\u7edc\u6700\u96c6\u4e2d\u7684\u4f4d\u7f6e\u3002</p> <p>\u4f60\u53ef\u4ee5\u7b80\u5355\u5730\u8fd0\u884c<code>plt.matshow(attentions)</code>\u6765\u67e5\u770b\u663e\u793a\u4e3a\u77e9\u9635\u7684\u6ce8\u610f\u529b\u8f93\u51fa\uff0c\u5217\u4e3a\u8f93\u5165\u6b65\u9aa4\uff0c\u884c\u4f4d\u8f93\u51fa\u6b65\u9aa4\u3002</p> <pre><code>output_words, attentions = evaluate(\n    encoder1, attn_decoder1, \"je suis trop froid .\")\nplt.matshow(attentions.numpy())\n\n</code></pre> <p></p> <p>\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u89c2\u770b\u4f53\u9a8c,\u6211\u4eec\u5c06\u989d\u5916\u6dfb\u52a0\u8f74\u548c\u6807\u7b7e:</p> <pre><code>def showAttention(input_sentence, output_words, attentions):\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['&lt;EOS&gt;'], rotation=90)\n    ax.set_yticklabels([''] + output_words)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions)\n\nevaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n\nevaluateAndShowAttention(\"elle est trop petit .\")\n\nevaluateAndShowAttention(\"je ne crains pas de mourir .\")\n\nevaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>input = elle a cinq ans de moins que moi .\noutput = she s five years younger than me . &lt;EOS&gt;\ninput = elle est trop petit .\noutput = she s too slow . &lt;EOS&gt;\ninput = je ne crains pas de mourir .\noutput = i m not scared to die . &lt;EOS&gt;\ninput = c est un jeune directeur plein de talent .\noutput = he s a talented young player . &lt;EOS&gt;\n\n</code></pre>"},{"location":"1.0/seq2seq_translation_tutorial/#_13","title":"\u7ec3\u4e60\u9898","text":"<ul> <li>\u5c1d\u8bd5\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6<ul> <li>\u53e6\u4e00\u79cd\u8bed\u8a00\u5bf9</li> <li>\u4eba \u2192 \u673a\u5668 (\u4f8b\u5982 IOT \u547d\u4ee4)</li> <li>\u804a\u5929 \u2192 \u54cd\u5e94</li> <li>\u95ee\u9898 \u2192 \u56de\u7b54</li> </ul> </li> <li>\u5c06\u5d4c\u5165\u66ff\u6362\u4e3a\u9884\u5148\u8bad\u7ec3\u8fc7\u7684\u5355\u8bcd\u5d4c\u5165\uff0c\u4f8b\u5982word2vec\u6216\u8005GloVe</li> <li>\u5c1d\u8bd5\u7528\u66f4\u591a\u7684\u5c42\u6b21\uff0c\u66f4\u591a\u7684\u9690\u85cf\u5355\u4f4d\uff0c\u66f4\u591a\u7684\u53e5\u5b50\u3002\u6bd4\u8f83\u8bad\u7ec3\u65f6\u95f4\u548c\u7ed3\u679c\u3002</li> <li>\u5982\u679c\u4f7f\u7528\u4e00\u4e2a\u7ffb\u8bd1\u6587\u4ef6\uff0c\u5176\u4e2d\u6210\u5bf9\u6709\u4e24\u4e2a\u76f8\u540c\u7684\u77ed\u8bed(<code>I am test \\t I am test</code>)\uff0c\u60a8\u53ef\u4ee5\u5c06\u5176\u7528\u4f5c\u81ea\u52a8\u7f16\u7801\u5668\u3002\u8bd5\u8bd5\u8fd9\u4e2a\uff1a<ul> <li>\u8bad\u7ec3\u4e3a\u81ea\u52a8\u7f16\u7801\u5668</li> <li>\u53ea\u4fdd\u5b58\u7f16\u7801\u5668\u7f51\u7edc</li> <li>\u8bad\u7ec3\u4e00\u79cd\u65b0\u7684\u7ffb\u8bd1\u89e3\u7801\u5668</li> </ul> </li> </ul>"},{"location":"1.0/sparse/","title":"torch.sparse","text":"<p>\u8bd1\u8005\uff1ahijkzzz</p> <p>\u8b66\u544a</p> <p>\u8fd9\u4e2aAPI\u76ee\u524d\u8fd8\u5904\u4e8e\u8bd5\u9a8c\u9636\u6bb5, \u53ef\u80fd\u5728\u4e0d\u4e45\u7684\u5c06\u6765\u4f1a\u53d1\u751f\u53d8\u5316. </p> <p>Torch\u652f\u6301COO(rdinate )\u683c\u5f0f\u7684\u7a00\u758f\u5f20\u91cf, \u8fd9\u53ef\u4ee5\u6709\u6548\u5730\u5b58\u50a8\u548c\u5904\u7406\u5927\u591a\u6570\u5143\u7d20\u4e3a\u96f6\u7684\u5f20\u91cf. </p> <p>\u7a00\u758f\u5f20\u91cf\u8868\u793a\u4e3a\u4e00\u5bf9\u7a20\u5bc6\u5f20\u91cf:\u4e00\u4e2a\u503c\u5f20\u91cf\u548c\u4e00\u4e2a\u4e8c\u7ef4\u6307\u6807\u5f20\u91cf. \u4e00\u4e2a\u7a00\u758f\u5f20\u91cf\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u8fd9\u4e24\u4e2a\u5f20\u91cf, \u4ee5\u53ca\u7a00\u758f\u5f20\u91cf\u7684\u5927\u5c0f\u6765\u6784\u9020(\u4ece\u8fd9\u4e9b\u5f20\u91cf\u662f\u65e0\u6cd5\u63a8\u5bfc\u51fa\u6765\u7684!)\u5047\u8bbe\u6211\u4eec\u8981\u5b9a\u4e49\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf, \u5b83\u7684\u5206\u91cf3\u5728(0,2)\u5904, \u5206\u91cf4\u5728(1,0)\u5904, \u5206\u91cf5\u5728(1,2)\u5904, \u7136\u540e\u6211\u4eec\u53ef\u4ee5\u8fd9\u6837\u5199</p> <pre><code>&gt;&gt;&gt; i = torch.LongTensor([[0, 1, 1],\n [2, 0, 2]])\n&gt;&gt;&gt; v = torch.FloatTensor([3, 4, 5])\n&gt;&gt;&gt; torch.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()\n 0  0  3\n 4  0  5\n[torch.FloatTensor of size 2x3]\n\n</code></pre> <p>\u6ce8\u610f, LongTensor\u7684\u8f93\u5165\u4e0d\u662f\u7d22\u5f15\u5143\u7ec4\u7684\u5217\u8868. \u5982\u679c\u4f60\u60f3\u8fd9\u6837\u5199\u4f60\u7684\u6307\u6807, \u4f60\u5e94\u8be5\u5728\u628a\u5b83\u4eec\u4f20\u9012\u7ed9\u7a00\u758f\u6784\u9020\u51fd\u6570\u4e4b\u524d\u8fdb\u884c\u8f6c\u7f6e:</p> <pre><code>&gt;&gt;&gt; i = torch.LongTensor([[0, 2], [1, 0], [1, 2]])\n&gt;&gt;&gt; v = torch.FloatTensor([3,      4,      5    ])\n&gt;&gt;&gt; torch.sparse.FloatTensor(i.t(), v, torch.Size([2,3])).to_dense()\n 0  0  3\n 4  0  5\n[torch.FloatTensor of size 2x3]\n\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u6784\u9020\u6df7\u5408\u7a00\u758f\u5f20\u91cf, \u5176\u4e2d\u53ea\u6709\u524dn\u4e2a\u7ef4\u5ea6\u662f\u7a00\u758f\u7684, \u5176\u4f59\u7ef4\u5ea6\u662f\u5bc6\u96c6\u7684. </p> <pre><code>&gt;&gt;&gt; i = torch.LongTensor([[2, 4]])\n&gt;&gt;&gt; v = torch.FloatTensor([[1, 3], [5, 7]])\n&gt;&gt;&gt; torch.sparse.FloatTensor(i, v).to_dense()\n 0  0\n 0  0\n 1  3\n 0  0\n 5  7\n[torch.FloatTensor of size 5x2]\n\n</code></pre> <p>\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a\u5176\u5927\u5c0f\u6765\u6784\u9020\u7a7a\u7684\u7a00\u758f\u5f20\u91cf\uff1a</p> <pre><code>&gt;&gt;&gt; torch.sparse.FloatTensor(2, 3)\nSparseFloatTensor of size 2x3 with indices:\n[torch.LongTensor with no dimension]\nand values:\n[torch.FloatTensor with no dimension]\n\n</code></pre> <pre><code>SparseTensor \u5177\u6709\u4ee5\u4e0b\u4e0d\u53d8\u91cf:\n</code></pre> <ol> <li>sparse_dim + dense_dim = len(SparseTensor.shape)</li> <li>SparseTensor._indices().shape = (sparse_dim, nnz)</li> <li>SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])</li> </ol> <p>\u56e0\u4e3aSparseTensor._indices()\u603b\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf, \u6700\u5c0f\u7684sparse_dim = 1. \u56e0\u6b64, sparse_dim = 0\u7684\u7a00\u758f\u5f20\u91cf\u7684\u8868\u793a\u5c31\u662f\u4e00\u4e2a\u7a20\u5bc6\u5f20\u91cf. </p> <p>\u6ce8\u610f</p> <p>\u6211\u4eec\u7684\u7a00\u758f\u5f20\u91cf\u683c\u5f0f\u5141\u8bb8_uncoalesced(\u672a\u5408\u5e76) \u7684\u7a00\u758f\u5f20\u91cf, \u5176\u4e2d\u7d22\u5f15\u4e2d\u53ef\u80fd\u6709\u91cd\u590d\u7684\u5750\u6807;\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b, \u89e3\u91ca\u662f\u7d22\u5f15\u5904\u7684\u503c\u662f\u6240\u6709\u91cd\u590d\u503c\u9879\u7684\u548c. _uncoalesced \u5f20\u91cf\u5141\u8bb8\u6211\u4eec\u66f4\u6709\u6548\u5730\u5b9e\u73b0\u67d0\u4e9b\u8fd0\u7b97\u7b26. </p> <p>\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b, \u4f60\u4e0d\u9700\u8981\u5173\u5fc3\u4e00\u4e2a\u7a00\u758f\u5f20\u91cf\u662f\u5426coalesced(\u5408\u5e76), \u56e0\u4e3a\u5927\u591a\u6570\u64cd\u4f5c\u5728\u7ed9\u51fa\u4e00\u4e2acoalesced\u6216uncoalesced\u7a00\u758f\u5f20\u91cf\u7684\u60c5\u51b5\u4e0b\u90fd\u662f\u4e00\u6837\u7684. \u7136\u800c, \u6709\u4e24\u79cd\u60c5\u51b5\u60a8\u53ef\u80fd\u9700\u8981\u6ce8\u610f. </p> <p>\u7b2c\u4e00, \u5982\u679c\u60a8\u91cd\u590d\u6267\u884c\u53ef\u4ee5\u4ea7\u751f\u91cd\u590d\u9879\u7684\u64cd\u4f5c (\u4f8b\u5982, <code>torch.sparse.FloatTensor.add()</code>), \u4f60\u5e94\u8be5\u5076\u5c14\u5c06\u7a00\u758f\u5f20\u91cfcoalesced\u4e00\u8d77, \u4ee5\u9632\u6b62\u5b83\u4eec\u53d8\u5f97\u592a\u5927.</p> <p>\u7b2c\u4e8c, \u4e00\u4e9b\u8fd0\u7b97\u7b26\u5c06\u6839\u636e\u5b83\u4eec\u662f\u5426coalesced\u4ea7\u751f\u4e0d\u540c\u7684\u503c (\u4f8b\u5982, <code>torch.sparse.FloatTensor._values()</code> and <code>torch.sparse.FloatTensor._indices()</code>, \u4ee5\u53ca <code>torch.Tensor.sparse_mask()</code>). \u8fd9\u4e9b\u64cd\u4f5c\u7b26\u4ee5\u4e0b\u5212\u7ebf\u4f5c\u4e3a\u524d\u7f00, \u8868\u793a\u5b83\u4eec\u63ed\u793a\u4e86\u5185\u90e8\u5b9e\u73b0\u7ec6\u8282, \u5e94\u8be5\u5c0f\u5fc3\u4f7f\u7528, \u56e0\u4e3a\u4f7f\u7528\u5408\u5e76\u7a00\u758f\u5f20\u91cf\u7684\u4ee3\u7801\u53ef\u80fd\u65e0\u6cd5\u4f7f\u7528\u672a\u5408\u5e76\u7a00\u758f\u5f20\u91cf;\u4e00\u822c\u6765\u8bf4, \u5728\u4f7f\u7528\u8fd9\u4e9b\u64cd\u4f5c\u7b26\u4e4b\u524d\u663e\u5f0f\u5730\u5408\u5e76\u662f\u6700\u5b89\u5168\u7684. </p> <p>\u4f8b\u5982, \u5047\u8bbe\u6211\u4eec\u60f3\u901a\u8fc7\u76f4\u63a5\u64cd\u4f5c<code>torch.sparse.FloatTensor._values()</code>.\u6765\u5b9e\u73b0\u4e00\u4e2a\u64cd\u4f5c\u7b26.\u6807\u91cf\u4e58\u6cd5\u53ef\u4ee5\u7528\u5f88\u660e\u663e\u7684\u65b9\u6cd5\u5b9e\u73b0, \u56e0\u4e3a\u4e58\u6cd5\u5206\u5e03\u4e8e\u52a0\u6cd5\u4e4b\u4e0a;\u4f46\u662f, \u5e73\u65b9\u6839\u4e0d\u80fd\u76f4\u63a5\u5b9e\u73b0, \u56e0\u4e3a<code>sqrt(a + b) != sqrt(a) + sqrt(b)</code>(\u5982\u679c\u7ed9\u5b9a\u4e00\u4e2auncoalesced\u7684\u5f20\u91cf, \u5c31\u4f1a\u8ba1\u7b97\u51fa\u8fd9\u4e2a\u7ed3\u679c). </p> <pre><code>class torch.sparse.FloatTensor\n</code></pre> <pre><code>add()\n</code></pre> <pre><code>add_()\n</code></pre> <pre><code>clone()\n</code></pre> <pre><code>dim()\n</code></pre> <pre><code>div()\n</code></pre> <pre><code>div_()\n</code></pre> <pre><code>get_device()\n</code></pre> <pre><code>hspmm()\n</code></pre> <pre><code>mm()\n</code></pre> <pre><code>mul()\n</code></pre> <pre><code>mul_()\n</code></pre> <pre><code>narrow_copy()\n</code></pre> <pre><code>resizeAs_()\n</code></pre> <pre><code>size()\n</code></pre> <pre><code>spadd()\n</code></pre> <pre><code>spmm()\n</code></pre> <pre><code>sspaddmm()\n</code></pre> <pre><code>sspmm()\n</code></pre> <pre><code>sub()\n</code></pre> <pre><code>sub_()\n</code></pre> <pre><code>t_()\n</code></pre> <pre><code>toDense()\n</code></pre> <pre><code>transpose()\n</code></pre> <pre><code>transpose_()\n</code></pre> <pre><code>zero_()\n</code></pre> <pre><code>coalesce()\n</code></pre> <pre><code>is_coalesced()\n</code></pre> <pre><code>_indices()\n</code></pre> <pre><code>_values()\n</code></pre> <pre><code>_nnz()\n</code></pre>"},{"location":"1.0/sparse/#_1","title":"\u51fd\u6570","text":"<pre><code>torch.sparse.addmm(mat, mat1, mat2, beta=1, alpha=1)\n</code></pre> <p>\u8fd9\u4e2a\u51fd\u6570\u548c <code>torch.addmm()</code> \u5728<code>forward</code>\u4e2d\u505a\u540c\u6837\u7684\u4e8b\u60c5, \u9664\u4e86\u5b83\u652f\u6301\u7a00\u758f\u77e9\u9635<code>mat1</code> \u7684 <code>backward</code>. <code>mat1</code>\u5e94\u5177\u6709 <code>sparse_dim = 2</code>.  \u8bf7\u6ce8\u610f, <code>mat1</code>\u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u5408\u5e76\u7684\u7a00\u758f\u5f20\u91cf.</p> <p>\u53c2\u6570: </p> <ul> <li>mat (Tensor) \u2013 \u88ab\u76f8\u52a0\u7684\u7a20\u5bc6\u77e9\u9635</li> <li>mat1 (SparseTensor) \u2013 \u88ab\u76f8\u4e58\u7684\u7a00\u758f\u77e9\u9635</li> <li>mat2 (Tensor) \u2013 \u88ab\u76f8\u4e58\u7684\u7a20\u5bc6\u77e9\u9635</li> <li>beta (Number__, optional) \u2013 \u4e58\u6570 <code>mat</code> ()</li> <li>alpha (Number__, optional) \u2013 \u4e58\u6570  ()</li> </ul> <pre><code>torch.sparse.mm(mat1, mat2)\n</code></pre> <p>\u6267\u884c\u7a00\u758f\u77e9\u9635<code>mat1</code> \u548c \u7a20\u5bc6\u77e9\u9635 <code>mat2</code>\u7684\u77e9\u9635\u4e58\u6cd5. \u7c7b\u4f3c\u4e8e <code>torch.mm()</code>, \u5982\u679c <code>mat1</code> \u662f\u4e00\u4e2a  tensor, <code>mat2</code> \u662f\u4e00\u4e2a  tensor, \u8f93\u51fa\u5c06\u4f1a\u662f  \u7a20\u5bc6\u7684 tensor. <code>mat1</code> \u5e94\u5177\u6709 <code>sparse_dim = 2</code>. \u6b64\u51fd\u6570\u4e5f\u652f\u6301\u4e24\u4e2a\u77e9\u9635\u7684\u5411\u540e. \u8bf7\u6ce8\u610f, <code>mat1</code>\u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u5408\u5e76\u7684\u7a00\u758f\u5f20\u91cf</p> <p>\u53c2\u6570: </p> <ul> <li>mat1 (SparseTensor) \u2013 \u7b2c\u4e00\u4e2a\u8981\u76f8\u4e58\u7684\u7a00\u758f\u77e9\u9635</li> <li>mat2 (Tensor) \u2013 \u7b2c\u4e8c\u4e2a\u8981\u76f8\u4e58\u7684\u7a20\u5bc6\u77e9\u9635</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n&gt;&gt;&gt; a\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n [0, 1, 2, 0, 1, 2]]),\n values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n\n&gt;&gt;&gt; b = torch.randn(3, 2, requires_grad=True)\n&gt;&gt;&gt; b\ntensor([[-0.6479,  0.7874],\n [-1.2056,  0.5641],\n [-1.1716, -0.9923]], requires_grad=True)\n\n&gt;&gt;&gt; y = torch.sparse.mm(a, b)\n&gt;&gt;&gt; y\ntensor([[-0.3323,  1.8723],\n [-1.8951,  0.7904]], grad_fn=&lt;SparseAddmmBackward&gt;)\n&gt;&gt;&gt; y.sum().backward()\n&gt;&gt;&gt; a.grad\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n [0, 1, 2, 0, 1, 2]]),\n values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n size=(2, 3), nnz=6, layout=torch.sparse_coo)\n\n</code></pre> <pre><code>torch.sparse.sum(input, dim=None, dtype=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4\u5ea6<code>dim</code>\u4e2d\u6bcf\u884cSparseTensor <code>input</code>\u7684\u603b\u548c. \u5982\u679c :attr::<code>dim</code> \u662f\u4e00\u4e2a\u7ef4\u5ea6\u7684<code>list</code>, reduce\u5c06\u5728\u5168\u90e8\u7ed9\u5b9a\u7ef4\u5ea6\u8fdb\u884c.\u5982\u679c\u5305\u62ec\u5168\u90e8\u7684 <code>sparse_dim</code>, \u6b64\u65b9\u6cd5\u5c06\u8fd4\u56de Tensor \u4ee3\u66ff SparseTensor.</p> <p>\u6240\u6709\u88ab\u6c42\u548c\u7684 <code>dim</code> \u5c06\u88ab squeezed (see <code>torch.squeeze()</code>),\u5bfc\u81f4\u901f\u51fa tensor \u7684 :attr::<code>dim</code> \u5c0f\u4e8e <code>input</code>.</p> <p>backward \u8fc7\u7a0b\u4e2d, \u4ec5\u4ec5 <code>input</code> \u7684 <code>nnz</code> \u4f4d\u7f6e\u88ab\u53cd\u5411\u4f20\u64ad.  \u8bf7\u6ce8\u610f, <code>input</code>\u7684\u68af\u5ea6\u662f\u5408\u5e76\u7684. </p> <p>\u53c2\u6570: </p> <ul> <li>input (Tensor) \u2013 t\u8f93\u5165 SparseTensor</li> <li>dim (int or tuple of python:ints) \u2013 \u7ef4\u5ea6\u6216\u8005\u7ef4\u5ea6\u5217\u8868. Default: \u6240\u6709\u7ef4\u5ea6.</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de Tensor \u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: dtype \u548c <code>input</code> \u4e00\u81f4.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; nnz = 3\n&gt;&gt;&gt; dims = [5, 5, 2, 3]\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])\n&gt;&gt;&gt; size = torch.Size(dims)\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size)\n&gt;&gt;&gt; S\ntensor(indices=tensor([[2, 0, 3],\n [2, 4, 1]]),\n values=tensor([[[-0.6438, -1.6467,  1.4004],\n [ 0.3411,  0.0918, -0.2312]],\n\n [[ 0.5348,  0.0634, -2.0494],\n [-0.7125, -1.0646,  2.1844]],\n\n [[ 0.1276,  0.1874, -0.6334],\n [-1.9682, -0.5340,  0.7483]]]),\n size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n# when sum over only part of sparse_dims, return a SparseTensor\n&gt;&gt;&gt; torch.sparse.sum(S, [1, 3])\ntensor(indices=tensor([[0, 2, 3]]),\n values=tensor([[-1.4512,  0.4073],\n [-0.8901,  0.2017],\n [-0.3183, -1.7539]]),\n size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n# when sum over all sparse dim, return a dense Tensor\n# with summed dims squeezed\n&gt;&gt;&gt; torch.sparse.sum(S, [0, 1, 3])\ntensor([-2.6596, -1.1450])\n\n</code></pre>"},{"location":"1.0/spatial_transformer_tutorial/","title":"\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u6559\u7a0b","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d </p> <p>\u4f5c\u8005: Ghassen HAMROUNI </p> <p>  \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u79f0\u4e3a\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u7684\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u6765\u6269\u5145\u60a8\u7684\u7f51\u7edc\u3002\u4f60\u53ef\u4ee5\u5728 DeepMind paper\u9605\u8bfb\u6709\u5173\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u7684\u66f4\u591a\u5185\u5bb9\u3002</p> <p>\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u662f\u5bf9\u4efb\u4f55\u7a7a\u95f4\u53d8\u6362\u7684\u5dee\u5f02\u5316\u5173\u6ce8\u7684\u6982\u62ec\u3002\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc(\u7b80\u79f0STN\uff09\u5141\u8bb8\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5982\u4f55\u5728\u8f93\u5165\u56fe\u50cf\u4e0a\u6267\u884c\u7a7a\u95f4\u53d8\u6362\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u51e0\u4f55\u4e0d\u53d8\u6027\u3002\u4f8b\u5982\uff0c\u5b83\u53ef\u4ee5\u88c1\u526a\u611f\u5174\u8da3\u7684\u533a\u57df\uff0c\u7f29\u653e\u5e76\u6821\u6b63\u56fe\u50cf\u7684\u65b9\u5411\u3002\u5b83\u53ef\u80fd\u662f\u4e00\u79cd\u6709\u7528\u7684\u673a\u5236\uff0c\u56e0\u4e3aCNN\u5bf9\u4e8e\u65cb\u8f6c\u548c\u7f29\u653e\u4ee5\u53ca\u66f4\u4e00\u822c\u7684\u4eff\u5c04\u53d8\u6362\u5e76\u4e0d\u662f\u4e0d\u53d8\u7684\u3002 \u5173\u4e8eSTN\u7684\u6700\u68d2\u7684\u4e8b\u60c5\u4e4b\u4e00\u662f\u80fd\u591f\u7b80\u5355\u5730\u5c06\u5176\u63d2\u5165\u4efb\u4f55\u73b0\u6709\u7684CNN\uff0c\u53ea\u9700\u5f88\u5c11\u7684\u4fee\u6539\u3002  </p> <pre><code># License: BSD\n# \u4f5c\u8005: Ghassen Hamrouni\n\nfrom __future__ import print_function  \nimport torch  \nimport torch.nn as nn  \nimport torch.nn.functional as F  \nimport torch.optim as optim  \nimport torchvision  \nfrom torchvision import datasets, transforms  \nimport matplotlib.pyplot as plt  \nimport numpy as np  \n\nplt.ion()   # \u4ea4\u4e92\u6a21\u5f0f\n\n</code></pre>"},{"location":"1.0/spatial_transformer_tutorial/#_2","title":"\u52a0\u8f7d\u6570\u636e","text":"<p>\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u5c1d\u8bd5\u4e86\u7ecf\u5178\u7684MNIST\u6570\u636e\u96c6\u3002\u4f7f\u7528\u6807\u51c6\u5377\u79ef\u7f51\u7edc\u589e\u5f3a\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u3002   </p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training dataset\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# Test dataset\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)\n\n</code></pre> <p>\u8f93\u51fa:</p> <pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ./MNIST/raw/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ./MNIST/raw/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ./MNIST/raw/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz\nProcessing...\nDone!\n\n</code></pre>"},{"location":"1.0/spatial_transformer_tutorial/#_3","title":"\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u53d9\u8ff0","text":"<p>\u7a7a\u95f4\u53d8\u6362\u5668\u7f51\u7edc\u5f52\u7ed3\u4e3a\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a</p> <ul> <li>\u672c\u5730\u7f51\u7edc(Localisation Network\uff09\u662f\u5e38\u89c4CNN\uff0c\u5176\u5bf9\u53d8\u6362\u53c2\u6570\u8fdb\u884c\u56de\u5f52\u3002\u4e0d\u4f1a\u4ece\u8be5\u6570\u636e\u96c6\u4e2d\u660e\u786e\u5730\u5b66\u4e60\u8f6c\u6362\uff0c\u800c\u662f\u7f51\u7edc\u81ea\u52a8\u5b66\u4e60\u589e\u5f3a\u5168\u5c40\u51c6\u786e\u6027\u7684\u7a7a\u95f4\u53d8\u6362\u3002</li> <li>\u7f51\u683c\u751f\u6210\u5668( Grid Genator)\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u751f\u6210\u4e0e\u8f93\u51fa\u56fe\u50cf\u4e2d\u7684\u6bcf\u4e2a\u50cf\u7d20\u76f8\u5bf9\u5e94\u7684\u5750\u6807\u7f51\u683c\u3002</li> <li>\u91c7\u6837\u5668(Sampler\uff09\u4f7f\u7528\u53d8\u6362\u7684\u53c2\u6570\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8f93\u5165\u56fe\u50cf\u3002  </li> </ul> <p> </p> <p>\u7b14\u8bb0  </p> <p>\u6211\u4eec\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684Pytorch\uff0c\u5b83\u5e94\u8be5\u5305\u542baffine_grid\u548cgrid_sample\u6a21\u5757\u3002  </p> <pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nmodel = Net().to(device)\n\n</code></pre>"},{"location":"1.0/spatial_transformer_tutorial/#_4","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u73b0\u5728\u6211\u4eec\u4f7f\u7528SGD(\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff09\u7b97\u6cd5\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u7f51\u7edc\u6b63\u5728\u4ee5\u6709\u76d1\u7763\u7684\u65b9\u5f0f\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u8be5\u6a21\u578b\u4ee5\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u81ea\u52a8\u5b66\u4e60STN\u3002  </p> <pre><code>optimizer = optim.SGD(model.parameters(), lr=0.01)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# A simple test procedure to measure STN the performances on MNIST.\n#\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))\n\n</code></pre>"},{"location":"1.0/spatial_transformer_tutorial/#stn","title":"\u53ef\u89c6\u5316STN\u7ed3\u679c","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u68c0\u67e5\u6211\u4eec\u5b66\u4e60\u7684\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u7684\u7ed3\u679c\u3002 \u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5c0f\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u4fbf\u5728\u8bad\u7ec3\u65f6\u53ef\u89c6\u5316\u53d8\u6362\u3002  </p> <pre><code>def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# We want to visualize the output of the spatial transformers layer\n# after the training, we visualize a batch of input images and\n# the corresponding transformed batch using STN.\n\ndef visualize_stn():\n    with torch.no_grad():\n        # Get a batch of training data\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()\n\n</code></pre> <p></p> <p>\u8f93\u51fa\uff1a  </p> <pre><code>Train Epoch: 1 [0/60000 (0%)]   Loss: 2.336866\nTrain Epoch: 1 [32000/60000 (53%)]      Loss: 0.841600\n\nTest set: Average loss: 0.2624, Accuracy: 9212/10000 (92%)\n\nTrain Epoch: 2 [0/60000 (0%)]   Loss: 0.527656\nTrain Epoch: 2 [32000/60000 (53%)]      Loss: 0.428908\n\nTest set: Average loss: 0.1176, Accuracy: 9632/10000 (96%)\n\nTrain Epoch: 3 [0/60000 (0%)]   Loss: 0.305364\nTrain Epoch: 3 [32000/60000 (53%)]      Loss: 0.263615\n\nTest set: Average loss: 0.1099, Accuracy: 9677/10000 (97%)\n\nTrain Epoch: 4 [0/60000 (0%)]   Loss: 0.169776\nTrain Epoch: 4 [32000/60000 (53%)]      Loss: 0.408683\n\nTest set: Average loss: 0.0861, Accuracy: 9734/10000 (97%)\n\nTrain Epoch: 5 [0/60000 (0%)]   Loss: 0.286635\nTrain Epoch: 5 [32000/60000 (53%)]      Loss: 0.122162\n\nTest set: Average loss: 0.0817, Accuracy: 9743/10000 (97%)\n\nTrain Epoch: 6 [0/60000 (0%)]   Loss: 0.331074\nTrain Epoch: 6 [32000/60000 (53%)]      Loss: 0.126413\n\nTest set: Average loss: 0.0589, Accuracy: 9822/10000 (98%)\n\nTrain Epoch: 7 [0/60000 (0%)]   Loss: 0.109780\nTrain Epoch: 7 [32000/60000 (53%)]      Loss: 0.172199\n\nTest set: Average loss: 0.0629, Accuracy: 9814/10000 (98%)\n\nTrain Epoch: 8 [0/60000 (0%)]   Loss: 0.078934\nTrain Epoch: 8 [32000/60000 (53%)]      Loss: 0.156452\n\nTest set: Average loss: 0.0563, Accuracy: 9839/10000 (98%)\n\nTrain Epoch: 9 [0/60000 (0%)]   Loss: 0.063500\nTrain Epoch: 9 [32000/60000 (53%)]      Loss: 0.186023\n\nTest set: Average loss: 0.0713, Accuracy: 9799/10000 (98%)\n\nTrain Epoch: 10 [0/60000 (0%)]  Loss: 0.199808\nTrain Epoch: 10 [32000/60000 (53%)]     Loss: 0.083502\n\nTest set: Average loss: 0.0528, Accuracy: 9850/10000 (98%)\n\nTrain Epoch: 11 [0/60000 (0%)]  Loss: 0.092909\nTrain Epoch: 11 [32000/60000 (53%)]     Loss: 0.204410\n\nTest set: Average loss: 0.0471, Accuracy: 9857/10000 (99%)\n\nTrain Epoch: 12 [0/60000 (0%)]  Loss: 0.078322\nTrain Epoch: 12 [32000/60000 (53%)]     Loss: 0.041391\n\nTest set: Average loss: 0.0634, Accuracy: 9796/10000 (98%)\n\nTrain Epoch: 13 [0/60000 (0%)]  Loss: 0.061228\nTrain Epoch: 13 [32000/60000 (53%)]     Loss: 0.137952\n\nTest set: Average loss: 0.0654, Accuracy: 9802/10000 (98%)\n\nTrain Epoch: 14 [0/60000 (0%)]  Loss: 0.068635\nTrain Epoch: 14 [32000/60000 (53%)]     Loss: 0.084583\n\nTest set: Average loss: 0.0515, Accuracy: 9853/10000 (99%)\n\nTrain Epoch: 15 [0/60000 (0%)]  Loss: 0.263158\nTrain Epoch: 15 [32000/60000 (53%)]     Loss: 0.127036\n\nTest set: Average loss: 0.0493, Accuracy: 9851/10000 (99%)\n\nTrain Epoch: 16 [0/60000 (0%)]  Loss: 0.083642\nTrain Epoch: 16 [32000/60000 (53%)]     Loss: 0.028274\n\nTest set: Average loss: 0.0461, Accuracy: 9867/10000 (99%)\n\nTrain Epoch: 17 [0/60000 (0%)]  Loss: 0.076734\nTrain Epoch: 17 [32000/60000 (53%)]     Loss: 0.034796\n\nTest set: Average loss: 0.0409, Accuracy: 9864/10000 (99%)\n\nTrain Epoch: 18 [0/60000 (0%)]  Loss: 0.122501\nTrain Epoch: 18 [32000/60000 (53%)]     Loss: 0.152187\n\nTest set: Average loss: 0.0474, Accuracy: 9860/10000 (99%)\n\nTrain Epoch: 19 [0/60000 (0%)]  Loss: 0.050512\nTrain Epoch: 19 [32000/60000 (53%)]     Loss: 0.270055\n\nTest set: Average loss: 0.0416, Accuracy: 9878/10000 (99%)\n\nTrain Epoch: 20 [0/60000 (0%)]  Loss: 0.073357\nTrain Epoch: 20 [32000/60000 (53%)]     Loss: 0.017542\n\nTest set: Average loss: 0.0713, Accuracy: 9816/10000 (98%)\n\n</code></pre>"},{"location":"1.0/storage/","title":"torch.Storage","text":"<p>\u8bd1\u8005\uff1ayuange250</p> <p><code>torch.Storage</code> \u8ddf\u7edd\u5927\u90e8\u5206\u57fa\u4e8e\u8fde\u7eed\u5b58\u50a8\u7684\u6570\u636e\u7ed3\u6784\u7c7b\u4f3c\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5355\u4e00\u6570\u636e\u7c7b\u578b\u7684\u4e00\u7ef4\u8fde\u7eed\u6570\u7ec4(array)\u3002</p> <p>\u6bcf\u4e00\u4e2a <code>torch.Tensor</code> \u90fd\u6709\u4e00\u4e2a\u4e0e\u4e4b\u76f8\u5bf9\u5e94\u7684<code>torch.Storage</code>\u5bf9\u8c61\uff0c\u4e24\u8005\u5b58\u50a8\u6570\u636e\u7684\u6570\u636e\u7c7b\u578b(data type)\u4fdd\u6301\u4e00\u81f4\u3002</p> <p>\u4e0b\u9762\u4ee5\u6570\u636e\u7c7b\u578b\u4e3afloat\u7684<code>torch.FloatStorage</code> \u4e3a\u4f8b\u4ecb\u7ecd\u4e00\u4e0b<code>torch.Storage</code>\u7684\u6210\u5458\u51fd\u6570\u3002</p> <pre><code>class torch.FloatStorage\n</code></pre> <pre><code>byte()\n</code></pre> <p>byte()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3abyte</p> <pre><code>char()\n</code></pre> <p>char()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3achar</p> <pre><code>clone()\n</code></pre> <p>clone()\u51fd\u6570\u53ef\u4ee5\u8fd4\u56de\u4e00\u4e2a\u6b64storage\u5bf9\u8c61\u7684\u590d\u5236 </p> <pre><code>copy_()\n</code></pre> <pre><code>cpu()\n</code></pre> <p>\u5982\u679c\u6b64storage\u5bf9\u8c61\u4e00\u5f00\u59cb\u4e0d\u5728cpu\u8bbe\u5907\u4e0a\uff0c\u8c03\u7528cpu()\u51fd\u6570\u8fd4\u56de\u6b64storage\u5bf9\u8c61\u7684\u4e00\u4e2acpu\u4e0a\u7684\u590d\u5236</p> <pre><code>cuda(device=None, non_blocking=False, **kwargs)\n</code></pre> <p>cuda()\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u5b58\u50a8\u5728CUDA\u5185\u5b58\u4e2d\u7684\u590d\u5236\uff0c\u5176\u4e2ddevice\u53ef\u4ee5\u6307\u5b9acuda\u8bbe\u5907\u3002 \u4f46\u5982\u679c\u6b64storage\u5bf9\u8c61\u65e9\u5df2\u5728CUDA\u5185\u5b58\u4e2d\u5b58\u50a8\uff0c\u5e76\u4e14\u5176\u6240\u5728\u7684\u8bbe\u5907\u7f16\u53f7\u4e0ecuda()\u51fd\u6570\u4f20\u5165\u7684device\u53c2\u6570\u4e00\u81f4\uff0c\u5219\u4e0d\u4f1a\u53d1\u751f\u590d\u5236\u64cd\u4f5c\uff0c\u8fd4\u56de\u539f\u5bf9\u8c61\u3002</p> <p>cuda()\u51fd\u6570\u7684\u53c2\u6570\u4fe1\u606f: </p> <ul> <li>device (int) \u2013 \u6307\u5b9a\u7684GPU\u8bbe\u5907id. \u9ed8\u8ba4\u4e3a\u5f53\u524d\u8bbe\u5907\uff0c\u5373 <code>torch.cuda.current_device()</code>\u7684\u8fd4\u56de\u503c\u3002</li> <li>non_blocking (bool) \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue, \u5e76\u4e14\u6b64\u5bf9\u8c61\u7684\u8d44\u6e90\u5b58\u50a8\u5728\u56fa\u5b9a\u5185\u5b58\u4e0a(pinned memory)\uff0c\u90a3\u4e48\u6b64cuda()\u51fd\u6570\u4ea7\u751f\u7684\u590d\u5236\u5c06\u4e0ehost\u7aef\u7684\u539fstorage\u5bf9\u8c61\u4fdd\u6301\u540c\u6b65\u3002\u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u4f5c\u7528\u3002</li> <li>**kwargs \u2013 \u4e3a\u4e86\u4fdd\u8bc1\u517c\u5bb9\u6027\uff0c\u4e5f\u652f\u6301async\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u7684\u4f5c\u7528\u4e0eno_blocking\u53c2\u6570\u7684\u4f5c\u7528\u5b8c\u5168\u76f8\u540c\uff0c\u65e7\u7248\u672c\u7684\u9057\u7559\u95ee\u9898\u4e4b\u4e00\u3002</li> </ul> <pre><code>data_ptr()\n</code></pre> <pre><code>double()\n</code></pre> <p>double()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3adouble</p> <pre><code>element_size()\n</code></pre> <pre><code>fill_()\n</code></pre> <pre><code>float()\n</code></pre> <p>float()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3afloat</p> <pre><code>static from_buffer()\n</code></pre> <pre><code>static from_file(filename, shared=False, size=0) \u2192 Storage\n</code></pre> <p>\u5bf9\u4e8efrom_file()\u51fd\u6570\uff0c\u5982\u679c<code>shared</code>\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c \u90a3\u4e48\u6b64\u90e8\u5206\u5185\u5b58\u53ef\u4ee5\u5728\u8fdb\u7a0b\u95f4\u5171\u4eab\uff0c\u4efb\u4f55\u5bf9storage\u5bf9\u8c61\u7684\u66f4\u6539\u90fd\u4f1a\u88ab\u5199\u5165\u5b58\u50a8\u6587\u4ef6\u3002 \u5982\u679c <code>shared</code> \u88ab\u7f6e\u4e3a <code>False</code>, \u90a3\u4e48\u5728\u5185\u5b58\u4e2d\u5bf9storage\u5bf9\u8c61\u7684\u66f4\u6539\u5219\u4e0d\u4f1a\u5f71\u54cd\u5230\u50a8\u5b58\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u3002</p> <p><code>size</code> \u53c2\u6570\u662f\u6b64storage\u5bf9\u8c61\u4e2d\u7684\u5143\u7d20\u4e2a\u6570\u3002 \u5982\u679c<code>shared</code>\u88ab\u7f6e\u4e3a<code>False</code>, \u90a3\u4e48\u6b64\u5b58\u50a8\u6587\u4ef6\u5fc5\u987b\u8981\u5305\u542b<code>size * sizeof(Type)</code>\u5b57\u8282\u5927\u5c0f\u7684\u6570\u636e (<code>Type</code>\u662f\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b)\u3002 \u5982\u679c <code>shared</code> \u88ab\u7f6e\u4e3a <code>True</code>\uff0c\u90a3\u4e48\u6b64\u5b58\u50a8\u6587\u4ef6\u53ea\u6709\u5728\u9700\u8981\u7684\u65f6\u5019\u624d\u4f1a\u88ab\u521b\u5efa\u3002</p> <p>from_file()\u51fd\u6570\u7684\u53c2\u6570\uff1a </p> <ul> <li>filename (str) \u2013 \u5bf9\u5e94\u7684\u5b58\u50a8\u6587\u4ef6\u540d</li> <li>shared (bool) \u2013 \u662f\u5426\u5171\u4eab\u5185\u5b58</li> <li>size (int) \u2013 \u6b64storage\u5bf9\u8c61\u4e2d\u7684\u5143\u7d20\u4e2a\u6570</li> </ul> <pre><code>half()\n</code></pre> <p>half()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3ahalf</p> <pre><code>int()\n</code></pre> <p>int()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3aint</p> <pre><code>is_cuda = False\n</code></pre> <pre><code>is_pinned()\n</code></pre> <pre><code>is_shared()\n</code></pre> <pre><code>is_sparse = False\n</code></pre> <pre><code>long()\n</code></pre> <p>long()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3along</p> <pre><code>new()\n</code></pre> <pre><code>pin_memory()\n</code></pre> <p>\u5982\u679c\u6b64storage\u5bf9\u8c61\u8fd8\u6ca1\u6709\u88ab\u5b58\u50a8\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d\uff0c\u5219pin_memory()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u5b58\u50a8\u5230\u56fa\u5b9a\u5185\u5b58\u4e2d</p> <pre><code>resize_()\n</code></pre> <pre><code>share_memory_()\n</code></pre> <p>share_memory_()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u8f6c\u79fb\u5230\u5171\u4eab\u5185\u5b58\u4e2d\u3002</p> <p>\u5bf9\u4e8e\u65e9\u5df2\u5728\u5171\u4eab\u5185\u5b58\u4e2d\u7684storage\u5bf9\u8c61\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u65e0\u6548\uff1b\u5bf9\u4e8e\u5b58\u50a8\u5728CUDA\u8bbe\u5907\u4e0a\u7684storage\u5bf9\u8c61\uff0c\u65e0\u9700\u79fb\u52a8\u5373\u53ef\u5b9e\u73b0\u6b64\u7c7b\u5bf9\u8c61\u5728\u8fdb\u7a0b\u95f4\u7684\u5171\u4eab\uff0c\u6240\u4ee5\u6b64\u64cd\u4f5c\u5bf9\u4e8e\u5b83\u4eec\u6765\u8bf4\u4e5f\u65e0\u6548\u3002</p> <p>\u5728\u5171\u4eab\u5185\u5b58\u4e2d\u5b58\u50a8\u7684storage\u5bf9\u8c61\u65e0\u6cd5\u88ab\u66f4\u6539\u5927\u5c0f\u3002</p> <p>share_memory_()\u51fd\u6570\u8fd4\u56de\u503c: self</p> <pre><code>short()\n</code></pre> <p>short()\u51fd\u6570\u53ef\u4ee5\u5c06\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u4e3ashort</p> <pre><code>size()\n</code></pre> <pre><code>tolist()\n</code></pre> <p>tolist()\u51fd\u6570\u53ef\u4ee5\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u6b64storage\u5bf9\u8c61\u6240\u6709\u5143\u7d20\u7684\u5217\u8868</p> <pre><code>type(dtype=None, non_blocking=False, **kwargs)\n</code></pre> <p>\u5982\u679c\u51fd\u6570\u8c03\u7528\u65f6\u6ca1\u6709\u63d0\u4f9b<code>dtype</code>\u53c2\u6570\uff0c\u5219type()\u51fd\u6570\u7684\u8c03\u7528\u7ed3\u679c\u662f\u8fd4\u56de\u6b64storage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u63d0\u4f9b\u4e86\u6b64\u53c2\u6570\uff0c\u5219\u5c06\u6b64storage\u5bf9\u8c61\u8f6c\u5316\u4e3a\u6b64\u53c2\u6570\u6307\u5b9a\u7684\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6240\u63d0\u4f9b\u53c2\u6570\u6240\u6307\u5b9a\u7684\u6570\u636e\u7c7b\u578b\u4e0e\u5f53\u524dstorage\u5bf9\u8c61\u7684\u6570\u636e\u7c7b\u578b\u4e00\u81f4\uff0c\u5219\u4e0d\u4f1a\u8fdb\u884c\u590d\u5236\u64cd\u4f5c\uff0c\u5c06\u539f\u5bf9\u8c61\u8fd4\u56de\u3002</p> <p>type()\u51fd\u6570\u7684\u53c2\u6570\u4fe1\u606f: </p> <ul> <li>dtype (type or string) \u2013 \u60f3\u8981\u8f6c\u5316\u4e3a\u7684\u6570\u636e\u7c7b\u578b</li> <li>non_blocking (bool) \u2013 \u5982\u679c\u6b64\u53c2\u6570\u88ab\u8bbe\u7f6e\u4e3aTrue, \u5e76\u4e14\u6b64\u5bf9\u8c61\u7684\u8d44\u6e90\u5b58\u50a8\u5728\u56fa\u5b9a\u5185\u5b58\u4e0a(pinned memory)\uff0c\u90a3\u4e48\u6b64cuda()\u51fd\u6570\u4ea7\u751f\u7684\u590d\u5236\u5c06\u4e0ehost\u7aef\u7684\u539fstorage\u5bf9\u8c61\u4fdd\u6301\u540c\u6b65\u3002\u5426\u5219\u6b64\u53c2\u6570\u4e0d\u8d77\u4f5c\u7528\u3002</li> <li>**kwargs \u2013 \u4e3a\u4e86\u4fdd\u8bc1\u517c\u5bb9\u6027\uff0c\u4e5f\u652f\u6301async\u53c2\u6570\uff0c\u6b64\u53c2\u6570\u7684\u4f5c\u7528\u4e0eno_blocking\u53c2\u6570\u7684\u4f5c\u7528\u5b8c\u5168\u76f8\u540c\uff0c\u65e7\u7248\u672c\u7684\u9057\u7559\u95ee\u9898\u4e4b\u4e00 (\u5df2\u7ecf\u88abdeprecated)\u3002</li> </ul>"},{"location":"1.0/super_resolution_with_caffe2/","title":"\u4f7f\u7528ONNX\u5c06\u6a21\u578b\u4ecePyTorch\u4f20\u8f93\u5230Caffe2\u548cMobile(\u79fb\u52a8\u7aef)","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d </p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528ONNX\u5c06PyTorch\u4e2d\u5b9a\u4e49\u7684\u6a21\u578b\u8f6c\u6362\u4e3aONNX\u683c\u5f0f\uff0c\u7136\u540e\u5c06\u5176\u52a0\u8f7d\u5230Caffe2\u4e2d\u3002\u4e00\u65e6\u8fdb\u5165Caffe2\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u8fd0\u884c\u6a21\u578b\u6765\u4ed4\u7ec6\u68c0\u67e5\u5b83\u662f\u5426\u6b63\u786e\u5bfc\u51fa\uff0c\u7136\u540e\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528Caffe2\u529f\u80fd(\u5982\u79fb\u52a8\u5bfc\u51fa\u5668\uff09\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b\u3002  </p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u9700\u8981\u5b89\u88c5onnx\u548cCaffe2\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528pip install onnx\u83b7\u53d6onnx\u7684\u4e8c\u8fdb\u5236\u7248\u672c\u3002  </p> <p><code>\u6ce8\u610f</code>: \u672c\u6559\u7a0b\u9700\u8981PyTorch master\u5206\u652f\uff0c\u53ef\u4ee5\u6309\u7167 \u8fd9\u91cc\u8bf4\u660e\u8fdb\u884c\u5b89\u88c5\u3002  </p> <pre><code># \u4e00\u4e9b\u5305\u7684\u5bfc\u5165\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx\n\n</code></pre> <p>\u8d85\u5206\u8fa8\u7387\u662f\u4e00\u79cd\u63d0\u9ad8\u56fe\u50cf\uff0c\u89c6\u9891\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\uff0c\u5e7f\u6cdb\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u6216\u89c6\u9891\u526a\u8f91\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u9996\u5148\u4f7f\u7528\u5e26\u6709\u865a\u62df\u8f93\u5165\u7684\u5c0f\u578b\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002  </p> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5728PyTorch\u4e2d\u521b\u5efa\u4e00\u4e2aSuperResolution\u6a21\u578b\u3002\u8fd9\u4e2a\u6a21\u578b \u76f4\u63a5\u6765\u81eaPyTorch\u7684\u4f8b\u5b50\uff0c\u6ca1\u6709\u4fee\u6539\uff1a  </p> <pre><code># PyTorch\u4e2d\u5b9a\u4e49\u7684Super Resolution\u6a21\u578b\nimport torch.nn as nn\nimport torch.nn.init as init\n\nclass SuperResolutionNet(nn.Module):\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = nn.ReLU(inplace=inplace)\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n        init.orthogonal_(self.conv4.weight)\n\n# \u4f7f\u7528\u4e0a\u9762\u6a21\u578b\u5b9a\u4e49\uff0c\u521b\u5efasuper-resolution\u6a21\u578b \ntorch_model = SuperResolutionNet(upscale_factor=3)\n\n</code></pre> <p>\u901a\u5e38\uff0c\u4f60\u73b0\u5728\u4f1a\u8bad\u7ec3\u8fd9\u4e2a\u6a21\u578b; \u4f46\u662f\uff0c\u5bf9\u4e8e\u672c\u6559\u7a0b\u6211\u4eec\u5c06\u4e0b\u8f7d\u4e00\u4e9b\u9884\u5148\u8bad\u7ec3\u7684\u6743\u91cd\u3002\u8bf7\u6ce8\u610f\uff0c\u6b64\u6a21\u578b\u672a\u7ecf\u8fc7\u5145\u5206\u8bad\u7ec3\u6765\u83b7\u5f97\u826f\u597d\u7684\u51c6\u786e\u6027\uff0c\u6b64\u5904\u4ec5\u7528\u4e8e\u6f14\u793a\u76ee\u7684\u3002</p> <pre><code># \u52a0\u8f7d\u9884\u5148\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6743\u91cd\ndel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u521d\u59cb\u5316\u6a21\u578b\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))\n\n# \u5c06\u8bad\u7ec3\u6a21\u5f0f\u8bbe\u7f6e\u4e3afalsesince we will only run the forward pass.\ntorch_model.train(False)\n\n</code></pre> <p>\u5728PyTorch\u4e2d\u5bfc\u51fa\u6a21\u578b\u901a\u8fc7\u8ddf\u8e2a\u5de5\u4f5c\u3002\u8981\u5bfc\u51fa\u6a21\u578b\uff0c\u8bf7\u8c03\u7528torch.onnx._export(\uff09\u51fd\u6570\u3002\u8fd9\u5c06\u6267\u884c\u6a21\u578b\uff0c\u8bb0\u5f55\u8fd0\u7b97\u7b26\u7528\u4e8e\u8ba1\u7b97\u8f93\u51fa\u7684\u8f68\u8ff9\u3002\u56e0\u4e3a_export\u8fd0\u884c\u6a21\u578b\uff0c\u6211\u4eec\u9700\u8981\u63d0\u4f9b\u8f93\u5165\u5f20\u91cfx\u3002\u8fd9\u4e2a\u5f20\u91cf\u7684\u503c\u5e76\u4e0d\u91cd\u8981; \u5b83\u53ef\u4ee5\u662f\u56fe\u50cf\u6216\u968f\u673a\u5f20\u91cf\uff0c\u53ea\u8981\u5b83\u662f\u6b63\u786e\u7684\u5927\u5c0f\u3002  </p> <p>\u8981\u4e86\u89e3\u6709\u5173PyTorch\u5bfc\u51fa\u754c\u9762\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770btorch.onnx documentation\u6587\u6863\u3002</p> <pre><code># \u8f93\u5165\u6a21\u578b\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\n\n# \u5bfc\u51fa\u6a21\u578b\ntorch_out = torch.onnx._export(torch_model,             # model being run\n                               x,                       # model input (or a tuple for multiple inputs)\n                               \"super_resolution.onnx\", # where to save the model (can be a file or file-like object)\n                               export_params=True)      # store the trained parameter weights inside the model file\n\n</code></pre> <p><code>torch_out</code> \u662f\u6267\u884c\u6a21\u578b\u540e\u7684\u8f93\u51fa\u3002\u901a\u5e38\u60a8\u53ef\u4ee5\u5ffd\u7565\u6b64\u8f93\u51fa\uff0c\u4f46\u5728\u8fd9\u91cc\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u6765\u9a8c\u8bc1\u6211\u4eec\u5bfc\u51fa\u7684\u6a21\u578b\u5728Caffe2\u4e2d\u8fd0\u884c\u65f6\u8ba1\u7b97\u76f8\u540c\u7684\u503c\u3002</p> <p>\u73b0\u5728\u8ba9\u6211\u4eec\u91c7\u7528ONNX\u8868\u793a\u5e76\u5728Caffe2\u4e2d\u4f7f\u7528\u5b83\u3002\u8fd9\u90e8\u5206\u901a\u5e38\u53ef\u4ee5\u5728\u4e00\u4e2a\u5355\u72ec\u7684\u8fdb\u7a0b\u4e2d\u6216\u5728\u53e6\u4e00\u53f0\u673a\u5668\u4e0a\u5b8c\u6210\uff0c\u4f46\u6211\u4eec\u5c06\u7ee7\u7eed\u5728\u540c\u4e00\u4e2a\u8fdb\u7a0b\u4e2d\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u9a8c\u8bc1Caffe2\u548cPyTorch\u662f\u5426\u4e3a\u7f51\u7edc\u8ba1\u7b97\u76f8\u540c\u7684\u503c\uff1a  </p> <pre><code>import onnx\nimport caffe2.python.onnx.backend as onnx_caffe2_backend\n\n# Load the ONNX ModelProto object. model is a standard Python protobuf object\nmodel = onnx.load(\"super_resolution.onnx\")\n\n# prepare the caffe2 backend for executing the model this converts the ONNX model into a\n# Caffe2 NetDef that can execute it. Other ONNX backends, like one for CNTK will be\n# availiable soon.\nprepared_backend = onnx_caffe2_backend.prepare(model)\n\n# run the model in Caffe2\n\n# Construct a map from input names to Tensor data.\n# The graph of the model itself contains inputs for all weight parameters, after the input image.\n# Since the weights are already embedded, we just need to pass the input image.\n# Set the first input.\nW = {model.graph.input[0].name: x.data.numpy()}\n\n# Run the Caffe2 net:\nc2_out = prepared_backend.run(W)[0]\n\n# Verify the numerical correctness upto 3 decimal places\nnp.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)\n\nprint(\"Exported model has been executed on Caffe2 backend, and the result looks good!\")\n\n</code></pre> <p>\u6211\u4eec\u5e94\u8be5\u770b\u5230PyTorch\u548cCaffe2\u7684\u8f93\u51fa\u5728\u6570\u5b57\u4e0a\u5339\u914d\u6700\u591a3\u4f4d\u5c0f\u6570\u3002\u4f5c\u4e3a\u65c1\u6ce8\uff0c\u5982\u679c\u5b83\u4eec\u4e0d\u5339\u914d\u5219\u5b58\u5728Caffe2\u548cPyTorch\u4e2d\u7684\u8fd0\u7b97\u7b26\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u8bf7\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u4e0e\u6211\u4eec\u8054\u7cfb\u3002  </p>"},{"location":"1.0/super_resolution_with_caffe2/#onnxsrresnet","title":"\u4f7f\u7528ONNX\u8f6c\u6362SRResNET","text":"<p>\u4f7f\u7528\u4e0e\u4e0a\u8ff0\u76f8\u540c\u7684\u8fc7\u7a0b\uff0c\u6211\u4eec\u53c2\u8003\u672c\u6587\u4e2d\u63d0\u51fa\u7684\u8d85\u5206\u8fa8\u7387\u8f6c\u79fb\u4e86\u4e00\u4e2a\u6709\u8da3\u7684\u65b0\u6a21\u578b\u201cSRResNet\u201d(\u611f\u8c22Twitter\u4e0a\u7684\u4f5c\u8005\u4e3a\u672c\u6559\u7a0b\u7684\u76ee\u7684\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u53c2\u6570\uff09\u3002\u53ef\u5728\u6b64\u5904\u627e\u5230\u6a21\u578b\u5b9a\u4e49\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u4e0b\u9762\u662fSRResNet\u6a21\u578b\u8f93\u5165\uff0c\u8f93\u51fa\u7684\u6837\u5b50\u3002 </p>"},{"location":"1.0/super_resolution_with_caffe2/#_1","title":"\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u6a21\u578b","text":"<p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u5df2\u7ecf\u4ecePyTorch\u5bfc\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u52a0\u8f7d\u5b83\u5e76\u5728Caffe2\u4e2d\u8fd0\u884c\u5b83\u3002\u73b0\u5728\u6a21\u578b\u5df2\u52a0\u8f7d\u5230Caffe2\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8f6c\u6362\u4e3a\u9002\u5408\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u683c\u5f0f\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u683c\u5f0f\u3002  </p> <p>\u6211\u4eec\u5c06\u4f7f\u7528Caffe2\u7684mobile_exporter\u751f\u6210\u53ef\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u4e24\u4e2a\u6a21\u578bprotobufs\u3002 \u7b2c\u4e00\u4e2a\u7528\u4e8e\u4f7f\u7528\u6b63\u786e\u7684\u6743\u91cd\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u7b2c\u4e8c\u4e2a\u5b9e\u9645\u8fd0\u884c\u6267\u884c\u6a21\u578b\u3002\u5728\u672c\u6559\u7a0b\u7684\u5176\u4f59\u90e8\u5206\uff0c\u6211\u4eec\u5c06\u7ee7\u7eed\u4f7f\u7528\u5c0f\u578b\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002  </p> <pre><code># extract the workspace and the model proto from the internal representation\nc2_workspace = prepared_backend.workspace\nc2_model = prepared_backend.predict_net\n\n# \u73b0\u5728\u5bfc\u5165caffe2mobile_exporter\nfrom caffe2.python.predictor import mobile_exporter\n\n# call the Export to get the predict_net, init_net. These nets are needed for running things on mobile\ninit_net, predict_net = mobile_exporter.Export(c2_workspace, c2_model, c2_model.external_input)\n\n# \u6211\u4eec\u8fd8\u5c06init_net\u548cpredict_net\u4fdd\u5b58\u5230\u6211\u4eec\u7a0d\u540e\u5c06\u7528\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u5b83\u4eec\u7684\u6587\u4ef6\u4e2d\nwith open('init_net.pb', \"wb\") as fopen:\n    fopen.write(init_net.SerializeToString())\nwith open('predict_net.pb', \"wb\") as fopen:\n    fopen.write(predict_net.SerializeToString())\n\n</code></pre> <p><code>init_net</code>\u5177\u6709\u6a21\u578b\u53c2\u6570\u548c\u5d4c\u5165\u5728\u5176\u4e2d\u7684\u6a21\u578b\u8f93\u5165\uff0c<code>predict_ne</code>t\u5c06\u7528\u4e8e\u6307\u5bfc\u8fd0\u884c\u65f6\u7684<code>init_net</code>\u6267\u884c\u3002 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4e0a\u9762\u751f\u6210\u7684<code>init_net</code>\u548c<code>predict_net</code>\uff0c\u5e76\u5728\u6b63\u5e38\u7684Caffe2\u540e\u7aef\u548c\u79fb\u52a8\u8bbe\u5907\u4e2d\u8fd0\u884c\u5b83\u4eec\uff0c\u5e76\u9a8c\u8bc1\u4e24\u6b21\u8fd0\u884c\u4e2d\u751f\u6210\u7684\u8f93\u51fa\u9ad8\u5206\u8fa8\u7387\u732b\u54aa\u56fe\u50cf\u662f\u5426\u76f8\u540c\u3002  </p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u5e7f\u6cdb\u4f7f\u7528\u7684\u8457\u540d\u732b\u54aa\u56fe\u50cf\uff0c\u5982\u4e0b\u6240\u793a\uff1a   </p> <p> </p> <pre><code># Some standard imports\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils\n\nimport numpy as np\nimport os\nimport subprocess\nfrom PIL import Image\nfrom matplotlib import pyplot\nfrom skimage import io, transform\n\n</code></pre> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u52a0\u8f7d\u56fe\u50cf\uff0c\u4f7f\u7528\u6807\u51c6\u7684skimage python\u5e93\u5bf9\u5176\u8fdb\u884c\u9884\u5904\u7406\u3002 \u8bf7\u6ce8\u610f\uff0c\u6b64\u9884\u5904\u7406\u662f\u5904\u7406\u7528\u4e8e\u8bad\u7ec3/\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u636e\u7684\u6807\u51c6\u505a\u6cd5\u3002  </p> <pre><code># \u52a0\u8f7d\u56fe\u50cf\nimg_in = io.imread(\"./_static/img/cat.jpg\")\n\n# \u8bbe\u7f6e\u56fe\u7247\u5206\u8fa8\u7387\u4e3a 224x224\nimg = transform.resize(img_in, [224, 224])\n\n# \u4fdd\u5b58\u597d\u8bbe\u7f6e\u7684\u56fe\u7247\u4f5c\u4e3a\u6a21\u578b\u7684\u8f93\u5165\nio.imsave(\"./_static/img/cat_224x224.jpg\", img)\n\n</code></pre> <p>\u73b0\u5728\uff0c\u4f5c\u4e3a\u4e0b\u4e00\u6b65\uff0c\u8ba9\u6211\u4eec\u62cd\u6444\u8c03\u6574\u5927\u5c0f\u7684\u732b\u56fe\u50cf\u5e76\u5728Caffe2\u540e\u7aef\u8fd0\u884c\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5e76\u4fdd\u5b58\u8f93\u51fa\u56fe\u50cf\u3002 \u8fd9\u91cc\u7684\u56fe\u50cf\u5904\u7406\u6b65\u9aa4\u5df2\u7ecf\u4ecePyTorch\u5b9e\u73b0\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u4e2d\u91c7\u7528\u3002  </p> <pre><code># \u52a0\u8f7d\u8bbe\u7f6e\u597d\u7684\u56fe\u7247\u5e76\u66f4\u6539\u4e3aYCbCr\u7684\u683c\u5f0f\nimg = Image.open(\"./_static/img/cat_224x224.jpg\")\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\n# Let's run the mobile nets that we generated above so that caffe2 workspace is properly initialized\nworkspace.RunNetOnce(init_net)\nworkspace.RunNetOnce(predict_net)\n\n# Caffe2 has a nice net_printer to be able to inspect what the net looks like and identify\n# what our input and output blob names are.\nprint(net_printer.to_string(predict_net))\n\n</code></pre> <p><code>\u5907\u6ce8</code>\uff1a YCbCr </p> <p>\u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8f93\u5165\u540d\u4e3a\u201c9\u201d\uff0c\u8f93\u51fa\u540d\u4e3a\u201c27\u201d(\u6211\u4eec\u5c06\u6570\u5b57\u4f5c\u4e3ablob\u540d\u79f0\u6709\u70b9\u5947\u602a\uff0c\u4f46\u8fd9\u662f\u56e0\u4e3a\u8ddf\u8e2aJIT\u4e3a\u6a21\u578b\u751f\u6210\u4e86\u7f16\u53f7\u6761\u76ee\uff09\u3002\u6709\u70b9\u95ee\u9898 \u540e\u7eed\u6821\u6b63\u3002  </p> <pre><code># Now, let's also pass in the resized cat image for processing by the model.\nworkspace.FeedBlob(\"9\", np.array(img_y)[np.newaxis, np.newaxis, :, :].astype(np.float32))\n\n# run the predict_net to get the model output\nworkspace.RunNetOnce(predict_net)\n\n# Now let's get the model output blob\nimg_out = workspace.FetchBlob(\"27\")\n\n</code></pre> <p>\u6211\u4eec\u5df2\u7ecf\u5b8c\u6210\u4e86\u5728\u7eafCaffe2\u540e\u7aef\u8fd0\u884c\u6211\u4eec\u7684\u79fb\u52a8\u7f51\u7edc\uff0c\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u5728Android\u8bbe\u5907\u4e0a\u6267\u884c\u8be5\u6a21\u578b\u5e76\u83b7\u53d6\u6a21\u578b\u8f93\u51fa\u3002  </p> <p><code>\u6ce8\u610f</code>\uff1a\u5bf9\u4e8eAndroid\u5f00\u53d1\uff0c\u9700\u8981<code>adb shell</code>\uff0c\u5426\u5219\u6559\u7a0b\u7684\u4ee5\u4e0b\u90e8\u5206\u5c06\u65e0\u6cd5\u8fd0\u884c\u3002  </p> <p>\u5728\u6211\u4eec\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u6a21\u578b\u7684\u7b2c\u4e00\u6b65\u4e2d\uff0c\u6211\u4eec\u628a\u57fa\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u672c\u673a\u901f\u5ea6\u6d4b\u8bd5\u57fa\u51c6\u4e8c\u8fdb\u5236\u6587\u4ef6\u63a8\u9001\u5230adb\u3002\u8fd9\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u5bfc\u51fa\u6211\u4eec\u7a0d\u540e\u53ef\u4ee5\u68c0\u7d22\u7684\u6a21\u578b\u8f93\u51fa\u3002\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u5728\u6b64\u5904\u83b7\u5f97\u3002\u8981\u6784\u5efa\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u8bf7\u6309\u7167\u6b64\u5904\u7684\u8bf4\u660e\u6267\u884c<code>build_android.sh</code>\u811a\u672c\u3002  </p> <p><code>\u6ce8\u610f</code>\uff1a \u4f60\u9700\u8981\u5df2\u7ecf\u5b89\u88c5\u4e86<code>ANDROID_NDK</code>,\u5e76\u4e14\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf<code>ANDROID_NDK=path to ndk root</code>\u3002  </p> <pre><code># let's first push a bunch of stuff to adb, specify the path for the binary\nCAFFE2_MOBILE_BINARY = ('caffe2/binaries/speed_benchmark')\n\n# we had saved our init_net and proto_net in steps above, we use them now.\n# Push the binary and the model protos\nos.system('adb push ' + CAFFE2_MOBILE_BINARY + ' /data/local/tmp/')\nos.system('adb push init_net.pb /data/local/tmp')\nos.system('adb push predict_net.pb /data/local/tmp')\n\n# Let's serialize the input image blob to a blob proto and then send it to mobile for execution.\nwith open(\"input.blobproto\", \"wb\") as fid:\n    fid.write(workspace.SerializeBlob(\"9\"))\n\n# push the input image blob to adb\nos.system('adb push input.blobproto /data/local/tmp/')\n\n# Now we run the net on mobile, look at the speed_benchmark --help for what various options mean\nos.system(\n    'adb shell /data/local/tmp/speed_benchmark '                     # binary to execute\n    '--init_net=/data/local/tmp/super_resolution_mobile_init.pb '    # mobile init_net\n    '--net=/data/local/tmp/super_resolution_mobile_predict.pb '      # mobile predict_net\n    '--input=9 '                                                     # name of our input image blob\n    '--input_file=/data/local/tmp/input.blobproto '                  # serialized input image\n    '--output_folder=/data/local/tmp '                               # destination folder for saving mobile output\n    '--output=27,9 '                                                 # output blobs we are interested in\n    '--iter=1 '                                                      # number of net iterations to execute\n    '--caffe2_log_level=0 '\n)\n\n# get the model output from adb and save to a file\nos.system('adb pull /data/local/tmp/27 ./output.blobproto')\n\n# We can recover the output content and post-process the model using same steps as we followed earlier\nblob_proto = caffe2_pb2.BlobProto()\nblob_proto.ParseFromString(open('./output.blobproto').read())\nimg_out = utils.Caffe2TensorToNumpyArray(blob_proto.tensor)\nimg_out_y = Image.fromarray(np.uint8((img_out[0,0]).clip(0, 255)), mode='L')\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\nfinal_img.save(\"./_static/img/cat_superres_mobile.jpg\")\n\n</code></pre> <p>\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u6bd4\u8f83\u56fe\u50cfcat_superres.jpg(\u6765\u81ea\u7eafcaffe2\u540e\u7aef\u6267\u884c\u7684\u6a21\u578b\u8f93\u51fa\uff09\u548ccat_superres_mobile.jpg(\u6765\u81ea\u79fb\u52a8\u6267\u884c\u7684\u6a21\u578b\u8f93\u51fa\uff09\uff0c\u5e76\u770b\u5230\u4e24\u4e2a\u56fe\u50cf\u770b\u8d77\u6765\u76f8\u540c\u3002 \u5982\u679c\u5b83\u4eec\u770b\u8d77\u6765\u4e0d\u4e00\u6837\uff0c\u90a3\u4e48\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u6267\u884c\u4f1a\u51fa\u73b0\u95ee\u9898\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8bf7\u8054\u7cfbCaffe2\u793e\u533a\u3002\u4f60\u5e94\u8be5\u671f\u671b\u770b\u5230\u8f93\u51fa\u56fe\u50cf\u5982\u4e0b\u6240\u793a\uff1a  </p> <p> </p> <p>\u4f7f\u7528\u4e0a\u8ff0\u6b65\u9aa4\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u6a21\u578b\u3002 \u53e6\u5916\uff0c\u6709\u5173caffe2\u79fb\u52a8\u540e\u7aef\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770bcaffe2-android-demo\u3002   </p>"},{"location":"1.0/tensor_attributes/","title":"Tensor(\u5f20\u91cf\uff09\u7684\u5c5e\u6027","text":"<p>\u8bd1\u8005\uff1a\u963f\u8fdc</p> <p>\u6bcf\u4e2a <code>torch.Tensor</code> \u5bf9\u8c61\u90fd\u6709\u4ee5\u4e0b\u51e0\u4e2a\u5c5e\u6027\uff1a <code>torch.dtype</code>, <code>torch.device</code>\uff0c \u548c <code>torch.layout</code>\u3002</p>"},{"location":"1.0/tensor_attributes/#torchdtype","title":"torch.dtype","text":"<pre><code>class torch.dtype\n</code></pre> <p><code>torch.dtype</code> \u5c5e\u6027\u6807\u8bc6\u4e86 <code>torch.Tensor</code>\u7684\u6570\u636e\u7c7b\u578b\u3002PyTorch \u6709\u516b\u79cd\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b\uff1a</p> Data type dtype Tensor types 32-bit floating point <code>torch.float32</code> or <code>torch.float</code> <code>torch.*.FloatTensor</code> 64-bit floating point <code>torch.float64</code> or <code>torch.double</code> <code>torch.*.DoubleTensor</code> 16-bit floating point <code>torch.float16</code> or <code>torch.half</code> <code>torch.*.HalfTensor</code> 8-bit integer (unsigned) <code>torch.uint8</code> <code>torch.*.ByteTensor</code> 8-bit integer (signed) <code>torch.int8</code> <code>torch.*.CharTensor</code> 16-bit integer (signed) <code>torch.int16</code> or <code>torch.short</code> <code>torch.*.ShortTensor</code> 32-bit integer (signed) <code>torch.int32</code> or <code>torch.int</code> <code>torch.*.IntTensor</code> 64-bit integer (signed) <code>torch.int64</code> or <code>torch.long</code> <code>torch.*.LongTensor</code>"},{"location":"1.0/tensor_attributes/#torchdevice","title":"torch.device","text":"<pre><code>class torch.device\n</code></pre> <p><code>torch.device</code> \u5c5e\u6027\u6807\u8bc6\u4e86<code>torch.Tensor</code>\u5bf9\u8c61\u5728\u521b\u5efa\u4e4b\u540e\u6240\u5b58\u50a8\u5728\u7684\u8bbe\u5907\u540d\u79f0\uff0c\u800c\u5728\u5bf9\u8c61\u521b\u5efa\u4e4b\u524d\u6b64\u5c5e\u6027\u6807\u8bc6\u4e86\u5373\u5c06\u4e3a\u6b64\u5bf9\u8c61\u7533\u8bf7\u5b58\u50a8\u7a7a\u95f4\u7684\u8bbe\u5907\u540d\u79f0\u3002</p> <p><code>torch.device</code> \u5305\u542b\u4e86\u4e24\u79cd\u8bbe\u5907\u7c7b\u578b (<code>'cpu'</code> \u6216\u8005 <code>'cuda'</code>) \uff0c\u5206\u522b\u6807\u8bc6\u5c06Tensor\u5bf9\u8c61\u50a8\u5b58\u4e8ecpu\u5185\u5b58\u6216\u8005gpu\u5185\u5b58\u4e2d\uff0c\u540c\u65f6\u652f\u6301\u6307\u5b9a\u8bbe\u5907\u7f16\u53f7\uff0c\u6bd4\u5982\u591a\u5f20gpu\uff0c\u53ef\u4ee5\u901a\u8fc7gpu\u7f16\u53f7\u6307\u5b9a\u67d0\u4e00\u5757gpu\u3002 \u5982\u679c\u6ca1\u6709\u6307\u5b9a\u8bbe\u5907\u7f16\u53f7\uff0c\u5219\u9ed8\u8ba4\u5c06\u5bf9\u8c61\u5b58\u50a8\u4e8ecurrent_device()\u5f53\u524d\u8bbe\u5907\u4e2d\uff1b \u4e3e\u4e2a\u4f8b\u5b50\uff0c \u4e00\u4e2a<code>torch.Tensor</code> \u5bf9\u8c61\u6784\u9020\u51fd\u6570\u4e2d\u7684\u8bbe\u5907\u5b57\u6bb5\u5982\u679c\u586b\u5199<code>'cuda'</code>\uff0c\u90a3\u7b49\u4ef7\u4e8e\u586b\u5199\u4e86<code>'cuda:X'</code>\uff0c\u5176\u4e2dX\u662f\u51fd\u6570 <code>torch.cuda.current_device()</code>\u7684\u8fd4\u56de\u503c\u3002</p> <p>\u5728<code>torch.Tensor</code>\u5bf9\u8c61\u521b\u5efa\u4e4b\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee<code>Tensor.device</code>\u5c5e\u6027\u5b9e\u65f6\u8bbf\u95ee\u5f53\u524d\u5bf9\u8c61\u6240\u5b58\u50a8\u5728\u7684\u8bbe\u5907\u540d\u79f0\u3002</p> <p><code>torch.device</code> \u5bf9\u8c61\u652f\u6301\u4f7f\u7528\u5b57\u7b26\u4e32\u6216\u8005\u5b57\u7b26\u4e32\u52a0\u8bbe\u5907\u7f16\u53f7\u8fd9\u4e24\u79cd\u65b9\u5f0f\u6765\u521b\u5efa\uff1a</p> <p>\u901a\u8fc7\u5b57\u7b26\u4e32\u521b\u5efa\uff1a</p> <pre><code>&gt;&gt;&gt; torch.device('cuda:0')\ndevice(type='cuda', index=0)  # \u7f16\u53f7\u4e3a0\u7684cuda\u8bbe\u5907\n\n&gt;&gt;&gt; torch.device('cpu')  # cpu\u5185\u5b58\ndevice(type='cpu')\n\n&gt;&gt;&gt; torch.device('cuda')  # \u5f53\u524dcuda\u8bbe\u5907\ndevice(type='cuda')\n\n</code></pre> <p>\u901a\u8fc7\u5b57\u7b26\u4e32\u52a0\u8bbe\u5907\u7f16\u53f7\u521b\u5efa\uff1a</p> <pre><code>&gt;&gt;&gt; torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n&gt;&gt;&gt; torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n\n</code></pre> <p>Note</p> <p>\u5f53<code>torch.device</code>\u4f5c\u4e3a\u51fd\u6570\u7684\u53c2\u6570\u7684\u65f6\u5019\uff0c \u53ef\u4ee5\u76f4\u63a5\u7528\u5b57\u7b26\u4e32\u66ff\u6362\u3002 \u8fd9\u6837\u6709\u52a9\u4e8e\u52a0\u5feb\u4ee3\u7801\u521b\u5efa\u539f\u578b\u7684\u901f\u5ea6\u3002</p> <pre><code>&gt;&gt;&gt; # \u4e00\u4e2a\u63a5\u53d7torch.device\u5bf9\u8c61\u4e3a\u53c2\u6570\u7684\u51fd\u6570\u4f8b\u5b50\n&gt;&gt;&gt; cuda1 = torch.device('cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=cuda1)\n\n</code></pre> <pre><code>&gt;&gt;&gt; # \u53ef\u4ee5\u7528\u4e00\u4e2a\u5b57\u7b26\u4e32\u66ff\u6362\u6389torch.device\u5bf9\u8c61\uff0c\u4e00\u6837\u7684\u6548\u679c\n&gt;&gt;&gt; torch.randn((2,3), 'cuda:1')\n\n</code></pre> <p>Note</p> <p>\u7531\u4e8e\u4e00\u4e9b\u5386\u53f2\u9057\u7559\u95ee\u9898, device\u5bf9\u8c61\u8fd8\u53ef\u4ee5\u4ec5\u901a\u8fc7\u4e00\u4e2a\u8bbe\u5907\u7f16\u53f7\u6765\u521b\u5efa\uff0c\u8fd9\u4e9b\u8bbe\u5907\u7f16\u53f7\u5bf9\u5e94\u7684\u90fd\u662f\u76f8\u5e94\u7684cuda\u8bbe\u5907\u3002 \u8fd9\u6b63\u597d\u5bf9\u5e94\u4e86 <code>Tensor.get_device()</code>\u51fd\u6570, \u8fd9\u4e2a\u4ec5\u652f\u6301cuda Tensor\u7684\u51fd\u6570\u8fd4\u56de\u7684\u5c31\u662f\u5f53\u524dtensor\u6240\u5728\u7684cuda\u8bbe\u5907\u7f16\u53f7\uff0ccpu Tensor\u4e0d\u652f\u6301\u8fd9\u4e2a\u51fd\u6570\u3002</p> <pre><code>&gt;&gt;&gt; torch.device(1)\ndevice(type='cuda', index=1)\n\n</code></pre> <p>Note</p> <p>\u63a5\u53d7device\u53c2\u6570\u7684\u51fd\u6570\u540c\u65f6\u4e5f\u53ef\u4ee5\u63a5\u53d7\u4e00\u4e2a\u6b63\u786e\u683c\u5f0f\u7684\u5b57\u7b26\u4e32\u6216\u8005\u6b63\u786e\u4ee3\u8868\u8bbe\u5907\u7f16\u53f7\u7684\u6570\u5b57(\u6570\u5b57\u8fd9\u4e2a\u662f\u5386\u53f2\u9057\u7559\u95ee\u9898\uff09\u4f5c\u4e3a\u53c2\u6570\uff0c\u4ee5\u4e0b\u7684\u64cd\u4f5c\u662f\u7b49\u4ef7\u7684\uff1a</p> <pre><code>&gt;&gt;&gt; torch.randn((2,3), device=torch.device('cuda:1'))\n&gt;&gt;&gt; torch.randn((2,3), device='cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=1)  # \u5386\u53f2\u9057\u7559\u505a\u6cd5\n\n</code></pre>"},{"location":"1.0/tensor_attributes/#torchlayout","title":"torch.layout","text":"<pre><code>class torch.layout\n</code></pre> <p><code>torch.layout</code> \u5c5e\u6027\u6807\u8bc6\u4e86<code>torch.Tensor</code> \u5728\u5185\u5b58\u4e2d\u7684\u5e03\u5c40\u6a21\u5f0f\u3002 \u73b0\u5728\uff0c \u6211\u4eec\u652f\u6301\u4e86\u4e24\u79cd\u5185\u5b58\u5e03\u5c40\u6a21\u5f0f <code>torch.strided</code> (dense Tensors) \u548c\u5c1a\u5904\u8bd5\u9a8c\u9636\u6bb5\u7684<code>torch.sparse_coo</code> (sparse COO Tensors\uff0c \u4e00\u79cd\u7ecf\u5178\u7684\u7a00\u758f\u77e9\u9635\u5b58\u50a8\u65b9\u5f0f).</p> <p><code>torch.strided</code> \u8de8\u6b65\u5b58\u50a8\u4ee3\u8868\u4e86\u5bc6\u96c6\u5f20\u91cf\u7684\u5b58\u50a8\u5e03\u5c40\u65b9\u5f0f\uff0c\u5f53\u7136\u4e5f\u662f\u6700\u5e38\u7528\u6700\u7ecf\u5178\u7684\u4e00\u79cd\u5e03\u5c40\u65b9\u5f0f\u3002 \u6bcf\u4e00\u4e2astrided tensor\u90fd\u6709\u4e00\u4e2a\u4e0e\u4e4b\u76f8\u8fde\u7684<code>torch.Storage</code>\u5bf9\u8c61, \u8fd9\u4e2a\u5bf9\u8c61\u5b58\u50a8\u7740tensor\u7684\u6570\u636e. \u8fd9\u4e9bStorage\u5bf9\u8c61\u4e3atensor\u63d0\u4f9b\u4e86\u4e00\u79cd\u591a\u7ef4\u7684\uff0c \u8de8\u6b65\u7684(strided)\u6570\u636e\u89c6\u56fe. \u8fd9\u4e00\u89c6\u56fe\u4e2d\u7684strides\u662f\u4e00\u4e2ainterger\u6574\u5f62\u5217\u8868\uff1a\u8fd9\u4e2a\u5217\u8868\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u7ed9\u51fa\u5f53\u524d\u5f20\u91cf\u7684\u5404\u4e2a\u7ef4\u5ea6\u7684\u6240\u5360\u5185\u5b58\u5927\u5c0f\uff0c\u4e25\u683c\u7684\u5b9a\u4e49\u5c31\u662f\uff0cstrides\u4e2d\u7684\u7b2ck\u4e2a\u5143\u7d20\u4ee3\u8868\u4e86\u5728\u7b2ck\u7ef4\u5ea6\u4e0b\uff0c\u4ece\u4e00\u4e2a\u5143\u7d20\u8df3\u8f6c\u5230\u4e0b\u4e00\u4e2a\u5143\u7d20\u6240\u9700\u8981\u8de8\u8d8a\u7684\u5185\u5b58\u5927\u5c0f\u3002 \u8de8\u6b65\u8fd9\u4e2a\u6982\u5ff5\u6709\u52a9\u4e8e\u63d0\u9ad8\u591a\u79cd\u5f20\u91cf\u8fd0\u7b97\u7684\u6548\u7387\u3002</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; x.stride() \n(5, 1)     # \u6b64\u65f6\u5728\u8fd9\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u4e2d\uff0c\u5728\u7b2c0\u7ef4\u5ea6\u4e0b\uff0c\u4ece\u4e00\u4e2a\u5143\u7d20\u5230\u4e0b\u4e00\u4e2a\u5143\u7d20\u9700\u8981\u8de8\u8d8a\u7684\u5185\u5b58\u5927\u5c0f\u662f5\uff0c\u6bd4\u5982x[0] \u5230x[1]\u9700\u8981\u8de8\u8d8ax[0]\u8fd95\u4e2a\u5143\u7d20, \u5728\u7b2c1\u7ef4\u5ea6\u4e0b\uff0c\u662f1\uff0c\u5982x[0, 0]\u5230x[0, 1]\u9700\u8981\u8de8\u8d8a1\u4e2a\u5143\u7d20\n\n&gt;&gt;&gt; x.t().stride()\n(1, 5)\n\n</code></pre> <p>\u66f4\u591a\u5173\u4e8e <code>torch.sparse_coo</code> tensors\u7684\u4fe1\u606f, \u8bf7\u770btorch.sparse.</p>"},{"location":"1.0/tensors/","title":"torch.Tensor","text":"<p>\u8bd1\u8005\uff1ahijkzzz</p> <p><code>torch.Tensor</code> \u662f\u4e00\u79cd\u5305\u542b\u5355\u4e00\u6570\u636e\u7c7b\u578b\u5143\u7d20\u7684\u591a\u7ef4\u77e9\u9635.</p> <p>Torch\u5b9a\u4e49\u4e86\u516b\u79cdCPU\u5f20\u91cf\u7c7b\u578b\u548c\u516b\u79cdGPU\u5f20\u91cf\u7c7b\u578b\uff1a</p> Data type dtype CPU tensor GPU tensor 32-bit floating point <code>torch.float32</code> or <code>torch.float</code> <code>torch.FloatTensor</code> <code>torch.cuda.FloatTensor</code> 64-bit floating point <code>torch.float64</code> or <code>torch.double</code> <code>torch.DoubleTensor</code> <code>torch.cuda.DoubleTensor</code> 16-bit floating point <code>torch.float16</code> or <code>torch.half</code> <code>torch.HalfTensor</code> <code>torch.cuda.HalfTensor</code> 8-bit integer (unsigned) <code>torch.uint8</code> <code>torch.ByteTensor</code> <code>torch.cuda.ByteTensor</code> 8-bit integer (signed) <code>torch.int8</code> <code>torch.CharTensor</code> <code>torch.cuda.CharTensor</code> 16-bit integer (signed) <code>torch.int16</code> or <code>torch.short</code> <code>torch.ShortTensor</code> <code>torch.cuda.ShortTensor</code> 32-bit integer (signed) <code>torch.int32</code> or <code>torch.int</code> <code>torch.IntTensor</code> <code>torch.cuda.IntTensor</code> 64-bit integer (signed) <code>torch.int64</code> or <code>torch.long</code> <code>torch.LongTensor</code> <code>torch.cuda.LongTensor</code> <p><code>torch.Tensor</code> \u662f\u9ed8\u8ba4\u7684tensor\u7c7b\u578b (<code>torch.FloatTensor</code>) \u7684\u7b80\u79f0.</p> <p>Tensor \u53ef\u4ee5\u7528<code>torch.tensor()</code>\u8f6c\u6362Python\u7684 <code>list</code> \u6216\u5e8f\u5217\u200b\u200b\u751f\u6210\uff1a</p> <pre><code>&gt;&gt;&gt; torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n [ 1.0000, -1.0000]])\n&gt;&gt;&gt; torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n [ 4,  5,  6]])\n\n</code></pre> <p>\u8b66\u544a</p> <p><code>torch.tensor()</code> \u603b\u662f\u62f7\u8d1d <code>data</code>. \u5982\u679c\u4f60\u6709\u4e00\u4e2a Tensor <code>data</code> \u5e76\u4e14\u4ec5\u4ec5\u60f3\u6539\u53d8\u5b83\u7684 <code>requires_grad</code> \u5c5e\u6027, \u53ef\u7528 <code>requires_grad_()</code> or <code>detach()</code> \u6765\u907f\u514d\u62f7\u8d1d. \u5982\u679c\u4f60\u6709\u4e00\u4e2a numpy \u6570\u7ec4\u5e76\u4e14\u60f3\u907f\u514d\u62f7\u8d1d, \u8bf7\u4f7f\u7528 <code>torch.as_tensor()</code>.</p> <p>\u6307\u5b9a\u6570\u636e\u7c7b\u578b\u7684Tensor\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u53c2\u6570 <code>torch.dtype</code> \u548c/\u6216\u8005  <code>torch.device</code> \u5230\u6784\u9020\u51fd\u6570\u751f\u6210\uff1a</p> <pre><code>&gt;&gt;&gt; torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n [ 0,  0,  0,  0]], dtype=torch.int32)\n&gt;&gt;&gt; cuda0 = torch.device('cuda:0')\n&gt;&gt;&gt; torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n\n</code></pre> <p>Tensor\u7684\u5185\u5bb9\u53ef\u4ee5\u901a\u8fc7Python\u7d22\u5f15\u6216\u8005\u5207\u7247\u8bbf\u95ee\u4ee5\u53ca\u4fee\u6539\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; print(x[1][2])\ntensor(6)\n&gt;&gt;&gt; x[0][1] = 8\n&gt;&gt;&gt; print(x)\ntensor([[ 1,  8,  3],\n [ 4,  5,  6]])\n\n</code></pre> <p>\u4f7f\u7528 <code>torch.Tensor.item()</code> \u4ece\u53ea\u6709\u4e00\u4e2a\u503c\u7684Tensor\u4e2d\u83b7\u53d6Python Number\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1]])\n&gt;&gt;&gt; x\ntensor([[ 1]])\n&gt;&gt;&gt; x.item()\n1\n&gt;&gt;&gt; x = torch.tensor(2.5)\n&gt;&gt;&gt; x\ntensor(2.5000)\n&gt;&gt;&gt; x.item()\n2.5\n\n</code></pre> <p>Tensor\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570 <code>requires_grad=True</code> \u521b\u5efa, \u8fd9\u6837 <code>torch.autograd</code> \u4f1a\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97\u5b9e\u73b0\u81ea\u52a8\u6c42\u5bfc.</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n&gt;&gt;&gt; out = x.pow(2).sum()\n&gt;&gt;&gt; out.backward()\n&gt;&gt;&gt; x.grad\ntensor([[ 2.0000, -2.0000],\n [ 2.0000,  2.0000]])\n\n</code></pre> <p>\u6bcf\u4e00\u4e2atensor\u90fd\u6709\u4e00\u4e2a\u76f8\u5e94\u7684 <code>torch.Storage</code> \u4fdd\u5b58\u5176\u6570\u636e. tensor \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u7ef4\u7684\u3001strided\u89c6\u56fe, \u5e76\u5b9a\u4e49\u4e86\u6570\u503c\u64cd\u4f5c.</p> <p>\u6ce8\u610f</p> <p>\u66f4\u591a\u5173\u4e8e <code>torch.dtype</code>, <code>torch.device</code>, \u548c <code>torch.layout</code>  \u7b49 <code>torch.Tensor</code>\u7684\u5c5e\u6027, \u89c1 Tensor Attributes.</p> <p>\u6ce8\u610f</p> <p>\u6ce8\u610f\uff1a\u4fee\u6539tensor\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e00\u4e2a\u4e0b\u5212\u7ebf\u540e\u7f00\u6765\u6807\u793a.\u6bd4\u5982, <code>torch.FloatTensor.abs_()</code> \u4f1a\u5728\u539f\u5730\u8ba1\u7b97\u7edd\u5bf9\u503c\u5e76\u8fd4\u56de\u4fee\u6539\u7684\u5f20\u91cf, \u800c <code>torch.FloatTensor.abs()</code> \u5c06\u4f1a\u5728\u65b0\u5f20\u91cf\u4e2d\u8ba1\u7b97\u7ed3\u679c.</p> <p>\u6ce8\u610f</p> <p>\u4e3a\u4e86\u6539\u53d8\u5df2\u6709\u7684 tensor \u7684 <code>torch.device</code> \u548c/\u6216\u8005 <code>torch.dtype</code>, \u8003\u8651\u4f7f\u7528 <code>to()</code> \u65b9\u6cd5.</p> <pre><code>class torch.Tensor\n</code></pre> <p>\u8fd9\u91cc\u6709\u5c11\u6570\u51e0\u79cd\u751f\u6210Tensor\u7684\u65b9\u6cd5, \u53d6\u51b3\u4e8e\u4f60\u7684\u5b9e\u9645\u60c5\u51b5.</p> <ul> <li>\u4ece\u5df2\u7ecf\u5b58\u5728\u7684\u6570\u636e\u751f\u6210, \u7528 <code>torch.tensor()</code>.</li> <li>\u751f\u6210\u7279\u6b8a\u5c3a\u5bf8\u7684Tensor, \u7528 <code>torch.*</code> creation ops (\u89c1 Creation Ops).</li> <li>\u751f\u6210\u4e0e\u5176\u5b83Tensor\u5c3a\u5bf8\u76f8\u540c\u7684Tensor (\u5e76\u4e14\u6570\u636e\u7c7b\u578b\u76f8\u540c), \u7528 <code>torch.*_like</code> creation ops (\u89c1 Creation Ops).</li> <li>\u751f\u6210\u4e0e\u5176\u5b83Tesor\u6570\u636e\u7c7b\u578b\u76f8\u540c\u4f46\u662f\u5c3a\u5bf8\u4e0d\u540c\u7684Tensor, \u7528 <code>tensor.new_*</code> creation ops.</li> </ul> <pre><code>new_tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684Tensor\u7528 <code>data</code> \u4f5c\u4e3atensor data.\u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u8fd4\u56de\u7684Tensor\u6709\u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code> .</p> <p>\u8b66\u544a</p> <p><code>new_tensor()</code> \u603b\u662f\u62f7\u8d1d <code>data</code>. \u5982\u679c \u4f60\u6709\u4e00\u4e2a Tensor <code>data</code> \u5e76\u4e14\u60f3\u907f\u514d\u62f7\u8d1d, \u4f7f\u7528 <code>torch.Tensor.requires_grad_()</code> \u6216\u8005 <code>torch.Tensor.detach()</code>. \u5982\u679c\u4f60\u6709\u4e00\u4e2a numpy \u6570\u7ec4\u5e76\u4e14\u60f3\u907f\u514d\u62f7\u8d1d, \u4f7f\u7528 <code>torch.from_numpy()</code>.</p> <p>\u8b66\u544a</p> <p>\u5f53 data \u662f\u4e00\u4e2a tensor <code>x</code>, <code>new_tensor()</code> \u8bfb\u53d6 x \u7684 'data' \u5e76\u4e14\u521b\u5efa\u4e00\u4e2a\u53f6\u5b50\u53d8\u91cf. \u56e0\u6b64 <code>tensor.new_tensor(x)</code> \u7b49\u4ef7\u4e8e <code>x.clone().detach()</code> \u5e76\u4e14 <code>tensor.new_tensor(x, requires_grad=True)</code> \u7b49\u4ef7\u4e8e <code>x.clone().detach().requires_grad_(True)</code>. \u63a8\u8350\u4f7f\u7528 <code>clone()</code> \u548c <code>detach()</code>.</p> <p>\u53c2\u6570: </p> <ul> <li>data (array_like) \u2013 \u8fd4\u56de\u7684 Tensor \u62f7\u8d1d <code>data</code>.</li> <li>dtype (<code>torch.dtype</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tensor\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.dtype</code>.</li> <li>device (<code>torch.device</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tesor\u6240\u5728\u8bbe\u5907. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.device</code>.</li> <li>requires_grad (bool, \u53ef\u9009) \u2013 \u662f\u5426\u4e3a\u81ea\u52a8\u6c42\u5bfc\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8)\n&gt;&gt;&gt; data = [[0, 1], [2, 3]]\n&gt;&gt;&gt; tensor.new_tensor(data)\ntensor([[ 0,  1],\n [ 2,  3]], dtype=torch.int8)\n\n</code></pre> <pre><code>new_full(size, fill_value, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2aTesnor\u7684\u5c3a\u5bf8\u7b49\u4e8e <code>size</code> \u7528 <code>fill_value</code>\u586b\u5145. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u8fd4\u56de\u7684 Tensor \u5177\u6709\u4e0e\u6b64Tensor\u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>.</p> <p>\u53c2\u6570: </p> <ul> <li>fill_value (scalar) \u2013 \u7528\u4e8e\u586b\u5145\u7684\u6570\u503c.</li> <li>dtype (<code>torch.dtype</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tensor\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.dtype</code>.</li> <li>device (<code>torch.device</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tesor\u6240\u5728\u8bbe\u5907. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.device</code>.</li> <li>requires_grad (bool, \u53ef\u9009) \u2013 \u662f\u5426\u4e3a\u81ea\u52a8\u6c42\u5bfc\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64)\n&gt;&gt;&gt; tensor.new_full((3, 4), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n [ 3.1416,  3.1416,  3.1416,  3.1416],\n [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n\n</code></pre> <pre><code>new_empty(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2aTesnor\u7684\u5c3a\u5bf8\u7b49\u4e8e <code>size</code> \u7528 <code>\u672a\u521d\u59cb\u5316\u7684\u503c</code>\u586b\u5145. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u8fd4\u56de\u7684 Tensor \u5177\u6709\u4e0e\u6b64Tensor\u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>.</p> <p>Parameters: </p> <ul> <li>dtype (<code>torch.dtype</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tensor\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.dtype</code>.</li> <li>device (<code>torch.device</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tesor\u6240\u5728\u8bbe\u5907. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.device</code>.</li> <li>requires_grad (bool, \u53ef\u9009) \u2013 \u662f\u5426\u4e3a\u81ea\u52a8\u6c42\u5bfc\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; tensor = torch.ones(())\n&gt;&gt;&gt; tensor.new_empty((2, 3))\ntensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n\n</code></pre> <pre><code>new_ones(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2aTesnor\u7684\u5c3a\u5bf8\u7b49\u4e8e <code>size</code> \u7528 <code>1</code>\u586b\u5145. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u8fd4\u56de\u7684 Tensor \u5177\u6709\u4e0e\u6b64Tensor\u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>.</p> <p>Parameters: </p> <ul> <li>size (int...) \u2013 list, tuple, \u6216\u8005 <code>torch.Size</code> \u5b9a\u4e49\u4e86\u8f93\u51faTensor\u7684\u5f62\u72b6.</li> <li>dtype (<code>torch.dtype</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tensor\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.dtype</code>.</li> <li>device (<code>torch.device</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tesor\u6240\u5728\u8bbe\u5907. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.device</code>.</li> <li>requires_grad (bool, \u53ef\u9009) \u2013 \u662f\u5426\u4e3a\u81ea\u52a8\u6c42\u5bfc\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32)\n&gt;&gt;&gt; tensor.new_ones((2, 3))\ntensor([[ 1,  1,  1],\n [ 1,  1,  1]], dtype=torch.int32)\n\n</code></pre> <pre><code>new_zeros(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2aTesnor\u7684\u5c3a\u5bf8\u7b49\u4e8e <code>size</code> \u7528 <code>0</code>\u586b\u5145. \u9ed8\u8ba4\u60c5\u51b5\u4e0b, \u8fd4\u56de\u7684 Tensor \u5177\u6709\u4e0e\u6b64Tensor\u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>.</p> <p>\u53c2\u6570: </p> <ul> <li>size (int...) \u2013 list, tuple, \u6216\u8005 <code>torch.Size</code> \u5b9a\u4e49\u4e86\u8f93\u51faTensor\u7684\u5f62\u72b6.</li> <li>dtype (<code>torch.dtype</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tensor\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.dtype</code>.</li> <li>device (<code>torch.device</code>, \u53ef\u9009) \u2013 \u671f\u671b\u8fd4\u56de\u7684Tesor\u6240\u5728\u8bbe\u5907. \u9ed8\u8ba4\u503c: \u5982\u679c\u662f None, \u7b49\u4e8e <code>torch.device</code>.</li> <li>requires_grad (bool, \u53ef\u9009) \u2013 \u662f\u5426\u4e3a\u81ea\u52a8\u6c42\u5bfc\u8bb0\u5f55\u76f8\u5173\u7684\u8fd0\u7b97. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64)\n&gt;&gt;&gt; tensor.new_zeros((2, 3))\ntensor([[ 0.,  0.,  0.],\n [ 0.,  0.,  0.]], dtype=torch.float64)\n\n</code></pre> <pre><code>is_cuda\n</code></pre> <p><code>True</code> \u5982\u679c Tensor \u5728 GPU \u4e0a, \u5426\u5219 <code>False</code>.</p> <pre><code>device\n</code></pre> <p><code>torch.device</code> Tensor \u6240\u5728\u7684\u8bbe\u5907.</p> <pre><code>abs() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.abs()</code></p> <pre><code>abs_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>abs()</code></p> <pre><code>acos() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.acos()</code></p> <pre><code>acos_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>acos()</code></p> <pre><code>add(value) \u2192 Tensor\n</code></pre> <p>add(value=1, other) -&gt; Tensor</p> <p>\u89c1 <code>torch.add()</code></p> <pre><code>add_(value) \u2192 Tensor\n</code></pre> <p>add_(value=1, other) -&gt; Tensor</p> <p>\u539f\u5730\u7248\u672c\u7684 <code>add()</code></p> <pre><code>addbmm(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addbmm()</code></p> <pre><code>addbmm_(beta=1, mat, alpha=1, batch1, batch2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addbmm()</code></p> <pre><code>addcdiv(value=1, tensor1, tensor2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addcdiv()</code></p> <pre><code>addcdiv_(value=1, tensor1, tensor2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addcdiv()</code></p> <pre><code>addcmul(value=1, tensor1, tensor2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addcmul()</code></p> <pre><code>addcmul_(value=1, tensor1, tensor2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addcmul()</code></p> <pre><code>addmm(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addmm()</code></p> <pre><code>addmm_(beta=1, mat, alpha=1, mat1, mat2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addmm()</code></p> <pre><code>addmv(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addmv()</code></p> <pre><code>addmv_(beta=1, tensor, alpha=1, mat, vec) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addmv()</code></p> <pre><code>addr(beta=1, alpha=1, vec1, vec2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.addr()</code></p> <pre><code>addr_(beta=1, alpha=1, vec1, vec2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>addr()</code></p> <pre><code>allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.allclose()</code></p> <pre><code>apply_(callable) \u2192 Tensor\n</code></pre> <p>\u5e94\u7528\u51fd\u6570 <code>callable</code> \u5230Tensor\u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20, \u7528 <code>callable</code>\u7684\u8fd4\u56de\u503c\u66ff\u6362\u6bcf\u4e00\u4e2a\u5143\u7d20.</p> <p>\u6ce8\u610f</p> <p>\u8fd9\u4e2a\u51fd\u6570\u4ec5\u4ec5\u80fd\u5728CPU\u4e0a\u5de5\u4f5c, \u5e76\u4e14\u4e0d\u8981\u7528\u4e8e\u9700\u8981\u9ad8\u6027\u80fd\u7684\u4ee3\u7801\u533a\u57df.</p> <pre><code>argmax(dim=None, keepdim=False)\n</code></pre> <p>\u89c1 <code>torch.argmax()</code></p> <pre><code>argmin(dim=None, keepdim=False)\n</code></pre> <p>\u89c1 <code>torch.argmin()</code></p> <pre><code>asin() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.asin()</code></p> <pre><code>asin_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>asin()</code></p> <pre><code>atan() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.atan()</code></p> <pre><code>atan2(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.atan2()</code></p> <pre><code>atan2_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>atan2()</code></p> <pre><code>atan_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>atan()</code></p> <pre><code>baddbmm(beta=1, alpha=1, batch1, batch2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.baddbmm()</code></p> <pre><code>baddbmm_(beta=1, alpha=1, batch1, batch2) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>baddbmm()</code></p> <pre><code>bernoulli(*, generator=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2aTensor, \u6bcf\u4e00\u4e2a  \u90fd\u662f\u72ec\u7acb\u91c7\u6837\u4e8e . <code>self</code> \u5fc5\u987b\u662f\u6d6e\u70b9\u578b <code>dtype</code>, \u5e76\u4e14\u8fd4\u56de\u503c\u6709\u76f8\u540c\u7684 <code>dtype</code>.</p> <p>\u89c1 <code>torch.bernoulli()</code></p> <pre><code>bernoulli_()\n</code></pre> <pre><code>bernoulli_(p=0.5, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u4ece  \u72ec\u7acb\u91c7\u6837\u586b\u5145 <code>self</code> \u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e.<code>self</code> \u53ef\u4ee5\u662f\u6574\u578b <code>dtype</code>.</p> <pre><code>bernoulli_(p_tensor, *, generator=None) \u2192 Tensor\n</code></pre> <p><code>p_tensor</code> \u5fc5\u987b\u662f\u4e00\u4e2a\u5305\u542b\u6982\u7387\u7684 Tensor \u7528\u4e8e\u53d6\u5f97\u4e8c\u5143\u968f\u673a\u6570.</p> <p><code>self</code> tensor \u7684  \u5143\u7d20\u5c06\u4f1a\u88ab\u8bbe\u7f6e\u4e3a\u91c7\u6837\u4e8e  \u7684\u503c.</p> <p><code>self</code> \u53ef\u4ee5\u6709\u6574\u578b <code>dtype</code>, \u4f46\u662f :attr<code>p_tensor</code> \u5fc5\u987b\u6709\u6d6e\u70b9\u578b <code>dtype</code>.</p> <p>\u53ef\u53c2\u8003 <code>bernoulli()</code> and <code>torch.bernoulli()</code></p> <pre><code>bmm(batch2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.bmm()</code></p> <pre><code>byte() \u2192 Tensor\n</code></pre> <p><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>. See <code>to()</code>.</p> <pre><code>btrifact(info=None, pivot=True)\n</code></pre> <p>\u89c1 <code>torch.btrifact()</code></p> <pre><code>btrifact_with_info(pivot=True) -&gt; (Tensor, Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.btrifact_with_info()</code></p> <pre><code>btrisolve(LU_data, LU_pivots) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.btrisolve()</code></p> <pre><code>cauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u7528\u53d6\u81ea Cauchy \u5206\u5e03\u5f97\u503c\u586b\u5145Tensor:</p> <p></p> <pre><code>ceil() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.ceil()</code></p> <pre><code>ceil_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>ceil()</code></p> <pre><code>char() \u2192 Tensor\n</code></pre> <p><code>self.char()</code> \u7b49\u4ef7\u4e8e <code>self.to(torch.int8)</code>. \u89c1 <code>to()</code>.</p> <pre><code>cholesky(upper=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cholesky()</code></p> <pre><code>chunk(chunks, dim=0) \u2192 List of Tensors\n</code></pre> <p>\u89c1 <code>torch.chunk()</code></p> <pre><code>clamp(min, max) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.clamp()</code></p> <pre><code>clamp_(min, max) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>clamp()</code></p> <pre><code>clone() \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4efd\u62f7\u8d1d\u7684 <code>self</code> tensor. \u8fd9\u4efd\u62f7\u8d1d\u6709 <code>self</code> \u76f8\u540c\u7684\u6570\u636e\u548c\u7c7b\u578b.</p> <p>\u6ce8\u610f</p> <p>\u4e0e<code>copy_()</code>\u4e0d\u540c, \u6b64\u51fd\u6570\u4f1a\u88ab\u8bb0\u5f55\u5728\u8ba1\u7b97\u56fe\u4e2d. \u4f20\u7ed9\u514b\u9686tensor\u7684\u68af\u5ea6\u5c06\u4f20\u64ad\u5230\u539f\u59cbtensor.</p> <pre><code>contiguous() \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u8fde\u7eed\u7684\u5f97Tensor, \u5176data\u4e0e <code>self</code> \u76f8\u540c. \u5982\u679c <code>self</code> tensor \u662f\u8fde\u7eed\u7684, \u6b64\u51fd\u6570\u8fd4\u56de <code>self</code> tensor \u81ea\u8eab.</p> <pre><code>copy_(src, non_blocking=False) \u2192 Tensor\n</code></pre> <p>\u4ece <code>src</code> \u62f7\u8d1d\u5143\u7d20\u5230 <code>self</code> tensor \u7136\u540e\u8fd4\u56de <code>self</code>.</p> <p><code>src</code> tensor \u5fc5\u987b\u4e0e <code>self</code> tensor \u662f broadcastable. \u4f46\u6570\u636e\u7c7b\u578b\u53ef\u4ee5\u4e0d\u540c, \u6240\u5728\u7684\u8bbe\u5907\u4e5f\u53ef\u4ee5\u4e0d\u540c.</p> <p>\u53c2\u6570: </p> <ul> <li>src (Tensor) \u2013 \u6e90 tensor</li> <li>non_blocking (bool) \u2013 \u5982\u679c\u662f <code>True</code> \u5e76\u4e14\u8fd9\u6b21\u590d\u5236\u5728 CPU \u548c GPU \u4e4b\u95f4\u8fdb\u884c, \u8fd9\u6b21\u590d\u5236\u5c06\u4f1a\u662f\u5f02\u6b65\u7684. \u5176\u4ed6\u60c5\u51b5\u5219\u6ca1\u6709\u5f71\u54cd.</li> </ul> <pre><code>cos() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cos()</code></p> <pre><code>cos_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>cos()</code></p> <pre><code>cosh() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cosh()</code></p> <pre><code>cosh_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>cosh()</code></p> <pre><code>cpu() \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u62f7\u8d1d\u5bf9\u8c61\u4e8e CPU \u5185\u5b58\u4e2d.</p> <p>\u5982\u679c\u8fd9\u4e2a\u5bf9\u8c61\u5df2\u7ecf\u5728 CPU \u5185\u5b58\u4e2d, \u5e76\u4e14\u5728\u8005\u6b63\u786e\u7684\u8bbe\u5907\u4e0a, \u90a3\u4e48\u5c06\u4f1a\u8fd4\u56de\u5176\u672c\u8eab.</p> <pre><code>cross(other, dim=-1) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cross()</code></p> <pre><code>cuda(device=None, non_blocking=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u62f7\u8d1d\u5bf9\u8c61\u4e8e CUDA \u5185\u5b58\u4e2d.</p> <p>\u5982\u679c\u8fd9\u4e2a\u5bf9\u8c61\u5df2\u7ecf\u5728 CUDA \u5185\u5b58\u4e2d, \u5e76\u4e14\u5728\u8005\u6b63\u786e\u7684\u8bbe\u5907\u4e0a, \u90a3\u4e48\u5c06\u4f1a\u8fd4\u56de\u5176\u672c\u8eab.</p> <p>\u53c2\u6570: </p> <ul> <li>device (<code>torch.device</code>) \u2013\u76ee\u6807GPU\u8bbe\u5907. \u9ed8\u8ba4\u503c\u662f\u5f53\u524dGPU.</li> <li>non_blocking (bool) \u2013 \u5982\u679c\u662f <code>True</code> \u5e76\u4e14\u6e90\u5728pinned memory\u4e2d, \u8fd9\u6b21\u62f7\u8d1d\u5c06\u662f\u5f02\u6b65\u7684.\u5426\u5219\u6b64\u53c2\u6570\u6ca1\u6709\u5f71\u54cd. \u9ed8\u8ba4\u503c: <code>False</code>.</li> </ul> <pre><code>cumprod(dim, dtype=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cumprod()</code></p> <pre><code>cumsum(dim, dtype=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.cumsum()</code></p> <pre><code>data_ptr() \u2192 int\n</code></pre> <p>\u8fd4\u56de <code>self</code> tensor \u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u7684\u6307\u9488.</p> <pre><code>det() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.det()</code></p> <pre><code>diag(diagonal=0) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.diag()</code></p> <pre><code>diag_embed(offset=0, dim1=-2, dim2=-1) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.diag_embed()</code></p> <pre><code>dim() \u2192 int\n</code></pre> <p>\u8fd4\u56de <code>self</code> tensor \u7684\u7ef4\u5ea6.</p> <pre><code>dist(other, p=2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.dist()</code></p> <pre><code>div(value) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.div()</code></p> <pre><code>div_(value) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>div()</code></p> <pre><code>dot(tensor2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.dot()</code></p> <pre><code>double() \u2192 Tensor\n</code></pre> <p><code>self.double()</code> \u7b49\u4ef7\u4e8e <code>self.to(torch.float64)</code>. \u89c1 <code>to()</code>.</p> <pre><code>eig(eigenvectors=False) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.eig()</code></p> <pre><code>element_size() \u2192 int\n</code></pre> <p>\u8fd4\u56de\u6bcf\u4e2a\u5143\u7d20\u5360\u7528\u7684\u5b57\u8282\u6570</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.tensor([]).element_size()\n4\n&gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size()\n1\n\n</code></pre> <pre><code>eq(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.eq()</code></p> <pre><code>eq_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>eq()</code></p> <pre><code>equal(other) \u2192 bool\n</code></pre> <p>\u89c1 <code>torch.equal()</code></p> <pre><code>erf() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.erf()</code></p> <pre><code>erf_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>erf()</code></p> <pre><code>erfc() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.erfc()</code></p> <pre><code>erfc_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>erfc()</code></p> <pre><code>erfinv() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.erfinv()</code></p> <pre><code>erfinv_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>erfinv()</code></p> <pre><code>exp() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.exp()</code></p> <pre><code>exp_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>exp()</code></p> <pre><code>expm1() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.expm1()</code></p> <pre><code>expm1_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>expm1()</code></p> <pre><code>expand(*sizes) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 <code>self</code> tensor \u7684\u89c6\u56fe, \u5176\u4e2d\u5355\u4e00\u7ef4\u5ea6\u6269\u5c55\u5230\u66f4\u5927\u7684\u5c3a\u5bf8.</p> <p>\u4f20\u9012<code>-1</code>\u610f\u5473\u7740\u4e0d\u6539\u53d8\u8be5\u7ef4\u5ea6\u7684\u5927\u5c0f.</p> <p>tensor \u4e5f\u53ef\u4ee5\u6269\u5c55\u5230\u66f4\u5927\u7684\u7ef4\u5ea6, \u65b0\u7684\u7ef4\u5ea6\u5c06\u4f1a\u9644\u52a0\u5728\u524d\u9762.\u5bf9\u4e8e\u65b0\u7ef4\u5ea6, \u5176\u5927\u5c0f\u4e0d\u80fd\u8bbe\u7f6e\u4e3a- 1.</p> <p>\u6269\u5c55\u5f20\u91cf\u4e0d\u4f1a\u5206\u914d\u65b0\u7684\u5185\u5b58, \u4f46\u53ea\u4f1a\u5728\u73b0\u6709\u5f20\u91cf\u4e0a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u89c6\u56fe, \u5176\u4e2d\u901a\u8fc7\u5c06<code>stride</code>\u8bbe\u7f6e\u4e3a0, \u7b2c\u4e00\u4e2a\u5c3a\u5bf8\u7684\u7ef4\u5ea6\u4f1a\u6269\u5c55\u5230\u66f4\u5927\u7684\u5c3a\u5bf8.\u5927\u5c0f\u4e3a1\u7684\u4efb\u4f55\u7ef4\u5ea6\u90fd\u53ef\u4ee5\u6269\u5c55\u5230\u4efb\u610f\u503c, \u800c\u65e0\u9700\u5206\u914d\u65b0\u5185\u5b58.</p> \u53c2\u6570: *sizes (torch.Size or int...) \u2013 \u671f\u671b\u6269\u5c55\u7684\u5c3a\u5bf8 <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])\n&gt;&gt;&gt; x.size()\ntorch.Size([3, 1])\n&gt;&gt;&gt; x.expand(3, 4)\ntensor([[ 1,  1,  1,  1],\n [ 2,  2,  2,  2],\n [ 3,  3,  3,  3]])\n&gt;&gt;&gt; x.expand(-1, 4)   # -1 \u610f\u5473\u7740\u4e0d\u4f1a\u6539\u53d8\u8be5\u7ef4\u5ea6\ntensor([[ 1,  1,  1,  1],\n [ 2,  2,  2,  2],\n [ 3,  3,  3,  3]])\n\n</code></pre> <pre><code>expand_as(other) \u2192 Tensor\n</code></pre> <p>\u6269\u5c55\u8fd9\u4e2a tensor \u4f7f\u5f97\u5176\u5c3a\u5bf8\u548c <code>other</code> \u76f8\u540c. <code>self.expand_as(other)</code> \u7b49\u4ef7\u4e8e <code>self.expand(other.size())</code>.</p> <p>\u8bf7\u770b <code>expand()</code> \u83b7\u5f97\u66f4\u591a\u5173\u4e8e <code>expand</code> \u7684\u4fe1\u606f.</p> \u53c2\u6570: other (<code>torch.Tensor</code>) \u2013 \u8fd4\u56de\u7684 tensor \u7684\u5c3a\u5bf8\u548c <code>other</code>. \u76f8\u540c <pre><code>exponential_(lambd=1, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u7528\u53d6\u81ea <code>exponential \u5206\u5e03</code> \u7684\u5143\u7d20\u586b\u5145 <code>self</code> tensor :</p> <p></p> <pre><code>fill_(value) \u2192 Tensor\n</code></pre> <p>\u7528\u6307\u5b9a\u7684\u503c\u586b\u5145 <code>self</code>.</p> <pre><code>flatten(input, start_dim=0, end_dim=-1) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.flatten()</code></p> <pre><code>flip(dims) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.flip()</code></p> <pre><code>float() \u2192 Tensor\n</code></pre> <p><code>self.float()</code> \u7b49\u4ef7\u4e8e <code>self.to(torch.float32)</code>. See <code>to()</code>.</p> <pre><code>floor() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.floor()</code></p> <pre><code>floor_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>floor()</code></p> <pre><code>fmod(divisor) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.fmod()</code></p> <pre><code>fmod_(divisor) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>fmod()</code></p> <pre><code>frac() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.frac()</code></p> <pre><code>frac_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>frac()</code></p> <pre><code>gather(dim, index) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.gather()</code></p> <pre><code>ge(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.ge()</code></p> <pre><code>ge_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>ge()</code></p> <pre><code>gels(A) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.gels()</code></p> <pre><code>geometric_(p, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u7528\u53d6\u81ea<code>geometric \u5206\u5e03</code>\u7684\u503c\u586b\u5145 <code>self</code> :</p> <p></p> <pre><code>geqrf() -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.geqrf()</code></p> <pre><code>ger(vec2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.ger()</code></p> <pre><code>gesv(A) \u2192 Tensor, Tensor\n</code></pre> <p>\u89c1 <code>torch.gesv()</code></p> <pre><code>get_device() -&gt; Device ordinal (Integer)\n</code></pre> <p>\u5bf9\u4e8e CUDA tensors, \u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a GPU \u5e8f\u53f7, \u5bf9\u5e94 tensor \u6240\u5728\u7684\u8bbe\u5907. \u5bf9\u4e8e CPU tensors, \u629b\u51fa\u4e00\u4e2a\u9519\u8bef.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 4, 5, device='cuda:0')\n&gt;&gt;&gt; x.get_device()\n0\n&gt;&gt;&gt; x.cpu().get_device()  # \u8fd0\u884c\u65f6\u9519\u8bef: get_device \u6ca1\u6709\u5728 torch.FloatTensor \u4e0a\u5b9e\u73b0\n\n</code></pre> <pre><code>gt(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.gt()</code></p> <pre><code>gt_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>gt()</code></p> <pre><code>half() \u2192 Tensor\n</code></pre> <p><code>self.half()</code> \u7b49\u4ef7\u4e8e <code>self.to(torch.float16)</code>. \u89c1 <code>to()</code>.</p> <pre><code>histc(bins=100, min=0, max=0) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.histc()</code></p> <pre><code>index_add_(dim, index, tensor) \u2192 Tensor\n</code></pre> <p>\u6839\u636e\u53c2\u6570<code>index</code> \u4e2d\u7684\u7d22\u5f15\u7684\u987a\u5e8f, \u7d2f\u52a0 <code>tensor</code> \u4e2d\u7684\u5143\u7d20\u5230 <code>self</code> tensor, \u4f8b\u5982, \u5982\u679c <code>dim == 0</code> \u5e76\u4e14 <code>index[i] == j</code>, \u5219\u7b2c <code>i</code> \u884c <code>tensor</code> \u4f1a\u88ab\u52a0\u5230\u7b2c <code>j</code>\u884c.</p> <p><code>tensor</code> \u7b2c <code>dim</code> \u7ef4\u5ea6 \u5fc5\u987b\u548c <code>index</code>(\u5fc5\u987b\u662f\u4e00\u4e2a\u5411\u91cf) \u7684\u957f\u5ea6\u76f8\u540c, \u5e76\u4e14\u5176\u5b83\u7ef4\u5ea6\u5fc5\u987b\u548c <code>self</code> \u5339\u914d, \u5426\u5219\u5c06\u4f1a\u629b\u51fa\u4e00\u4e2a\u9519\u8bef.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528 CUDA \u4f5c\u4e3a\u540e\u7aef, \u8fd9\u4e2a\u64cd\u4f5c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u884c\u4e3a, \u4e14\u4e0d\u5bb9\u6613\u5173\u95ed. \u8bf7\u770b Reproducibility.</p> <p>Parameters: </p> <ul> <li>dim (int) \u2013 \u8981\u7d22\u5f15\u7684\u7ef4\u5ea6</li> <li>index (LongTensor) \u2013 \u4ece <code>tensor</code> \u4e2d\u9009\u62e9\u7684\u7d22\u5f15</li> <li>tensor (Tensor) \u2013 \u7528\u4e8e\u76f8\u52a0\u7684tensor</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.ones(5, 3)\n&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 4, 2])\n&gt;&gt;&gt; x.index_add_(0, index, t)\ntensor([[  2.,   3.,   4.],\n [  1.,   1.,   1.],\n [  8.,   9.,  10.],\n [  1.,   1.,   1.],\n [  5.,   6.,   7.]])\n\n</code></pre> <pre><code>index_copy_(dim, index, tensor) \u2192 Tensor\n</code></pre> <p>\u6839\u636e\u53c2\u6570<code>index</code> \u4e2d\u7684\u9009\u62e9\u7684\u7d22\u5f15, \u590d\u5236 <code>tensor</code> \u4e2d\u7684\u5143\u7d20\u5230 <code>self</code> tensor, \u4f8b\u5982, \u5982\u679c <code>dim == 0</code> \u5e76\u4e14 <code>index[i] == j</code>, \u5219\u7b2c <code>i</code> \u884c <code>tensor</code> \u4f1a\u88ab\u52a0\u5230\u7b2c <code>j</code>\u884c.</p> <p><code>tensor</code> \u7b2c <code>dim</code> \u7ef4\u5ea6 \u5fc5\u987b\u548c <code>index</code>(\u5fc5\u987b\u662f\u4e00\u4e2a\u5411\u91cf) \u7684\u957f\u5ea6\u76f8\u540c, \u5e76\u4e14\u5176\u5b83\u7ef4\u5ea6\u5fc5\u987b\u548c <code>self</code> \u5339\u914d, \u5426\u5219\u5c06\u4f1a\u629b\u51fa\u4e00\u4e2a\u9519\u8bef.</p> <p>Parameters: </p> <ul> <li>dim (int) \u2013 \u8981\u7d22\u5f15\u7684\u7ef4\u5ea6</li> <li>index (LongTensor) \u2013 \u4ece <code>tensor</code> \u4e2d\u9009\u62e9\u7684\u7d22\u5f15</li> <li>tensor (Tensor) \u2013 \u7528\u4e8e\u590d\u5236\u7684tensor</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.zeros(5, 3)\n&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 4, 2])\n&gt;&gt;&gt; x.index_copy_(0, index, t)\ntensor([[ 1.,  2.,  3.],\n [ 0.,  0.,  0.],\n [ 7.,  8.,  9.],\n [ 0.,  0.,  0.],\n [ 4.,  5.,  6.]])\n\n</code></pre> <pre><code>index_fill_(dim, index, val) \u2192 Tensor\n</code></pre> <p>\u6839\u636e <code>index</code> \u4e2d\u6307\u5b9a\u7684\u987a\u5e8f\u7d22\u5f15, \u7528\u503c <code>val</code>\u586b\u5145 <code>self</code> tensor \u4e2d\u7684\u5143\u7d20.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u6307\u5b9a\u7d22\u5f15\u5bf9\u5e94\u7684\u7ef4\u5ea6</li> <li>index (LongTensor) \u2013  <code>self</code> tensor \u4e2d\u5c06\u88ab\u586b\u5145\u7684\u7d22\u5f15\u503c</li> <li>val (float) \u2013 \u7528\u4e8e\u586b\u5145\u7684\u503c</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 2])\n&gt;&gt;&gt; x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n [-1.,  5., -1.],\n [-1.,  8., -1.]])\n\n</code></pre> <pre><code>index_put_(indices, value, accumulate=False) \u2192 Tensor\n</code></pre> <p>\u6839\u636e <code>indices</code> (\u662f\u4e00\u4e2a Tensors \u7684tuple)\u4e2d\u6307\u5b9a\u7684\u7d22\u5f15, \u53d6\u51fa tensor <code>value</code> \u4e2d\u7684\u503c\u653e\u5165 tensor <code>self</code> . \u8868\u8fbe\u5f0f <code>tensor.index_put_(indices, value)</code> \u7b49\u4ef7\u4e8e <code>tensor[indices] = value</code>. \u8fd4\u56de <code>self</code>.</p> <p>\u5982\u679c <code>accumulate</code> \u7b49\u4e8e <code>True</code>,  <code>tensor</code> \u4e2d\u7684\u5143\u7d20\u4f1a\u88ab\u52a0\u5230 <code>self</code>. \u5982\u679c\u662f <code>False</code>, \u4e14 <code>indices</code> \u4e2d\u542b\u6709\u91cd\u590d\u7684\u5143\u7d20, \u5219\u884c\u4e3a\u662f\u672a\u5b9a\u4e49\u7684.</p> <p>\u53c2\u6570: </p> <ul> <li>indices (tuple of LongTensor) \u2013 tensors \u7528\u4e8e\u7d22\u5f15 <code>self</code>.</li> <li>value (Tensor) \u2013 \u4e0e <code>self</code> \u6709\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684 tensor.</li> <li>accumulate (bool) \u2013 \u662f\u5426\u7d2f\u52a0\u5230\u81ea\u8eab</li> </ul> <pre><code>index_select(dim, index) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.index_select()</code></p> <pre><code>int() \u2192 Tensor\n</code></pre> <p><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>. See <code>to()</code>.</p> <pre><code>inverse() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.inverse()</code></p> <pre><code>is_contiguous() \u2192 bool\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c <code>self</code> tensor \u5728\u5185\u5b58\u4e2d\u662f\u8fde\u7eed\u5b58\u50a8\u7684.</p> <pre><code>is_pinned()\n</code></pre> <p>\u8fd4\u56de true \u5982\u679c tensor \u50a8\u5b58\u5728pinned memory</p> <pre><code>is_set_to(tensor) \u2192 bool\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c\u6b64\u5bf9\u8c61\u5728 Torch C API \u4e2d\u5f15\u7528\u7684 <code>THTensor</code> \u5bf9\u8c61\u548c\u7ed9\u5b9a tensor \u662f\u76f8\u540c\u7684.</p> <pre><code>is_signed()\n</code></pre> <pre><code>item() \u2192 number\n</code></pre> <p>\u8fd4\u56de tensor \u4e2d\u7684\u503c\u4f5c\u4e3a\u4e00\u4e2a\u6807\u51c6\u7684 Python number. \u4ec5\u5728\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u65f6\u5019\u6709\u6548. \u5bf9\u4e8e\u5176\u4ed6\u60c5\u51b5, \u89c1 <code>tolist()</code>.</p> <p>\u8fd9\u4e2a\u64cd\u4f5c\u662f\u4e0d\u53ef\u5fae\u5206\u7684.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1.0])\n&gt;&gt;&gt; x.item()\n1.0\n\n</code></pre> <pre><code>kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u89c1 <code>torch.kthvalue()</code></p> <pre><code>le(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.le()</code></p> <pre><code>le_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>le()</code></p> <pre><code>lerp(start, end, weight) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.lerp()</code></p> <pre><code>lerp_(start, end, weight) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>lerp()</code></p> <pre><code>log() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.log()</code></p> <pre><code>log_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>log()</code></p> <pre><code>logdet() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.logdet()</code></p> <pre><code>log10() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.log10()</code></p> <pre><code>log10_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>log10()</code></p> <pre><code>log1p() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.log1p()</code></p> <pre><code>log1p_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>log1p()</code></p> <pre><code>log2() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.log2()</code></p> <pre><code>log2_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>log2()</code></p> <pre><code>log_normal_(mean=1, std=2, *, generator=None)\n</code></pre> <p>\u7528 <code>mean</code> \u548c<code>std</code> \u521d\u59cb\u5316\u7684 <code>log-normal \u5206\u5e03</code> \u4e2d\u53d6\u51fa\u7684\u503c\u586b\u5145 <code>self</code>. \u6ce8\u610f <code>mean</code> \u548c <code>std</code> \u662f\u4e0b\u9762\u7684 normal \u5206\u5e03\u7684\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee, \u800c\u4e0d\u662f\u8fd4\u56de\u7684\u5206\u5e03:</p> <p></p> <pre><code>logsumexp(dim, keepdim=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.logsumexp()</code></p> <pre><code>long() \u2192 Tensor\n</code></pre> <p><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>. See <code>to()</code>.</p> <pre><code>lt(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.lt()</code></p> <pre><code>lt_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>lt()</code></p> <pre><code>map_(tensor, callable)\n</code></pre> <p>\u5bf9 <code>self</code> tensor \u548c \u7ed9\u5b9a\u7684 <code>tensor</code> \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u5e94\u7528 <code>callable</code> \u7136\u540e\u628a\u7ed3\u679c\u5b58\u4e8e <code>self</code> tensor. <code>self</code> tensor \u548c\u7ed9\u5b9a\u7684 <code>tensor</code> \u5fc5\u987b\u53ef\u5e7f\u64ad broadcastable.</p> <p><code>callable</code> \u5e94\u8be5\u6709\u4e0b\u9762\u7684\u51fd\u6570\u7b7e\u540d:</p> <pre><code>def callable(a, b) -&gt; number\n\n</code></pre> <pre><code>masked_scatter_(mask, source)\n</code></pre> <p>\u4ece <code>source</code> \u590d\u5236\u5143\u7d20\u5230 <code>self</code> tensor \u5f53\u5bf9\u5e94 <code>mask</code> \u5bf9\u5e94\u7684\u503c\u662f 1.  <code>mask</code> \u7684\u5f62\u72b6\u5fc5\u987b\u548c\u5e95\u5c42 tensor \u53ef\u5e7f\u64ad broadcastable.  <code>source</code> \u7684\u5143\u7d20\u6570\u91cf\u81f3\u5c11\u548c <code>mask</code>\u91cc\u9762\u76841\u4e00\u6837\u591a</p> <p>Parameters: </p> <ul> <li>mask (ByteTensor) \u2013 \u4e8c\u503c\u63a9\u7801</li> <li>source (Tensor) \u2013 \u6e90 tensor</li> </ul> <p>\u6ce8\u610f</p> <p><code>mask</code> \u64cd\u4f5c\u4e8e <code>self</code> tensor, \u800c\u4e0d\u662f\u7ed9\u5b9a\u7684 <code>source</code> tensor.</p> <pre><code>masked_fill_(mask, value)\n</code></pre> <p>\u7528<code>value</code>\u586b\u5145 <code>self</code> tensor \u4e2d\u7684\u5143\u7d20, \u5f53\u5bf9\u5e94\u4f4d\u7f6e\u7684 <code>mask</code> \u662f1. <code>mask</code> \u7684\u5f62\u72b6\u5fc5\u987b\u548c\u5e95\u5c42 tensor broadcastable.</p> <p>\u53c2\u6570: </p> <ul> <li>mask (ByteTensor) \u2013 \u4e8c\u503c\u63a9\u7801</li> <li>value (float) \u2013 \u7528\u4e8e\u586b\u5145\u7684\u503c</li> </ul> <pre><code>masked_select(mask) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.masked_select()</code></p> <pre><code>matmul(tensor2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.matmul()</code></p> <pre><code>matrix_power(n) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.matrix_power()</code></p> <pre><code>max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.max()</code></p> <pre><code>mean(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.mean()</code></p> <pre><code>median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u89c1 <code>torch.median()</code></p> <pre><code>min(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.min()</code></p> <pre><code>mm(mat2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.mm()</code></p> <pre><code>mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u89c1 <code>torch.mode()</code></p> <pre><code>mul(value) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.mul()</code></p> <pre><code>mul_(value)\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>mul()</code></p> <pre><code>multinomial(num_samples, replacement=False, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.multinomial()</code></p> <pre><code>mv(vec) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.mv()</code></p> <pre><code>mvlgamma(p) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.mvlgamma()</code></p> <pre><code>mvlgamma_(p) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>mvlgamma()</code></p> <pre><code>narrow(dimension, start, length) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.narrow()</code></p> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; x.narrow(0, 0, 2)\ntensor([[ 1,  2,  3],\n [ 4,  5,  6]])\n&gt;&gt;&gt; x.narrow(1, 1, 2)\ntensor([[ 2,  3],\n [ 5,  6],\n [ 8,  9]])\n\n</code></pre> <pre><code>ndimension() \u2192 int\n</code></pre> <p>Alias for <code>dim()</code></p> <pre><code>ne(other) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.ne()</code></p> <pre><code>ne_(other) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>ne()</code></p> <pre><code>neg() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.neg()</code></p> <pre><code>neg_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>neg()</code></p> <pre><code>nelement() \u2192 int\n</code></pre> <p>\u522b\u540d <code>numel()</code></p> <pre><code>nonzero() \u2192 LongTensor\n</code></pre> <p>\u89c1 <code>torch.nonzero()</code></p> <pre><code>norm(p='fro', dim=None, keepdim=False)\n</code></pre> <p>\u89c1 :func: <code>torch.norm</code></p> <pre><code>normal_(mean=0, std=1, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u7528\u91c7\u6837\u4e8e normal \u5206\u5e03\u7684\u5143\u7d20\u586b\u5145 <code>self</code> tensor, normal \u5206\u5e03\u4f7f\u7528\u53c2\u6570 <code>mean</code> and <code>std</code>\u521d\u59cb\u5316.</p> <pre><code>numel() \u2192 int\n</code></pre> <p>\u89c1 <code>torch.numel()</code></p> <pre><code>numpy() \u2192 numpy.ndarray\n</code></pre> <p>\u8fd4\u56de <code>self</code> tensor \u4f5c\u4e3a\u4e00\u4e2a NumPy <code>ndarray</code>. \u6b64 tensor \u548c\u8fd4\u56de\u7684 <code>ndarray</code> \u5171\u4eab\u540c\u4e00\u4e2a\u5e95\u5c42\u5b58\u50a8. \u6539\u53d8<code>self</code> tensor \u5c06\u4f1a\u540c\u65f6\u6539\u53d8 <code>ndarray</code> .</p> <pre><code>orgqr(input2) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.orgqr()</code></p> <pre><code>ormqr(input2, input3, left=True, transpose=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.ormqr()</code></p> <pre><code>permute(*dims) \u2192 Tensor\n</code></pre> <p>\u6392\u5217 tensor \u7684\u7ef4\u5ea6.</p> \u53c2\u6570: *dims (int...) \u2013 \u7ef4\u5ea6\u7684\u6392\u5217\u987a\u5e8f <p>Example</p> <pre><code>&gt;&gt;&gt; x = torch.randn(2, 3, 5)\n&gt;&gt;&gt; x.size()\ntorch.Size([2, 3, 5])\n&gt;&gt;&gt; x.permute(2, 0, 1).size()\ntorch.Size([5, 2, 3])\n\n</code></pre> <pre><code>pin_memory()\n</code></pre> <pre><code>pinverse() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.pinverse()</code></p> <pre><code>potrf(upper=True)\n</code></pre> <p>\u89c1 <code>torch.cholesky()</code></p> <pre><code>potri(upper=True) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.potri()</code></p> <pre><code>potrs(input2, upper=True) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.potrs()</code></p> <pre><code>pow(exponent) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.pow()</code></p> <pre><code>pow_(exponent) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>pow()</code></p> <pre><code>prod(dim=None, keepdim=False, dtype=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.prod()</code></p> <pre><code>pstrf(upper=True, tol=-1) -&gt; (Tensor, IntTensor)\n</code></pre> <p>\u89c1 <code>torch.pstrf()</code></p> <pre><code>put_(indices, tensor, accumulate=False) \u2192 Tensor\n</code></pre> <p>\u4ece <code>tensor</code> \u4e2d\u590d\u5236\u5143\u7d20\u5230 indices \u6307\u5b9a\u7684\u4f4d\u7f6e. \u5bf9\u4e8e\u76ee\u7684\u7d22\u5f15,  <code>self</code> tensor \u88ab\u5f53\u4f5c\u4e00\u4e2a 1-D tensor.</p> <p>\u5982\u679c <code>accumulate</code> \u662f <code>True</code>, <code>tensor</code> \u4e2d\u7684\u5143\u7d20\u88ab\u88ab\u52a0\u5230 <code>self</code>. \u5982\u679c accumulate \u662f <code>False</code>, \u5f53 indices \u4e2d\u6709\u91cd\u590d\u7d22\u5f15\u65f6\u884c\u4e3a\u672a\u5b9a\u4e49.</p> <p>Parameters: </p> <ul> <li>indices (LongTensor) \u2013 self \u7684\u7d22\u5f15\u4f4d\u7f6e</li> <li>tensor (Tensor) \u2013 \u5305\u542b\u5f85\u590d\u5236\u5143\u7d20\u7684 tensor</li> <li>accumulate (bool) \u2013 \u662f\u5426\u7d2f\u52a0\u5230 self</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],\n [6, 7, 8]])\n&gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\ntensor([[  4,   9,   5],\n [ 10,   7,   8]])\n\n</code></pre> <pre><code>qr() -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.qr()</code></p> <pre><code>random_(from=0, to=None, *, generator=None) \u2192 Tensor\n</code></pre> <p>\u7528\u79bb\u6563\u5747\u5300\u5206\u5e03\u4ecb\u4e8e  <code>[from, to - 1]</code> \u91c7\u6837\u7684\u6570\u5b57\u586b\u5145 <code>self</code> tensor. \u5982\u679c\u6ca1\u6709\u7279\u522b\u6307\u5b9a, \u8fd9\u4e9b\u91c7\u6837\u7684\u6570\u503c\u88ab <code>self</code> tensor's \u6570\u636e\u7c7b\u578b\u754c\u5b9a. \u7136\u800c, \u5bf9\u4e8e\u6d6e\u70b9\u578b, \u5982\u679c\u6ca1\u6709\u7279\u522b\u6307\u5b9a, \u8303\u56f4\u5c06\u662f <code>[0, 2^mantissa]</code> \u6765\u786e\u4fdd\u6bcf\u4e00\u4e2a\u503c\u662f\u53ef\u8868\u793a\u7684. \u4f8b\u5982, <code>torch.tensor(1, dtype=torch.double).random_()</code> \u5c06\u4f1a\u88ab\u8bbe\u4e3a <code>[0, 2^53]</code>.</p> <pre><code>reciprocal() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.reciprocal()</code></p> <pre><code>reciprocal_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>reciprocal()</code></p> <pre><code>remainder(divisor) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.remainder()</code></p> <pre><code>remainder_(divisor) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>remainder()</code></p> <pre><code>renorm(p, dim, maxnorm) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.renorm()</code></p> <pre><code>renorm_(p, dim, maxnorm) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>renorm()</code></p> <pre><code>repeat(*sizes) \u2192 Tensor\n</code></pre> <p>\u5728\u6307\u5b9a\u7684\u7ef4\u5ea6\u91cd\u590d\u8fd9\u4e2a tensor.</p> <p>\u4e0d\u50cf <code>expand()</code>, \u8fd9\u4e2a\u51fd\u6570\u4f1a\u62f7\u8d1d\u5e95\u5c42\u6570\u636e.</p> <p>\u8b66\u544a</p> <p><code>torch.repeat()</code> \u7684\u884c\u4e3a\u548c numpy.repeat \u4e0d\u4e00\u6837, \u66f4\u7c7b\u4f3c\u4e8e numpy.tile.</p> \u53c2\u6570: sizes (torch.Size or int...) \u2013 \u6bcf\u4e2a\u7ef4\u5ea6\u91cd\u590d\u7684\u6b21\u6570 <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; x.repeat(4, 2)\ntensor([[ 1,  2,  3,  1,  2,  3],\n [ 1,  2,  3,  1,  2,  3],\n [ 1,  2,  3,  1,  2,  3],\n [ 1,  2,  3,  1,  2,  3]])\n&gt;&gt;&gt; x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n\n</code></pre> <pre><code>requires_grad_(requires_grad=True) \u2192 Tensor\n</code></pre> <p>\u8bbe\u7f6e\u662f\u5426\u5e94\u8be5\u81ea\u52a8\u6c42\u5bfc: \u539f\u5730\u8bbe\u7f6e\u8fd9\u4e2a tensor \u7684 <code>requires_grad</code> \u5c5e\u6027.\u8fd4\u56de\u8fd9\u4e2a tensor.</p> <p><code>require_grad_()</code> \u7684\u4e3b\u8981\u4f7f\u7528\u60c5\u51b5\u662f\u544a\u8bc9\u81ea\u52a8\u6c42\u5bfc\u5f00\u59cb\u8bb0\u5f55Tensor <code>tensor</code>\u4e0a\u7684\u64cd\u4f5c. \u5982\u679c <code>tensor</code> \u7684 <code>requires_grad=False</code> (\u56e0\u4e3a\u5b83\u662f\u901a\u8fc7 DataLoader \u83b7\u5f97\u6216\u8005\u9700\u8981\u9884\u5904\u7406\u6216\u521d\u59cb\u5316), <code>tensor.requires_grad_()</code> \u5c06\u4f1a\u4f7f\u5f97\u81ea\u52a8\u6c42\u5bfc\u5f00\u59cb\u751f\u6548.</p> \u53c2\u6570: requires_grad (bool) \u2013 \u662f\u5426\u81ea\u52a8\u6c42\u5bfc\u5e94\u8be5\u8bb0\u5f55\u76f8\u5173\u64cd\u4f5c. Default: <code>True</code>. <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; # Let's say we want to preprocess some saved weights and use\n&gt;&gt;&gt; # the result as new weights.\n&gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25]\n&gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights)\n&gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function\n&gt;&gt;&gt; weights\ntensor([-0.5503,  0.4926, -2.1158, -0.8303])\n\n&gt;&gt;&gt; # Now, start to record operations done to weights\n&gt;&gt;&gt; weights.requires_grad_()\n&gt;&gt;&gt; out = weights.pow(2).sum()\n&gt;&gt;&gt; out.backward()\n&gt;&gt;&gt; weights.grad\ntensor([-1.1007,  0.9853, -4.2316, -1.6606])\n\n</code></pre> <pre><code>reshape(*shape) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a tensor, \u5176data\u548c\u5143\u7d20\u6570\u91cf\u4e0e <code>self</code> \u4e00\u6837, \u4f46\u662f\u6539\u53d8\u6210\u6307\u5b9a\u7684\u5f62\u72b6. \u8fd9\u4e2a\u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2atensor\u7684\u8bd5\u56fe \u5982\u679c <code>shape</code> \u548c\u5f53\u524d\u7684\u5f62\u72b6\u662f\u517c\u5bb9\u7684. \u89c1 <code>torch.Tensor.view()</code> \u5173\u4e8e\u662f\u4ec0\u4e48\u65f6\u5019\u8fd4\u56de\u4e00\u4e2a view.</p> <p>\u89c1 <code>torch.reshape()</code></p> \u53c2\u6570: shape (tuple of python:ints or int...) \u2013 \u671f\u671b\u53d8\u6210\u7684\u5f62\u72b6 <pre><code>reshape_as(other) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2atensor\u5f62\u72b6\u4e0e <code>other</code> \u76f8\u540c. <code>self.reshape_as(other)</code> \u7b49\u4ef7\u4e8e <code>self.reshape(other.sizes())</code>. \u8fd9\u4e2a\u65b9\u6cd5\u8fd4\u56de\u4e00\u4e2atensor\u7684\u8bd5\u56fe \u5982\u679c <code>self.reshape(other.sizes())</code> \u548c\u5f53\u524d\u7684\u5f62\u72b6\u662f\u517c\u5bb9\u7684. \u89c1 <code>torch.Tensor.view()</code> \u5173\u4e8e\u662f\u4ec0\u4e48\u65f6\u5019\u8fd4\u56de\u4e00\u4e2a view.</p> <p>\u8bf7\u53c2\u8003 <code>reshape()</code> \u83b7\u5f97\u66f4\u591a\u5173\u4e8e <code>reshape</code> \u7684\u4fe1\u606f.</p> \u53c2\u6570: other (<code>torch.Tensor</code>) \u2013 \u8fd4\u56de\u7684tensor\u5f62\u72b6\u4e0e <code>other</code> \u4e00\u81f4. <pre><code>resize_(*sizes) \u2192 Tensor\n</code></pre> <p>\u7f29\u653e <code>self</code> tensor\u5230\u6307\u5b9a\u7684\u5927\u5c0f. \u5982\u679c\u6307\u5b9a\u7684\u5143\u7d20\u6570\u91cf\u6bd4\u5f53\u524d\u7684\u8981\u5927, \u5e95\u5c42\u7684\u5b58\u50a8\u7ed3\u6784\u4f1a\u7f29\u653e\u5230\u5408\u9002\u7684\u5927\u5c0f. \u5982\u679c\u6570\u91cf\u66f4\u5c0f, \u5e95\u5c42\u5b58\u50a8\u4e0d\u53d8. \u5f53\u524d\u7684\u5143\u7d20\u90fd\u4f1a\u88ab\u4fdd\u7559, \u6ca1\u6709\u4efb\u4f55\u7684\u65b0\u7684\u521d\u59cb\u5316.</p> <p>\u8b66\u544a</p> <p>\u8fd9\u662f\u4e00\u4e2a\u5e95\u5c42\u7684\u64cd\u4f5c. \u5b58\u50a8\u88ab\u91cd\u65b0\u89e3\u91ca\u4e3aC-contiguous, \u5ffd\u7565\u5f53\u524dstride(\u9664\u975e\u76ee\u6807\u5927\u5c0f\u7b49\u4e8e\u5f53\u524d\u5927\u5c0f, \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0btensor\u4fdd\u6301\u4e0d\u53d8\uff09.\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b, \u60a8\u5c06\u8981\u4f7f\u7528 <code>view()</code>, \u5b83\u4f1a\u68c0\u67e5\u8fde\u7eed\u6027, \u6216\u8005 <code>reshape()</code>, \u5728\u5fc5\u8981\u7684\u65f6\u5019\u4f1a\u62f7\u8d1d\u6570\u636e. \u5982\u679c\u60f3\u8981\u6539\u53d8\u5927\u5c0f\u5e76\u4e14\u81ea\u5b9a\u4e49stride, \u89c1 <code>set_()</code>.</p> \u53c2\u6570: sizes (torch.Size or int...) \u2013 \u671f\u671b\u7684\u5927\u5c0f <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; x.resize_(2, 2)\ntensor([[ 1,  2],\n [ 3,  4]])\n\n</code></pre> <pre><code>resize_as_(tensor) \u2192 Tensor\n</code></pre> <p>\u7f29\u653e <code>self</code> tensor \u7684\u5927\u5c0f\u4e0e\u53c2\u6570 <code>tensor</code> \u76f8\u540c. \u7b49\u4ef7\u4e8e <code>self.resize_(tensor.size())</code>.</p> <pre><code>round() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.round()</code></p> <pre><code>round_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>round()</code></p> <pre><code>rsqrt() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.rsqrt()</code></p> <pre><code>rsqrt_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>rsqrt()</code></p> <pre><code>scatter_(dim, index, src) \u2192 Tensor\n</code></pre> <p>\u6839\u636e <code>index</code> tensor \u4e2d\u6307\u5b9a\u7684\u7d22\u5f15, \u5c06\u6240\u6709 tensor <code>src</code> \u4e2d\u7684\u503c\u5199\u5165<code>self</code> . \u5bf9\u4e8e  <code>src</code> \u4e2d\u7684\u6bcf\u4e00\u4e2a\u503c, \u5f53 <code>dimension != dim</code>, \u5b83\u7684\u8f93\u51fa\u7684\u7d22\u5f15\u7531 <code>src</code> \u4e2d\u7684\u7d22\u5f15\u6307\u5b9a, \u5f53 <code>dimension = dim</code>,  \u7531 <code>index</code> \u4e2d\u5bf9\u5e94\u7684\u503c\u6307\u5b9a.</p> <p>\u5bf9\u4e8e\u4e00\u4e2a 3-D tensor, <code>self</code> \u7684\u66f4\u65b0\u89c4\u5219\u5982\u4e0b:</p> <pre><code>self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n\n</code></pre> <p>\u8fd9\u662f <code>gather()</code> \u4e2d\u63cf\u8ff0\u7684\u65b9\u5f0f\u7684\u9006\u5411\u64cd\u4f5c.</p> <p><code>self</code>, <code>index</code> and <code>src</code> (if it is a Tensor) \u5e94\u8be5\u6709\u76f8\u540c\u6570\u91cf\u7684\u7ef4\u5ea6. \u540c\u65f6\u4e5f\u8981\u6c42 <code>index.size(d) &lt;= src.size(d)</code> \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u7ef4\u5ea6 <code>d</code>, \u800c\u4e14 <code>index.size(d) &lt;= self.size(d)</code> \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u7ef4\u5ea6 <code>d != dim</code>.</p> <p>\u6b64\u5916, \u5173\u4e8e <code>gather()</code>,  <code>index</code> \u7684\u503c\u5fc5\u987b\u4ecb\u4e8e <code>0</code> \u548c <code>self.size(dim) - 1</code> (\u5305\u62ec), \u5e76\u4e14\u6cbf\u7740\u6307\u5b9a\u7ef4\u5ea6<code>dim</code>\u7684\u884c\u4e2d\u7684\u6240\u6709\u503c\u5fc5\u987b\u662f\u552f\u4e00\u7684.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u8981\u7d22\u5f15\u7684\u8f74</li> <li>index (LongTensor) \u2013 \u9700\u8981 scatter \u7684\u5143\u7d20\u7684\u7d22\u5f15, \u53ef\u4ee5\u662f\u7a7a\u7684\uff0c\u4e5f\u53ef\u4ee5\u4e0esrc\u5927\u5c0f\u76f8\u540c\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u64cd\u4f5c\u8fd4\u56de\u6052\u7b49</li> <li>src (Tensor or float) \u2013 scatter \u6e90</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)\n&gt;&gt;&gt; x\ntensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],\n [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])\n&gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\ntensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],\n [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],\n [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])\n\n&gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)\n&gt;&gt;&gt; z\ntensor([[ 0.0000,  0.0000,  1.2300,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  1.2300]])\n\n</code></pre> <pre><code>scatter_add_(dim, index, other) \u2192 Tensor\n</code></pre> <p>\u6839\u636e <code>index</code> tensor \u4e2d\u6307\u5b9a\u7684\u7d22\u5f15(\u65b9\u5f0f\u548c<code>scatter_()</code>\u7c7b\u4f3c), \u5c06\u6240\u6709 tensor <code>other</code> \u4e2d\u7684\u503c\u52a0\u5230<code>self</code> . \u5bf9\u4e8e  <code>other</code> \u4e2d\u7684\u6bcf\u4e00\u4e2a\u503c, \u5f53 <code>dimension != dim</code>, \u5b83\u7684\u8f93\u51fa\u7684\u7d22\u5f15\u7531 <code>other</code> \u4e2d\u7684\u7d22\u5f15\u6307\u5b9a, \u5f53 <code>dimension = dim</code>,  \u7531 <code>index</code> \u4e2d\u5bf9\u5e94\u7684\u503c\u6307\u5b9a.</p> <p>\u5bf9\u4e8e\u4e00\u4e2a 3-D tensor, <code>self</code> \u7684\u66f4\u65b0\u89c4\u5219\u5982\u4e0b:</p> <pre><code>self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2\n\n</code></pre> <p><code>self</code>, <code>index</code> and <code>other</code> \u5e94\u8be5\u6709\u76f8\u540c\u6570\u91cf\u7684\u7ef4\u5ea6. \u4e5f\u8981\u6c42 <code>index.size(d) &lt;= other.size(d)</code> \u5bf9\u4e8e\u6240\u6709\u7684\u7ef4\u5ea6 <code>d</code>, \u5e76\u4e14 <code>index.size(d) &lt;= self.size(d)</code> \u5bf9\u4e8e\u6240\u6709\u7684\u7ef4\u5ea6 <code>d != dim</code>.</p> <p>\u6b64\u5916, \u5173\u4e8e <code>gather()</code>,  <code>index</code> \u7684\u503c\u5fc5\u987b\u4ecb\u4e8e <code>0</code> \u548c <code>self.size(dim) - 1</code> (\u5305\u62ec), \u5e76\u4e14\u6cbf\u7740\u6307\u5b9a\u7ef4\u5ea6<code>dim</code>\u7684\u884c\u4e2d\u7684\u6240\u6709\u503c\u5fc5\u987b\u662f\u552f\u4e00\u7684.</p> <p>\u6ce8\u610f</p> <p>\u5f53\u4f7f\u7528 CUDA \u4f5c\u4e3a\u540e\u7aef, \u8fd9\u4e2a\u64cd\u4f5c\u5c06\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u884c\u4e3a, \u5e76\u4e14\u96be\u4ee5\u505c\u6b62. \u8bf7\u53c2\u8003 Reproducibility \u83b7\u5f97\u76f8\u5173\u80cc\u666f.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u8981\u7d22\u5f15\u7684\u8f74</li> <li>index (LongTensor) \u2013 \u9700\u8981 scatter add \u7684\u5143\u7d20\u7684\u7d22\u5f15, \u53ef\u4ee5\u662f\u7a7a\u7684\uff0c\u4e5f\u53ef\u4ee5\u4e0esrc\u5927\u5c0f\u76f8\u540c\u3002\u5f53\u4e3a\u7a7a\u65f6\uff0c\u64cd\u4f5c\u8fd4\u56de\u6052\u7b49</li> <li>src (Tensor or float) \u2013 scatter \u6e90</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.rand(2, 5)\n&gt;&gt;&gt; x\ntensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],\n [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])\n&gt;&gt;&gt; torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\ntensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],\n [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],\n [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])\n\n</code></pre> <pre><code>select(dim, index) \u2192 Tensor\n</code></pre> <p>\u6cbf\u7740\u9009\u62e9\u7684\u7ef4\u5ea6\u5728\u7ed9\u5b9a\u7684\u7d22\u5f15\u5904\u5207\u53d6 <code>self</code> tensor.\u8fd9\u4e2a\u51fd\u6570\u8fd4\u56de\u7684 tensor \u6307\u5b9a\u7684\u7ef4\u5ea6\u88ab\u79fb\u9664\u4e86.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u8981\u5207\u7247\u7684\u7ef4\u5ea6</li> <li>index (int) \u2013 \u9009\u62e9\u7684\u7d22\u5f15</li> </ul> <p>\u6ce8\u610f</p> <p><code>select()</code> \u7b49\u4ef7\u4e8e\u5207\u7247. \u4f8b\u5982, <code>tensor.select(0, index)</code> \u7b49\u4ef7\u4e8e <code>tensor[index]</code> and <code>tensor.select(2, index)</code> \u7b49\u4ef7\u4e8e <code>tensor[:,:,index]</code>.</p> <pre><code>set_(source=None, storage_offset=0, size=None, stride=None) \u2192 Tensor\n</code></pre> <p>\u8bbe\u7f6e\u5e95\u5c42\u5b58\u50a8, \u5927\u5c0f, \u548c strides. \u5982\u679c <code>source</code> \u662f\u4e00\u4e2a tensor, <code>self</code> tensor \u5c06\u4f1a\u548c <code>source</code> \u5171\u4eab\u5e95\u5c42\u5b58\u50a8, \u5e76\u6709\u7528\u4e00\u6837\u7684\u5927\u5c0f\u548c strides. \u5728\u4e00\u4e2a tensor \u4e2d\u6539\u53d8\u5143\u7d20\u5c06\u4f1a\u53cd\u5e94\u5230\u53e6\u4e00\u4e2atensor.</p> <p>\u5982\u679c <code>source</code> \u662f\u4e00\u4e2a <code>Storage</code>, \u6b64\u65b9\u6cd5\u8bbe\u7f6e\u5e95\u5c42\u5b58\u50a8, offset, \u5927\u5c0f, \u548c stride.</p> <p>\u53c2\u6570: </p> <ul> <li>source (Tensor or Storage) \u2013 \u8981\u8bbe\u7f6e\u7684 tensor \u6216\u8005 storage</li> <li>storage_offset (int, optional) \u2013 storage \u7684 offset</li> <li>size (torch.Size__, optional) \u2013 \u671f\u671b\u7684\u5927\u5c0f.\u9ed8\u8ba4\u662f source \u7684\u5927\u5c0f.</li> <li>stride (tuple, optional) \u2013 \u671f\u671b\u7684 stride.\u9ed8\u8ba4\u503c\u662f C-contiguous strides.</li> </ul> <pre><code>share_memory_()\n</code></pre> <p>\u79fb\u52a8\u5e95\u5c42\u5b58\u50a8\u5230\u5171\u4eab\u5185\u5b58.</p> <p>\u8fd9\u662f\u4e00\u4e2a\u7a7a\u64cd\u4f5c\u5982\u679c\u5e95\u5c42\u5b58\u50a8\u5df2\u7ecf\u5728\u5171\u4eab\u5185\u5b58\u4e2d\u6216\u8005\u662f CUDA tensors. \u5171\u4eab\u5185\u5b58\u4e2d\u7684 tensor \u4e0d\u80fd resize.</p> <pre><code>short() \u2192 Tensor\n</code></pre> <p><code>self.short()</code> \u7b49\u4ef7\u4e8e <code>self.to(torch.int16)</code>. \u89c1 <code>to()</code>.</p> <pre><code>sigmoid() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sigmoid()</code></p> <pre><code>sigmoid_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sigmoid()</code></p> <pre><code>sign() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sign()</code></p> <pre><code>sign_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sign()</code></p> <pre><code>sin() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sin()</code></p> <pre><code>sin_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sin()</code></p> <pre><code>sinh() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sinh()</code></p> <pre><code>sinh_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sinh()</code></p> <pre><code>size() \u2192 torch.Size\n</code></pre> <p>\u8fd4\u56de <code>self</code> tensor \u7684\u5c3a\u5bf8. \u8fd4\u56de\u503c\u662f  [<code>tuple</code>] \u7684\u5b50\u7c7b(https://docs.python.org/3/library/stdtypes.html#tuple \"(in Python v3.7)\").</p> <p>\u4f8b\u5982:</p> <pre><code>&gt;&gt;&gt; torch.empty(3, 4, 5).size()\ntorch.Size([3, 4, 5])\n\n</code></pre> <pre><code>slogdet() -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.slogdet()</code></p> <pre><code>sort(dim=None, descending=False) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u89c1 <code>torch.sort()</code></p> <pre><code>split(split_size, dim=0)\n</code></pre> <p>\u89c1 <code>torch.split()</code></p> <pre><code>sparse_mask(input, mask) \u2192 Tensor\n</code></pre> <p>\u7528  <code>mask</code> \u7684\u7d22\u5f15\u8fc7\u6ee4 Tensor <code>input</code>, \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 SparseTensor. <code>input</code> \u548c <code>mask</code> \u5fc5\u987b\u6709\u76f8\u540c\u7684\u5f62\u72b6.</p> <p>\u53c2\u6570: </p> <ul> <li>input (Tensor) \u2013 \u8f93\u5165 Tensor</li> <li>mask (SparseTensor) \u2013 SparseTensor \u7528\u5176\u7d22\u5f15\u8fc7\u6ee4 <code>input</code> </li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; nnz = 5\n&gt;&gt;&gt; dims = [5, 5, 2, 2]\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n&gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3])\n&gt;&gt;&gt; size = torch.Size(dims)\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce()\n&gt;&gt;&gt; D = torch.randn(dims)\n&gt;&gt;&gt; D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n [0, 1, 4, 3]]),\n values=tensor([[[ 1.6550,  0.2397],\n [-0.1611, -0.0779]],\n\n [[ 0.2326, -1.0558],\n [ 1.4711,  1.9678]],\n\n [[-0.5138, -0.0411],\n [ 1.9417,  0.5158]],\n\n [[ 0.0793,  0.0036],\n [-0.2569, -0.1055]]]),\n size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n\n</code></pre> <pre><code>sqrt() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sqrt()</code></p> <pre><code>sqrt_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sqrt()</code></p> <pre><code>squeeze(dim=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.squeeze()</code></p> <pre><code>squeeze_(dim=None) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>squeeze()</code></p> <pre><code>std(dim=None, unbiased=True, keepdim=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.std()</code></p> <pre><code>storage() \u2192 torch.Storage\n</code></pre> <p>\u8fd4\u56de\u5e95\u5c42\u7684 storage</p> <pre><code>storage_offset() \u2192 int\n</code></pre> <p>\u6839\u636e\u5b58\u50a8\u5143\u7d20\u7684\u6570\u91cf(\u800c\u4e0d\u662f\u5b57\u8282)\uff0c\u8fd4\u56de\u5e95\u5c42\u5b58\u50a8\u4e2d\u7684<code>tesor</code>\u504f\u79fb\u91cf(offset)\u3002</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5])\n&gt;&gt;&gt; x.storage_offset()\n0\n&gt;&gt;&gt; x[3:].storage_offset()\n3\n\n</code></pre> <pre><code>storage_type()\n</code></pre> <pre><code>stride(dim) \u2192 tuple or int\n</code></pre> <p>\u8fd4\u56de <code>self</code> tensor \u7684 stride.</p> <p>stride \u662f\u5fc5\u8981\u7684\u7528\u4e8e\u5728\u6307\u5b9a\u7684\u7ef4\u5ea6 <code>dim</code> \u627e\u5230\u4e0b\u4e00\u4e2a\u5143\u7d20. \u5982\u679c\u4f20\u5165\u7a7a, \u5219\u8fd4\u56de\u4e00\u4e2a tuple \u5305\u542b\u6240\u6709\u7ef4\u5ea6\u7684 stride. \u5426\u5219, \u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a int \u8868\u793a\u6307\u5b9a\u7ef4\u5ea6 <code>dim</code> \u7684 stride.</p> \u53c2\u6570: dim (int, optional) \u2013 \u9700\u8981\u8fd4\u56de stride \u7684\u7ef4\u5ea6 <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; x.stride()\n(5, 1)\n&gt;&gt;&gt;x.stride(0)\n5\n&gt;&gt;&gt; x.stride(-1)\n1\n\n</code></pre> <pre><code>sub(value, other) \u2192 Tensor\n</code></pre> <p><code>self</code> tensor \u51cf\u53bb\u4e00\u4e2a scalar \u6216\u8005 tensor. \u5982\u679c <code>value</code> \u548c <code>other</code> \u90fd\u88ab\u6307\u5b9a,  \u5728\u76f8\u51cf\u4e4b\u524d, <code>other</code> \u7684\u6bcf\u4e2a\u5143\u7d20\u5c06\u4f1a\u7528 <code>value</code> \u7f29\u653e.</p> <p>\u5f53 <code>other</code> \u662f\u4e00\u4e2a tensor,  <code>other</code> \u7684\u5f62\u72b6\u5fc5\u987b\u548c\u5e95\u5c42\u5b58\u50a8\u662f\u53ef\u5e7f\u64ad\u7684 broadcastable .</p> <pre><code>sub_(x) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>sub()</code></p> <pre><code>sum(dim=None, keepdim=False, dtype=None) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.sum()</code></p> <pre><code>svd(some=True, compute_uv=True) -&gt; (Tensor, Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.svd()</code></p> <pre><code>symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.symeig()</code></p> <pre><code>t() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.t()</code></p> <pre><code>t_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>t()</code></p> <pre><code>to(*args, **kwargs) \u2192 Tensor\n</code></pre> <p>\u6267\u884c tensor \u7c7b\u578b\u6216\u8005\u8bbe\u5907\u8f6c\u6362. <code>torch.dtype</code> \u548c <code>torch.device</code> \u662f\u4ece\u53c2\u6570\u4e2d\u63a8\u65ad\u7684 <code>self.to(*args, **kwargs)</code>.</p> <p>\u6ce8\u610f</p> <p>\u5982\u679c <code>self</code> Tensor \u5df2\u7ecf\u6709\u6b63\u786e\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>, \u5219 <code>self</code> \u88ab\u8fd4\u56de. \u5426\u5219, \u5c06\u8fd4\u56de\u590d\u5236\u7684 <code>self</code> \u671f\u671b\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>.</p> <p>\u4e0b\u9762\u662f\u8c03\u7528\u7684\u65b9\u6cd5 <code>to</code>:</p> <pre><code>to(dtype, non_blocking=False, copy=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a Tensor \u6307\u5b9a\u7c7b\u578b <code>dtype</code></p> <pre><code>to(device=None, dtype=None, non_blocking=False, copy=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a Tensor \u5e76\u6307\u5b9a <code>device</code> \u548c (\u53ef\u9009\u7684) <code>dtype</code>. \u5982\u679c <code>dtype</code> \u662f <code>None</code> \u5219\u63a8\u65ad\u4e3a <code>self.dtype</code> . \u5f53\u542f\u7528 <code>non_blocking</code>, \u8bd5\u56fe\u5728\u4e3b\u673a\u4e0a\u6267\u884c\u5f02\u6b65\u8f6c\u6362, \u4f8b\u5982, \u8f6c\u6362\u4e00\u4e2a pinned memory \u7684 CPU Tensor \u5230 CUDA Tensor. \u5f53 <code>copy</code> \u88ab\u8bbe\u7f6e, \u4e00\u4e2a\u65b0\u7684 tensor \u88ab\u521b\u5efa.</p> <pre><code>to(other, non_blocking=False, copy=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a Tensor \u5e76\u6709\u548c Tensor <code>other</code> \u76f8\u540c\u7684 <code>torch.dtype</code> \u548c <code>torch.device</code>. \u5f53\u542f\u7528 <code>non_blocking</code>, \u8bd5\u56fe\u5728\u4e3b\u673a\u4e0a\u6267\u884c\u5f02\u6b65\u8f6c\u6362, \u4f8b\u5982, \u8f6c\u6362\u4e00\u4e2a pinned memory \u7684 CPU Tensor \u5230 CUDA Tensor. \u5f53 <code>copy</code> \u88ab\u8bbe\u7f6e, \u4e00\u4e2a\u65b0\u7684 tensor \u88ab\u521b\u5efa.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n&gt;&gt;&gt; tensor.to(torch.float64)\ntensor([[-0.5044,  0.0005],\n [ 0.3310, -0.0584]], dtype=torch.float64)\n\n&gt;&gt;&gt; cuda0 = torch.device('cuda:0')\n&gt;&gt;&gt; tensor.to(cuda0)\ntensor([[-0.5044,  0.0005],\n [ 0.3310, -0.0584]], device='cuda:0')\n\n&gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64)\ntensor([[-0.5044,  0.0005],\n [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n&gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0)\n&gt;&gt;&gt; tensor.to(other, non_blocking=True)\ntensor([[-0.5044,  0.0005],\n [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n</code></pre> <pre><code>take(indices) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.take()</code></p> <pre><code>tan()\n</code></pre> <pre><code>tan_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>tan()</code></p> <pre><code>tanh() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.tanh()</code></p> <pre><code>tanh_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>tanh()</code></p> <pre><code>tolist()\n</code></pre> <p>\u201d tolist() -&gt; list or number</p> <p>\u8fd4\u56detensor \u4f5c\u4e3a(\u5d4c\u5957\u7684) list. \u5bf9\u4e8e scalars,\u4e00\u4e2a\u6807\u51c6\u7684 Python number \u88ab\u8fd4\u56de, \u5c31\u50cf <code>item()</code> \u4e00\u6837. Tensors \u4f1a\u81ea\u52a8\u79fb\u52a8\u5230 CPU \u4e0a\u5982\u679c\u6709\u5fc5\u8981.</p> <p>\u8fd9\u4e2a\u64cd\u4f5c\u662f\u4e0d\u53ef\u5fae\u5206\u7684.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(2, 2)\n&gt;&gt;&gt; a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n&gt;&gt;&gt; a[0,0].tolist()\n0.012766935862600803\n\n</code></pre> <pre><code>topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u89c1 <code>torch.topk()</code></p> <pre><code>to_sparse(sparseDims) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7a00\u758f\u590d\u5236\u7684 tensor. PyTorch \u652f\u6301 coordinate \u683c\u5f0f \u7684\u7a00\u758f tensors. :param sparseDims: \u8981\u5305\u542b\u5728\u65b0\u7a00\u758ftensor\u4e2d\u7684\u7a00\u758f\u7ef4\u6570 :type sparseDims: int, \u53ef\u9009\u7684</p> <pre><code>\u4f8b\u5b50::\n</code></pre> <pre><code>&gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n&gt;&gt;&gt; d\ntensor([[ 0,  0,  0],\n [ 9,  0, 10],\n [ 0,  0,  0]])\n&gt;&gt;&gt; d.to_sparse()\ntensor(indices=tensor([[1, 1],\n [0, 2]]),\n values=tensor([ 9, 10]),\n size=(3, 3), nnz=2, layout=torch.sparse_coo)\n&gt;&gt;&gt; d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n values=tensor([[ 9,  0, 10]]),\n size=(3, 3), nnz=1, layout=torch.sparse_coo)\n\n</code></pre> <pre><code>trace() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.trace()</code></p> <pre><code>transpose(dim0, dim1) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.transpose()</code></p> <pre><code>transpose_(dim0, dim1) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>transpose()</code></p> <pre><code>tril(k=0) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.tril()</code></p> <pre><code>tril_(k=0) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>tril()</code></p> <pre><code>triu(k=0) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.triu()</code></p> <pre><code>triu_(k=0) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>triu()</code></p> <pre><code>trtrs(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u89c1 <code>torch.trtrs()</code></p> <pre><code>trunc() \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.trunc()</code></p> <pre><code>trunc_() \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>trunc()</code></p> <pre><code>type(dtype=None, non_blocking=False, **kwargs) \u2192 str or Tensor\n</code></pre> <p>\u8fd4\u56de type \u5982\u679c <code>dtype</code> \u6ca1\u6709\u88ab\u8bbe\u7f6e, \u5426\u5219\u5c06\u4f1a\u5f3a\u5236\u8f6c\u6362\u6210 <code>dtype</code> \u7c7b\u578b.</p> <p>\u5982\u679c\u8fd9\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b\uff0c\u5219\u4e0d\u6267\u884c\u590d\u5236\uff0c\u5e76\u8fd4\u56de\u539f\u59cb\u5bf9\u8c61.</p> <p>\u53c2\u6570: </p> <ul> <li>dtype (type or string) \u2013 \u671f\u671b\u7c7b\u578b</li> <li>non_blocking (bool) \u2013  \u5982\u679c <code>True</code>\uff0c\u5e76\u4e14\u6e90\u5728pinned memory\u4e2d\uff0c\u76ee\u7684\u5730\u5728GPU\u4e0a\uff0c\u5219\u62f7\u8d1d\u76f8\u5bf9\u4e8e\u4e3b\u673a\u5f02\u6b65\u6267\u884c\u3002\u5426\u5219\uff0c\u8fd9\u4e2a\u53c2\u6570\u6ca1\u6709\u4efb\u4f55\u4f5c\u7528\u3002</li> <li>**kwargs \u2013 \u4e3a\u4e86\u517c\u5bb9\u6027, \u53ef\u80fd\u5305\u542b <code>async</code> \u7528\u6765\u7f6e\u6362 <code>non_blocking</code> \u53c2\u6570.  <code>async</code> \u53c2\u6570\u88ab\u5e9f\u5f03\u4e86.</li> </ul> <pre><code>type_as(tensor) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de tensor \u5f3a\u5236\u8f6c\u6362\u4e3a tensor \u7684\u6570\u636e\u7c7b\u578b.</p> <p>\u5982\u679c\u8fd9\u5df2\u7ecf\u662f\u6b63\u786e\u7684\u7c7b\u578b\uff0c\u5219\u662f\u7a7a\u64cd\u4f5c. \u7b49\u4ef7\u4e8e:</p> <pre><code>self.type(tensor.type())\n\n</code></pre> <pre><code>Params:\n</code></pre> <p>tensor (Tensor): \u62e5\u6709\u76ee\u6807\u6570\u636e\u7c7b\u578b\u7684 tensor</p> <pre><code>unfold(dim, size, step) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a tensor \u5305\u542b <code>self</code> tensor \u5728\u7ef4\u5ea6 <code>dim</code> \u4e0a\u7684\u6240\u6709\u5207\u7247, \u6bcf\u4e00\u4e2a\u7684\u5927\u5c0f\u4e3a size.</p> <p><code>step</code> \u6307\u5b9a\u6bcf\u4e00\u4e2a\u5207\u7247\u7684\u95f4\u8ddd.</p> <p>\u5982\u679c <code>sizedim</code> \u662f <code>self</code> dim \u7ef4\u5ea6\u7684\u5927\u5c0f, \u8fd4\u56de\u7684 tensor \u7684\u7ef4\u5ea6 <code>dim</code> \u5927\u5c0f\u662f <code>(sizedim - size) / step + 1</code>.</p> <p>\u4e00\u4e2a\u9644\u52a0\u7684size size\u7684\u7ef4\u5ea6\u8ffd\u52a0\u4e8e\u8fd4\u56de\u7684 tensor.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u6307\u5b9a unfold \u7684\u7ef4\u5ea6</li> <li>size (int) \u2013 \u6307\u5b9a\u6bcf\u4e2aslice\u7684\u5927\u5c0f</li> <li>step (int) \u2013 \u6307\u5b9a\u6b65\u957f</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(1., 8)\n&gt;&gt;&gt; x\ntensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n&gt;&gt;&gt; x.unfold(0, 2, 1)\ntensor([[ 1.,  2.],\n [ 2.,  3.],\n [ 3.,  4.],\n [ 4.,  5.],\n [ 5.,  6.],\n [ 6.,  7.]])\n&gt;&gt;&gt; x.unfold(0, 2, 2)\ntensor([[ 1.,  2.],\n [ 3.,  4.],\n [ 5.,  6.]])\n\n</code></pre> <pre><code>uniform_(from=0, to=1) \u2192 Tensor\n</code></pre> <p>\u7528\u8fde\u7eed\u5747\u5300\u5206\u5e03\u7684\u91c7\u6837\u503c\u586b\u5145 <code>self</code> tensor:</p> <p></p> <pre><code>unique(sorted=False, return_inverse=False, dim=None)\n</code></pre> <p>\u8fd4\u56de tensor \u4e2d\u552f\u4e00\u7684\u6807\u91cf\u4f5c\u4e3a 1-D tensor.</p> <p>\u89c1 <code>torch.unique()</code></p> <pre><code>unsqueeze(dim) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.unsqueeze()</code></p> <pre><code>unsqueeze_(dim) \u2192 Tensor\n</code></pre> <p>\u539f\u5730\u7248\u672c\u7684 <code>unsqueeze()</code></p> <pre><code>var(dim=None, unbiased=True, keepdim=False) \u2192 Tensor\n</code></pre> <p>\u89c1 <code>torch.var()</code></p> <pre><code>view(*shape) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684 tersor, \u548c <code>self</code> \u6709\u76f8\u540c\u7684\u6570\u636e, \u4f46\u662f\u6709\u4e0d\u540c\u7684 <code>shape</code>.</p> <p>\u8fd4\u56de\u7684 tensor \u5171\u4eab\u76f8\u540c\u7684\u6570\u636e\uff0c\u5e76\u4e14\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u5143\u7d20\uff0c\u4f46\u662f\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u5927\u5c0f\u3002\u8981 <code>view()</code> \u4e00\u4e2atensor\uff0c\u65b0\u89c6\u56fe\u5927\u5c0f\u5fc5\u987b\u4e0e\u5176\u539f\u59cb\u5927\u5c0f\u548c stride \u517c\u5bb9, \u4f8b\u5982, \u6bcf\u4e2a\u65b0\u89c6\u56fe\u7ef4\u5ea6\u5fc5\u987b\u662f\u539f\u59cb\u7ef4\u5ea6\u7684\u5b50\u7a7a\u95f4\uff0c\u6216\u8005\u4ec5\u8de8\u8d8a\u539f\u59cb\u7ef4\u5ea6  \u6ee1\u8db3\u4ee5\u4e0b\u8fde\u7eed\u6027\u6761\u4ef6 ,</p> <p></p> <p>\u5426\u5219\u5728 <code>view()</code> \u4e4b\u524d, <code>contiguous()</code> \u9700\u8981\u88ab\u8c03\u7528. \u53ef\u53c2\u8003: <code>reshape()</code>, \u8fd4\u56de\u4e00\u4e2aview \u5f53\u5f62\u72b6\u662f\u517c\u5bb9\u7684, \u5426\u5219\u590d\u5236 (\u7b49\u4ef7\u4e8e\u8c03\u7528 <code>contiguous()</code>).</p> \u53c2\u6570: shape (torch.Size or int...) \u2013 the desired size <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)\n&gt;&gt;&gt; x.size()\ntorch.Size([4, 4])\n&gt;&gt;&gt; y = x.view(16)\n&gt;&gt;&gt; y.size()\ntorch.Size([16])\n&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n&gt;&gt;&gt; z.size()\ntorch.Size([2, 8])\n\n</code></pre> <pre><code>view_as(other) \u2192 Tensor\n</code></pre> <p>\u4f7f\u7528 <code>other</code> \u7684\u5927\u5c0f View tensor . <code>self.view_as(other)</code> \u7b49\u4ef7\u4e8e <code>self.view(other.size())</code>.</p> <p>\u8bf7\u53c2\u8003 <code>view()</code> \u83b7\u5f97\u66f4\u591a\u4fe1\u606f\u5173\u4e8e <code>view</code>.</p> \u53c2\u6570: other (<code>torch.Tensor</code>) \u2013 \u8fd4\u56de\u7684tensor \u548c <code>other</code> \u5927\u5c0f\u76f8\u540c. <pre><code>zero_() \u2192 Tensor\n</code></pre> <p>\u7528 0 \u586b\u5145 <code>self</code> tensor.</p> <pre><code>class torch.ByteTensor\n</code></pre> <p>\u4e0b\u9762\u7684\u65b9\u6cd5\u662f <code>torch.ByteTensor</code> \u72ec\u5360.</p> <pre><code>all()\n</code></pre> <pre><code>all() \u2192 bool\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c\u6240\u6709\u7684\u5143\u7d20\u975e\u96f6, \u5426\u5219 False.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3).byte() % 2\n&gt;&gt;&gt; a\ntensor([[1, 0, 0]], dtype=torch.uint8)\n&gt;&gt;&gt; a.all()\ntensor(0, dtype=torch.uint8)\n\n</code></pre> <pre><code>all(dim, keepdim=False, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c tensor \u5728\u6307\u5b9a\u7ef4\u5ea6<code>dim</code>\u6bcf\u4e00\u884c\u7684\u6240\u6709\u7684\u5143\u7d20\u975e\u96f6, \u5426\u5219 False.</p> <p>\u5982\u679c <code>keepdim</code> \u662f <code>True</code>, \u5219\u8f93\u51fa tensor \u7684\u5927\u5c0f\u4e0e <code>input</code>\u76f8\u540c, \u4f46\u5c3a\u5bf8\u4e3a1\u7684\u7ef4\u5ea6<code>dim</code>\u9664\u5916. \u5426\u5219, <code>dim</code> \u4f1a\u88ab\u538b\u7f29 (\u89c1 <code>torch.squeeze()</code>), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4<code>input</code>\u5c111\u7ef4.</p> <p>Parameters: </p> <ul> <li>dim (int) \u2013 \u8981reduce\u7684\u7ef4\u5ea6</li> <li>keepdim (bool) \u2013 output tensor \u662f\u5426\u4fdd\u7559 <code>dim</code> </li> <li>out (Tensor, \u53ef\u9009\u7684) \u2013 output tensor</li> </ul> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 2).byte() % 2\n&gt;&gt;&gt; a\ntensor([[0, 0],\n [0, 0],\n [0, 1],\n [1, 1]], dtype=torch.uint8)\n&gt;&gt;&gt; a.all(dim=1)\ntensor([0, 0, 0, 1], dtype=torch.uint8)\n\n</code></pre> <pre><code>any()\n</code></pre> <pre><code>any() \u2192 bool\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c\u4efb\u610f\u5143\u7d20\u975e\u96f6, \u5426\u5219 False.</p> <p>\u4f8b\u5b50:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3).byte() % 2\n&gt;&gt;&gt; a\ntensor([[0, 0, 1]], dtype=torch.uint8)\n&gt;&gt;&gt; a.any()\ntensor(1, dtype=torch.uint8)\n\n</code></pre> <pre><code>any(dim, keepdim=False, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de True \u5982\u679c tensor \u5728\u6307\u5b9a\u7ef4\u5ea6<code>dim</code>\u6bcf\u4e00\u884c\u7684\u4efb\u610f\u7684\u5143\u7d20\u975e\u96f6, \u5426\u5219 False.</p> <p>\u5982\u679c <code>keepdim</code> \u662f <code>True</code>, \u5219\u8f93\u51fa tensor \u7684\u5927\u5c0f\u4e0e <code>input</code>\u76f8\u540c, \u4f46\u5c3a\u5bf8\u4e3a1\u7684\u7ef4\u5ea6<code>dim</code>\u9664\u5916. \u5426\u5219, <code>dim</code> \u4f1a\u88ab\u538b\u7f29 (\u89c1 <code>torch.squeeze()</code>), \u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4<code>input</code>\u5c111\u7ef4.</p> <p>\u53c2\u6570: </p> <ul> <li>dim (int) \u2013 \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim (bool) \u2013 output tensor \u662f\u5426\u4fdd\u7559 <code>dim</code> </li> <li>out (Tensor, \u53ef\u9009\u7684) \u2013 output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 2).byte() % 2\n&gt;&gt;&gt; a\ntensor([[1, 0],\n [0, 0],\n [0, 1],\n [0, 0]], dtype=torch.uint8)\n&gt;&gt;&gt; a.any(dim=1)\ntensor([1, 0, 1, 0], dtype=torch.uint8)\n\n</code></pre>"},{"location":"1.0/torch/","title":"torch","text":"<p>\u8bd1\u8005\uff1acluster</p> <p>torch package \u5305\u542b \u591a\u7ef4\u5f20\u91cf\u548c\u4e4b\u524d\u5b9a\u4e49\u8fc7\u6570\u5b66\u8fd0\u7b97\u7684\u6570\u636e\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5b83\u63d0\u4f9b\u4e86\u652f\u6301\u8bb8\u591a\u9ad8\u6548\u5e8f\u5217\u5316\u7684\u5f20\u91cf(\u6216\u8005\u4efb\u610f\u7c7b\u578b) \u7684\u7a0b\u5e8f\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e9b\u6709\u7528\u5de5\u5177\u3002</p> <p>\u5b83\u652f\u6301CUDA\u73af\u5883\uff0c\u4f7f\u4f60\u53ef\u4ee5\u5728NVIDIA GPU\u8fdb\u884c\u5f20\u91cf\u8fd0\u7b97(\u8981\u6c42compute capability \u7248\u672c3.0\u53ca\u4ee5\u4e0a)</p>"},{"location":"1.0/torch_math_operations/","title":"Torch math operations","text":""},{"location":"1.0/torch_math_operations/#math-operations","title":"Math operations","text":""},{"location":"1.0/torch_math_operations_blas_lapack_ops/","title":"BLAS and LAPACK Operations","text":""},{"location":"1.0/torch_math_operations_blas_lapack_ops/#blaslapack","title":"BLAS\u548cLAPACK\u64cd\u4f5c","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u5b58\u50a8\u5728<code>batch1</code>\u548c<code>batch2</code>\u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u77e9\u9635 - \u77e9\u9635\u4e58\u79ef\uff0c\u51cf\u5c11\u52a0\u6cd5\u6b65\u9aa4(\u6240\u6709\u77e9\u9635\u4e58\u6cd5\u6cbf\u7b2c\u4e00\u7ef4\u79ef\u7d2f\uff09\u3002 <code>mat</code>\u88ab\u6dfb\u52a0\u5230\u6700\u7ec8\u7ed3\u679c\u4e2d\u3002</p> <p><code>batch1</code>\u548c<code>batch2</code>\u5fc5\u987b\u662f3-D\u5f20\u91cf\uff0c\u6bcf\u4e2a\u5f20\u91cf\u5305\u542b\u76f8\u540c\u6570\u91cf\u7684\u77e9\u9635\u3002</p> <p>\u5982\u679c<code>batch1</code>\u662f  \u5f20\u91cf\uff0c<code>batch2</code>\u662f  \u5f20\u91cf\uff0c<code>mat</code>\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u548c  \u5f20\u91cf\u548c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002</p> <p></p> <p>\u5bf9\u4e8e<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\u7c7b\u578b\u7684\u8f93\u5165\uff0c\u53c2\u6570<code>beta</code>\u548c<code>alpha</code>\u5fc5\u987b\u662f\u5b9e\u6570\uff0c\u5426\u5219\u5b83\u4eec\u5e94\u8be5\u662f\u6574\u6570\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>beta (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 - <code>mat</code> ()\u7684\u4e58\u6570</li> <li>mat  (Tensor\uff09 - \u8981\u6dfb\u52a0\u7684\u57fa\u8d28</li> <li>alpha (\u6570 \uff0c \u4efb\u9009\uff09 - <code>batch1 @ batch2</code> ()\u7684\u4e58\u6570</li> <li>batch1  (Tensor\uff09 - \u7b2c\u4e00\u6279\u8981\u4e58\u7684\u77e9\u9635</li> <li>batch2  (Tensor\uff09 - \u7b2c\u4e8c\u6279\u77e9\u9635\u88ab\u4e58\u4ee5</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; M = torch.randn(3, 5)\n&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)\n&gt;&gt;&gt; torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n\n</code></pre> <pre><code>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u77e9\u9635<code>mat1</code>\u548c<code>mat2</code>\u7684\u77e9\u9635\u4e58\u6cd5\u3002\u77e9\u9635<code>mat</code>\u88ab\u6dfb\u52a0\u5230\u6700\u7ec8\u7ed3\u679c\u4e2d\u3002</p> <p>\u5982\u679c<code>mat1</code>\u662f  \u5f20\u91cf\uff0c<code>mat2</code>\u662f  \u5f20\u91cf\uff0c\u90a3\u4e48<code>mat</code>\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u548c [] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/42cdcd96fd628658ac0e3e7070ba08d5.jpg) \u5f20\u91cf\u548c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002</p> <p><code>alpha</code>\u548c<code>beta</code>\u5206\u522b\u662f<code>mat1</code>\u548c\uff1aattr <code>mat2</code>\u4e0e\u6dfb\u52a0\u7684\u57fa\u8d28<code>mat</code>\u4e4b\u95f4\u7684\u57fa\u8d28 - \u8f7d\u4f53\u4ea7\u7269\u7684\u6bd4\u4f8b\u56e0\u5b50\u3002</p> <p></p> <p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers.</p> <p>Parameters:</p> <ul> <li>beta (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 - <code>mat</code> ()\u7684\u4e58\u6570</li> <li>mat  (Tensor\uff09 - \u8981\u6dfb\u52a0\u7684\u57fa\u8d28</li> <li>alpha (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 -   ()\u7684\u4e58\u6570</li> <li>mat1  (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u88ab\u4e58\u6cd5\u7684\u77e9\u9635</li> <li>mat2  (Tensor\uff09 - \u8981\u500d\u589e\u7684\u7b2c\u4e8c\u4e2a\u77e9\u9635</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; M = torch.randn(2, 3)\n&gt;&gt;&gt; mat1 = torch.randn(2, 3)\n&gt;&gt;&gt; mat2 = torch.randn(3, 3)\n&gt;&gt;&gt; torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n [ 0.7573, -3.9555, -2.8681]])\n\n</code></pre> <pre><code>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u77e9\u9635<code>mat</code>\u548c\u5411\u91cf<code>vec</code>\u7684\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u3002\u5c06\u8f7d\u4f53 <code>tensor</code> \u6dfb\u52a0\u5230\u6700\u7ec8\u7ed3\u679c\u4e2d\u3002</p> <p>\u5982\u679c<code>mat</code>\u662f  \u5f20\u91cf\uff0c<code>vec</code>\u662f\u5927\u5c0f\u4e3a<code>m</code>\u76841-D\u5f20\u91cf\uff0c\u5219 <code>tensor</code> \u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u5177\u67091-D\u5f20\u91cf\u7684<code>n</code>\u548c<code>out</code>\u5c06\u662f1-D\u5f20\u91cf\u7684\u5927\u5c0f<code>n</code>\u3002</p> <p><code>alpha</code>\u548c<code>beta</code>\u5206\u522b\u662f<code>mat</code>\u548c<code>vec</code>\u4e4b\u95f4\u7684\u57fa\u8d28 - \u8f7d\u4f53\u4ea7\u7269\u548c\u52a0\u5165\u7684\u5f20\u91cf <code>tensor</code> \u7684\u6bd4\u4f8b\u56e0\u5b50\u3002</p> <p></p> <p>\u5bf9\u4e8e<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\u7c7b\u578b\u7684\u8f93\u5165\uff0c\u53c2\u6570<code>beta</code>\u548c<code>alpha</code>\u5fc5\u987b\u662f\u5b9e\u6570\uff0c\u5426\u5219\u5b83\u4eec\u5e94\u8be5\u662f\u6574\u6570</p> <p>Parameters:</p> <ul> <li>beta (\uff0c \u4efb\u9009\uff09 - <code>tensor</code>  ()\u7684\u4e58\u6570</li> <li>\u5f20\u91cf (Tensor\uff09 - \u8981\u6dfb\u52a0\u7684\u8f7d\u4f53</li> <li>alpha (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 -   ()\u7684\u4e58\u6570</li> <li>mat  (Tensor\uff09 - \u77e9\u9635\u6210\u500d\u589e\u52a0</li> <li>vec  (Tensor\uff09 - \u8f7d\u4f53\u500d\u589e</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; M = torch.randn(2)\n&gt;&gt;&gt; mat = torch.randn(2, 3)\n&gt;&gt;&gt; vec = torch.randn(3)\n&gt;&gt;&gt; torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n\n</code></pre> <pre><code>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u5411\u91cf<code>vec1</code>\u548c<code>vec2</code>\u7684\u5916\u79ef\u5e76\u5c06\u5176\u6dfb\u52a0\u5230\u77e9\u9635<code>mat</code>\u3002</p> <p>\u53ef\u9009\u503c<code>beta</code>\u548c<code>alpha</code>\u5206\u522b\u662f<code>vec1</code>\u548c<code>vec2</code>\u4e4b\u95f4\u7684\u5916\u79ef\u548c\u6dfb\u52a0\u7684\u77e9\u9635<code>mat</code>\u7684\u7f29\u653e\u56e0\u5b50\u3002</p> <p></p> <p>\u5982\u679c<code>vec1</code>\u662f\u5927\u5c0f\u4e3a<code>n</code>\u7684\u77e2\u91cf\u800c<code>vec2</code>\u662f\u5927\u5c0f\u4e3a<code>m</code>\u7684\u77e2\u91cf\uff0c\u90a3\u4e48<code>mat</code>\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684\uff0c\u5176\u5927\u5c0f\u4e3a\u77e9\u9635  \u548c<code>out</code>\u5c06\u662f\u5927\u5c0f  \u7684\u57fa\u8d28\u3002</p> <p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers</p> <p>Parameters:</p> <ul> <li>beta (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 - <code>mat</code> ()\u7684\u4e58\u6570</li> <li>mat  (Tensor\uff09 - \u8981\u6dfb\u52a0\u7684\u57fa\u8d28</li> <li>alpha (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 -   ()\u7684\u4e58\u6570</li> <li>vec1  (Tensor\uff09 - \u5916\u90e8\u4ea7\u54c1\u7684\u7b2c\u4e00\u4e2a\u8f7d\u4f53</li> <li>vec2  (Tensor\uff09 - \u5916\u4ea7\u54c1\u7684\u7b2c\u4e8c\u4e2a\u8f7d\u4f53</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; vec1 = torch.arange(1., 4.)\n&gt;&gt;&gt; vec2 = torch.arange(1., 3.)\n&gt;&gt;&gt; M = torch.zeros(3, 2)\n&gt;&gt;&gt; torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n [ 2.,  4.],\n [ 3.,  6.]])\n\n</code></pre> <pre><code>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None) \u2192 Tensor\n</code></pre> <p>\u5728<code>batch1</code>\u548c<code>batch2</code>\u4e2d\u6267\u884c\u77e9\u9635\u7684\u6279\u91cf\u77e9\u9635 - \u77e9\u9635\u4e58\u79ef\u3002 <code>mat</code>\u88ab\u6dfb\u52a0\u5230\u6700\u7ec8\u7ed3\u679c\u4e2d\u3002</p> <p><code>batch1</code> and <code>batch2</code> must be 3-D tensors each containing the same number of matrices.</p> <p>\u5982\u679c<code>batch1</code>\u662f  \u5f20\u91cf\uff0c<code>batch2</code>\u662f  \u5f20\u91cf\uff0c\u90a3\u4e48<code>mat</code>\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u548c [] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/29f0e4a370460668f7e257b22d08622d.jpg) \u5f20\u91cf\u548c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002 <code>alpha</code>\u548c<code>beta</code>\u5747\u4e0e <code>torch.addbmm()</code> \u4e2d\u4f7f\u7528\u7684\u6bd4\u4f8b\u56e0\u5b50\u76f8\u540c\u3002</p> <p></p> <p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, arguments <code>beta</code> and <code>alpha</code> must be real numbers, otherwise they should be integers.</p> <p>Parameters:</p> <ul> <li>beta (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 - <code>mat</code> ()\u7684\u4e58\u6570</li> <li>\u57ab (Tensor\uff09 - \u8981\u52a0\u7684\u5f20\u91cf</li> <li>alpha (\u7f16\u53f7 \uff0c \u4efb\u9009\uff09 -   ()\u7684\u4e58\u6570</li> <li>batch1  (Tensor\uff09 - \u7b2c\u4e00\u6279\u8981\u4e58\u7684\u77e9\u9635</li> <li>batch2  (Tensor\uff09 - \u7b2c\u4e8c\u6279\u77e9\u9635\u88ab\u4e58\u4ee5</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; M = torch.randn(10, 3, 5)\n&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)\n&gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n\n</code></pre> <pre><code>torch.bmm(batch1, batch2, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u5b58\u50a8\u5728<code>batch1</code>\u548c<code>batch2</code>\u4e2d\u7684\u77e9\u9635\u7684\u6279\u91cf\u77e9\u9635 - \u77e9\u9635\u4e58\u79ef\u3002</p> <p><code>batch1</code> and <code>batch2</code> must be 3-D tensors each containing the same number of matrices.</p> <p>\u5982\u679c<code>batch1</code>\u662f  \u5f20\u91cf\uff0c<code>batch2</code>\u662f  \u5f20\u91cf\uff0c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002</p> <p></p> <p>\u6ce8\u610f</p> <p>\u6b64\u529f\u80fd\u4e0d\u5e7f\u64ad\u3002\u6709\u5173\u5e7f\u64ad\u77e9\u9635\u4ea7\u54c1\uff0c\u8bf7\u53c2\u9605 <code>torch.matmul()</code> \u3002</p> <p>Parameters:</p> <ul> <li>batch1  (Tensor\uff09 - \u7b2c\u4e00\u6279\u8981\u4e58\u7684\u77e9\u9635</li> <li>batch2  (Tensor\uff09 - \u7b2c\u4e8c\u6279\u77e9\u9635\u88ab\u4e58\u4ee5</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; batch1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; batch2 = torch.randn(10, 4, 5)\n&gt;&gt;&gt; res = torch.bmm(batch1, batch2)\n&gt;&gt;&gt; res.size()\ntorch.Size([10, 3, 5])\n\n</code></pre> <pre><code>torch.btrifact(A, info=None, pivot=True)\n</code></pre> <p>\u6279\u91cfLU\u5206\u89e3\u3002</p> <p>\u8fd4\u56de\u5305\u542bLU\u5206\u89e3\u548c\u67a2\u8f74\u7684\u5143\u7ec4\u3002\u5982\u679c\u8bbe\u7f6e\u4e86<code>pivot</code>\uff0c\u5219\u5b8c\u6210\u65cb\u8f6c\u3002</p> <p>\u5982\u679c\u6bcf\u4e2aminibatch\u793a\u4f8b\u7684\u5206\u89e3\u6210\u529f\uff0c\u5219\u53ef\u9009\u53c2\u6570<code>info</code>\u5b58\u50a8\u4fe1\u606f\u3002 <code>info</code>\u4f5c\u4e3a<code>IntTensor</code>\u63d0\u4f9b\uff0c\u5176\u503c\u5c06\u4ecedgetrf\u586b\u5145\uff0c\u975e\u96f6\u503c\u8868\u793a\u53d1\u751f\u9519\u8bef\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5982\u679c\u4f7f\u7528cuda\uff0c\u5219\u503c\u6765\u81eacublas\uff0c\u5426\u5219\u4e3aLAPACK\u3002</p> <p>\u8b66\u544a</p> <p><code>info</code>\u53c2\u6570\u4e0d\u63a8\u8350\u4f7f\u7528 <code>torch.btrifact_with_info()</code> \u3002</p> <p>Parameters:</p> <ul> <li>A  (tensor) - \u56e0\u5b50\u7684\u5f20\u91cf</li> <li>info  (IntTensor \uff0c \u53ef\u9009\uff09 - (\u5f03\u7528\uff09<code>IntTensor</code>\u5b58\u50a8\u6307\u793a\u5206\u89e3\u662f\u5426\u6210\u529f\u7684\u503c</li> <li>pivot  (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u5b8c\u6210\u65cb\u8f6c</li> </ul> \u8fd4\u56de\uff1a \u5305\u542b\u5206\u89e3\u548c\u67a2\u8f74\u7684\u5143\u7ec4\u3002 <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)\n&gt;&gt;&gt; A_LU, pivots = torch.btrifact(A)\n&gt;&gt;&gt; A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n [ 0.1684,  1.1551,  0.1940],\n [ 0.1193,  0.6189, -0.5497]],\n\n [[ 0.4526,  1.2526, -0.3285],\n [-0.7988,  0.7175, -0.9701],\n [ 0.2634, -0.9255, -0.3459]]])\n\n&gt;&gt;&gt; pivots\ntensor([[ 3,  3,  3],\n [ 3,  3,  3]], dtype=torch.int32)\n\n</code></pre> <pre><code>torch.btrifact_with_info(A, pivot=True) -&gt; (Tensor, IntTensor, IntTensor)\n</code></pre> <p>\u6279\u91cfLU\u5206\u89e3\u548c\u5176\u4ed6\u9519\u8bef\u4fe1\u606f\u3002</p> <p>\u8fd9\u662f <code>torch.btrifact()</code> \u7684\u4e00\u4e2a\u7248\u672c\uff0c\u5b83\u59cb\u7ec8\u521b\u5efa\u4e00\u4e2ainfo <code>IntTensor</code>\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7b2c\u4e09\u4e2a\u8fd4\u56de\u503c\u8fd4\u56de\u3002</p> <p>Parameters:</p> <ul> <li>A  (tensor) - \u56e0\u5b50\u7684\u5f20\u91cf</li> <li>pivot  (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u5b8c\u6210\u65cb\u8f6c</li> </ul> Returns: \u5305\u542b\u56e0\u5f0f\u5206\u89e3\uff0c\u67a2\u8f74\u548c<code>IntTensor</code>\u7684\u5143\u7ec4\uff0c\u5176\u4e2d\u975e\u96f6\u503c\u8868\u793a\u6bcf\u4e2a\u5c0f\u6279\u91cf\u6837\u672c\u7684\u5206\u89e3\u662f\u5426\u6210\u529f\u3002 <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)\n&gt;&gt;&gt; A_LU, pivots, info = A.btrifact_with_info()\n&gt;&gt;&gt; if info.nonzero().size(0) == 0:\n&gt;&gt;&gt;   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!\n\n</code></pre> <pre><code>torch.btrisolve(b, LU_data, LU_pivots) \u2192 Tensor\n</code></pre> <p>\u6279\u91cfLU\u89e3\u51b3\u3002</p> <p>\u8fd4\u56de\u7ebf\u6027\u7cfb\u7edf  \u7684LU\u6c42\u89e3\u3002</p> <p>Parameters:</p> <ul> <li>b  (tensor) - RHS\u5f20\u91cf</li> <li>LU_data  (Tensor\uff09 - \u6765\u81ea <code>btrifact()</code> \u7684A\u7684\u65cb\u8f6cLU\u5206\u89e3\u3002</li> <li>LU_pivots  (IntTensor ) - LU\u5206\u89e3\u7684\u5173\u952e\u70b9</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)\n&gt;&gt;&gt; b = torch.randn(2, 3)\n&gt;&gt;&gt; A_LU = torch.btrifact(A)\n&gt;&gt;&gt; x = torch.btrisolve(b, *A_LU)\n&gt;&gt;&gt; torch.norm(torch.bmm(A, x.unsqueeze(2)) - b.unsqueeze(2))\ntensor(1.00000e-07 *\n 2.8312)\n\n</code></pre> <pre><code>torch.btriunpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)\n</code></pre> <p>\u4ece\u5f20\u91cf\u7684\u5206\u6bb5LU\u5206\u89e3(btrifact\uff09\u89e3\u5305\u6570\u636e\u548c\u67a2\u8f74\u3002</p> <p>\u8fd4\u56de\u5f20\u91cf\u7684\u5143\u7ec4\u4f5c\u4e3a<code>(the pivots, the L tensor, the U tensor)</code>\u3002</p> <p>Parameters:</p> <ul> <li>LU_data  (Tensor\uff09 - \u6253\u5305\u7684LU\u5206\u89e3\u6570\u636e</li> <li>LU_pivots  (Tensor\uff09 - \u6253\u5305\u7684LU\u5206\u89e3\u67a2\u8f74</li> <li>unpack_data  (bool\uff09 - \u6307\u793a\u6570\u636e\u662f\u5426\u5e94\u89e3\u5305\u7684\u6807\u5fd7</li> <li>unpack_pivots  (bool\uff09 - \u6307\u793a\u67a2\u8f74\u662f\u5426\u5e94\u89e3\u5305\u7684\u6807\u5fd7</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(2, 3, 3)\n&gt;&gt;&gt; A_LU, pivots = A.btrifact()\n&gt;&gt;&gt; P, A_L, A_U = torch.btriunpack(A_LU, pivots)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # can recover A from factorization\n&gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n</code></pre> <pre><code>torch.chain_matmul(*matrices)\n</code></pre> <p>\u8fd4\u56de  2-D\u5f20\u91cf\u7684\u77e9\u9635\u4e58\u79ef\u3002\u4f7f\u7528\u77e9\u9635\u94fe\u5e8f\u7b97\u6cd5\u6709\u6548\u5730\u8ba1\u7b97\u8be5\u4e58\u79ef\uff0c\u8be5\u7b97\u6cd5\u9009\u62e9\u5728\u7b97\u672f\u8fd0\u7b97\u65b9\u9762\u4ea7\u751f\u6700\u4f4e\u6210\u672c\u7684\u987a\u5e8f ([CLRS])\u3002\u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u8fd9\u662f\u8ba1\u7b97\u4ea7\u54c1\u7684\u51fd\u6570\uff0c  \u9700\u8981\u5927\u4e8e\u6216\u7b49\u4e8e2;\u5982\u679c\u7b49\u4e8e2\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u5e73\u51e1\u7684\u77e9\u9635 - \u77e9\u9635\u4e58\u79ef\u3002\u5982\u679c  \u4e3a1\uff0c\u90a3\u4e48\u8fd9\u662f\u4e00\u4e2a\u65e0\u64cd\u4f5c - \u539f\u59cb\u77e9\u9635\u6309\u539f\u6837\u8fd4\u56de\u3002</p> \u53c2\u6570\uff1a \u77e9\u9635(\u5f20\u91cf... ) - 2\u4e2a\u6216\u66f4\u591a\u4e2a2-D\u5f20\u91cf\u7684\u5e8f\u5217\uff0c\u5176\u4ea7\u7269\u5c06\u88ab\u786e\u5b9a\u3002 \u8fd4\u56de\uff1a \u5982\u679c  \u5f20\u91cf\u5177\u6709  \u7684\u7ef4\u5ea6\uff0c\u5219\u4ea7\u7269\u7684\u5c3a\u5bf8\u4e3a  \u3002 \u8fd4\u56de\u7c7b\u578b\uff1a Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 4)\n&gt;&gt;&gt; b = torch.randn(4, 5)\n&gt;&gt;&gt; c = torch.randn(5, 6)\n&gt;&gt;&gt; d = torch.randn(6, 7)\n&gt;&gt;&gt; torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n\n</code></pre> <pre><code>torch.cholesky(A, upper=False, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635  \u7684Cholesky\u5206\u89e3\u6216\u5bf9\u79f0\u6279\u6b63\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u3002</p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>True</code>\uff0c\u5219\u8fd4\u56de\u7684\u77e9\u9635<code>U</code>\u4e3a\u4e0a\u4e09\u89d2\u5f62\uff0c\u5206\u89e3\u7684\u5f62\u5f0f\u4e3a\uff1a</p> <p></p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>False</code>\uff0c\u5219\u8fd4\u56de\u7684\u77e9\u9635<code>L</code>\u4e3a\u4f4e\u4e09\u89d2\u5f62\uff0c\u5206\u89e3\u7684\u5f62\u5f0f\u4e3a\uff1a</p> <p></p> <p>\u5982\u679c<code>upper</code>\u662f<code>True</code>\uff0c\u5e76\u4e14<code>A</code>\u662f\u4e00\u6279\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\uff0c\u5219\u8fd4\u56de\u7684\u5f20\u91cf\u5c06\u7531\u6bcf\u4e2a\u5355\u72ec\u77e9\u9635\u7684\u4e0a\u4e09\u89d2\u5f62Cholesky\u56e0\u5b50\u7ec4\u6210\u3002\u7c7b\u4f3c\u5730\uff0c\u5f53<code>upper</code>\u662f<code>False</code>\u65f6\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u5c06\u7531\u6bcf\u4e2a\u5355\u4e2a\u77e9\u9635\u7684\u4e0b\u4e09\u89d2\u5f62Cholesky\u56e0\u5b50\u7ec4\u6210\u3002</p> <p>Parameters:</p> <ul> <li>a  (tensor) - \u8f93\u5165\u5f20\u91cf\u5927\u5c0f (* \uff0cn\uff0cn\uff09\u5176\u4e2d<code>*</code>\u4e3a\u96f6\u6216\u66f4\u591a\u6279\u7531\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7ec4\u6210\u7684\u7ef4\u6570\u3002</li> <li>\u4e0a (bool\uff0c \u53ef\u9009\uff09 - \u8868\u793a\u662f\u5426\u8fd4\u56de\u4e0a\u4e0b\u4e09\u89d2\u77e9\u9635\u7684\u6807\u5fd7\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> <li>out  (Tensor\uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u77e9\u9635</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite\n&gt;&gt;&gt; l = torch.cholesky(a)\n&gt;&gt;&gt; a\ntensor([[ 2.4112, -0.7486,  1.4551],\n [-0.7486,  1.3544,  0.1294],\n [ 1.4551,  0.1294,  1.6724]])\n&gt;&gt;&gt; l\ntensor([[ 1.5528,  0.0000,  0.0000],\n [-0.4821,  1.0592,  0.0000],\n [ 0.9371,  0.5487,  0.7023]])\n&gt;&gt;&gt; torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n [-0.7486,  1.3544,  0.1294],\n [ 1.4551,  0.1294,  1.6724]])\n&gt;&gt;&gt; a = torch.randn(3, 2, 2)\n&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n&gt;&gt;&gt; l = torch.cholesky(a)\n&gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2))\n&gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n\n</code></pre> <pre><code>torch.dot(tensor1, tensor2) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u4e24\u4e2a\u5f20\u91cf\u7684\u70b9\u79ef(\u5185\u79ef\uff09\u3002</p> <p>Note</p> <p>\u6b64\u529f\u80fd\u4e0d\u5e7f\u64ad\u3002</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n\n</code></pre> <pre><code>torch.eig(a, eigenvectors=False, out=None) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8ba1\u7b97\u5b9e\u65b9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u3002</p> <p>Parameters:</p> <ul> <li>a  (Tensor\uff09 - \u5f62\u72b6  \u7684\u65b9\u9635\uff0c\u5176\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u5c06\u88ab\u8ba1\u7b97</li> <li>\u7279\u5f81\u5411\u91cf (bool\uff09 - <code>True</code>\u8ba1\u7b97\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf;\u5426\u5219\uff0c\u53ea\u8ba1\u7b97\u7279\u5f81\u503c</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>|\u8fd4\u56de\uff1a|\u5305\u542b\u5143\u7ec4\u7684\u5143\u7ec4</p> <p>\uff06GT; * e (tensor\uff09\uff1a\u5f62\u72b6  \u3002\u6bcf\u884c\u662f<code>a</code>\u7684\u7279\u5f81\u503c\uff0c\u5176\u4e2d\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5b9e\u90e8\uff0c\u7b2c\u4e8c\u4e2a\u5143\u7d20\u662f\u865a\u90e8\u3002\u7279\u5f81\u503c\u4e0d\u4e00\u5b9a\u662f\u6709\u5e8f\u7684\u3002 \uff06GT; * v  (Tensor )\uff1a\u5982\u679c<code>eigenvectors=False</code>\uff0c\u5b83\u662f\u4e00\u4e2a\u7a7a\u5f20\u91cf\u3002\u5426\u5219\uff0c\u8be5\u5f20\u91cf\u5f62\u72b6  \u53ef\u7528\u4e8e\u8ba1\u7b97\u76f8\u5e94\u7279\u5f81\u503c<code>e</code>\u7684\u5f52\u4e00\u5316(\u5355\u4f4d\u957f\u5ea6\uff09\u7279\u5f81\u5411\u91cf\uff0c\u5982\u4e0b\u6240\u8ff0\u3002\u5982\u679c\u5bf9\u5e94\u7684e [j]\u662f\u5b9e\u6570\uff0c\u5219\u5217v [\uff1a\uff0cj]\u662f\u5bf9\u5e94\u4e8e\u7279\u5f81\u503ce [j]\u7684\u7279\u5f81\u5411\u91cf\u3002\u5982\u679c\u76f8\u5e94\u7684e [j]\u548ce [j + 1]\u7279\u5f81\u503c\u5f62\u6210\u590d\u5171\u8f6d\u5bf9\uff0c\u90a3\u4e48\u771f\u5b9e\u7684\u7279\u5f81\u5411\u91cf\u53ef\u4ee5\u88ab\u8ba1\u7b97\u4e3a  \uff0c  \u3002</p> \u8fd4\u56de\u7c7b\u578b\uff1a (Tensor \uff0c Tensor) <pre><code>torch.gels(B, A, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u5927\u5c0f  \u7684\u5168\u79e9\u77e9\u9635  \u548c\u5927\u5c0f\u7684\u77e9\u9635  \u7684\u6700\u5c0f\u4e8c\u4e58\u548c\u6700\u5c0f\u8303\u6570\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848  \u3002</p> <p>\u5982\u679c  \uff0c <code>gels()</code> \u89e3\u51b3\u4e86\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff1a</p> <p></p> <p>\u5982\u679c  \uff0c <code>gels()</code> \u89e3\u51b3\u4e86\u6700\u5c0f\u8303\u6570\u95ee\u9898\uff1a</p> <p></p> <p>\u8fd4\u56de\u5f20\u91cf  \u5177\u6709  \u7684\u5f62\u72b6\u3002  \u7684\u7b2c\u4e00  \u884c\u5305\u542b\u8be5\u6eb6\u6db2\u3002\u5982\u679c  \uff0c\u5219\u6bcf\u5217\u4e2d\u6eb6\u6db2\u7684\u6b8b\u4f59\u5e73\u65b9\u548c\u7531\u8be5\u5217\u7684\u5269\u4f59  \u884c\u4e2d\u7684\u5143\u7d20\u7684\u5e73\u65b9\u548c\u7ed9\u51fa\u3002</p> <p>Parameters:</p> <ul> <li>B  (tensor) - \u57fa\u8d28 </li> <li>(tensor) -  </li> <li>out  (\u5143\u7ec4 \uff0c \u53ef\u9009\uff09 - \u53ef\u9009\u76ee\u7684\u5730\u5f20\u91cf</li> </ul> <p>|\u8fd4\u56de\uff1a|\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\u7684\u5143\u7ec4\uff1a</p> <p>\uff06GT; * X (tensor\uff09\uff1a\u6700\u5c0f\u4e8c\u4e58\u89e3\uff06gt; * qr  (Tensor )\uff1aQR\u5206\u89e3\u7684\u7ec6\u8282</p> Return type: (Tensor, Tensor) <p>Note</p> <p>\u65e0\u8bba\u8f93\u5165\u77e9\u9635\u7684\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u77e9\u9635\u5c06\u59cb\u7ec8\u88ab\u8f6c\u7f6e\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u4ed6\u4eec\u5c06\u6709<code>(1, m)</code>\u800c\u4e0d\u662f<code>(m, 1)</code>\u3002</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.tensor([[1., 1, 1],\n [2, 3, 4],\n [3, 5, 2],\n [4, 2, 5],\n [5, 4, 3]])\n&gt;&gt;&gt; B = torch.tensor([[-10., -3],\n [ 12, 14],\n [ 14, 12],\n [ 16, 16],\n [ 18, 16]])\n&gt;&gt;&gt; X, _ = torch.gels(B, A)\n&gt;&gt;&gt; X\ntensor([[  2.0000,   1.0000],\n [  1.0000,   1.0000],\n [  1.0000,   2.0000],\n [ 10.9635,   4.8501],\n [  8.9332,   5.2418]])\n\n</code></pre> <pre><code>torch.geqrf(input, out=None) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8fd9\u662f\u4e00\u4e2a\u76f4\u63a5\u8c03\u7528LAPACK\u7684\u4f4e\u7ea7\u51fd\u6570\u3002</p> <p>\u60a8\u901a\u5e38\u5e0c\u671b\u4f7f\u7528 <code>torch.qr()</code> \u3002</p> <p>\u8ba1\u7b97<code>input</code>\u7684QR\u5206\u89e3\uff0c\u4f46\u4e0d\u6784\u9020  \u548c  \u4f5c\u4e3a\u663e\u5f0f\u5355\u72ec\u7684\u77e9\u9635\u3002</p> <p>\u76f8\u53cd\uff0c\u8fd9\u76f4\u63a5\u8c03\u7528\u5e95\u5c42LAPACK\u51fd\u6570<code>?geqrf</code>\uff0c\u5b83\u4ea7\u751f\u4e00\u7cfb\u5217\u201c\u57fa\u672c\u53cd\u5c04\u5668\u201d\u3002</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605geqrf \u7684 LAPACK\u6587\u6863\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u77e9\u9635</li> <li>out  (\u5143\u7ec4 \uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5143\u7ec4(Tensor\uff0cTensor\uff09</li> </ul> <pre><code>torch.ger(vec1, vec2, out=None) \u2192 Tensor\n</code></pre> <p><code>vec1</code>\u548c<code>vec2</code>\u7684\u5916\u4ea7\u7269\u3002\u5982\u679c<code>vec1</code>\u662f\u5927\u5c0f  \u7684\u8f7d\u4f53\uff0c<code>vec2</code>\u662f\u5927\u5c0f  \u7684\u8f7d\u4f53\uff0c\u90a3\u4e48<code>out</code>\u5fc5\u987b\u662f\u5927\u5c0f\u7684\u77e9\u9635  \u3002</p> <p>Note</p> <p>This function does not broadcast.</p> <p>Parameters:</p> <ul> <li>vec1  (tensor) - 1-D\u8f93\u5165\u5411\u91cf</li> <li>vec2  (tensor) - 1-D\u8f93\u5165\u5411\u91cf</li> <li>out  (Tensor\uff0c \u53ef\u9009\uff09 - \u53ef\u9009\u8f93\u51fa\u77e9\u9635</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; v1 = torch.arange(1., 5.)\n&gt;&gt;&gt; v2 = torch.arange(1., 4.)\n&gt;&gt;&gt; torch.ger(v1, v2)\ntensor([[  1.,   2.,   3.],\n [  2.,   4.,   6.],\n [  3.,   6.,   9.],\n [  4.,   8.,  12.]])\n\n</code></pre> <pre><code>torch.gesv(B, A) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8be5\u51fd\u6570\u5c06\u89e3\u51b3\u65b9\u6848\u8fd4\u56de\u5230\u7531  \u8868\u793a\u7684\u7ebf\u6027\u65b9\u7a0b\u7ec4\u548cA\u7684LU\u5206\u89e3\uff0c\u6309\u987a\u5e8f\u4f5c\u4e3a\u5143\u7ec4<code>X, LU</code>\u3002</p> <p><code>LU</code>\u5305\u542b<code>A</code>\u7684LU\u5206\u89e3\u7684<code>L</code>\u548c<code>U</code>\u56e0\u5b50\u3002</p> <p><code>torch.gesv(B, A)</code>\u53ef\u4ee5\u63a5\u65362D\u8f93\u5165<code>B, A</code>\u6216\u4e24\u62792D\u77e9\u9635\u7684\u8f93\u5165\u3002\u5982\u679c\u8f93\u5165\u662f\u6279\u6b21\uff0c\u5219\u8fd4\u56de\u6279\u91cf\u8f93\u51fa<code>X, LU</code>\u3002</p> <p>Note</p> <p><code>out</code>\u5173\u952e\u5b57\u4ec5\u652f\u63012D\u77e9\u9635\u8f93\u5165\uff0c\u5373<code>B, A</code>\u5fc5\u987b\u662f2D\u77e9\u9635\u3002</p> <p>Note</p> <p>\u4e0d\u7ba1\u539f\u59cb\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u77e9\u9635<code>X</code>\u548c<code>LU</code>\u5c06\u88ab\u8f6c\u7f6e\uff0c\u5373\u5206\u522b\u5177\u6709\u8bf8\u5982<code>B.contiguous().transpose(-1, -2).strides()</code>\u548c<code>A.contiguous().transpose(-1, -2).strides()</code>\u7684\u6b65\u5e45\u3002</p> <p>Parameters:</p> <ul> <li>B  (Tensor\uff09 - \u5927\u5c0f  \u7684\u8f93\u5165\u77e9\u9635\uff0c\u5176\u4e2d  \u4e3a\u96f6\u6216\u6279\u91cf\u7ef4\u5ea6\u66f4\u591a\u3002</li> <li>A  (tensor) - \u8f93\u5165\u65b9\u5f62\u77e9\u9635  \uff0c\u5176\u4e2d  \u4e3a\u96f6\u6216\u66f4\u591a\u6279\u91cf\u7ef4\u5ea6\u3002</li> <li>\u51fa(( tensor \uff0c tensor ]\uff09__\uff0c \u53ef\u9009\uff09 - \u53ef\u9009\u8f93\u51fa\u5143\u7ec4\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n [-6.05, -3.30,  5.36, -4.44,  1.08],\n [-0.45,  2.58, -2.70,  0.27,  9.04],\n [8.32,  2.71,  4.35,  -7.17,  2.14],\n [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n&gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n [-1.56,  4.00, -8.67,  1.75,  2.86],\n [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n&gt;&gt;&gt; X, LU = torch.gesv(B, A)\n&gt;&gt;&gt; torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n 7.0977)\n\n&gt;&gt;&gt; # Batched solver example\n&gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4)\n&gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6)\n&gt;&gt;&gt; X, LU = torch.gesv(B, A)\n&gt;&gt;&gt; torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n 3.6386)\n\n</code></pre> <pre><code>torch.inverse(input, out=None) \u2192 Tensor\n</code></pre> <p>\u91c7\u7528\u65b9\u9635<code>input</code>\u7684\u5012\u6570\u3002 <code>input</code>\u53ef\u4ee5\u662f2D\u65b9\u5f62\u5f20\u91cf\u7684\u6279\u6b21\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8be5\u51fd\u6570\u5c06\u8fd4\u56de\u7531\u5355\u4e2a\u53cd\u8f6c\u7ec4\u6210\u7684\u5f20\u91cf\u3002</p> <p>Note</p> <p>\u65e0\u8bba\u539f\u59cb\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u5f20\u91cf\u90fd\u5c06\u88ab\u8f6c\u7f6e\uff0c\u5373\u50cf<code>input.contiguous().transpose(-2, -1).strides()</code>\u8fd9\u6837\u7684\u6b65\u5e45</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf\u5927\u5c0f (* \uff0cn\uff0cn\uff09\u5176\u4e2d<code>*</code>\u4e3a\u96f6\u6216\u66f4\u591a\u6279\u5c3a\u5bf8</li> <li>out  (Tensor\uff0c \u53ef\u9009\uff09 - \u53ef\u9009\u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.rand(4, 4)\n&gt;&gt;&gt; y = torch.inverse(x)\n&gt;&gt;&gt; z = torch.mm(x, y)\n&gt;&gt;&gt; z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n [ 0.0000,  1.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  1.0000,  0.0000],\n [ 0.0000, -0.0000, -0.0000,  1.0000]])\n&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero\ntensor(1.1921e-07)\n&gt;&gt;&gt; # Batched inverse example\n&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)\n&gt;&gt;&gt; y = torch.inverse(x)\n&gt;&gt;&gt; z = torch.matmul(x, y)\n&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero\ntensor(1.9073e-06)\n\n</code></pre> <pre><code>torch.det(A) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b972D\u5e73\u65b9\u5f20\u91cf\u7684\u884c\u5217\u5f0f\u3002</p> <p>Note</p> <p>\u5f53<code>A</code>\u4e0d\u53ef\u9006\u65f6\uff0c\u5411\u540e\u901a\u8fc7 <code>det()</code> \u5728\u5185\u90e8\u4f7f\u7528SVD\u7ed3\u679c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5f53<code>A</code>\u6ca1\u6709\u660e\u663e\u7684\u5947\u5f02\u503c\u65f6\uff0c\u901a\u8fc7 <code>det()</code> \u7684\u53cc\u5411\u540e\u5c06\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 <code>svd()</code> \u3002</p> Parameters: A  (Tensor\uff09 - \u8f93\u51652D\u5e73\u65b9\u5f20\u91cf <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)\n&gt;&gt;&gt; torch.det(A)\ntensor(3.7641)\n\n</code></pre> <pre><code>torch.logdet(A) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b972D\u5e73\u65b9\u5f20\u91cf\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u3002</p> <p>Note</p> <p>\u5982\u679c<code>A</code>\u5177\u6709\u96f6\u5bf9\u6570\u884c\u5217\u5f0f\uff0c\u5219\u7ed3\u679c\u4e3a<code>-inf</code>\uff0c\u5982\u679c<code>A</code>\u5177\u6709\u8d1f\u7684\u884c\u5217\u5f0f\uff0c\u5219\u7ed3\u679c\u4e3a<code>nan</code>\u3002</p> <p>Note</p> <p>\u5f53<code>A</code>\u4e0d\u53ef\u9006\u65f6\uff0c\u5411\u540e\u901a\u8fc7 <code>logdet()</code> \u5728\u5185\u90e8\u4f7f\u7528SVD\u7ed3\u679c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5f53<code>A</code>\u6ca1\u6709\u660e\u663e\u7684\u5947\u5f02\u503c\u65f6\uff0c\u901a\u8fc7 <code>logdet()</code> \u7684\u53cc\u5411\u540e\u5c06\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 <code>svd()</code> \u3002</p> Parameters: A (Tensor) \u2013 The input 2D square tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)\n&gt;&gt;&gt; torch.det(A)\ntensor(0.2611)\n&gt;&gt;&gt; torch.logdet(A)\ntensor(-1.3430)\n\n</code></pre> <pre><code>torch.slogdet(A) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8ba1\u7b972D\u5e73\u65b9\u5f20\u91cf\u7684\u884c\u5217\u5f0f\u7684\u7b26\u53f7\u548c\u5bf9\u6570\u503c\u3002</p> <p>Note</p> <p>\u5982\u679c<code>A</code>\u7684\u884c\u5217\u5f0f\u4e3a\u96f6\uff0c\u5219\u8fd4\u56de<code>(0, -inf)</code>\u3002</p> <p>Note</p> <p>\u5f53<code>A</code>\u4e0d\u53ef\u9006\u65f6\uff0c\u5411\u540e\u901a\u8fc7 <code>slogdet()</code> \u5728\u5185\u90e8\u4f7f\u7528SVD\u7ed3\u679c\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5f53<code>A</code>\u6ca1\u6709\u660e\u663e\u7684\u5947\u5f02\u503c\u65f6\uff0c\u901a\u8fc7 <code>slogdet()</code> \u7684\u53cc\u5411\u540e\u5c06\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 <code>svd()</code> \u3002</p> Parameters: A (Tensor) \u2013 The input 2D square tensor Returns: \u5305\u542b\u884c\u5217\u5f0f\u7b26\u53f7\u7684\u5143\u7ec4\uff0c\u4ee5\u53ca\u7edd\u5bf9\u884c\u5217\u5f0f\u7684\u5bf9\u6570\u503c\u3002 <p>Example:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(3, 3)\n&gt;&gt;&gt; torch.det(A)\ntensor(-4.8215)\n&gt;&gt;&gt; torch.logdet(A)\ntensor(nan)\n&gt;&gt;&gt; torch.slogdet(A)\n(tensor(-1.), tensor(1.5731))\n\n</code></pre> <pre><code>torch.matmul(tensor1, tensor2, out=None) \u2192 Tensor\n</code></pre> <p>\u4e24\u4e2a\u5f20\u91cf\u7684\u77e9\u9635\u4e58\u79ef\u3002</p> <p>\u884c\u4e3a\u53d6\u51b3\u4e8e\u5f20\u91cf\u7684\u7ef4\u5ea6\u5982\u4e0b\uff1a</p> <ul> <li>\u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u90fd\u662f1\u7ef4\u7684\uff0c\u5219\u8fd4\u56de\u70b9\u79ef(\u6807\u91cf\uff09\u3002</li> <li>\u5982\u679c\u4e24\u4e2a\u53c2\u6570\u90fd\u662f\u4e8c\u7ef4\u7684\uff0c\u5219\u8fd4\u56de\u77e9\u9635 - \u77e9\u9635\u4e58\u79ef\u3002</li> <li>\u5982\u679c\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f1\u7ef4\u4e14\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f2\u7ef4\uff0c\u5219\u4e3a\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u76ee\u7684\uff0c\u5728\u5176\u7ef4\u5ea6\u4e4b\u524d\u52a01\u3002\u5728\u77e9\u9635\u4e58\u6cd5\u4e4b\u540e\uff0c\u79fb\u9664\u524d\u7f6e\u7ef4\u5ea6\u3002</li> <li>\u5982\u679c\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f2\u7ef4\u4e14\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f1\u7ef4\uff0c\u5219\u8fd4\u56de\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u3002</li> <li>\u5982\u679c\u4e24\u4e2a\u53c2\u6570\u90fd\u662f\u81f3\u5c11\u4e00\u7ef4\u7684\u5e76\u4e14\u81f3\u5c11\u4e00\u4e2a\u53c2\u6570\u662fN\u7ef4\u7684(\u5176\u4e2dN&gt; 2\uff09\uff0c\u5219\u8fd4\u56de\u6279\u91cf\u77e9\u9635\u4e58\u6cd5\u3002\u5982\u679c\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f1\u7ef4\u7684\uff0c\u5219\u4e3a\u4e86\u6279\u91cf\u77e9\u9635\u7684\u76ee\u7684\uff0c\u5c061\u52a0\u5728\u5176\u7ef4\u5ea6\u4e4b\u524d\uff0c\u7136\u540e\u5c06\u5176\u5220\u9664\u3002\u5982\u679c\u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f1\u7ef4\u7684\uff0c\u5219\u4e3a\u4e86\u6279\u5904\u7406\u77e9\u9635\u7684\u591a\u4e2a\u76ee\u7684\uff0c\u5c061\u9644\u52a0\u5230\u5176\u7ef4\u5ea6\uff0c\u5e76\u5728\u4e4b\u540e\u5220\u9664\u3002\u975e\u77e9\u9635(\u5373\u6279\u91cf\uff09\u7ef4\u5ea6\u662f\u5e7f\u64ad(\u56e0\u6b64\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684\uff09\u3002\u4f8b\u5982\uff0c\u5982\u679c<code>tensor1</code>\u662f  \u5f20\u91cf\u800c<code>tensor2</code>\u662f  \u5f20\u91cf\uff0c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002</li> </ul> <p>Note</p> <p>\u6b64\u529f\u80fd\u76841\u7ef4\u70b9\u79ef\u7248\u672c\u4e0d\u652f\u6301<code>out</code>\u53c2\u6570\u3002</p> <p>Parameters:</p> <ul> <li>tensor1  (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u8981\u4e58\u7684\u5f20\u91cf</li> <li>tensor2  (Tensor\uff09 - \u8981\u589e\u52a0\u7684\u7b2c\u4e8c\u4e2a\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; # vector x vector\n&gt;&gt;&gt; tensor1 = torch.randn(3)\n&gt;&gt;&gt; tensor2 = torch.randn(3)\n&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n&gt;&gt;&gt; # matrix x vector\n&gt;&gt;&gt; tensor1 = torch.randn(3, 4)\n&gt;&gt;&gt; tensor2 = torch.randn(4)\n&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n&gt;&gt;&gt; # batched matrix x broadcasted vector\n&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; tensor2 = torch.randn(4)\n&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n&gt;&gt;&gt; # batched matrix x batched matrix\n&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5)\n&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n&gt;&gt;&gt; # batched matrix x broadcasted matrix\n&gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4)\n&gt;&gt;&gt; tensor2 = torch.randn(4, 5)\n&gt;&gt;&gt; torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n\n</code></pre> <pre><code>torch.matrix_power(input, n) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e3a\u77e9\u5f62\u77e9\u9635\u63d0\u5347\u5230\u5e42<code>n</code>\u7684\u77e9\u9635\u3002\u5bf9\u4e8e\u4e00\u6279\u77e9\u9635\uff0c\u6bcf\u4e2a\u5355\u72ec\u7684\u77e9\u9635\u88ab\u63d0\u5347\u5230\u529f\u7387<code>n</code>\u3002</p> <p>\u5982\u679c<code>n</code>\u4e3a\u8d1f\uff0c\u5219\u77e9\u9635\u7684\u53cd\u8f6c(\u5982\u679c\u53ef\u9006\uff09\u5c06\u5347\u81f3\u529f\u7387<code>n</code>\u3002\u5bf9\u4e8e\u4e00\u6279\u77e9\u9635\uff0c\u6279\u91cf\u53cd\u8f6c(\u5982\u679c\u53ef\u9006\uff09\u5219\u4e0a\u5347\u5230\u529f\u7387<code>n</code>\u3002\u5982\u679c<code>n</code>\u4e3a0\uff0c\u5219\u8fd4\u56de\u5355\u4f4d\u77e9\u9635\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>n  (int\uff09 - \u5c06\u77e9\u9635\u63d0\u5347\u5230</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(2, 2, 2)\n&gt;&gt;&gt; a\ntensor([[[-1.9975, -1.9610],\n [ 0.9592, -2.3364]],\n\n [[-1.2534, -1.3429],\n [ 0.4153, -1.4664]]])\n&gt;&gt;&gt; torch.matrix_power(a, 3)\ntensor([[[  3.9392, -23.9916],\n [ 11.7357,  -0.2070]],\n\n [[  0.2468,  -6.7168],\n [  2.0774,  -0.8187]]])\n\n</code></pre> <pre><code>torch.matrix_rank(input, tol=None, bool symmetric=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e8c\u7ef4\u5f20\u91cf\u7684\u6570\u503c\u7b49\u7ea7\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528SVD\u5b8c\u6210\u8ba1\u7b97\u77e9\u9635\u79e9\u7684\u65b9\u6cd5\u3002\u5982\u679c<code>symmetric</code>\u662f<code>True</code>\uff0c\u5219\u5047\u8bbe<code>input</code>\u662f\u5bf9\u79f0\u7684\uff0c\u5e76\u4e14\u901a\u8fc7\u83b7\u5f97\u7279\u5f81\u503c\u6765\u5b8c\u6210\u79e9\u7684\u8ba1\u7b97\u3002</p> <p><code>tol</code>\u662f\u4e00\u4e2a\u9608\u503c\uff0c\u4f4e\u4e8e\u8be5\u9608\u503c\u65f6\uff0c\u5947\u5f02\u503c(\u6216<code>symmetric</code>\u4e3a<code>True</code>\u65f6\u7684\u7279\u5f81\u503c\uff09\u88ab\u8ba4\u4e3a\u662f0.\u5982\u679c\u672a\u6307\u5b9a<code>tol</code>\uff0c\u5219<code>tol</code>\u8bbe\u7f6e\u4e3a<code>S.max() * max(S.size()) * eps</code>\uff0c\u5176\u4e2d<code>S</code> ]\u662f\u5947\u5f02\u503c(\u6216<code>symmetric</code>\u4e3a<code>True</code>\u65f6\u7684\u7279\u5f81\u503c\uff09\uff0c<code>eps</code>\u662f<code>input</code>\u6570\u636e\u7c7b\u578b\u7684epsilon\u503c\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u51652-D\u5f20\u91cf</li> <li>tol  (float\uff0c \u4efb\u9009\uff09 - \u8010\u53d7\u503c\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code></li> <li>\u5bf9\u79f0 (bool\uff0c \u4efb\u9009\uff09 - \u8868\u793a<code>input</code>\u662f\u5426\u5bf9\u79f0\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.eye(10)\n&gt;&gt;&gt; torch.matrix_rank(a)\ntensor(10)\n&gt;&gt;&gt; b = torch.eye(10)\n&gt;&gt;&gt; b[0, 0] = 0\n&gt;&gt;&gt; torch.matrix_rank(b)\ntensor(9)\n\n</code></pre> <pre><code>torch.mm(mat1, mat2, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u77e9\u9635<code>mat1</code>\u548c<code>mat2</code>\u7684\u77e9\u9635\u4e58\u6cd5\u3002</p> <p>\u5982\u679c<code>mat1</code>\u662f  \u5f20\u91cf\uff0c<code>mat2</code>\u662f  \u5f20\u91cf\uff0c<code>out</code>\u5c06\u662f  \u5f20\u91cf\u3002</p> <p>Note</p> <p>This function does not broadcast. For broadcasting matrix products, see <code>torch.matmul()</code>.</p> <p>Parameters:</p> <ul> <li>mat1  (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u88ab\u4e58\u6cd5\u7684\u77e9\u9635</li> <li>mat2  (Tensor\uff09 - \u8981\u500d\u589e\u7684\u7b2c\u4e8c\u4e2a\u77e9\u9635</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; mat1 = torch.randn(2, 3)\n&gt;&gt;&gt; mat2 = torch.randn(3, 3)\n&gt;&gt;&gt; torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n [-0.0760, -3.6705,  2.4784]])\n\n</code></pre> <pre><code>torch.mv(mat, vec, out=None) \u2192 Tensor\n</code></pre> <p>\u6267\u884c\u77e9\u9635<code>mat</code>\u548c\u5411\u91cf<code>vec</code>\u7684\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u3002</p> <p>\u5982\u679c<code>mat</code>\u662f  \u5f20\u91cf\uff0c<code>vec</code>\u662f1-D\u5f20\u91cf\u5927\u5c0f  \uff0c<code>out</code>\u5c06\u662f1-D\u5927\u5c0f [] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/493731e423d5db62086d0b8705dda0c8.jpg) \u3002</p> <p>Note</p> <p>This function does not broadcast.</p> <p>Parameters:</p> <ul> <li>mat  (Tensor\uff09 - \u77e9\u9635\u6210\u500d\u589e\u52a0</li> <li>vec  (Tensor\uff09 - \u8f7d\u4f53\u500d\u589e</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; mat = torch.randn(2, 3)\n&gt;&gt;&gt; vec = torch.randn(3)\n&gt;&gt;&gt; torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])\n\n</code></pre> <pre><code>torch.orgqr(a, tau) \u2192 Tensor\n</code></pre> <p>\u4ece <code>torch.geqrf()</code> \u8fd4\u56de\u7684<code>(a, tau)</code>\u5143\u7ec4\u8ba1\u7b97QR\u5206\u89e3\u7684\u6b63\u4ea4\u77e9\u9635<code>Q</code>\u3002</p> <p>\u8fd9\u76f4\u63a5\u8c03\u7528\u5e95\u5c42LAPACK\u51fd\u6570<code>?orgqr</code>\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605orgqr \u7684 LAPACK\u6587\u6863\u3002</p> <p>Parameters:</p> <ul> <li>a  (tensor) - \u6765\u81ea <code>torch.geqrf()</code> \u7684<code>a</code>\u3002</li> <li>tau  (tensor) - \u6765\u81ea <code>torch.geqrf()</code> \u7684<code>tau</code>\u3002</li> </ul> <pre><code>torch.ormqr(a, tau, mat, left=True, transpose=False) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u5c06<code>mat</code>\u4e58\u4ee5\u7531<code>(a, tau)</code>\u8868\u793a\u7684 <code>torch.geqrf()</code> \u5f62\u6210\u7684QR\u5206\u89e3\u7684\u6b63\u4ea4<code>Q</code>\u77e9\u9635\u3002</p> <p>\u8fd9\u76f4\u63a5\u8c03\u7528\u5e95\u5c42LAPACK\u51fd\u6570<code>?ormqr</code>\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605ormqr \u7684 LAPACK\u6587\u6863\u3002</p> <p>Parameters:</p> <ul> <li>a  (tensor) - \u6765\u81ea <code>torch.geqrf()</code> \u7684<code>a</code>\u3002</li> <li>tau  (tensor) - \u6765\u81ea <code>torch.geqrf()</code> \u7684<code>tau</code>\u3002</li> <li>mat  (Tensor\uff09 - \u8981\u500d\u589e\u7684\u77e9\u9635\u3002</li> </ul> <pre><code>torch.pinverse(input, rcond=1e-15) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b972D\u5f20\u91cf\u7684\u4f2a\u9006(\u4e5f\u79f0\u4e3aMoore-Penrose\u9006\uff09\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b Moore-Penrose\u9006</p> <p>Note</p> <p>\u8be5\u65b9\u6cd5\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u6765\u5b9e\u73b0\u3002</p> <p>Note</p> <p>\u4f2a\u9006\u4e0d\u4e00\u5b9a\u662f\u77e9\u9635 [1] \u7684\u5143\u7d20\u4e2d\u7684\u8fde\u7eed\u51fd\u6570\u3002\u56e0\u6b64\uff0c\u884d\u751f\u7269\u5e76\u4e0d\u603b\u662f\u5b58\u5728\uff0c\u53ea\u5b58\u5728\u4e8e\u6052\u5b9a\u7b49\u7ea7 [2] \u3002\u4f46\u662f\uff0c\u7531\u4e8e\u4f7f\u7528SVD\u7ed3\u679c\u5b9e\u73b0\uff0c\u6b64\u65b9\u6cd5\u53ef\u4ee5\u53cd\u5411\u4f7f\u7528\uff0c\u5e76\u4e14\u53ef\u80fd\u4e0d\u7a33\u5b9a\u3002\u7531\u4e8e\u5728\u5185\u90e8\u4f7f\u7528SVD\uff0c\u53cc\u5411\u540e\u4e5f\u5c06\u4e0d\u7a33\u5b9a\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 <code>svd()</code> \u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u7ef4\u5ea6  \u7684\u8f93\u51652D\u5f20\u91cf</li> <li>rcond  (float\uff09 - \u4e00\u4e2a\u6d6e\u70b9\u503c\uff0c\u7528\u4e8e\u786e\u5b9a\u5c0f\u5947\u5f02\u503c\u7684\u622a\u6b62\u503c\u3002\u9ed8\u8ba4\u503c\uff1a1e-15</li> </ul> Returns: \u7ef4\u5ea6  \u7684<code>input</code>\u7684\u4f2a\u9006 <p>Example:</p> <pre><code>&gt;&gt;&gt; input = torch.randn(3, 5)\n&gt;&gt;&gt; input\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n&gt;&gt;&gt; torch.pinverse(input)\ntensor([[ 0.0600, -0.1933, -0.2090],\n [-0.0903, -0.0817, -0.4752],\n [-0.7124, -0.1631, -0.2272],\n [ 0.1356,  0.3933, -0.5023],\n [-0.0308, -0.1725, -0.5216]])\n\n</code></pre> <pre><code>torch.potrf(a, upper=True, out=None)\n</code></pre> <p>\u8ba1\u7b97\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635  \u7684Cholesky\u5206\u89e3\u3002</p> <p>\u6709\u5173 <code>torch.potrf()</code> \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b <code>torch.cholesky()</code> \u3002</p> <p>Warning</p> <p>torch.potrf\u4e0d\u8d5e\u6210\u4f7f\u7528torch.cholesky\uff0c\u5c06\u5728\u4e0b\u4e00\u4e2a\u7248\u672c\u4e2d\u5220\u9664\u3002\u8bf7\u6539\u7528torch.cholesky\u5e76\u6ce8\u610ftorch.cholesky\u4e2d\u7684<code>upper</code>\u53c2\u6570\u9ed8\u8ba4\u4e3a<code>False</code>\u3002</p> <pre><code>torch.potri(u, upper=True, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u6b63\u534a\u5b9a\u77e9\u9635\u7684\u5012\u6570\uff0c\u7ed9\u51fa\u5176Cholesky\u56e0\u5b50<code>u</code>\uff1a\u8fd4\u56de\u77e9\u9635<code>inv</code></p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>True</code>\u6216\u672a\u63d0\u4f9b\uff0c\u5219<code>u</code>\u4e3a\u4e0a\u4e09\u89d2\u5f62\uff0c\u4f7f\u5f97\u8fd4\u56de\u7684\u5f20\u91cf\u4e3a</p> <p></p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>False</code>\uff0c\u5219<code>u</code>\u4e3a\u4e0b\u4e09\u89d2\u5f62\uff0c\u4f7f\u5f97\u8fd4\u56de\u7684\u5f20\u91cf\u4e3a</p> <p></p> <p>Parameters:</p> <ul> <li>u  (Tensor\uff09 - \u8f93\u51652-D\u5f20\u91cf\uff0c\u4e0a\u4e0b\u4e09\u89d2Cholesky\u56e0\u5b50</li> <li>\u4e0a (bool\uff0c \u53ef\u9009\uff09 - \u662f\u5426\u8fd4\u56de\u4e0a\u9650(\u9ed8\u8ba4\uff09\u6216\u4e0b\u4e09\u89d2\u77e9\u9635</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - <code>inv</code>\u7684\u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite\n&gt;&gt;&gt; u = torch.cholesky(a)\n&gt;&gt;&gt; a\ntensor([[  0.9935,  -0.6353,   1.5806],\n [ -0.6353,   0.8769,  -1.7183],\n [  1.5806,  -1.7183,  10.6618]])\n&gt;&gt;&gt; torch.potri(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n [ 1.2251,  2.4439,  0.2122],\n [-0.0889,  0.2122,  0.1412]])\n&gt;&gt;&gt; a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n [ 1.2251,  2.4439,  0.2122],\n [-0.0889,  0.2122,  0.1412]])\n\n</code></pre> <pre><code>torch.potrs(b, u, upper=True, out=None) \u2192 Tensor\n</code></pre> <p>\u6c42\u89e3\u5177\u6709\u6b63\u534a\u5b9a\u77e9\u9635\u7684\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff0c\u7ed9\u5b9a\u5176Cholesky\u56e0\u5b50\u77e9\u9635<code>u</code>\u3002</p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>True</code>\u6216\u672a\u63d0\u4f9b\uff0c\u5219<code>u</code>\u4e3a\u4e0a\u4e09\u89d2\u5f62\u5e76\u8fd4\u56de<code>c</code>\uff0c\u4ee5\u4fbf\uff1a</p> <p></p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>False</code>\uff0c\u5219<code>u</code>\u4e3a\u4e0b\u4e09\u89d2\u5f62\u5e76\u8fd4\u56de<code>c</code>\uff0c\u4ee5\u4fbf\uff1a</p> <p></p> <p><code>torch.potrs(b, u)</code>\u53ef\u4ee5\u63a5\u65362D\u8f93\u5165<code>b, u</code>\u6216\u4e24\u62792D\u77e9\u9635\u7684\u8f93\u5165\u3002\u5982\u679c\u8f93\u5165\u662f\u6279\u6b21\uff0c\u5219\u8fd4\u56de\u6279\u91cf\u8f93\u51fa<code>c</code></p> <p>Note</p> <p><code>out</code>\u5173\u952e\u5b57\u4ec5\u652f\u63012D\u77e9\u9635\u8f93\u5165\uff0c\u5373<code>b, u</code>\u5fc5\u987b\u662f2D\u77e9\u9635\u3002</p> <p>Parameters:</p> <ul> <li>b  (tensor) - \u5927\u5c0f  \u7684\u8f93\u5165\u77e9\u9635\uff0c\u5176\u4e2d  \u4e3a\u96f6\u6216\u6279\u91cf\u7ef4\u5ea6\u66f4\u591a</li> <li>u  (Tensor\uff09 - \u5927\u5c0f\u4e3a  \u7684\u8f93\u5165\u77e9\u9635\uff0c\u5176\u4e2d  \u4e3a\u96f6\u66f4\u591a\u6279\u91cf\u5c3a\u5bf8\u7531\u4e0a\u90e8\u6216\u4e0b\u90e8\u4e09\u89d2\u5f62Cholesky\u56e0\u5b50\u7ec4\u6210</li> <li>\u4e0a (bool\uff0c \u53ef\u9009\uff09 - \u662f\u5426\u8fd4\u56de\u4e0a\u9650(\u9ed8\u8ba4\uff09\u6216\u4e0b\u4e09\u89d2\u77e9\u9635</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - <code>c</code>\u7684\u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite\n&gt;&gt;&gt; u = torch.cholesky(a)\n&gt;&gt;&gt; a\ntensor([[ 0.7747, -1.9549,  1.3086],\n [-1.9549,  6.7546, -5.4114],\n [ 1.3086, -5.4114,  4.8733]])\n&gt;&gt;&gt; b = torch.randn(3, 2)\n&gt;&gt;&gt; b\ntensor([[-0.6355,  0.9891],\n [ 0.1974,  1.4706],\n [-0.4115, -0.6225]])\n&gt;&gt;&gt; torch.potrs(b,u)\ntensor([[ -8.1625,  19.6097],\n [ -5.8398,  14.2387],\n [ -4.3771,  10.4173]])\n&gt;&gt;&gt; torch.mm(a.inverse(),b)\ntensor([[ -8.1626,  19.6097],\n [ -5.8398,  14.2387],\n [ -4.3771,  10.4173]])\n\n</code></pre> <pre><code>torch.pstrf(a, upper=True, out=None) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8ba1\u7b97\u6b63\u534a\u5b9a\u77e9\u9635<code>a</code>\u7684\u65cb\u8f6cCholesky\u5206\u89e3\u3002\u8fd4\u56de\u77e9\u9635<code>u</code>\u548c<code>piv</code>\u3002</p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>True</code>\u6216\u672a\u63d0\u4f9b\uff0c\u5219<code>u</code>\u4e3a\u4e0a\u4e09\u89d2\u5f62\uff0c\u4f7f\u5f97  \uff0c<code>p</code>\u4e3a<code>piv</code>\u7ed9\u51fa\u7684\u7f6e\u6362\u3002</p> <p>\u5982\u679c<code>upper</code>\u4e3a<code>False</code>\uff0c\u5219<code>u</code>\u4e3a\u4e09\u89d2\u5f62\uff0c\u4f7f\u5f97  \u3002</p> <p>Parameters:</p> <ul> <li>a  (tensor) - \u8f93\u5165\u4e8c\u7ef4\u5f20\u91cf</li> <li>\u4e0a (bool\uff0c \u53ef\u9009\uff09 - \u662f\u5426\u8fd4\u56de\u4e0a\u9650(\u9ed8\u8ba4\uff09\u6216\u4e0b\u4e09\u89d2\u77e9\u9635</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - <code>u</code>\u548c<code>piv</code>\u5f20\u91cf\u7684\u5143\u7ec4</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite\n&gt;&gt;&gt; a\ntensor([[ 3.5405, -0.4577,  0.8342],\n [-0.4577,  1.8244, -0.1996],\n [ 0.8342, -0.1996,  3.7493]])\n&gt;&gt;&gt; u,piv = torch.pstrf(a)\n&gt;&gt;&gt; u\ntensor([[ 1.9363,  0.4308, -0.1031],\n [ 0.0000,  1.8316, -0.2256],\n [ 0.0000,  0.0000,  1.3277]])\n&gt;&gt;&gt; piv\ntensor([ 2,  0,  1], dtype=torch.int32)\n&gt;&gt;&gt; p = torch.eye(3).index_select(0,piv.long()).index_select(0,piv.long()).t() # make pivot permutation\n&gt;&gt;&gt; torch.mm(torch.mm(p.t(),torch.mm(u.t(),u)),p) # reconstruct\ntensor([[ 3.5405, -0.4577,  0.8342],\n [-0.4577,  1.8244, -0.1996],\n [ 0.8342, -0.1996,  3.7493]])\n\n</code></pre> <pre><code>torch.qr(input, out=None) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8ba1\u7b97\u77e9\u9635<code>input</code>\u7684QR\u5206\u89e3\uff0c\u5e76\u8fd4\u56de\u77e9\u9635<code>Q</code>\u548c<code>R</code>\uff0c\u4f7f  \uff0c  \u4e3a\u6b63\u4ea4\u77e9\u9635\u548c [] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/502cdd9c79852b33d2a6d18ba5ec3102.jpg) \u662f\u4e00\u4e2a\u4e0a\u4e09\u89d2\u77e9\u9635\u3002</p> <p>\u8fd9\u5c06\u8fd4\u56de\u7626(\u51cf\u5c11\uff09QR\u5206\u89e3\u3002</p> <p>Note</p> <p>\u5982\u679c<code>input</code>\u7684\u5143\u7d20\u7684\u5927\u5c0f\u5f88\u5927\uff0c\u5219\u7cbe\u5ea6\u53ef\u80fd\u4f1a\u4e22\u5931</p> <p>Note</p> <p>\u867d\u7136\u5b83\u5e94\u8be5\u603b\u662f\u7ed9\u4f60\u4e00\u4e2a\u6709\u6548\u7684\u5206\u89e3\uff0c\u5b83\u53ef\u80fd\u4e0d\u4f1a\u8de8\u5e73\u53f0\u7ed9\u4f60\u76f8\u540c\u7684 - \u5b83\u5c06\u53d6\u51b3\u4e8e\u4f60\u7684LAPACK\u5b9e\u73b0\u3002</p> <p>Note</p> <p>\u4e0d\u7ba1\u539f\u59cb\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u57fa\u8d28  \u5c06\u88ab\u8f6c\u7f6e\uff0c\u5373\u6b65\u5e45\u4e3a<code>(1, m)</code>\u800c\u4e0d\u662f<code>(m, 1)</code>\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u51652-D\u5f20\u91cf</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - <code>Q</code>\u548c<code>R</code>\u5f20\u91cf\u7684\u5143\u7ec4</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n&gt;&gt;&gt; q, r = torch.qr(a)\n&gt;&gt;&gt; q\ntensor([[-0.8571,  0.3943,  0.3314],\n [-0.4286, -0.9029, -0.0343],\n [ 0.2857, -0.1714,  0.9429]])\n&gt;&gt;&gt; r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n [   0.0000, -175.0000,   70.0000],\n [   0.0000,    0.0000,  -35.0000]])\n&gt;&gt;&gt; torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n [   6.,  167.,  -68.],\n [  -4.,   24.,  -41.]])\n&gt;&gt;&gt; torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n [ 0.,  1., -0.],\n [ 0., -0.,  1.]])\n\n</code></pre> <pre><code>torch.svd(input, some=True, compute_uv=True, out=None) -&gt; (Tensor, Tensor, Tensor)\n</code></pre> <p><code>U, S, V = torch.svd(A)</code>\u8fd4\u56de\u5927\u5c0f\u4e3a<code>(n x m)</code>\u7684\u5b9e\u77e9\u9635<code>A</code>\u7684\u5947\u5f02\u503c\u5206\u89e3\uff0c\u4f7f\u5f97  \u3002</p> <p><code>U</code>\u5177\u6709  \u7684\u5f62\u72b6\u3002</p> <p><code>S</code>\u662f\u5f62\u72b6  \u7684\u5bf9\u89d2\u77e9\u9635\uff0c\u8868\u793a\u4e3a\u5305\u542b\u975e\u8d1f\u5bf9\u89d2\u7ebf\u6761\u76ee\u7684\u5927\u5c0f  \u7684\u5411\u91cf\u3002</p> <p><code>V</code>\u5177\u6709  \u7684\u5f62\u72b6\u3002</p> <p>\u5982\u679c<code>some</code>\u4e3a<code>True</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u8fd4\u56de\u7684<code>U</code>\u548c<code>V</code>\u77e9\u9635\u5c06\u4ec5\u5305\u542b  \u6b63\u4ea4\u5217\u3002</p> <p>\u5982\u679c<code>compute_uv</code>\u662f<code>False</code>\uff0c\u5219\u8fd4\u56de\u7684<code>U</code>\u548c<code>V</code>\u77e9\u9635\u5c06\u5206\u522b\u4e3a\u5f62\u72b6  \u548c  \u7684\u96f6\u77e9\u9635\u3002\u8fd9\u91cc\u5c06\u5ffd\u7565<code>some</code>\u3002</p> <p>Note</p> <p>\u5728CPU\u4e0a\u5b9e\u73b0SVD\u4f7f\u7528LAPACK\u4f8b\u7a0b<code>?gesdd</code>(\u5206\u800c\u6cbb\u4e4b\u7b97\u6cd5\uff09\u800c\u4e0d\u662f<code>?gesvd</code>\u6765\u63d0\u9ad8\u901f\u5ea6\u3002\u7c7b\u4f3c\u5730\uff0cGPU\u4e0a\u7684SVD\u4e5f\u4f7f\u7528MAGMA\u4f8b\u7a0b<code>gesdd</code>\u3002</p> <p>Note</p> <p>\u4e0d\u7ba1\u539f\u59cb\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u77e9\u9635<code>U</code>\u5c06\u88ab\u8f6c\u7f6e\uff0c\u5373\u7528\u6b65\u5e45<code>(1, n)</code>\u4ee3\u66ff<code>(n, 1)</code>\u3002</p> <p>Note</p> <p>\u5411\u540e\u901a\u8fc7<code>U</code>\u548c<code>V</code>\u8f93\u51fa\u65f6\u9700\u8981\u7279\u522b\u5c0f\u5fc3\u3002\u5f53<code>input</code>\u5177\u6709\u6240\u6709\u4e0d\u540c\u7684\u5947\u5f02\u503c\u7684\u6ee1\u79e9\u65f6\uff0c\u8fd9\u79cd\u64cd\u4f5c\u5b9e\u9645\u4e0a\u662f\u7a33\u5b9a\u7684\u3002\u5426\u5219\uff0c<code>NaN</code>\u53ef\u80fd\u4f1a\u51fa\u73b0\uff0c\u56e0\u4e3a\u672a\u6b63\u786e\u5b9a\u4e49\u6e10\u53d8\u3002\u6b64\u5916\uff0c\u8bf7\u6ce8\u610f\uff0c\u5373\u4f7f\u539f\u59cb\u540e\u5411\u4ec5\u5728<code>S</code>\u4e0a\uff0c\u53cc\u5411\u540e\u901a\u5e38\u4f1a\u901a\u8fc7<code>U</code>\u548c<code>V</code>\u5411\u540e\u8fdb\u884c\u3002</p> <p>Note</p> <p>\u5f53<code>some</code> = <code>False</code>\u65f6\uff0c<code>U[:, min(n, m):]</code>\u548c<code>V[:, min(n, m):]</code>\u4e0a\u7684\u68af\u5ea6\u5c06\u88ab\u53cd\u5411\u5ffd\u7565\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u77e2\u91cf\u53ef\u4ee5\u662f\u5b50\u7a7a\u95f4\u7684\u4efb\u610f\u57fa\u6570\u3002</p> <p>Note</p> <p>\u5f53<code>compute_uv</code> = <code>False</code>\u65f6\uff0c\u7531\u4e8e\u540e\u5411\u64cd\u4f5c\u9700\u8981\u524d\u5411\u901a\u9053\u7684<code>U</code>\u548c<code>V</code>\uff0c\u56e0\u6b64\u65e0\u6cd5\u6267\u884c\u540e\u9000\u64cd\u4f5c\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u51652-D\u5f20\u91cf</li> <li>\u4e00\u4e9b (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u8fd4\u56de<code>U</code>\u548c<code>V</code>\u7684\u5f62\u72b6</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - \u5f20\u91cf\u7684\u8f93\u51fa\u5143\u7ec4</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.tensor([[8.79,  6.11, -9.15,  9.57, -3.49,  9.84],\n [9.93,  6.91, -7.93,  1.64,  4.02,  0.15],\n [9.83,  5.04,  4.86,  8.83,  9.80, -8.99],\n [5.45, -0.27,  4.85,  0.74, 10.00, -6.02],\n [3.16,  7.98,  3.01,  5.80,  4.27, -5.31]]).t()\n\n&gt;&gt;&gt; u, s, v = torch.svd(a)\n&gt;&gt;&gt; u\ntensor([[-0.5911,  0.2632,  0.3554,  0.3143,  0.2299],\n [-0.3976,  0.2438, -0.2224, -0.7535, -0.3636],\n [-0.0335, -0.6003, -0.4508,  0.2334, -0.3055],\n [-0.4297,  0.2362, -0.6859,  0.3319,  0.1649],\n [-0.4697, -0.3509,  0.3874,  0.1587, -0.5183],\n [ 0.2934,  0.5763, -0.0209,  0.3791, -0.6526]])\n&gt;&gt;&gt; s\ntensor([ 27.4687,  22.6432,   8.5584,   5.9857,   2.0149])\n&gt;&gt;&gt; v\ntensor([[-0.2514,  0.8148, -0.2606,  0.3967, -0.2180],\n [-0.3968,  0.3587,  0.7008, -0.4507,  0.1402],\n [-0.6922, -0.2489, -0.2208,  0.2513,  0.5891],\n [-0.3662, -0.3686,  0.3859,  0.4342, -0.6265],\n [-0.4076, -0.0980, -0.4933, -0.6227, -0.4396]])\n&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(1.00000e-06 *\n 9.3738)\n\n</code></pre> <pre><code>torch.symeig(input, eigenvectors=False, upper=True, out=None) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u8be5\u51fd\u6570\u8fd4\u56de\u5b9e\u5bf9\u79f0\u77e9\u9635<code>input</code>\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\uff0c\u7531\u5143\u7ec4  \u8868\u793a\u3002</p> <p><code>input</code>\u548c  \u662f  \u57fa\u8d28\uff0c  \u662f  \u7ef4\u5411\u91cf\u3002</p> <p>\u8be5\u51fd\u6570\u8ba1\u7b97<code>input</code>\u7684\u6240\u6709\u7279\u5f81\u503c(\u548c\u5411\u91cf\uff09\uff0c\u4f7f\u5f97  \u3002</p> <p>\u5e03\u5c14\u53c2\u6570<code>eigenvectors</code>\u4ec5\u5b9a\u4e49\u7279\u5f81\u5411\u91cf\u6216\u7279\u5f81\u503c\u7684\u8ba1\u7b97\u3002</p> <p>\u5982\u679c\u662f<code>False</code>\uff0c\u5219\u4ec5\u8ba1\u7b97\u7279\u5f81\u503c\u3002\u5982\u679c\u662f<code>True</code>\uff0c\u5219\u8ba1\u7b97\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\u3002</p> <p>\u7531\u4e8e\u8f93\u5165\u77e9\u9635<code>input</code>\u5e94\u8be5\u662f\u5bf9\u79f0\u7684\uff0c\u56e0\u6b64\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4ec5\u4f7f\u7528\u4e0a\u4e09\u89d2\u5f62\u90e8\u5206\u3002</p> <p>\u5982\u679c<code>upper</code>\u662f<code>False</code>\uff0c\u5219\u4f7f\u7528\u4e0b\u4e09\u89d2\u5f62\u90e8\u5206\u3002</p> <p>\u6ce8\u610f\uff1a\u65e0\u8bba\u539f\u59cb\u6b65\u5e45\u5982\u4f55\uff0c\u8fd4\u56de\u7684\u77e9\u9635<code>V</code>\u90fd\u5c06\u88ab\u8f6c\u7f6e\uff0c\u5373\u4f7f\u7528\u6b65\u5e45<code>(1, m)</code>\u800c\u4e0d\u662f<code>(m, 1)</code>\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5bf9\u79f0\u77e9\u9635</li> <li>\u7279\u5f81\u5411\u91cf(\u5e03\u5c14 \uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u5fc5\u987b\u8ba1\u7b97\u7279\u5f81\u5411\u91cf</li> <li>\u4e0a(\u5e03\u5c14 \uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8003\u8651\u4e0a\u4e09\u89d2\u6216\u4e0b\u4e09\u89d2\u533a\u57df</li> <li>out  (\u5143\u7ec4 \uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5143\u7ec4(Tensor\uff0cTensor\uff09</li> </ul> <p>| Returns: | A tuple containing</p> <p>\uff06GT; * e (tensor\uff09\uff1a\u5f62\u72b6  \u3002\u6bcf\u4e2a\u5143\u7d20\u662f<code>input</code>\u7684\u7279\u5f81\u503c\uff0c\u7279\u5f81\u503c\u6309\u5347\u5e8f\u6392\u5217\u3002 \uff06GT; * V (tensor\uff09\uff1a\u5f62\u72b6  \u3002\u5982\u679c<code>eigenvectors=False</code>\uff0c\u5b83\u662f\u4e00\u4e2a\u5145\u6ee1\u96f6\u7684\u5f20\u91cf\u3002\u5426\u5219\uff0c\u8be5\u5f20\u91cf\u5305\u542b<code>input</code>\u7684\u6807\u51c6\u6b63\u4ea4\u7279\u5f81\u5411\u91cf\u3002</p> Return type: (Tensor, Tensor) <p>\u4f8b\u5b50\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.tensor([[ 1.96,  0.00,  0.00,  0.00,  0.00],\n [-6.49,  3.80,  0.00,  0.00,  0.00],\n [-0.47, -6.39,  4.17,  0.00,  0.00],\n [-7.20,  1.50, -1.51,  5.70,  0.00],\n [-0.65, -6.34,  2.67,  1.80, -7.10]]).t()\n&gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True)\n&gt;&gt;&gt; e\ntensor([-11.0656,  -6.2287,   0.8640,   8.8655,  16.0948])\n&gt;&gt;&gt; v\ntensor([[-0.2981, -0.6075,  0.4026, -0.3745,  0.4896],\n [-0.5078, -0.2880, -0.4066, -0.3572, -0.6053],\n [-0.0816, -0.3843, -0.6600,  0.5008,  0.3991],\n [-0.0036, -0.4467,  0.4553,  0.6204, -0.4564],\n [-0.8041,  0.4480,  0.1725,  0.3108,  0.1622]])\n\n</code></pre> <pre><code>torch.trtrs(b, A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)\n</code></pre> <p>\u6c42\u89e3\u5177\u6709\u4e09\u89d2\u7cfb\u6570\u77e9\u9635  \u548c\u591a\u4e2a\u53f3\u4fa7<code>b</code>\u7684\u65b9\u7a0b\u7ec4\u3002</p> <p>\u7279\u522b\u662f\uff0c\u89e3\u51b3  \u5e76\u5047\u8bbe  \u662f\u9ed8\u8ba4\u5173\u952e\u5b57\u53c2\u6570\u7684\u4e0a\u4e09\u89d2\u5f62\u3002</p> <p>Parameters:</p> <ul> <li>A  (Tensor\uff09 - \u8f93\u5165\u4e09\u89d2\u7cfb\u6570\u77e9\u9635</li> <li>b  (Tensor\uff09 - \u591a\u4e2a\u53f3\u4fa7\u3002  \u7684\u6bcf\u4e00\u5217\u662f\u65b9\u7a0b\u7ec4\u7684\u53f3\u4fa7\u3002</li> <li>\u4e0a (bool\uff0c \u53ef\u9009\uff09 - \u662f\u5426\u89e3\u51b3\u4e0a\u4e09\u89d2\u65b9\u7a0b\u7ec4(\u9ed8\u8ba4\uff09\u6216\u8005\u4e0b\u4e09\u89d2\u65b9\u7a0b\u7ec4\u3002\u9ed8\u8ba4\u503c\uff1aTrue\u3002</li> <li>\u8f6c\u5ea7 (bool\uff0c \u4efb\u9009\uff09 -  \u662f\u5426\u5e94\u8f6c\u7f6e\u5728\u88ab\u9001\u5230\u89e3\u7b97\u5668\u4e4b\u524d\u3002\u9ed8\u8ba4\u503c\uff1aFalse\u3002</li> <li>\u4e09\u89d2 (bool\uff0c \u4efb\u9009\uff09 -  \u662f\u5355\u4f4d\u4e09\u89d2\u5f62\u3002\u5982\u679c\u4e3aTrue\uff0c\u5219\u5047\u5b9a  \u7684\u5bf9\u89d2\u5143\u7d20\u4e3a1\uff0c\u5e76\u4e14\u672a\u53c2\u8003  \u3002\u9ed8\u8ba4\u503c\uff1aFalse\u3002</li> </ul> Returns: \u5143\u7ec4  \u5176\u4e2d  \u662f  \u548c  \u7684\u514b\u9686\u662f[\u7684\u89e3\u51b3\u65b9\u6848] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/79ccd3754eebf815ed3195b42f93bacb.jpg) (\u6216\u7b49\u5f0f\u7cfb\u7edf\u7684\u4efb\u4f55\u53d8\u4f53\uff0c\u53d6\u51b3\u4e8e\u5173\u952e\u5b57\u53c2\u6570\u3002\uff09 <pre><code>Shape:\n</code></pre> <ul> <li>\u7b54\uff1a </li> <li>b\uff1a </li> <li>\u8f93\u51fa[0]\uff1a </li> <li>\u8f93\u51fa[1]\uff1a </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; A = torch.randn(2, 2).triu()\n&gt;&gt;&gt; A\ntensor([[ 1.1527, -1.0753],\n [ 0.0000,  0.7986]])\n&gt;&gt;&gt; b = torch.randn(2, 3)\n&gt;&gt;&gt; b\ntensor([[-0.0210,  2.3513, -1.5492],\n [ 1.5429,  0.7403, -1.0243]])\n&gt;&gt;&gt; torch.trtrs(b, A)\n(tensor([[ 1.7840,  2.9045, -2.5405],\n [ 1.9319,  0.9269, -1.2826]]), tensor([[ 1.1527, -1.0753],\n [ 0.0000,  0.7986]]))\n\n</code></pre>"},{"location":"1.0/torch_math_operations_comparison_ops/","title":"Comparison Ops","text":""},{"location":"1.0/torch_math_operations_comparison_ops/#_1","title":"\u6bd4\u8f83\u884c\u52a8","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.allclose(self, other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 bool\n</code></pre> <p>\u6b64\u51fd\u6570\u68c0\u67e5\u6240\u6709<code>self</code>\u548c<code>other</code>\u662f\u5426\u6ee1\u8db3\u6761\u4ef6\uff1a</p> <p></p> <p>\u5143\u7d20\uff0c\u5bf9\u4e8e<code>self</code>\u548c<code>other</code>\u7684\u6240\u6709\u5143\u7d20\u3002\u6b64\u51fd\u6570\u7684\u884c\u4e3a\u7c7b\u4f3c\u4e8e numpy.allclose</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u81ea (tensor) - \u9996\u5148\u8fdb\u884c\u5f20\u91cf\u6bd4\u8f83</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u5f20\u91cf\u6765\u6bd4\u8f83</li> <li>atol  (\u6f02\u6d6e \uff0c \u4efb\u9009\uff09 - \u7edd\u5bf9\u8010\u53d7\u3002\u9ed8\u8ba4\u503c\uff1a1e-08</li> <li>rtol  (\u6f02\u6d6e \uff0c \u4efb\u9009\uff09 - \u76f8\u5bf9\u8010\u53d7\u3002\u9ed8\u8ba4\u503c\uff1a1e-05</li> <li>equal_nan  (\u6f02\u6d6e \uff0c \u4efb\u9009\uff09 - \u5982\u679c<code>True</code>\uff0c\u90a3\u4e48\u4e24\u4e2a<code>NaN</code> s\u5c06\u662f\u6bd4\u8f83\u5e73\u7b49\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> </ul> <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n&gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n&gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n\n</code></pre> <pre><code>torch.argsort(input, dim=None, descending=False)\n</code></pre> <p>\u8fd4\u56de\u6309\u503c\u6309\u5347\u5e8f\u5bf9\u7ed9\u5b9a\u7ef4\u5ea6\u7684\u5f20\u91cf\u8fdb\u884c\u6392\u5e8f\u7684\u7d22\u5f15\u3002</p> <p>\u8fd9\u662f <code>torch.sort()</code> \u8fd4\u56de\u7684\u7b2c\u4e8c\u4e2a\u503c\u3002\u6709\u5173\u6b64\u65b9\u6cd5\u7684\u786e\u5207\u8bed\u4e49\uff0c\u8bf7\u53c2\u9605\u5176\u6587\u6863\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff0c \u53ef\u9009\uff09 - \u6392\u5e8f\u7684\u7ef4\u5ea6</li> <li>\u964d\u5e8f (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u6392\u5e8f\u987a\u5e8f(\u5347\u5e8f\u6216\u964d\u5e8f\uff09</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n [ 0.1598,  0.0788, -0.0745, -1.2700],\n [ 1.2208,  1.0722, -0.7064,  1.2564],\n [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n&gt;&gt;&gt; torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n [3, 2, 1, 0],\n [2, 1, 0, 3],\n [3, 2, 1, 0]])\n\n</code></pre> <pre><code>torch.eq(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u5143\u7d20\u660e\u786e\u7684\u5e73\u7b49</p> <p>\u7b2c\u4e8c\u4e2a\u53c2\u6570\u53ef\u4ee5\u662f\u6570\u5b57\u6216\u5f20\u91cf\uff0c\u5176\u5f62\u72b6\u4e3a\u53ef\u5e7f\u64ad\u7684\u7b2c\u4e00\u4e2a\u53c2\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u3002\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> \u8fd4\u56de\uff1a \u5728\u6bd4\u8f83\u4e3a\u771f\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5305\u542b1\u7684<code>torch.ByteTensor</code> \u8fd4\u56de\u7c7b\u578b\uff1a Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 1,  0],\n [ 0,  1]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.equal(tensor1, tensor2) \u2192 bool\n</code></pre> <p><code>True</code>\u5982\u679c\u4e24\u4e2a\u5f20\u91cf\u5177\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\u548c\u5143\u7d20\uff0c\u5219<code>False</code>\u3002</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n\n</code></pre> <pre><code>torch.ge(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u6309\u5143\u7d20\u8ba1\u7b97  \u3002</p> <p>The second argument can be a number or a tensor whose shape is broadcastable with the first argument.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> Returns: A <code>torch.ByteTensor</code> containing a 1 at each location where comparison is true Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 1,  1],\n [ 0,  1]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.gt(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u6309\u5143\u7d20\u8ba1\u7b97  \u3002</p> <p>The second argument can be a number or a tensor whose shape is broadcastable with the first argument.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> Returns: A <code>torch.ByteTensor</code> containing a 1 at each location where comparison is true Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 0,  1],\n [ 0,  0]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.isfinite(tensor)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5e03\u5c14\u5143\u7d20\u8868\u793a\u6bcf\u4e2a\u5143\u7d20\u662f\u5426\u4e3a<code>Finite</code>\u3002</p> \u53c2\u6570\uff1a \u5f20\u91cf (tensor) - \u5f20\u91cf\u6765\u68c0\u67e5 \u8fd4\u56de\uff1a <code>torch.ByteTensor</code>\u5728\u6709\u9650\u5143\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5305\u542b1\uff0c\u5426\u5219\u4e3a0 Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([ 1,  0,  1,  0,  0], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.isinf(tensor)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5e03\u5c14\u5143\u7d20\u8868\u793a\u6bcf\u4e2a\u5143\u7d20\u662f\u5426\u4e3a<code>+/-INF</code>\u3002</p> Parameters: tensor (Tensor) \u2013 A tensor to check Returns: <code>torch.ByteTensor</code>\u5728<code>+/-INF</code>\u5143\u7d20\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5305\u542b1\uff0c\u5426\u5219\u4e3a0 Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([ 0,  1,  0,  1,  0], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.isnan(tensor)\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5e03\u5c14\u5143\u7d20\u8868\u793a\u6bcf\u4e2a\u5143\u7d20\u662f\u5426\u4e3a<code>NaN</code>\u3002</p> Parameters: tensor (Tensor) \u2013 A tensor to check Returns: <code>torch.ByteTensor</code>\u5728<code>NaN</code>\u5143\u7d20\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5305\u542b1\u3002 Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([ 0,  1,  0], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.kthvalue(input, k, dim=None, keepdim=False, out=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a\u7ed9\u5b9a<code>input</code>\u5f20\u91cf\u7684<code>k</code>\u4e2a\u6700\u5c0f\u5143\u7d20\u3002</p> <p>\u5982\u679c\u672a\u7ed9\u51fa<code>dim</code>\uff0c\u5219\u9009\u62e9<code>input</code>\u7684\u6700\u540e\u4e00\u4e2a\u5c3a\u5bf8\u3002</p> <p>\u8fd4\u56de<code>(values, indices)</code>\u7684\u5143\u7ec4\uff0c\u5176\u4e2d<code>indices</code>\u662f\u7ef4\u5ea6<code>dim</code>\u4e2d\u539f\u59cb<code>input</code>\u5f20\u91cf\u4e2d\u7b2ck\u4e2a\u6700\u5c0f\u5143\u7d20\u7684\u7d22\u5f15\u3002</p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c<code>values</code>\u548c<code>indices</code>\u5f20\u91cf\u90fd\u4e0e<code>input</code>\u7684\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a<code>dim</code>\u7684\u5c3a\u5bf8\u9664\u5916\u3002\u5426\u5219\uff0c<code>dim</code>\u88ab\u6324\u538b(\u89c1 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4<code>values</code>\u548c<code>indices</code>\u5f20\u91cf\u7684\u5c3a\u5bf8\u6bd4<code>input</code>\u5f20\u91cf\u5c0f1\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>k  (int\uff09 - k\u4e3a\u7b2ck\u4e2a\u6700\u5c0f\u5143\u7d20</li> <li>\u660f\u6697 (int\uff0c \u53ef\u9009\uff09 - \u627e\u5230kth\u503c\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - (Tensor\uff0cLongTensor\uff09\u7684\u8f93\u51fa\u5143\u7ec4\u53ef\u4ee5\u4efb\u610f\u7ed9\u51fa\u7528\u4f5c\u8f93\u51fa\u7f13\u51b2\u533a</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)\n&gt;&gt;&gt; x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n&gt;&gt;&gt; torch.kthvalue(x, 4)\n(tensor(4.), tensor(3))\n\n&gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3)\n&gt;&gt;&gt; x\ntensor([[ 1.,  2.,  3.],\n [ 4.,  5.,  6.]])\n&gt;&gt;&gt; torch.kthvalue(x,2,0,True)\n(tensor([[ 4.,  5.,  6.]]), tensor([[ 1,  1,  1]]))\n\n</code></pre> <pre><code>torch.le(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u6309\u5143\u7d20\u8ba1\u7b97  \u3002</p> <p>The second argument can be a number or a tensor whose shape is broadcastable with the first argument.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> Returns: A <code>torch.ByteTensor</code> containing a 1 at each location where comparison is true Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 1,  0],\n [ 1,  1]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.lt(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u6309\u5143\u7d20\u8ba1\u7b97  \u3002</p> <p>The second argument can be a number or a tensor whose shape is broadcastable with the first argument.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> Returns: A <code>torch.ByteTensor</code> containing a 1 at each location where comparison is true Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 0,  0],\n [ 1,  0]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.max()\n</code></pre> <pre><code>torch.max(input) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u6700\u5927\u503c\u3002</p> Parameters: \u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n&gt;&gt;&gt; torch.max(a)\ntensor(0.7445)\n\n</code></pre> <pre><code>torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u6700\u5927\u503c\u3002\u7b2c\u4e8c\u4e2a\u8fd4\u56de\u503c\u662f\u627e\u5230\u7684\u6bcf\u4e2a\u6700\u5927\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e(argmax\uff09\u3002</p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c\u5219\u8f93\u51fa\u5f20\u91cf\u4e0e<code>input</code>\u7684\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a<code>dim</code>\u7684\u5c3a\u5bf8\u4e3a1.\u5426\u5219\uff0c<code>dim</code>\u88ab\u6324\u538b(\u53c2\u89c1 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u7684\u5c3a\u5bf8\u6bd4<code>input</code>\u5c111\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>out  (\u5143\u7ec4 \uff0c \u53ef\u9009\uff09 - \u4e24\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u7ed3\u679c\u5143\u7ec4(max\uff0cmax_indices\uff09</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n [ 1.1949, -1.1127, -2.2379, -0.6702],\n [ 1.5717, -0.9207,  0.1297, -1.8768],\n [-0.6172,  1.0036, -0.6060, -0.2432]])\n&gt;&gt;&gt; torch.max(a, 1)\n(tensor([ 0.8475,  1.1949,  1.5717,  1.0036]), tensor([ 3,  0,  0,  1]))\n\n</code></pre> <pre><code>torch.max(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u5f20\u91cf<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u5f20\u91cf<code>other</code>\u7684\u5bf9\u5e94\u5143\u7d20\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u91c7\u7528\u9010\u5143\u7d20\u6700\u5927\u503c\u3002</p> <p><code>input</code>\u548c<code>other</code>\u7684\u5f62\u72b6\u4e0d\u9700\u8981\u5339\u914d\uff0c\u4f46\u5b83\u4eec\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u7684\u3002</p> <p></p> <p>\u6ce8\u610f</p> <p>\u5f53\u5f62\u72b6\u4e0d\u5339\u914d\u65f6\uff0c\u8fd4\u56de\u7684\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6\u9075\u5faa\u5e7f\u64ad\u89c4\u5219\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.2942, -0.7416,  0.2653, -0.1584])\n&gt;&gt;&gt; b = torch.randn(4)\n&gt;&gt;&gt; b\ntensor([ 0.8722, -1.7421, -0.4141, -0.5055])\n&gt;&gt;&gt; torch.max(a, b)\ntensor([ 0.8722, -0.7416,  0.2653, -0.1584])\n\n</code></pre> <pre><code>torch.min()\n</code></pre> <pre><code>torch.min(input) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u6700\u5c0f\u503c\u3002</p> Parameters: input (Tensor) \u2013 the input tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n&gt;&gt;&gt; torch.min(a)\ntensor(0.6750)\n\n</code></pre> <pre><code>torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u6700\u5c0f\u503c\u3002\u7b2c\u4e8c\u4e2a\u8fd4\u56de\u503c\u662f\u627e\u5230\u7684\u6bcf\u4e2a\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e(argmin\uff09\u3002</p> <p>If <code>keepdim</code> is <code>True</code>, the output tensors are of the same size as <code>input</code> except in the dimension <code>dim</code> where they are of size 1. Otherwise, <code>dim</code> is squeezed (see <code>torch.squeeze()</code>), resulting in the output tensors having 1 fewer dimension than <code>input</code>.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - \u4e24\u4e2a\u8f93\u51fa\u5f20\u91cf\u7684\u5143\u7ec4(min\uff0cmin_indices\uff09</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n [-1.4644, -0.2635, -0.3651,  0.6134],\n [ 0.2457,  0.0384,  1.0128,  0.7015],\n [-0.1153,  2.9849,  2.1458,  0.5788]])\n&gt;&gt;&gt; torch.min(a, 1)\n(tensor([-1.1899, -1.4644,  0.0384, -0.1153]), tensor([ 2,  0,  1,  0]))\n\n</code></pre> <pre><code>torch.min(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u5c06\u5f20\u91cf<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u5f20\u91cf<code>other</code>\u7684\u5bf9\u5e94\u5143\u7d20\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u91c7\u7528\u9010\u5143\u7d20\u6700\u5c0f\u503c\u3002\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\u3002</p> <p>The shapes of <code>input</code> and <code>other</code> don't need to match, but they must be broadcastable.</p> <p></p> <p>Note</p> <p>When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.8137, -1.1740, -0.6460,  0.6308])\n&gt;&gt;&gt; b = torch.randn(4)\n&gt;&gt;&gt; b\ntensor([-0.1369,  0.1555,  0.4019, -0.1929])\n&gt;&gt;&gt; torch.min(a, b)\ntensor([-0.1369, -1.1740, -0.6460, -0.1929])\n\n</code></pre> <pre><code>torch.ne(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u6309\u5143\u7d20\u8ba1\u7b97  \u3002</p> <p>The second argument can be a number or a tensor whose shape is broadcastable with the first argument.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8981\u6bd4\u8f83\u7684\u5f20\u91cf</li> <li>\u5176\u4ed6 (tensor \u6216 \u6f02\u6d6e) - \u5f20\u91cf\u6216\u503c\u6bd4\u8f83</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u5fc5\u987b\u662f<code>ByteTensor</code></li> </ul> Returns: \u5728\u6bd4\u8f83\u4e3a\u771f\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5305\u542b1\u7684<code>torch.ByteTensor</code>\u3002 Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 0,  1],\n [ 1,  0]], dtype=torch.uint8)\n\n</code></pre> <pre><code>torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u6309\u503c\u6309\u5347\u5e8f\u5bf9\u7ed9\u5b9a\u7ef4\u5ea6\u7684<code>input</code>\u5f20\u91cf\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\u3002</p> <p>If <code>dim</code> is not given, the last dimension of the <code>input</code> is chosen.</p> <p>\u5982\u679c<code>descending</code>\u662f<code>True</code>\uff0c\u5219\u5143\u7d20\u6309\u503c\u6309\u964d\u5e8f\u6392\u5e8f\u3002</p> <p>\u8fd4\u56de\u5143\u7ec4(sorted_tensor\uff0csorted_indices\uff09\uff0c\u5176\u4e2dsorted_indices\u662f\u539f\u59cb<code>input</code>\u5f20\u91cf\u4e2d\u5143\u7d20\u7684\u7d22\u5f15\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff0c \u53ef\u9009\uff09 - \u6392\u5e8f\u7684\u7ef4\u5ea6</li> <li>\u964d\u5e8f (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u6392\u5e8f\u987a\u5e8f(\u5347\u5e8f\u6216\u964d\u5e8f\uff09</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - (<code>Tensor</code>\uff0c<code>LongTensor</code>\uff09\u7684\u8f93\u51fa\u5143\u7ec4\u53ef\u4ee5\u9009\u62e9\u5c06\u5176\u7528\u4f5c\u8f93\u51fa\u7f13\u51b2\u533a</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)\n&gt;&gt;&gt; sorted, indices = torch.sort(x)\n&gt;&gt;&gt; sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n [-0.5793,  0.0061,  0.6058,  0.9497],\n [-0.5071,  0.3343,  0.9553,  1.0960]])\n&gt;&gt;&gt; indices\ntensor([[ 1,  0,  2,  3],\n [ 3,  1,  0,  2],\n [ 0,  3,  1,  2]])\n\n&gt;&gt;&gt; sorted, indices = torch.sort(x, 0)\n&gt;&gt;&gt; sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n [ 0.0608,  0.0061,  0.9497,  0.3343],\n [ 0.6058,  0.9553,  1.0960,  2.3332]])\n&gt;&gt;&gt; indices\ntensor([[ 2,  0,  0,  1],\n [ 0,  1,  1,  2],\n [ 1,  2,  2,  0]])\n\n</code></pre> <pre><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4\u5ea6\u4e0a\u7ed9\u5b9a<code>input</code>\u5f20\u91cf\u7684<code>k</code>\u6700\u5927\u5143\u7d20\u3002</p> <p>If <code>dim</code> is not given, the last dimension of the <code>input</code> is chosen.</p> <p>\u5982\u679c<code>largest</code>\u4e3a<code>False</code>\uff0c\u5219\u8fd4\u56de<code>k</code>\u6700\u5c0f\u5143\u7d20\u3002</p> <p>\u8fd4\u56de<code>(values, indices)</code>\u5143\u7ec4\uff0c\u5176\u4e2d<code>indices</code>\u662f\u539f\u59cb<code>input</code>\u5f20\u91cf\u4e2d\u5143\u7d20\u7684\u7d22\u5f15\u3002</p> <p>\u5e03\u5c14\u9009\u9879<code>sorted</code>\u5982\u679c<code>True</code>\uff0c\u5c06\u786e\u4fdd\u8fd4\u56de\u7684<code>k</code>\u5143\u7d20\u672c\u8eab\u5df2\u6392\u5e8f</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>k  (int\uff09 - \u201ctop-k\u201d\u4e2d\u7684k</li> <li>\u660f\u6697 (int\uff0c \u53ef\u9009\uff09 - \u6392\u5e8f\u7684\u7ef4\u5ea6</li> <li>\u6700\u5927 (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u6700\u5927\u6216\u6700\u5c0f\u5143\u7d20</li> <li>\u6392\u5e8f (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u6309\u6392\u5e8f\u987a\u5e8f\u8fd4\u56de\u5143\u7d20</li> <li>out  (\u5143\u7ec4 \uff0c \u4efb\u9009\uff09 - (Tensor\uff0cLongTensor\uff09\u7684\u8f93\u51fa\u5143\u7ec4\uff0c\u53ef\u4ee5\u9009\u62e9\u6027\u7ed9\u4e88\u7528\u4f5c\u8f93\u51fa\u7f13\u51b2\u533a</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(1., 6.)\n&gt;&gt;&gt; x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n&gt;&gt;&gt; torch.topk(x, 3)\n(tensor([ 5.,  4.,  3.]), tensor([ 4,  3,  2]))\n\n</code></pre>"},{"location":"1.0/torch_math_operations_other_ops/","title":"Other Operations","text":""},{"location":"1.0/torch_math_operations_other_ops/#_1","title":"\u5176\u4ed6\u884c\u52a8","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.bincount(self, weights=None, minlength=0) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u975e\u8d1f\u7684int\u6570\u7ec4\u4e2d\u6bcf\u4e2a\u503c\u7684\u9891\u7387\u3002</p> <p>\u9664\u975e<code>input</code>\u4e3a\u7a7a\uff0c\u5426\u5219\u7bb1\u6570(\u5927\u5c0f\u4e3a1\uff09\u6bd4<code>input</code>\u4e2d\u7684\u6700\u5927\u503c\u59271\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u7ed3\u679c\u662f\u5927\u5c0f\u4e3a0.\u5982\u679c\u6307\u5b9a<code>minlength</code>\uff0c\u5219\u7bb1\u6570\u4e3a\u81f3\u5c11<code>minlength</code>\u5e76\u4e14\u5982\u679c<code>input</code>\u4e3a\u7a7a\uff0c\u5219\u7ed3\u679c\u662f\u586b\u5145\u96f6\u7684\u5927\u5c0f<code>minlength</code>\u7684\u5f20\u91cf\u3002\u5982\u679c<code>n</code>\u662f\u4f4d\u7f6e<code>i</code>\u7684\u503c\uff0c<code>out[n] += weights[i]</code>\u5982\u679c\u6307\u5b9a\u4e86<code>weights</code>\uff0c\u5219<code>out[n] += 1</code>\u3002</p> <p>\u6ce8\u610f</p> <p>\u4f7f\u7528CUDA\u540e\u7aef\u65f6\uff0c\u6b64\u64cd\u4f5c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0d\u5bb9\u6613\u5173\u95ed\u7684\u4e0d\u786e\u5b9a\u884c\u4e3a\u3002\u6709\u5173\u80cc\u666f\uff0c\u8bf7\u53c2\u9605\u518d\u73b0\u6027\u7684\u6ce8\u91ca\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - 1-d int\u5f20\u91cf</li> <li>\u6743\u91cd (Tensor\uff09 - \u53ef\u9009\uff0c\u8f93\u5165\u5f20\u91cf\u4e2d\u6bcf\u4e2a\u503c\u7684\u6743\u91cd\u3002\u5e94\u4e0e\u8f93\u5165\u5f20\u91cf\u5927\u5c0f\u76f8\u540c\u3002</li> <li>minlength  (int\uff09 - \u53ef\u9009\u7684\u6700\u5c0f\u4e8c\u8fdb\u5236\u6570\u3002\u5e94\u8be5\u662f\u975e\u8d1f\u9762\u7684\u3002</li> </ul> \u8fd4\u56de\uff1a \u5982\u679c<code>input</code>\u975e\u7a7a\uff0c\u5219\u4e3a\u5f62\u72b6\u5f20\u91cf<code>Size([max(input) + 1])</code>\uff0c\u5426\u5219\u4e3a<code>Size(0)</code> \u8fd4\u56de\u7c7b\u578b\uff1a \u8f93\u51fa (Tensor) <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64)\n&gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5)\n&gt;&gt;&gt; input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n&gt;&gt;&gt; torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n&gt;&gt;&gt; input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n\n</code></pre> <pre><code>torch.broadcast_tensors(*tensors) \u2192 List of Tensors\n</code></pre> <p>\u6839\u636e_broadcasting-semantics\u5e7f\u64ad\u7ed9\u5b9a\u7684\u5f20\u91cf\u3002</p> \u53c2\u6570\uff1a * \u5f20\u91cf - \u4efb\u4f55\u6570\u91cf\u7684\u76f8\u540c\u7c7b\u578b\u7684\u5f20\u91cf <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(3).view(1, 3)\n&gt;&gt;&gt; y = torch.arange(2).view(2, 1)\n&gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y)\n&gt;&gt;&gt; a.size()\ntorch.Size([2, 3])\n&gt;&gt;&gt; a\ntensor([[0, 1, 2],\n [0, 1, 2]])\n\n</code></pre> <pre><code>torch.cross(input, other, dim=-1, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u548c<code>other</code>\u7684\u7ef4\u5ea6<code>dim</code>\u4e2d\u77e2\u91cf\u7684\u53c9\u79ef\u3002</p> <p><code>input</code>\u548c<code>other</code>\u5fc5\u987b\u5177\u6709\u76f8\u540c\u7684\u5c3a\u5bf8\uff0c\u5e76\u4e14<code>dim</code>\u5c3a\u5bf8\u7684\u5927\u5c0f\u5e94\u4e3a3\u3002</p> <p>\u5982\u679c\u672a\u7ed9\u51fa<code>dim</code>\uff0c\u5219\u9ed8\u8ba4\u4e3a\u627e\u5230\u5927\u5c0f\u4e3a3\u7684\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>dim  (int\uff0c \u4efb\u9009\uff09 - \u91c7\u53d6\u4ea4\u53c9\u79ef\u7684\u7ef4\u5ea6\u3002</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 3)\n&gt;&gt;&gt; a\ntensor([[-0.3956,  1.1455,  1.6895],\n [-0.5849,  1.3672,  0.3599],\n [-1.1626,  0.7180, -0.0521],\n [-0.1339,  0.9902, -2.0225]])\n&gt;&gt;&gt; b = torch.randn(4, 3)\n&gt;&gt;&gt; b\ntensor([[-0.0257, -1.4725, -1.2251],\n [-1.1479, -0.7005, -1.9757],\n [-1.3904,  0.3726, -1.1836],\n [-0.9688, -0.7153,  0.2159]])\n&gt;&gt;&gt; torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n [-2.4490, -1.5687,  1.9792],\n [-0.8304, -1.3037,  0.5650],\n [-1.2329,  1.9883,  1.0551]])\n&gt;&gt;&gt; torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n [-2.4490, -1.5687,  1.9792],\n [-0.8304, -1.3037,  0.5650],\n [-1.2329,  1.9883,  1.0551]])\n\n</code></pre> <pre><code>torch.diag(input, diagonal=0, out=None) \u2192 Tensor\n</code></pre> <ul> <li>\u5982\u679c<code>input</code>\u662f\u77e2\u91cf(1-D\u5f20\u91cf\uff09\uff0c\u5219\u8fd4\u56de2-D\u5e73\u65b9\u5f20\u91cf\uff0c\u5176\u4e2d<code>input</code>\u7684\u5143\u7d20\u4f5c\u4e3a\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c<code>input</code>\u662f\u77e9\u9635(2-D\u5f20\u91cf\uff09\uff0c\u5219\u8fd4\u56de\u5177\u6709<code>input</code>\u7684\u5bf9\u89d2\u5143\u7d20\u76841-D\u5f20\u91cf\u3002</li> </ul> <p>\u53c2\u6570 <code>diagonal</code> \u63a7\u5236\u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf\uff1a</p> <ul> <li>\u5982\u679c <code>diagonal</code> = 0\uff0c\u5219\u5b83\u662f\u4e3b\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c <code>diagonal</code> &gt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u3002</li> <li>\u5982\u679c <code>diagonal</code> \uff06lt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u9762\u3002</li> </ul> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5bf9\u89d2\u7ebf (int\uff0c \u53ef\u9009\uff09 - \u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>\u4e5f\u53ef\u4ee5\u770b\u770b</p> <p><code>torch.diagonal()</code> \u59cb\u7ec8\u8fd4\u56de\u5176\u8f93\u5165\u7684\u5bf9\u89d2\u7ebf\u3002</p> <p><code>torch.diagflat()</code> \u603b\u662f\u6784\u9020\u4e00\u4e2a\u7531\u8f93\u5165\u6307\u5b9a\u7684\u5bf9\u89d2\u5143\u7d20\u7684\u5f20\u91cf\u3002</p> <p>\u4f8b\u5b50\uff1a</p> <p>\u83b7\u53d6\u8f93\u5165\u5411\u91cf\u4e3a\u5bf9\u89d2\u7ebf\u7684\u65b9\u9635\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3)\n&gt;&gt;&gt; a\ntensor([ 0.5950,-0.0872, 2.3298])\n&gt;&gt;&gt; torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n [ 0.0000,-0.0872, 0.0000],\n [ 0.0000, 0.0000, 2.3298]])\n&gt;&gt;&gt; torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n [ 0.0000, 0.0000,-0.0872, 0.0000],\n [ 0.0000, 0.0000, 0.0000, 2.3298],\n [ 0.0000, 0.0000, 0.0000, 0.0000]])\n\n</code></pre> <p>\u83b7\u53d6\u7ed9\u5b9a\u77e9\u9635\u7684\u7b2ck\u4e2a\u5bf9\u89d2\u7ebf\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a\ntensor([[-0.4264, 0.0255,-0.1064],\n [ 0.8795,-0.2429, 0.1374],\n [ 0.1029,-0.6482,-1.6300]])\n&gt;&gt;&gt; torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n&gt;&gt;&gt; torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n\n</code></pre> <pre><code>torch.diag_embed(input, offset=0, dim1=-2, dim2=-1) \u2192 Tensor\n</code></pre> <p>\u521b\u5efa\u4e00\u4e2a\u5f20\u91cf\uff0c\u5176\u67d0\u4e9b2D\u5e73\u9762\u7684\u5bf9\u89d2\u7ebf(\u7531<code>dim1</code>\u548c<code>dim2</code>\u6307\u5b9a\uff09\u7531<code>input</code>\u586b\u5145\u3002\u4e3a\u4e86\u4fbf\u4e8e\u521b\u5efa\u6279\u91cf\u5bf9\u89d2\u77e9\u9635\uff0c\u9ed8\u8ba4\u9009\u62e9\u7531\u8fd4\u56de\u5f20\u91cf\u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\u5f62\u6210\u76842D\u5e73\u9762\u3002</p> <p>\u53c2\u6570<code>offset</code>\u63a7\u5236\u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf\uff1a</p> <ul> <li>\u5982\u679c<code>offset</code> = 0\uff0c\u5219\u5b83\u662f\u4e3b\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c<code>offset</code>&gt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u3002</li> <li>\u5982\u679c<code>offset</code>\uff06lt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u9762\u3002</li> </ul> <p>\u5c06\u8ba1\u7b97\u65b0\u77e9\u9635\u7684\u5927\u5c0f\u4ee5\u4f7f\u5f97\u6307\u5b9a\u7684\u5bf9\u89d2\u7ebf\u5177\u6709\u6700\u540e\u8f93\u5165\u7ef4\u5ea6\u7684\u5927\u5c0f\u3002\u6ce8\u610f\uff0c\u5bf9\u4e8e  \u4ee5\u5916\u7684<code>offset</code>\uff0c<code>dim1</code>\u548c<code>dim2</code>\u7684\u987a\u5e8f\u5f88\u91cd\u8981\u3002\u4ea4\u6362\u5b83\u4eec\u76f8\u5f53\u4e8e\u6539\u53d8<code>offset</code>\u7684\u7b26\u53f7\u3002</p> <p>\u5c06 <code>torch.diagonal()</code> \u5e94\u7528\u4e8e\u5177\u6709\u76f8\u540c\u53c2\u6570\u7684\u6b64\u51fd\u6570\u7684\u8f93\u51fa\uff0c\u5c06\u4ea7\u751f\u4e0e\u8f93\u5165\u76f8\u540c\u7684\u77e9\u9635\u3002\u4f46\u662f\uff0c <code>torch.diagonal()</code> \u5177\u6709\u4e0d\u540c\u7684\u9ed8\u8ba4\u5c3a\u5bf8\uff0c\u56e0\u6b64\u9700\u8981\u660e\u786e\u6307\u5b9a\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf\u3002\u5fc5\u987b\u81f3\u5c11\u662f\u4e00\u7ef4\u7684\u3002</li> <li>\u504f\u79fb (int\uff0c \u4efb\u9009\uff09 - \u5bf9\u89d2\u7ebf\u8003\u8651\u3002\u9ed8\u8ba4\u503c\uff1a0(\u4e3b\u5bf9\u89d2\u7ebf\uff09\u3002</li> <li>dim1  (int\uff0c \u4efb\u9009\uff09 - \u76f8\u5bf9\u4e8e\u5176\u91c7\u53d6\u5bf9\u89d2\u7ebf\u7684\u7b2c\u4e00\u7ef4\u5ea6\u3002\u9ed8\u8ba4\u503c\uff1a-2\u3002</li> <li>dim2  (int\uff0c \u4efb\u9009\uff09 - \u76f8\u5bf9\u4e8e\u5176\u91c7\u53d6\u5bf9\u89d2\u7ebf\u7684\u7b2c\u4e8c\u7ef4\u5ea6\u3002\u9ed8\u8ba4\u503c\uff1a-1\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(2, 3)\n&gt;&gt;&gt; torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n [ 0.0000, -0.2934,  0.0000],\n [ 0.0000,  0.0000, -2.1788]],\n\n [[ 0.5684,  0.0000,  0.0000],\n [ 0.0000, -1.0845,  0.0000],\n [ 0.0000,  0.0000, -1.3986]]])\n\n&gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n [[ 0.0000,  0.0000, -0.2934,  0.0000],\n [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n [[ 0.0000,  0.0000,  0.0000, -2.1788],\n [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n [[ 0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n\n</code></pre> <pre><code>torch.diagflat(input, diagonal=0) \u2192 Tensor\n</code></pre> <ul> <li>\u5982\u679c<code>input</code>\u662f\u77e2\u91cf(1-D\u5f20\u91cf\uff09\uff0c\u5219\u8fd4\u56de2-D\u5e73\u65b9\u5f20\u91cf\uff0c\u5176\u4e2d<code>input</code>\u7684\u5143\u7d20\u4f5c\u4e3a\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c<code>input</code>\u662f\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u7ef4\u5ea6\u7684\u5f20\u91cf\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\uff0c\u5176\u5bf9\u89d2\u7ebf\u5143\u7d20\u7b49\u4e8e\u4e00\u4e2a\u5c55\u5e73\u7684<code>input</code>\u3002</li> </ul> <p>The argument <code>offset</code> controls which diagonal to consider:</p> <ul> <li>\u5982\u679c<code>offset</code> = 0\uff0c\u5219\u5b83\u662f\u4e3b\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c<code>offset</code>&gt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u3002</li> <li>\u5982\u679c<code>offset</code>\uff06lt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u9762\u3002</li> </ul> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u504f\u79fb (int\uff0c \u4efb\u9009\uff09 - \u5bf9\u89d2\u7ebf\u8003\u8651\u3002\u9ed8\u8ba4\u503c\uff1a0(\u4e3b\u5bf9\u89d2\u7ebf\uff09\u3002</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3)\n&gt;&gt;&gt; a\ntensor([-0.2956, -0.9068,  0.1695])\n&gt;&gt;&gt; torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n [ 0.0000, -0.9068,  0.0000],\n [ 0.0000,  0.0000,  0.1695]])\n&gt;&gt;&gt; torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n [ 0.0000,  0.0000, -0.9068,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  0.1695],\n [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n&gt;&gt;&gt; a = torch.randn(2, 2)\n&gt;&gt;&gt; a\ntensor([[ 0.2094, -0.3018],\n [-0.1516,  1.9342]])\n&gt;&gt;&gt; torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n [ 0.0000, -0.3018,  0.0000,  0.0000],\n [ 0.0000,  0.0000, -0.1516,  0.0000],\n [ 0.0000,  0.0000,  0.0000,  1.9342]])\n\n</code></pre> <pre><code>torch.diagonal(input, offset=0, dim1=0, dim2=1) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u7684\u5c40\u90e8\u89c6\u56fe\uff0c\u5176\u5bf9\u89d2\u7ebf\u5143\u7d20\u76f8\u5bf9\u4e8e<code>dim1</code>\u548c<code>dim2</code>\u4f5c\u4e3a\u5f62\u72b6\u672b\u5c3e\u7684\u5c3a\u5bf8\u9644\u52a0\u3002</p> <p>The argument <code>offset</code> controls which diagonal to consider:</p> <ul> <li>\u5982\u679c<code>offset</code> = 0\uff0c\u5219\u5b83\u662f\u4e3b\u5bf9\u89d2\u7ebf\u3002</li> <li>\u5982\u679c<code>offset</code>&gt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u3002</li> <li>\u5982\u679c<code>offset</code>\uff06lt; 0\uff0c\u5b83\u5728\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u9762\u3002</li> </ul> <p>\u5c06 <code>torch.diag_embed()</code> \u5e94\u7528\u4e8e\u5177\u6709\u76f8\u540c\u53c2\u6570\u7684\u6b64\u51fd\u6570\u7684\u8f93\u51fa\uff0c\u5c06\u751f\u6210\u5e26\u6709\u8f93\u5165\u5bf9\u89d2\u7ebf\u6761\u76ee\u7684\u5bf9\u89d2\u77e9\u9635\u3002\u4f46\u662f\uff0c <code>torch.diag_embed()</code> \u5177\u6709\u4e0d\u540c\u7684\u9ed8\u8ba4\u5c3a\u5bf8\uff0c\u56e0\u6b64\u9700\u8981\u660e\u786e\u6307\u5b9a\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf\u3002\u5fc5\u987b\u81f3\u5c11\u662f\u4e8c\u7ef4\u7684\u3002</li> <li>\u504f\u79fb (int\uff0c \u4efb\u9009\uff09 - \u5bf9\u89d2\u7ebf\u8003\u8651\u3002\u9ed8\u8ba4\u503c\uff1a0(\u4e3b\u5bf9\u89d2\u7ebf\uff09\u3002</li> <li>dim1  (int\uff0c \u4efb\u9009\uff09 - \u76f8\u5bf9\u4e8e\u5176\u91c7\u53d6\u5bf9\u89d2\u7ebf\u7684\u7b2c\u4e00\u7ef4\u5ea6\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>dim2  (int\uff0c \u4efb\u9009\uff09 - \u76f8\u5bf9\u4e8e\u5176\u91c7\u53d6\u5bf9\u89d2\u7ebf\u7684\u7b2c\u4e8c\u7ef4\u5ea6\u3002\u9ed8\u8ba4\u503c\uff1a1\u3002</li> </ul> <p>Note</p> <p>\u8981\u91c7\u7528\u6279\u5bf9\u89d2\u7ebf\uff0c\u4f20\u5165dim1 = -2\uff0cdim2 = -1\u3002</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a\ntensor([[-1.0854,  1.1431, -0.1752],\n [ 0.8536, -0.0905,  0.0360],\n [ 0.6927, -0.3735, -0.4945]])\n\n&gt;&gt;&gt; torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n&gt;&gt;&gt; torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n&gt;&gt;&gt; x = torch.randn(2, 5, 4, 2)\n&gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n [[-1.7325, -0.3081,  0.6166,  0.2335],\n [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n\n</code></pre> <pre><code>torch.einsum(equation, *operands) \u2192 Tensor\n</code></pre> <p>\u8be5\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f7f\u7528\u7231\u56e0\u65af\u5766\u6c42\u548c\u7ea6\u5b9a\u6765\u8ba1\u7b97\u591a\u7ebf\u6027\u8868\u8fbe\u5f0f(\u5373\u4e58\u79ef\u548c\uff09\u7684\u65b9\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>\u7b49\u5f0f (string ) - \u8be5\u7b49\u5f0f\u6839\u636e\u4e0e\u64cd\u4f5c\u6570\u548c\u7ed3\u679c\u7684\u6bcf\u4e2a\u7ef4\u5ea6\u76f8\u5173\u8054\u7684\u5c0f\u5199\u5b57\u6bcd(\u7d22\u5f15\uff09\u7ed9\u51fa\u3002\u5de6\u4fa7\u5217\u51fa\u4e86\u64cd\u4f5c\u6570\u5c3a\u5bf8\uff0c\u4ee5\u9017\u53f7\u5206\u9694\u3002\u6bcf\u4e2a\u5f20\u91cf\u7ef4\u5ea6\u5e94\u8be5\u6709\u4e00\u4e2a\u7d22\u5f15\u5b57\u6bcd\u3002\u53f3\u4fa7\u8ddf\u5728<code>-&amp;gt;</code>\u4e4b\u540e\uff0c\u5e76\u7ed9\u51fa\u8f93\u51fa\u7684\u7d22\u5f15\u3002\u5982\u679c\u7701\u7565<code>-&amp;gt;</code>\u548c\u53f3\u4fa7\uff0c\u5219\u5b83\u9690\u5f0f\u5730\u5b9a\u4e49\u4e3a\u5728\u5de6\u4fa7\u6070\u597d\u51fa\u73b0\u4e00\u6b21\u7684\u6240\u6709\u7d22\u5f15\u7684\u6309\u5b57\u6bcd\u987a\u5e8f\u6392\u5e8f\u7684\u5217\u8868\u3002\u5728\u64cd\u4f5c\u6570\u8f93\u5165\u4e4b\u540e\uff0c\u5c06\u8f93\u51fa\u4e2d\u672a\u663e\u793a\u7684\u7d22\u5f15\u6c42\u548c\u3002\u5982\u679c\u7d22\u5f15\u5bf9\u540c\u4e00\u64cd\u4f5c\u6570\u591a\u6b21\u51fa\u73b0\uff0c\u5219\u91c7\u7528\u5bf9\u89d2\u7ebf\u3002\u7701\u7565\u53f7<code>\u2026</code>\u8868\u793a\u56fa\u5b9a\u6570\u91cf\u7684\u7ef4\u5ea6\u3002\u5982\u679c\u63a8\u65ad\u51fa\u53f3\u4fa7\uff0c\u5219\u7701\u7565\u53f7\u7ef4\u5ea6\u4f4d\u4e8e\u8f93\u51fa\u7684\u5f00\u5934\u3002</li> <li>\u64cd\u4f5c\u6570(\u5f20\u91cf\u5217\u8868\uff09 - \u8ba1\u7b97\u7231\u56e0\u65af\u5766\u548c\u7684\u64cd\u4f5c\u6570\u3002\u8bf7\u6ce8\u610f\uff0c\u64cd\u4f5c\u6570\u4f5c\u4e3a\u5217\u8868\u4f20\u9012\uff0c\u800c\u4e0d\u662f\u4f5c\u4e3a\u5355\u4e2a\u53c2\u6570\u4f20\u9012\u3002</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(5)\n&gt;&gt;&gt; y = torch.randn(4)\n&gt;&gt;&gt; torch.einsum('i,j-&gt;ij', x, y)  # outer product\ntensor([[-0.0570, -0.0286, -0.0231,  0.0197],\n [ 1.2616,  0.6335,  0.5113, -0.4351],\n [ 1.4452,  0.7257,  0.5857, -0.4984],\n [-0.4647, -0.2333, -0.1883,  0.1603],\n [-1.1130, -0.5588, -0.4510,  0.3838]])\n\n&gt;&gt;&gt; A = torch.randn(3,5,4)\n&gt;&gt;&gt; l = torch.randn(2,5)\n&gt;&gt;&gt; r = torch.randn(2,4)\n&gt;&gt;&gt; torch.einsum('bn,anm,bm-&gt;ba', l, A, r) # compare torch.nn.functional.bilinear\ntensor([[-0.3430, -5.2405,  0.4494],\n [ 0.3311,  5.5201, -3.0356]])\n\n&gt;&gt;&gt; As = torch.randn(3,2,5)\n&gt;&gt;&gt; Bs = torch.randn(3,5,4)\n&gt;&gt;&gt; torch.einsum('bij,bjk-&gt;bik', As, Bs) # batch matrix multiplication\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n [[ 4.2239,  0.3107, -0.5756, -0.2354],\n [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n [[ 2.8153,  1.8787, -4.3839, -1.2112],\n [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n&gt;&gt;&gt; A = torch.randn(3, 3)\n&gt;&gt;&gt; torch.einsum('ii-&gt;i', A) # diagonal\ntensor([-0.7825,  0.8291, -0.1936])\n\n&gt;&gt;&gt; A = torch.randn(4, 3, 3)\n&gt;&gt;&gt; torch.einsum('...ii-&gt;...i', A) # batch diagonal\ntensor([[-1.0864,  0.7292,  0.0569],\n [-0.9725, -1.0270,  0.6493],\n [ 0.5832, -1.1716, -1.5084],\n [ 0.4041, -1.1690,  0.8570]])\n\n&gt;&gt;&gt; A = torch.randn(2, 3, 4, 5)\n&gt;&gt;&gt; torch.einsum('...ij-&gt;...ji', A).shape # batch permute\ntorch.Size([2, 3, 5, 4])\n\n</code></pre> <pre><code>torch.flatten(input, start_dim=0, end_dim=-1) \u2192 Tensor\n</code></pre> <p>\u5728\u5f20\u91cf\u4e2d\u5c55\u5e73\u8fde\u7eed\u7684\u4e00\u7cfb\u5217\u53d8\u6697\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>start_dim  (int\uff09 - \u7b2c\u4e00\u4e2a\u6697\u6de1\u53d8\u5e73</li> <li>end_dim  (int\uff09 - \u6700\u540e\u7684\u6697\u6de1\u53d8\u5e73</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; t = torch.tensor([[[1, 2],\n [3, 4]],\n [[5, 6],\n [7, 8]]])\n&gt;&gt;&gt; torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n [5, 6, 7, 8]])\n\n</code></pre> <pre><code>torch.flip(input, dims) \u2192 Tensor\n</code></pre> <p>\u5728dims\u4e2d\u6cbf\u7ed9\u5b9a\u8f74\u53cd\u8f6cn-D\u5f20\u91cf\u7684\u987a\u5e8f\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u6697\u6de1(\u4e00\u4e2a\u5217\u8868 \u6216 \u5143\u7ec4 - \u8f74\u8981\u7ffb\u8f6c</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2)\n&gt;&gt;&gt; x\ntensor([[[ 0,  1],\n [ 2,  3]],\n\n [[ 4,  5],\n [ 6,  7]]])\n&gt;&gt;&gt; torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n [ 4,  5]],\n\n [[ 2,  3],\n [ 0,  1]]])\n\n</code></pre> <pre><code>torch.histc(input, bins=100, min=0, max=0, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u5f20\u91cf\u7684\u76f4\u65b9\u56fe\u3002</p> <p>\u5143\u7d20\u5728 <code>min</code> \u548c <code>max</code> \u4e4b\u95f4\u5206\u6210\u76f8\u7b49\u7684\u5bbd\u5ea6\u533a\u95f4\u3002\u5982\u679c <code>min</code> \u548c <code>max</code> \u90fd\u4e3a\u96f6\uff0c\u5219\u4f7f\u7528\u6570\u636e\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u7bb1 (int\uff09 - \u76f4\u65b9\u56fe\u7bb1\u6570</li> <li>min  (int\uff09 - \u8303\u56f4\u7684\u4e0b\u9650(\u542b\uff09</li> <li>max  (int\uff09 - \u8303\u56f4\u7684\u4e0a\u9650(\u542b\uff09</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> Returns: \u76f4\u65b9\u56fe\u8868\u793a\u4e3a\u5f20\u91cf Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])\n\n</code></pre> <pre><code>torch.meshgrid(*tensors, **kwargs)\n</code></pre> <p>\u53d6  \u5f20\u91cf\uff0c\u6bcf\u4e2a\u5f20\u91cf\u53ef\u4ee5\u662f\u6807\u91cf\u62161\u7ef4\u5411\u91cf\uff0c\u5e76\u521b\u5efa  N\u7ef4\u7f51\u683c\uff0c\u5176\u4e2d\uff1amath\uff1a[<code>](#id2)i</code>\u901a\u8fc7\u6269\u5c55\uff1amath\uff1a[<code>](#id4)i</code> th\u8f93\u5165\u5b9a\u4e49\u7531\u5176\u4ed6\u8f93\u5165\u5b9a\u4e49\u7684\u7ef4\u5ea6\u6765\u5b9a\u4e49\u7f51\u683c\u3002</p> <p><code>py Args:</code></p> <p>\u5f20\u91cf(Tensor\u5217\u8868\uff09\uff1a\u6807\u91cf\u5217\u8868\u62161\u7ef4\u5f20\u91cf\u3002\u6807\u91cf\u5c06\u88ab\u81ea\u52a8\u89c6\u4e3a\u5927\u5c0f  \u7684\u5f20\u91cf</p> <p><code>py Returns:</code></p> <p>seq(\u5f20\u91cf\u5e8f\u5217\uff09\uff1a\u5982\u679c\u8f93\u5165\u7684  \u5f20\u91cf\u5927\u5c0f\u4e3a  \uff0c\u90a3\u4e48\u8f93\u51fa\u4e5f\u4f1a\u6709  \u5f20\u91cf\uff0c\u5176\u4e2d\u6240\u6709\u5f20\u91cf\u5747\u4e3a  \u3002</p> <p>Example:</p> <p>```py &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; y = torch.tensor([4, 5, 6]) &gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y) &gt;&gt;&gt; grid_x tensor([[1, 1, 1],  [2, 2, 2],  [3, 3, 3]]) &gt;&gt;&gt; grid_y tensor([[4, 5, 6],  [4, 5, 6],  [4, 5, 6]])</p> <p>```</p> <pre><code>torch.renorm(input, p, dim, maxnorm, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5f20\u91cf\uff0c\u5176\u4e2d\u6cbf\u7740\u7ef4\u5ea6<code>dim</code>\u7684<code>input</code>\u7684\u6bcf\u4e2a\u5b50\u5f20\u91cf\u88ab\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u5b50\u5f20\u91cf\u7684<code>p</code> - \u8303\u6570\u4f4e\u4e8e\u503c<code>maxnorm</code></p> <p>Note</p> <p>\u5982\u679c\u884c\u7684\u8303\u6570\u4f4e\u4e8e<code>maxnorm</code>\uff0c\u5219\u8be5\u884c\u4e0d\u53d8</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>p  (float\uff09 - \u89c4\u8303\u8ba1\u7b97\u7684\u52a8\u529b</li> <li>dim  (int\uff09 - \u5207\u7247\u4ee5\u83b7\u5f97\u5b50\u5f20\u91cf\u7684\u7ef4\u6570</li> <li>maxnorm  (float\uff09 - \u4fdd\u6301\u6bcf\u4e2a\u5b50\u5f20\u91cf\u7684\u6700\u5927\u8303\u6570</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.ones(3, 3)\n&gt;&gt;&gt; x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n&gt;&gt;&gt; x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n&gt;&gt;&gt; x\ntensor([[ 1.,  1.,  1.],\n [ 2.,  2.,  2.],\n [ 3.,  3.,  3.]])\n&gt;&gt;&gt; torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n [ 1.6667,  1.6667,  1.6667],\n [ 1.6667,  1.6667,  1.6667]])\n\n</code></pre> <pre><code>torch.tensordot(a, b, dims=2)\n</code></pre> <p>\u8fd4\u56de\u591a\u7ef4\u5ea6\u4e0aa\u548cb\u7684\u6536\u7f29\u3002</p> <p><code>tensordot</code> \u5b9e\u73b0\u4e86\u77e9\u9635\u4e58\u79ef\u7684\u63a8\u5e7f\u3002</p> <p>Parameters:</p> <ul> <li>a  (tensor) - \u5de6\u5f20\u91cf\u6536\u7f29</li> <li>b  (tensor) - \u53f3\u5f20\u91cf\u6536\u7f29</li> <li>\u6697\u6de1 (int\u6216 \u5143\u7ec4\u7684\u4e24\u4e2apython\u5217\u8868\uff1a\u6574\u6570\uff09 - \u8981\u6536\u7f29\u7684\u7ef4\u6570\u6216<code>a</code>\u548c<code>b</code>\u7684\u660e\u786e\u7ef4\u5ea6\u5217\u8868</li> </ul> <p>\u5f53\u7528\u6574\u6570\u53c2\u6570<code>dims</code> =  \u8c03\u7528\u65f6\uff0c<code>a</code>\u548c<code>b</code>\u7684\u7ef4\u6570\u662f  \u548c  \uff0c\u5b83\u5206\u522b\u8ba1\u7b97</p> <p></p> <p>\u5f53\u4f7f\u7528\u5217\u8868\u5f62\u5f0f\u7684<code>dims</code>\u8c03\u7528\u65f6\uff0c\u5c06\u6536\u7f29\u7ed9\u5b9a\u7684\u7ef4\u5ea6\u6765\u4ee3\u66ff<code>a</code>\u7684\u6700\u540e  \u548c\u7684\u7b2c\u4e00\u4e2a  ]  \u3002\u8fd9\u4e9b\u5c3a\u5bf8\u7684\u5c3a\u5bf8\u5fc5\u987b\u5339\u914d\uff0c\u4f46 <code>tensordot</code> \u5c06\u5904\u7406\u5e7f\u64ad\u5c3a\u5bf8\u3002</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5)\n&gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2)\n&gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n [4532., 4874.],\n [4664., 5018.],\n [4796., 5162.],\n [4928., 5306.]])\n\n&gt;&gt;&gt; a = torch.randn(3, 4, 5, device='cuda')\n&gt;&gt;&gt; b = torch.randn(4, 5, 6, device='cuda')\n&gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n</code></pre> <pre><code>torch.trace(input) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u8f93\u51652-D\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u5143\u7d20\u7684\u603b\u548c\u3002</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3)\n&gt;&gt;&gt; x\ntensor([[ 1.,  2.,  3.],\n [ 4.,  5.,  6.],\n [ 7.,  8.,  9.]])\n&gt;&gt;&gt; torch.trace(x)\ntensor(15.)\n\n</code></pre> <pre><code>torch.tril(input, diagonal=0, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u77e9\u9635\u7684\u4e0b\u4e09\u89d2\u90e8\u5206(2-D\u5f20\u91cf\uff09<code>input</code>\uff0c\u7ed3\u679c\u5f20\u91cf<code>out</code>\u7684\u5176\u4ed6\u5143\u7d20\u8bbe\u7f6e\u4e3a0\u3002</p> <p>\u77e9\u9635\u7684\u4e0b\u4e09\u89d2\u5f62\u90e8\u5206\u88ab\u5b9a\u4e49\u4e3a\u5bf9\u89d2\u7ebf\u4e0a\u548c\u4e0b\u65b9\u7684\u5143\u7d20\u3002</p> <p>\u53c2\u6570 <code>diagonal</code> \u63a7\u5236\u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf\u3002\u5982\u679c <code>diagonal</code> = 0\uff0c\u5219\u4fdd\u7559\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u548c\u4e0b\u65b9\u7684\u6240\u6709\u5143\u7d20\u3002\u6b63\u503c\u5305\u62ec\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u7684\u5bf9\u89d2\u7ebf\u6570\u91cf\uff0c\u540c\u6837\u8d1f\u503c\u4e5f\u4e0d\u5305\u62ec\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u65b9\u7684\u5bf9\u89d2\u7ebf\u6570\u91cf\u3002\u4e3b\u5bf9\u89d2\u7ebf\u662f  \u7684\u6307\u6570  \u7684\u96c6\u5408\uff0c\u5176\u4e2d  \u662f\u57fa\u8d28\u7684\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5bf9\u89d2\u7ebf (int\uff0c \u53ef\u9009\uff09 - \u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a\ntensor([[-1.0813, -0.8619,  0.7105],\n [ 0.0935,  0.1380,  2.2112],\n [-0.3409, -0.9828,  0.0289]])\n&gt;&gt;&gt; torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n [ 0.0935,  0.1380,  0.0000],\n [-0.3409, -0.9828,  0.0289]])\n\n&gt;&gt;&gt; b = torch.randn(4, 6)\n&gt;&gt;&gt; b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n&gt;&gt;&gt; torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n&gt;&gt;&gt; torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n\n</code></pre> <pre><code>torch.triu(input, diagonal=0, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u77e9\u9635\u7684\u4e0a\u4e09\u89d2\u90e8\u5206(2-D\u5f20\u91cf\uff09<code>input</code>\uff0c\u7ed3\u679c\u5f20\u91cf<code>out</code>\u7684\u5176\u4ed6\u5143\u7d20\u8bbe\u7f6e\u4e3a0\u3002</p> <p>\u77e9\u9635\u7684\u4e0a\u4e09\u89d2\u5f62\u90e8\u5206\u88ab\u5b9a\u4e49\u4e3a\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u548c\u4e0a\u65b9\u7684\u5143\u7d20\u3002</p> <p>\u53c2\u6570 <code>diagonal</code> \u63a7\u5236\u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf\u3002\u5982\u679c <code>diagonal</code> = 0\uff0c\u5219\u4fdd\u7559\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u548c\u4e0b\u65b9\u7684\u6240\u6709\u5143\u7d20\u3002\u6b63\u503c\u6392\u9664\u4e86\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u65b9\u7684\u5bf9\u89d2\u7ebf\u6570\u91cf\uff0c\u540c\u6837\u8d1f\u503c\u4e5f\u5305\u62ec\u4e3b\u5bf9\u89d2\u7ebf\u4e0b\u65b9\u7684\u5bf9\u89d2\u7ebf\u6570\u91cf\u3002\u4e3b\u5bf9\u89d2\u7ebf\u662f  \u7684\u6307\u6570  \u7684\u96c6\u5408\uff0c\u5176\u4e2d  \u662f\u57fa\u8d28\u7684\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5bf9\u89d2\u7ebf (int\uff0c \u53ef\u9009\uff09 - \u8981\u8003\u8651\u7684\u5bf9\u89d2\u7ebf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; a\ntensor([[ 0.2309,  0.5207,  2.0049],\n [ 0.2072, -1.0680,  0.6602],\n [ 0.3480, -0.5211, -0.4573]])\n&gt;&gt;&gt; torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n [ 0.0000, -1.0680,  0.6602],\n [ 0.0000,  0.0000, -0.4573]])\n&gt;&gt;&gt; torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n [ 0.0000,  0.0000,  0.6602],\n [ 0.0000,  0.0000,  0.0000]])\n&gt;&gt;&gt; torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n [ 0.2072, -1.0680,  0.6602],\n [ 0.0000, -0.5211, -0.4573]])\n\n&gt;&gt;&gt; b = torch.randn(4, 6)\n&gt;&gt;&gt; b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n&gt;&gt;&gt; torch.tril(b, diagonal=1)\ntensor([[ 0.5876, -0.0794,  0.0000,  0.0000,  0.0000,  0.0000],\n [-0.2447,  0.9556, -1.2919,  0.0000,  0.0000,  0.0000],\n [ 0.4333,  0.3146,  0.6576, -1.0432,  0.0000,  0.0000],\n [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.0000]])\n&gt;&gt;&gt; torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [-0.2447,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n [ 0.4333,  0.3146,  0.0000,  0.0000,  0.0000,  0.0000],\n [-0.9888,  1.0679, -1.3337,  0.0000,  0.0000,  0.0000]])\n\n</code></pre>"},{"location":"1.0/torch_math_operations_pointwise_ops/","title":"Pointwise Ops","text":""},{"location":"1.0/torch_math_operations_pointwise_ops/#_1","title":"\u9010\u70b9\u884c\u52a8","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.abs(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u7ed9\u5b9a<code>input</code>\u5f20\u91cf\u7684\u9010\u5143\u7d20\u7edd\u5bf9\u503c\u3002</p> <p></p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n\n</code></pre> <pre><code>torch.acos(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input</code>\u5143\u7d20\u7684\u53cd\u4f59\u5f26\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n&gt;&gt;&gt; torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n\n</code></pre> <pre><code>torch.add()\n</code></pre> <pre><code>torch.add(input, value, out=None)\n</code></pre> <p>\u5c06\u6807\u91cf<code>value</code>\u6dfb\u52a0\u5230\u8f93\u5165<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u5e76\u8fd4\u56de\u65b0\u7684\u7ed3\u679c\u5f20\u91cf\u3002</p> <p></p> <p>\u5982\u679c<code>input</code>\u7684\u7c7b\u578b\u4e3aFloatTensor\u6216DoubleTensor\uff0c\u5219<code>value</code>\u5fc5\u987b\u662f\u5b9e\u6570\uff0c\u5426\u5219\u5e94\u4e3a\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u53f7\u7801\uff09 - \u8981\u6dfb\u52a0\u5230<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u6570\u5b57</li> </ul> \u5173\u952e\u5b57\u53c2\u6570\uff1a \uff1f <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n&gt;&gt;&gt; torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n\n</code></pre> <pre><code>torch.add(input, value=1, other, out=None)\n</code></pre> <p>\u5f20\u91cf<code>other</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u6807\u91cf<code>value</code>\u5e76\u6dfb\u52a0\u5230\u5f20\u91cf<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u3002\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\u3002</p> <p><code>input</code>\u548c<code>other</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p></p> <p>\u5982\u679c<code>other</code>\u7684\u7c7b\u578b\u4e3aFloatTensor\u6216DoubleTensor\uff0c\u5219<code>value</code>\u5fc5\u987b\u662f\u5b9e\u6570\uff0c\u5426\u5219\u5e94\u4e3a\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u6570\u5b57\uff09 - <code>other</code>\u7684\u6807\u91cf\u4e58\u6570</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf</li> </ul> Keyword Arguments: ? <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n&gt;&gt;&gt; b = torch.randn(4, 1)\n&gt;&gt;&gt; b\ntensor([[ 0.3743],\n [-1.7724],\n [-0.5811],\n [-0.8017]])\n&gt;&gt;&gt; torch.add(a, 10, b)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n [-18.6971, -18.0736, -17.0994, -17.3216],\n [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n\n</code></pre> <pre><code>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor\n</code></pre> <p>\u901a\u8fc7<code>tensor2</code>\u6267\u884c<code>tensor1</code>\u7684\u9010\u5143\u7d20\u5212\u5206\uff0c\u5c06\u7ed3\u679c\u4e58\u4ee5\u6807\u91cf<code>value</code>\u5e76\u5c06\u5176\u6dfb\u52a0\u5230 <code>tensor</code> \u3002</p> <p></p> <p><code>tensor</code> \uff0c<code>tensor1</code>\u548c<code>tensor2</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p>\u5bf9\u4e8e<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\u7c7b\u578b\u7684\u8f93\u5165\uff0c<code>value</code>\u5fc5\u987b\u662f\u5b9e\u6570\uff0c\u5426\u5219\u662f\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u5f20\u91cf (Tensor\uff09 - \u8981\u52a0\u7684\u5f20\u91cf</li> <li>\u503c(\u6570 \uff0c \u53ef\u9009\uff09 -  \u7684\u4e58\u6570</li> <li>tensor1  (Tensor\uff09 - \u5206\u5b50\u5f20\u91cf</li> <li>\u5f20\u91cf2  (tensor) - \u5206\u6bcd\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)\n&gt;&gt;&gt; t1 = torch.randn(3, 1)\n&gt;&gt;&gt; t2 = torch.randn(1, 3)\n&gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2)\ntensor([[-0.2312, -3.6496,  0.1312],\n [-1.0428,  3.4292, -0.1030],\n [-0.5369, -0.9829,  0.0430]])\n\n</code></pre> <pre><code>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) \u2192 Tensor\n</code></pre> <p>\u901a\u8fc7<code>tensor2</code>\u6267\u884c<code>tensor1</code>\u7684\u9010\u5143\u7d20\u4e58\u6cd5\uff0c\u5c06\u7ed3\u679c\u4e58\u4ee5\u6807\u91cf<code>value</code>\u5e76\u5c06\u5176\u6dfb\u52a0\u5230 <code>tensor</code> \u3002</p> <p></p> <p>The shapes of <code>tensor</code>, <code>tensor1</code>, and <code>tensor2</code> must be broadcastable.</p> <p>For inputs of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code> must be a real number, otherwise an integer.</p> <p>Parameters:</p> <ul> <li>\u5f20\u91cf (Tensor\uff09 - \u8981\u52a0\u7684\u5f20\u91cf</li> <li>\u503c(\u6570 \uff0c \u53ef\u9009\uff09 -  \u7684\u4e58\u6570</li> <li>tensor1  (Tensor\uff09 - \u8981\u500d\u589e\u7684\u5f20\u91cf</li> <li>tensor2  (Tensor\uff09 - \u8981\u500d\u589e\u7684\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; t = torch.randn(1, 3)\n&gt;&gt;&gt; t1 = torch.randn(3, 1)\n&gt;&gt;&gt; t2 = torch.randn(1, 3)\n&gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2)\ntensor([[-0.8635, -0.6391,  1.6174],\n [-0.7617, -0.5879,  1.7388],\n [-0.8353, -0.6249,  1.6511]])\n\n</code></pre> <pre><code>torch.asin(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u53cd\u6b63\u5f26\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n&gt;&gt;&gt; torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n\n</code></pre> <pre><code>torch.atan(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input</code>\u5143\u7d20\u53cd\u6b63\u5207\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n&gt;&gt;&gt; torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n\n</code></pre> <pre><code>torch.atan2(input1, input2, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input1</code>\u548c<code>input2</code>\u5143\u7d20\u7684\u53cd\u6b63\u5207\u7684\u65b0\u5f20\u91cf\u3002</p> <p><code>input1</code>\u548c<code>input2</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p>Parameters:</p> <ul> <li>input1  (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>input2  (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n&gt;&gt;&gt; torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n\n</code></pre> <pre><code>torch.ceil(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684ceil\u7684\u65b0\u5f20\u91cf\uff0c\u8be5\u5143\u7d20\u662f\u5927\u4e8e\u6216\u7b49\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u6574\u6570\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n&gt;&gt;&gt; torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n\n</code></pre> <pre><code>torch.clamp(input, min, max, out=None) \u2192 Tensor\n</code></pre> <p>\u5c06<code>input</code>\u4e2d\u7684\u6240\u6709\u5143\u7d20\u94b3\u4f4d\u5230<code>[</code> <code>min</code> \uff0c <code>max</code> <code>]</code>\u8303\u56f4\u5185\u5e76\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\uff1a</p> <p></p> <p>\u5982\u679c<code>input</code>\u7684\u7c7b\u578b\u4e3a<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\uff0c\u5219 <code>min</code> \u548c <code>max</code> \u5fc5\u987b\u4e3a\u5b9e\u6570\uff0c\u5426\u5219\u5b83\u4eec\u5e94\u4e3a\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>min  (Number ) - \u8981\u88ab\u94b3\u4f4d\u7684\u8303\u56f4\u7684\u4e0b\u9650</li> <li>max  (Number ) - \u8981\u94b3\u4f4d\u7684\u8303\u56f4\u7684\u4e0a\u9650</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n&gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n</code></pre> <pre><code>torch.clamp(input, *, min, out=None) \u2192 Tensor\n</code></pre> <p>\u5c06<code>input</code>\u4e2d\u7684\u6240\u6709\u5143\u7d20\u94b3\u4f4d\u4e3a\u5927\u4e8e\u6216\u7b49\u4e8e <code>min</code> \u3002</p> <p>\u5982\u679c<code>input</code>\u7684\u7c7b\u578b\u4e3a<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\uff0c\u5219<code>value</code>\u5e94\u4e3a\u5b9e\u6570\uff0c\u5426\u5219\u5e94\u4e3a\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u6570\u5b57\uff09 - \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5c0f\u503c</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.0299, -2.3184,  2.1593, -0.8883])\n&gt;&gt;&gt; torch.clamp(a, min=0.5)\ntensor([ 0.5000,  0.5000,  2.1593,  0.5000])\n\n</code></pre> <pre><code>torch.clamp(input, *, max, out=None) \u2192 Tensor\n</code></pre> <p>\u5c06<code>input</code>\u4e2d\u7684\u6240\u6709\u5143\u7d20\u94b3\u4f4d\u4e3a\u5c0f\u4e8e\u6216\u7b49\u4e8e <code>max</code> \u3002</p> <p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code> should be a real number, otherwise it should be an integer.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u6570\u5b57\uff09 - \u8f93\u51fa\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u6700\u5927\u503c</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.7753, -0.4702, -0.4599,  1.1899])\n&gt;&gt;&gt; torch.clamp(a, max=0.5)\ntensor([ 0.5000, -0.4702, -0.4599,  0.5000])\n\n</code></pre> <pre><code>torch.cos(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u4f59\u5f26\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n&gt;&gt;&gt; torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n\n</code></pre> <pre><code>torch.cosh(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u53cc\u66f2\u4f59\u5f26\u503c\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n&gt;&gt;&gt; torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n\n</code></pre> <pre><code>torch.div()\n</code></pre> <pre><code>torch.div(input, value, out=None) \u2192 Tensor\n</code></pre> <p>\u5c06\u8f93\u5165<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u6807\u91cf<code>value</code>\u5206\u5f00\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf\u3002</p> <p></p> <p>\u5982\u679c<code>input</code>\u7684\u7c7b\u578b\u4e3a<code>FloatTensor</code>\u6216<code>DoubleTensor</code>\uff0c<code>value</code>\u5e94\u4e3a\u5b9e\u6570\uff0c\u5426\u5219\u5e94\u4e3a\u6574\u6570</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u53f7\u7801\uff09 - \u8981\u5206\u914d\u7ed9<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u6570\u5b57</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(5)\n&gt;&gt;&gt; a\ntensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n&gt;&gt;&gt; torch.div(a, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])\n\n</code></pre> <pre><code>torch.div(input, other, out=None) \u2192 Tensor\n</code></pre> <p>\u5f20\u91cf<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u9664\u4ee5\u5f20\u91cf<code>other</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u3002\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\u3002 <code>input</code>\u548c<code>other</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u5206\u5b50\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u5206\u6bcd\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n [ 0.1815, -1.0111,  0.9805, -1.5923],\n [ 0.1062,  1.4581,  0.7759, -1.2344],\n [-0.1830, -0.0313,  1.1908, -1.4757]])\n&gt;&gt;&gt; b = torch.randn(4)\n&gt;&gt;&gt; b\ntensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n&gt;&gt;&gt; torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2637],\n [ 0.2260, -3.4507, -1.2086,  6.8988],\n [ 0.1322,  4.9764, -0.9564,  5.3480],\n [-0.2278, -0.1068, -1.4678,  6.3936]])\n\n</code></pre> <pre><code>torch.digamma(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97<code>input</code>\u4e0a\u4f3d\u739b\u51fd\u6570\u7684\u5bf9\u6570\u5bfc\u6570\u3002</p> <p></p> \u53c2\u6570\uff1a \u8f93\u5165 (Tensor\uff09 - \u8ba1\u7b97digamma\u51fd\u6570\u7684\u5f20\u91cf <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.tensor([1, 0.5])\n&gt;&gt;&gt; torch.digamma(a)\ntensor([-0.5772, -1.9635])\n\n</code></pre> <pre><code>torch.erf(tensor, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u6bcf\u4e2a\u5143\u7d20\u7684\u9519\u8bef\u51fd\u6570\u3002\u9519\u8bef\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <p></p> <p>Parameters:</p> <ul> <li>\u5f20\u91cf (tensor) - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n\n</code></pre> <pre><code>torch.erfc(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u4e92\u8865\u8bef\u5dee\u51fd\u6570\u3002\u4e92\u8865\u8bef\u5dee\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <p></p> <p>Parameters:</p> <ul> <li>\u5f20\u91cf (tensor) - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n\n</code></pre> <pre><code>torch.erfinv(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u53cd\u5411\u8bef\u5dee\u51fd\u6570\u3002\u53cd\u5411\u8bef\u5dee\u51fd\u6570\u5728  \u8303\u56f4\u5185\u5b9a\u4e49\u4e3a\uff1a</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n\n</code></pre> <pre><code>torch.exp(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709\u8f93\u5165\u5f20\u91cf<code>input</code>\u5143\u7d20\u7684\u6307\u6570\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n\n</code></pre> <pre><code>torch.expm1(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5143\u7d20\u7684\u6307\u6570\u51cf\u53bb<code>input</code>\u76841\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n\n</code></pre> <pre><code>torch.floor(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u5305\u542b<code>input</code>\u5143\u7d20\u7684\u6700\u4f4e\u503c\uff0c\u8fd9\u662f\u6bcf\u4e2a\u5143\u7d20\u5c0f\u4e8e\u6216\u7b49\u4e8e\u7684\u6700\u5927\u6574\u6570\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n&gt;&gt;&gt; torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n\n</code></pre> <pre><code>torch.fmod(input, divisor, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97\u9664\u6cd5\u7684\u5143\u7d20\u4f59\u6570\u3002</p> <p>\u88ab\u9664\u6570\u548c\u9664\u6570\u53ef\u4ee5\u5305\u542b\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3002\u4f59\u6570\u4e0e\u88ab\u9664\u6570<code>input</code>\u5177\u6709\u76f8\u540c\u7684\u7b26\u53f7\u3002</p> <p>\u5f53<code>divisor</code>\u662f\u5f20\u91cf\u65f6\uff0c<code>input</code>\u548c<code>divisor</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u80a1\u606f</li> <li>\u9664\u6570 (tensor \u6216 \u6f02\u6d6e) - \u9664\u6570\uff0c\u53ef\u80fd\u662f\u4e0e\u88ab\u9664\u6570\u76f8\u540c\u5f62\u72b6\u7684\u6570\u5b57\u6216\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n&gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n\n</code></pre> <pre><code>torch.frac(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8ba1\u7b97<code>input</code>\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u5c0f\u6570\u90e8\u5206\u3002</p> <p></p> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n\n</code></pre> <pre><code>torch.lerp(start, end, weight, out=None)\n</code></pre> <p>\u662f\u5426\u57fa\u4e8e\u6807\u91cf<code>weight</code>\u5bf9\u4e24\u4e2a\u5f20\u91cf<code>start</code>\u548c<code>end</code>\u8fdb\u884c\u7ebf\u6027\u63d2\u503c\uff0c\u5e76\u8fd4\u56de\u5f97\u5230\u7684<code>out</code>\u5f20\u91cf\u3002</p> <p></p> <p><code>start</code>\u548c<code>end</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p>Parameters:</p> <ul> <li>\u542f\u52a8 (Tensor\uff09 - \u5f20\u91cf\u4e0e\u8d77\u70b9</li> <li>\u7ed3\u675f (Tensor\uff09 - \u5e26\u6709\u7ec8\u70b9\u7684\u5f20\u91cf</li> <li>\u4f53\u91cd (\u6f02\u6d6e) - \u63d2\u503c\u516c\u5f0f\u7684\u6743\u91cd</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; start = torch.arange(1., 5.)\n&gt;&gt;&gt; end = torch.empty(4).fill_(10)\n&gt;&gt;&gt; start\ntensor([ 1.,  2.,  3.,  4.])\n&gt;&gt;&gt; end\ntensor([ 10.,  10.,  10.,  10.])\n&gt;&gt;&gt; torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n\n</code></pre> <pre><code>torch.log(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u81ea\u7136\u5bf9\u6570\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(5)\n&gt;&gt;&gt; a\ntensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])\n&gt;&gt;&gt; torch.log(a)\ntensor([ nan,  nan,  nan,  nan,  nan])\n\n</code></pre> <pre><code>torch.log10(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5bf9\u6570\u4e3a<code>input</code>\u5143\u7d20\u7684\u57fa\u657010\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.rand(5)\n&gt;&gt;&gt; a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n&gt;&gt;&gt; torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\n\n</code></pre> <pre><code>torch.log1p(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u81ea\u7136\u5bf9\u6570\u4e3a(1 + <code>input</code>\uff09\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>\u6ce8\u610f</p> <p>\u5bf9\u4e8e<code>input</code>\u7684\u5c0f\u503c\uff0c\u6b64\u51fd\u6570\u6bd4 <code>torch.log()</code> \u66f4\u51c6\u786e</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(5)\n&gt;&gt;&gt; a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n&gt;&gt;&gt; torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n\n</code></pre> <pre><code>torch.log2(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5bf9\u6570\u4e3a<code>input</code>\u5143\u7d20\u7684\u57fa\u65702\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.rand(5)\n&gt;&gt;&gt; a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n&gt;&gt;&gt; torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\n\n</code></pre> <pre><code>torch.mul()\n</code></pre> <pre><code>torch.mul(input, value, out=None)\n</code></pre> <p>\u5c06\u8f93\u5165<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e0e\u6807\u91cf<code>value</code>\u76f8\u4e58\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u7ed3\u679c\u5f20\u91cf\u3002</p> <p></p> <p>If <code>input</code> is of type <code>FloatTensor</code> or <code>DoubleTensor</code>, <code>value</code> should be a real number, otherwise it should be an integer</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u503c(\u53f7\u7801\uff09 - \u8981\u4e0e<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u76f8\u4e58\u7684\u6570\u5b57</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(3)\n&gt;&gt;&gt; a\ntensor([ 0.2015, -0.4255,  2.6087])\n&gt;&gt;&gt; torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n\n</code></pre> <pre><code>torch.mul(input, other, out=None)\n</code></pre> <p>\u5f20\u91cf<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u4e58\u4ee5\u5f20\u91cf<code>other</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u3002\u8fd4\u56de\u7ed3\u679c\u5f20\u91cf\u3002</p> <p>The shapes of <code>input</code> and <code>other</code> must be broadcastable.</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u7b2c\u4e00\u4e2a\u88ab\u4e58\u6570\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u7b2c\u4e8c\u4e2a\u88ab\u4e58\u6570\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 1)\n&gt;&gt;&gt; a\ntensor([[ 1.1207],\n [-0.3137],\n [ 0.0700],\n [ 0.8378]])\n&gt;&gt;&gt; b = torch.randn(1, 4)\n&gt;&gt;&gt; b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n&gt;&gt;&gt; torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n [-0.1614, -0.0382,  0.1645, -0.7021],\n [ 0.0360,  0.0085, -0.0367,  0.1567],\n [ 0.4312,  0.1019, -0.4394,  1.8753]])\n\n</code></pre> <pre><code>torch.mvlgamma(input, p) \u2192 Tensor\n</code></pre> <p>\u7528\u7ef4\u5ea6  \u5143\u7d20\u8ba1\u7b97\u591a\u53d8\u91cflog-gamma\u51fd\u6570\uff0c\u7531\u4e0b\u5f0f\u7ed9\u51fa\uff1a</p> <p></p> <p>\u5176\u4e2d  \u548c  \u662fGamma\u51fd\u6570\u3002</p> <p>\u5982\u679c\u4efb\u4f55\u5143\u7d20\u5c0f\u4e8e\u6216\u7b49\u4e8e  \uff0c\u5219\u629b\u51fa\u9519\u8bef\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8ba1\u7b97\u591a\u53d8\u91cflog-gamma\u51fd\u6570\u7684\u5f20\u91cf</li> <li>p  (int\uff09 - \u7ef4\u6570</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2)\n&gt;&gt;&gt; a\ntensor([[1.6835, 1.8474, 1.1929],\n [1.0475, 1.7162, 1.4180]])\n&gt;&gt;&gt; torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n [1.0311, 0.3901, 0.5049]])\n\n</code></pre> <pre><code>torch.neg(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5143\u7d20\u4e3a<code>input</code>\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(5)\n&gt;&gt;&gt; a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n&gt;&gt;&gt; torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\n\n</code></pre> <pre><code>torch.pow()\n</code></pre> <pre><code>torch.pow(input, exponent, out=None) \u2192 Tensor\n</code></pre> <p>\u4f7f\u7528<code>exponent</code>\u83b7\u53d6<code>input</code>\u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684\u529f\u6548\uff0c\u5e76\u8fd4\u56de\u5e26\u6709\u7ed3\u679c\u7684\u5f20\u91cf\u3002</p> <p><code>exponent</code>\u53ef\u4ee5\u662f\u5355\u4e2a<code>float</code>\u7f16\u53f7\uff0c\u4e5f\u53ef\u4ee5\u662f<code>Tensor</code>\uff0c\u5176\u5143\u7d20\u6570\u4e0e<code>input</code>\u76f8\u540c\u3002</p> <p>\u5f53<code>exponent</code>\u662f\u6807\u91cf\u503c\u65f6\uff0c\u5e94\u7528\u7684\u64cd\u4f5c\u662f\uff1a</p> <p></p> <p>\u5f53<code>exponent</code>\u662f\u5f20\u91cf\u65f6\uff0c\u5e94\u7528\u7684\u64cd\u4f5c\u662f\uff1a</p> <p></p> <p>\u5f53<code>exponent</code>\u662f\u5f20\u91cf\u65f6\uff0c<code>input</code>\u548c<code>exponent</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u5e7f\u64ad\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u6307\u6570 (float\u6216 tensor\uff09 - \u6307\u6570\u503c</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n&gt;&gt;&gt; torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n&gt;&gt;&gt; exp = torch.arange(1., 5.)\n\n&gt;&gt;&gt; a = torch.arange(1., 5.)\n&gt;&gt;&gt; a\ntensor([ 1.,  2.,  3.,  4.])\n&gt;&gt;&gt; exp\ntensor([ 1.,  2.,  3.,  4.])\n&gt;&gt;&gt; torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])\n\n</code></pre> <pre><code>torch.pow(base, input, out=None) \u2192 Tensor\n</code></pre> <p><code>base</code>\u662f\u6807\u91cf<code>float</code>\u503c\uff0c<code>input</code>\u662f\u5f20\u91cf\u3002\u8fd4\u56de\u7684\u5f20\u91cf<code>out</code>\u4e0e<code>input</code>\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6</p> <p>\u9002\u7528\u7684\u64cd\u4f5c\u662f\uff1a</p> <p></p> <p>Parameters:</p> <ul> <li>base  (float\uff09 - \u7535\u6e90\u64cd\u4f5c\u7684\u6807\u91cf\u57fa\u503c</li> <li>\u8f93\u5165 (Tensor\uff09 - \u6307\u6570\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; exp = torch.arange(1., 5.)\n&gt;&gt;&gt; base = 2\n&gt;&gt;&gt; torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])\n\n</code></pre> <pre><code>torch.reciprocal(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u5012\u6570\u7684\u65b0\u5f20\u91cf</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n&gt;&gt;&gt; torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\n\n</code></pre> <pre><code>torch.remainder(input, divisor, out=None) \u2192 Tensor\n</code></pre> <p>Computes the element-wise remainder of division.</p> <p>\u9664\u6570\u548c\u88ab\u9664\u6570\u53ef\u4ee5\u5305\u542b\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3002\u5176\u4f59\u90e8\u5206\u4e0e\u9664\u6570\u5177\u6709\u76f8\u540c\u7684\u7b26\u53f7\u3002</p> <p>When <code>divisor</code> is a tensor, the shapes of <code>input</code> and <code>divisor</code> must be broadcastable.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u80a1\u606f</li> <li>\u9664\u6570 (tensor \u6216 \u6f02\u6d6e) - \u53ef\u80fd\u662f\u4e00\u4e2a\u9664\u6570\u6570\u5b57\u6216\u4e0e\u88ab\u9664\u6570\u76f8\u540c\u5f62\u72b6\u7684\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n&gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u770b\u770b</p> <p><code>torch.fmod()</code> \uff0c\u5b83\u8ba1\u7b97\u4e0eC\u5e93\u51fd\u6570<code>fmod()</code>\u7b49\u6548\u7684\u9664\u6cd5\u7684\u5143\u7d20\u4f59\u6570\u3002</p> <pre><code>torch.round(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u56db\u820d\u4e94\u5165\u5230\u6700\u63a5\u8fd1\u7684\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n&gt;&gt;&gt; torch.round(a)\ntensor([ 1.,  1.,  1., -1.])\n\n</code></pre> <pre><code>torch.rsqrt(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u5177\u6709<code>input</code>\u7684\u6bcf\u4e2a\u5143\u7d20\u7684\u5e73\u65b9\u6839\u7684\u5012\u6570\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n&gt;&gt;&gt; torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])\n\n</code></pre> <pre><code>torch.sigmoid(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input</code>\u5143\u7d20\u7684sigmoid\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n&gt;&gt;&gt; torch.sigmoid(a)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n\n</code></pre> <pre><code>torch.sign(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input</code>\u5143\u7d20\u7b26\u53f7\u7684\u65b0\u5f20\u91cf\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3])\n&gt;&gt;&gt; a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n&gt;&gt;&gt; torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])\n\n</code></pre> <pre><code>torch.sin(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5e26\u6709<code>input</code>\u5143\u7d20\u6b63\u5f26\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n&gt;&gt;&gt; torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\n\n</code></pre> <pre><code>torch.sinh(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5f26\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n&gt;&gt;&gt; torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n\n</code></pre> <pre><code>torch.sqrt(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u5e73\u65b9\u6839\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n&gt;&gt;&gt; torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])\n\n</code></pre> <pre><code>torch.tan(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u6b63\u5207\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n&gt;&gt;&gt; torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\n\n</code></pre> <pre><code>torch.tanh(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u53cc\u66f2\u6b63\u5207\u7684\u65b0\u5f20\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n&gt;&gt;&gt; torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n\n</code></pre> <pre><code>torch.trunc(input, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5177\u6709<code>input</code>\u5143\u7d20\u7684\u622a\u65ad\u6574\u6570\u503c\u7684\u65b0\u5f20\u91cf\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4)\n&gt;&gt;&gt; a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n&gt;&gt;&gt; torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])\n\n</code></pre>"},{"location":"1.0/torch_math_operations_reduction_ops/","title":"Reduction Ops","text":""},{"location":"1.0/torch_math_operations_reduction_ops/#_1","title":"\u51cf\u5c11\u884c\u52a8","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.argmax(input, dim=None, keepdim=False)\n</code></pre> <p>\u8fd4\u56de\u7ef4\u5ea6\u4e0a\u5f20\u91cf\u7684\u6700\u5927\u503c\u7684\u7d22\u5f15\u3002</p> <p>\u8fd9\u662f <code>torch.max()</code> \u8fd4\u56de\u7684\u7b2c\u4e8c\u4e2a\u503c\u3002\u6709\u5173\u6b64\u65b9\u6cd5\u7684\u786e\u5207\u8bed\u4e49\uff0c\u8bf7\u53c2\u9605\u5176\u6587\u6863\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>dim  (int\uff09 - \u964d\u4f4e\u7684\u7ef4\u6570\u3002\u5982\u679c<code>None</code>\uff0c\u5219\u8fd4\u56de\u5c55\u5e73\u8f93\u5165\u7684argmax\u3002</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code>\u3002\u5982\u679c<code>dim=None</code>\uff0c\u5219\u5ffd\u7565\u3002</li> </ul> <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n [-0.7401, -0.8805, -0.3402, -1.1936],\n [ 0.4907, -1.3948, -1.0691, -0.3132],\n [-1.6092,  0.5419, -0.2993,  0.3195]])\n\n&gt;&gt;&gt; torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n\n</code></pre> <pre><code>torch.argmin(input, dim=None, keepdim=False)\n</code></pre> <p>\u8fd4\u56de\u7ef4\u5ea6\u4e0a\u5f20\u91cf\u7684\u6700\u5c0f\u503c\u7684\u7d22\u5f15\u3002</p> <p>\u8fd9\u662f <code>torch.min()</code> \u8fd4\u56de\u7684\u7b2c\u4e8c\u4e2a\u503c\u3002\u6709\u5173\u6b64\u65b9\u6cd5\u7684\u786e\u5207\u8bed\u4e49\uff0c\u8bf7\u53c2\u9605\u5176\u6587\u6863\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>dim  (int\uff09 - \u964d\u4f4e\u7684\u7ef4\u6570\u3002\u5982\u679c<code>None</code>\uff0c\u5219\u8fd4\u56de\u5c55\u5e73\u8f93\u5165\u7684argmin\u3002</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code>\u3002\u5982\u679c<code>dim=None</code>\uff0c\u5219\u5ffd\u7565\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n [ 1.0100, -1.1975, -0.0102, -0.4732],\n [-0.9240,  0.1207, -0.7506, -1.0213],\n [ 1.7809, -1.2960,  0.9384,  0.1438]])\n\n&gt;&gt;&gt; torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n\n</code></pre> <pre><code>torch.cumprod(input, dim, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ef4\u5ea6<code>dim</code>\u4e2d<code>input</code>\u5143\u7d20\u7684\u7d2f\u79ef\u4e58\u79ef\u3002</p> <p>\u4f8b\u5982\uff0c\u5982\u679c<code>input</code>\u662f\u5927\u5c0f\u4e3aN\u7684\u5411\u91cf\uff0c\u5219\u7ed3\u679c\u4e5f\u5c06\u662f\u5177\u6709\u5143\u7d20\u7684\u5927\u5c0f\u4e3aN\u7684\u5411\u91cf\u3002</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u6267\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(10)\n&gt;&gt;&gt; a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n -0.2129, -0.4206,  0.1968])\n&gt;&gt;&gt; torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n 0.0014, -0.0006, -0.0001])\n\n&gt;&gt;&gt; a[5] = 0.0\n&gt;&gt;&gt; torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n 0.0000, -0.0000, -0.0000])\n\n</code></pre> <pre><code>torch.cumsum(input, dim, out=None, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ef4\u5ea6<code>dim</code>\u4e2d<code>input</code>\u7684\u5143\u7d20\u7684\u7d2f\u79ef\u548c\u3002</p> <p>For example, if <code>input</code> is a vector of size N, the result will also be a vector of size N, with elements.</p> <p></p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u6267\u884c\u64cd\u4f5c\u7684\u7ef4\u5ea6</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(10)\n&gt;&gt;&gt; a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n 0.1850, -1.1571, -0.4243])\n&gt;&gt;&gt; torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n -1.8209, -2.9780, -3.4022])\n\n</code></pre> <pre><code>torch.dist(input, other, p=2) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de(<code>input</code> - <code>other</code>\uff09\u7684p\u8303\u6570</p> <p><code>input</code>\u548c<code>other</code>\u7684\u5f62\u72b6\u5fc5\u987b\u662f\u53ef\u64ad\u653e\u7684\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u5176\u4ed6 (Tensor\uff09 - \u53f3\u4fa7\u8f93\u5165\u5f20\u91cf</li> <li>p  (\u6f02\u6d6e \uff0c \u4efb\u9009\uff09 - \u8981\u8ba1\u7b97\u7684\u8303\u6570</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(4)\n&gt;&gt;&gt; x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n&gt;&gt;&gt; y = torch.randn(4)\n&gt;&gt;&gt; y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n&gt;&gt;&gt; torch.dist(x, y, 3.5)\ntensor(1.6727)\n&gt;&gt;&gt; torch.dist(x, y, 3)\ntensor(1.6973)\n&gt;&gt;&gt; torch.dist(x, y, 0)\ntensor(inf)\n&gt;&gt;&gt; torch.dist(x, y, 1)\ntensor(2.6537)\n\n</code></pre> <pre><code>torch.logsumexp(input, dim, keepdim=False, out=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u6c42\u548c\u6307\u6570\u7684\u5bf9\u6570\u3002\u8ba1\u7b97\u5728\u6570\u503c\u4e0a\u662f\u7a33\u5b9a\u7684\u3002</p> <p>\u5bf9\u4e8e\u7531<code>dim</code>\u548c\u5176\u4ed6\u6307\u6570  \u7ed9\u51fa\u7684\u603b\u548c\u6307\u6570  \uff0c\u7ed3\u679c\u662f</p> <p></p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c\u5219\u8f93\u51fa\u5f20\u91cf\u4e0e<code>input</code>\u7684\u5927\u5c0f\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a<code>dim</code>\u7684\u5927\u5c0f\u4e3a1.\u5426\u5219\uff0c<code>dim</code>\u88ab\u6324\u538b(\u53c2\u89c1 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4<code>input</code>\u5c111\u4e2a\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\u6216 \u5143\u7ec4python\uff1a\u6574\u6570\uff09 - \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6\u6216\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <pre><code>Example::\n</code></pre> <pre><code>&gt;&gt;&gt; a = torch.randn(3, 3)\n&gt;&gt;&gt; torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])\n\n</code></pre> <pre><code>torch.mean()\n</code></pre> <pre><code>torch.mean(input) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u5e73\u5747\u503c\u3002</p> \u53c2\u6570\uff1a \u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n&gt;&gt;&gt; torch.mean(a)\ntensor(0.3367)\n\n</code></pre> <pre><code>torch.mean(input, dim, keepdim=False, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u5e73\u5747\u503c\u3002\u5982\u679c<code>dim</code>\u662f\u7ef4\u5ea6\u5217\u8868\uff0c\u8bf7\u51cf\u5c11\u6240\u6709\u7ef4\u5ea6\u3002</p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c\u5219\u8f93\u51fa\u5f20\u91cf\u4e0e<code>input</code>\u7684\u5927\u5c0f\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a1\u7684\u5c3a\u5bf8<code>dim</code>\u9664\u5916\u3002<code>dim</code>\u88ab\u6324\u538b(\u89c1\uff09 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u5177\u67091(\u6216<code>len(dim)</code>\uff09\u66f4\u5c11\u7684\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>out  (Tensor\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n [-0.9644,  1.0131, -0.6549, -1.4279],\n [-0.2951, -1.3350, -0.7694,  0.5600],\n [ 1.0842, -0.9580,  0.3623,  0.2343]])\n&gt;&gt;&gt; torch.mean(a, 1)\ntensor([-0.0163, -0.5085, -0.4599,  0.1807])\n&gt;&gt;&gt; torch.mean(a, 1, True)\ntensor([[-0.0163],\n [-0.5085],\n [-0.4599],\n [ 0.1807]])\n\n</code></pre> <pre><code>torch.median()\n</code></pre> <pre><code>torch.median(input) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u4e2d\u503c\u3002</p> Parameters: input (Tensor) \u2013 the input tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n&gt;&gt;&gt; torch.median(a)\ntensor(0.2202)\n\n</code></pre> <pre><code>torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u4e2d\u503c\u3002\u8fd8\u5c06\u4e2d\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e\u8fd4\u56de\u4e3a<code>LongTensor</code>\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c<code>dim</code>\u662f<code>input</code>\u5f20\u91cf\u7684\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u3002</p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c\u5219\u8f93\u51fa\u5f20\u91cf\u4e0e<code>input</code>\u7684\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a<code>dim</code>\u7684\u5c3a\u5bf8\u4e3a1.\u5426\u5219\uff0c<code>dim</code>\u88ab\u6324\u538b(\u53c2\u89c1 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u6bd4<code>input</code>\u5c111\u4e2a\u7ef4\u5ea6\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>\u503c (tensor \uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>\u6307\u6570 (tensor \uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u6307\u6570\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 5)\n&gt;&gt;&gt; a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n&gt;&gt;&gt; torch.median(a, 1)\n(tensor([-0.3982,  0.2270,  0.2488,  0.4742]), tensor([ 1,  4,  4,  3]))\n\n</code></pre> <pre><code>torch.mode(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u6a21\u5f0f\u503c\u3002\u8fd8\u5c06\u6a21\u5f0f\u503c\u7684\u7d22\u5f15\u4f4d\u7f6e\u4f5c\u4e3a<code>LongTensor</code>\u8fd4\u56de\u3002</p> <p>By default, <code>dim</code> is the last dimension of the <code>input</code> tensor.</p> <p>\u5982\u679c<code>keepdim</code>\u4e3a<code>True</code>\uff0c\u5219\u8f93\u51fa\u5f20\u91cf\u4e0e<code>input</code>\u7684\u5c3a\u5bf8\u76f8\u540c\uff0c\u4f46\u5c3a\u5bf8\u4e3a<code>dim</code>\u7684\u5c3a\u5bf8\u4e3a1.\u5426\u5219\uff0c<code>dim</code>\u88ab\u6324\u538b(\u53c2\u89c1 <code>torch.squeeze()</code>)\uff0c\u5bfc\u81f4\u8f93\u51fa\u5f20\u91cf\u7684\u5c3a\u5bf8\u6bd4<code>input</code>\u5c111\u3002</p> <p>\u6ce8\u610f</p> <p>\u5c1a\u672a\u4e3a<code>torch.cuda.Tensor</code>\u5b9a\u4e49\u6b64\u529f\u80fd\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>\u503c (tensor \uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>\u6307\u6570 (tensor \uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u6307\u6570\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 5)\n&gt;&gt;&gt; a\ntensor([[-1.2808, -1.0966, -1.5946, -0.1148,  0.3631],\n [ 1.1395,  1.1452, -0.6383,  0.3667,  0.4545],\n [-0.4061, -0.3074,  0.4579, -1.3514,  1.2729],\n [-1.0130,  0.3546, -1.4689, -0.1254,  0.0473]])\n&gt;&gt;&gt; torch.mode(a, 1)\n(tensor([-1.5946, -0.6383, -1.3514, -1.4689]), tensor([ 2,  2,  3,  2]))\n\n</code></pre> <pre><code>torch.norm(input, p='fro', dim=None, keepdim=False, out=None)\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u5f20\u91cf\u7684\u77e9\u9635\u8303\u6570\u6216\u5411\u91cf\u8303\u6570\u3002</p> <p>Parameters:</p> <ul> <li> <p>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</p> </li> <li> <p>p  (int\uff0c \u6f02\u6d6e \uff0c ] inf \uff0c -inf \uff0c '\u6765'__\uff0c 'nuc'__\uff0c \u4efb\u9009\uff09 -</p> <p>\u89c4\u8303\u7684\u987a\u5e8f\u3002\u9ed8\u8ba4\u503c\uff1a<code>'fro'</code>\u53ef\u4ee5\u8ba1\u7b97\u4ee5\u4e0b\u89c4\u8303\uff1a</p> <p>| ord |\u77e9\u9635\u89c4\u8303|\u77e2\u91cf\u89c4\u8303| | --- | --- | --- | |\u6ca1\u6709| Frobenius\u89c4\u8303| 2\u8303\u6570| | '\u6765'| Frobenius\u89c4\u8303| - | | 'nuc'|\u6838\u89c4\u8303| - | |\u5176\u4ed6|\u5f53\u660f\u6697\u662f\u65e0|\u65f6\uff0c\u4f5c\u4e3avec\u89c4\u8303sum(abs(x\uff09 ord\uff09(1./ord\uff09|</p> </li> <li> <p>\u660f\u6697 (int\uff0c 2\u5143\u7ec4python\uff1aints \uff0c 2-list of python\uff1aints \uff0c \u53ef\u9009\uff09 - \u5982\u679c\u662fint\uff0c\u5c06\u8ba1\u7b97\u5411\u91cf\u8303\u6570\uff0c\u5982\u679c\u662f2\u5143\u7ec4\u7684int\uff0c\u5c06\u8ba1\u7b97\u77e9\u9635\u8303\u6570\u3002\u5982\u679c\u503c\u4e3aNone\uff0c\u5219\u5f53\u8f93\u5165\u5f20\u91cf\u4ec5\u5177\u6709\u4e24\u4e2a\u7ef4\u5ea6\u65f6\u5c06\u8ba1\u7b97\u77e9\u9635\u8303\u6570\uff0c\u5f53\u8f93\u5165\u5f20\u91cf\u4ec5\u5177\u6709\u4e00\u4e2a\u7ef4\u5ea6\u65f6\u5c06\u8ba1\u7b97\u5411\u91cf\u8303\u6570\u3002\u5982\u679c\u8f93\u5165\u5f20\u91cf\u5177\u6709\u4e24\u4e2a\u4ee5\u4e0a\u7684\u7ef4\u5ea6\uff0c\u5219\u5411\u91cf\u8303\u6570\u5c06\u5e94\u7528\u4e8e\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u3002</p> </li> <li> <p>keepdim  (bool\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code>\u3002\u5982\u679c<code>dim</code> = <code>None</code>\u548c<code>out</code> = <code>None</code>\uff0c\u5219\u5ffd\u7565\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></p> </li> <li> <p>out  (Tensor\uff0c \u53ef\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf\u3002\u5982\u679c<code>dim</code> = <code>None</code>\u548c<code>out</code> = <code>None</code>\uff0c\u5219\u5ffd\u7565\u3002</p> </li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4\n&gt;&gt;&gt; b = a.reshape((3, 3))\n&gt;&gt;&gt; torch.norm(a)\ntensor(7.7460)\n&gt;&gt;&gt; torch.norm(b)\ntensor(7.7460)\n&gt;&gt;&gt; torch.norm(a, float('inf'))\ntensor(4.)\n&gt;&gt;&gt; torch.norm(b, float('inf'))\ntensor([4., 3., 4.])\n&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n&gt;&gt;&gt; torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n&gt;&gt;&gt; torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n&gt;&gt;&gt; torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n&gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n&gt;&gt;&gt; torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n\n</code></pre> <pre><code>torch.prod()\n</code></pre> <pre><code>torch.prod(input, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u4e58\u79ef\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[-0.8020,  0.5428, -1.5854]])\n&gt;&gt;&gt; torch.prod(a)\ntensor(0.6902)\n\n</code></pre> <pre><code>torch.prod(input, dim, keepdim=False, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u4e58\u79ef\u3002</p> <p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 fewer dimension than <code>input</code>.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 2)\n&gt;&gt;&gt; a\ntensor([[ 0.5261, -0.3837],\n [ 1.1857, -0.2498],\n [-1.1646,  0.0705],\n [ 1.1131, -1.0629]])\n&gt;&gt;&gt; torch.prod(a, 1)\ntensor([-0.2018, -0.2962, -0.0821, -1.1831])\n\n</code></pre> <pre><code>torch.std()\n</code></pre> <pre><code>torch.std(input, unbiased=True) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u6807\u51c6\u504f\u5dee\u3002</p> <p>\u5982\u679c<code>unbiased</code>\u4e3a<code>False</code>\uff0c\u5219\u5c06\u901a\u8fc7\u504f\u5dee\u4f30\u7b97\u5668\u8ba1\u7b97\u6807\u51c6\u504f\u5dee\u3002\u5426\u5219\uff0c\u5c06\u4f7f\u7528\u8d1d\u585e\u5c14\u7684\u4fee\u6b63\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u65e0\u504f (bool\uff09 - \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[-0.8166, -1.3802, -0.3560]])\n&gt;&gt;&gt; torch.std(a)\ntensor(0.5130)\n\n</code></pre> <pre><code>torch.std(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u6807\u51c6\u504f\u5dee\u3002</p> <p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 fewer dimension than <code>input</code>.</p> <p>If <code>unbiased</code> is <code>False</code>, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>\u65e0\u504f (bool\uff09 - \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[ 0.2035,  1.2959,  1.8101, -0.4644],\n [ 1.5027, -0.3270,  0.5905,  0.6538],\n [-1.5745,  1.3330, -0.5596, -0.6548],\n [ 0.1264, -0.5080,  1.6420,  0.1992]])\n&gt;&gt;&gt; torch.std(a, dim=1)\ntensor([ 1.0311,  0.7477,  1.2204,  0.9087])\n\n</code></pre> <pre><code>torch.sum()\n</code></pre> <pre><code>torch.sum(input, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u603b\u548c\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n&gt;&gt;&gt; torch.sum(a)\ntensor(-0.5475)\n\n</code></pre> <pre><code>torch.sum(input, dim, keepdim=False, dtype=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u603b\u548c\u3002\u5982\u679c<code>dim</code>\u662f\u7ef4\u5ea6\u5217\u8868\uff0c\u8bf7\u51cf\u5c11\u6240\u6709\u7ef4\u5ea6\u3002</p> <p>If <code>keepdim</code> is <code>True</code>, the output tensor is of the same size as <code>input</code> except in the dimension(s) <code>dim</code> where it is of size 1. Otherwise, <code>dim</code> is squeezed (see <code>torch.squeeze()</code>), resulting in the output tensor having 1 (or <code>len(dim)</code>) fewer dimension(s).</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\u6216 \u5143\u7ec4python\uff1a\u6574\u6570\uff09 - \u8981\u51cf\u5c11\u7684\u7ef4\u5ea6\u6216\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6307\u5b9a\uff0c\u5219\u5728\u6267\u884c\u64cd\u4f5c\u4e4b\u524d\u5c06\u8f93\u5165\u5f20\u91cf\u8f6c\u6362\u4e3a<code>dtype</code>\u3002\u8fd9\u5bf9\u4e8e\u9632\u6b62\u6570\u636e\u7c7b\u578b\u6ea2\u51fa\u5f88\u6709\u7528\u3002\u9ed8\u8ba4\u503c\uff1a\u65e0\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n [-0.2993,  0.9138,  0.9337, -1.6864],\n [ 0.1132,  0.7892, -0.1003,  0.5688],\n [ 0.3637, -0.9906, -0.4752, -1.5197]])\n&gt;&gt;&gt; torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n&gt;&gt;&gt; torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])\n\n</code></pre> <pre><code>torch.unique(input, sorted=False, return_inverse=False, dim=None)\n</code></pre> <p>\u8fd4\u56de\u8f93\u5165\u5f20\u91cf\u7684\u552f\u4e00\u6807\u91cf\u5143\u7d20\u4f5c\u4e3a1-D\u5f20\u91cf\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u6392\u5e8f (bool\uff09 - \u662f\u5426\u5728\u8fd4\u56de\u4f5c\u4e3a\u8f93\u51fa\u4e4b\u524d\u6309\u5347\u5e8f\u5bf9\u552f\u4e00\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\u3002</li> <li>return_inverse  (bool\uff09 - \u662f\u5426\u8fd8\u8fd4\u56de\u539f\u59cb\u8f93\u5165\u4e2d\u5143\u7d20\u5728\u8fd4\u56de\u7684\u552f\u4e00\u5217\u8868\u4e2d\u7ed3\u675f\u7684\u7d22\u5f15\u3002</li> <li>dim  (int\uff09 - \u5e94\u7528\u552f\u4e00\u7684\u7ef4\u5ea6\u3002\u5982\u679c\u662f<code>None</code>\uff0c\u5219\u8fd4\u56de\u5c55\u5e73\u8f93\u5165\u7684\u552f\u4e00\u503c\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code></li> </ul> <p>|\u8fd4\u56de\uff1a|\u5305\u542b\u5f20\u91cf\u7684\u5f20\u91cf\u6216\u5143\u7ec4</p> <p>\uff06GT; * \u8f93\u51fa (Tensor )\uff1a\u552f\u4e00\u6807\u91cf\u5143\u7d20\u7684\u8f93\u51fa\u5217\u8868\u3002 \uff06GT; * inverse_indices  (Tensor ):(\u53ef\u9009\uff09\u5982\u679c<code>return_inverse</code>\u4e3aTrue\uff0c\u5c06\u4f1a\u6709\u7b2c\u4e8c\u4e2a\u8fd4\u56de\u7684\u5f20\u91cf(\u4e0e\u8f93\u5165\u76f8\u540c\u7684\u5f62\u72b6\uff09\uff0c\u8868\u793a\u539f\u59cb\u5143\u7d20\u7684\u7d22\u5f15\u8f93\u5165\u6620\u5c04\u5230\u8f93\u51fa\u4e2d;\u5426\u5219\uff0c\u6b64\u51fd\u6570\u53ea\u8fd4\u56de\u5355\u4e2a\u5f20\u91cf\u3002</p> \u8fd4\u56de\u7c7b\u578b\uff1a (Tensor \uff0c Tensor (\u53ef\u9009\uff09\uff09 <p>Example:</p> <pre><code>&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n&gt;&gt;&gt; output\ntensor([ 2,  3,  1])\n\n&gt;&gt;&gt; output, inverse_indices = torch.unique(\n torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n&gt;&gt;&gt; output\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; inverse_indices\ntensor([ 0,  2,  1,  2])\n\n&gt;&gt;&gt; output, inverse_indices = torch.unique(\n torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n&gt;&gt;&gt; output\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; inverse_indices\ntensor([[ 0,  2],\n [ 1,  2]])\n\n</code></pre> <pre><code>torch.var()\n</code></pre> <pre><code>torch.var(input, unbiased=True) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de<code>input</code>\u5f20\u91cf\u4e2d\u6240\u6709\u5143\u7d20\u7684\u65b9\u5dee\u3002</p> <p>\u5982\u679c<code>unbiased</code>\u662f<code>False</code>\uff0c\u5219\u901a\u8fc7\u504f\u5dee\u4f30\u8ba1\u5668\u8ba1\u7b97\u65b9\u5dee\u3002\u5426\u5219\uff0c\u5c06\u4f7f\u7528\u8d1d\u585e\u5c14\u7684\u4fee\u6b63\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u65e0\u504f (bool\uff09 - \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 3)\n&gt;&gt;&gt; a\ntensor([[-0.3425, -1.2636, -0.4864]])\n&gt;&gt;&gt; torch.var(a)\ntensor(0.2455)\n\n</code></pre> <pre><code>torch.var(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7ed9\u5b9a\u7ef4<code>dim</code>\u4e2d<code>input</code>\u5f20\u91cf\u7684\u6bcf\u4e00\u884c\u7684\u65b9\u5dee\u3002</p> <p>If <code>keepdim</code> is <code>True</code>, the output tensors are of the same size as <code>input</code> except in the dimension <code>dim</code> where they are of size 1. Otherwise, <code>dim</code> is squeezed (see <code>torch.squeeze()</code>), resulting in the outputs tensor having 1 fewer dimension than <code>input</code>.</p> <p>If <code>unbiased</code> is <code>False</code>, then the variance will be calculated via the biased estimator. Otherwise, Bessel's correction will be used.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>\u660f\u6697 (int\uff09 - \u51cf\u5c11\u7684\u7ef4\u5ea6</li> <li>keepdim  (bool\uff09 - \u8f93\u51fa\u5f20\u91cf\u662f\u5426\u4fdd\u7559<code>dim</code></li> <li>\u65e0\u504f (bool\uff09 - \u662f\u5426\u4f7f\u7528\u65e0\u504f\u4f30\u8ba1</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)\n&gt;&gt;&gt; a\ntensor([[-0.3567,  1.7385, -1.3042,  0.7423],\n [ 1.3436, -0.1015, -0.9834, -0.8438],\n [ 0.6056,  0.1089, -0.3112, -1.4085],\n [-0.7700,  0.6074, -0.1469,  0.7777]])\n&gt;&gt;&gt; torch.var(a, 1)\ntensor([ 1.7444,  1.1363,  0.7356,  0.5112])\n\n</code></pre>"},{"location":"1.0/torch_math_operations_spectral_ops/","title":"Spectral Ops","text":""},{"location":"1.0/torch_math_operations_spectral_ops/#_1","title":"\u5149\u8c31\u884c\u52a8","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.fft(input, signal_ndim, normalized=False) \u2192 Tensor\n</code></pre> <p>\u590d\u6742\u5230\u590d\u6742\u7684\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362</p> <p>\u8be5\u65b9\u6cd5\u8ba1\u7b97\u590d\u6570\u5230\u590d\u6570\u7684\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362\u3002\u5ffd\u7565\u6279\u91cf\u7ef4\u5ea6\uff0c\u5b83\u8ba1\u7b97\u4ee5\u4e0b\u8868\u8fbe\u5f0f\uff1a</p> <p></p> <p>\u5176\u4e2d  = <code>signal_ndim</code>\u662f\u4fe1\u53f7\u7684\u7ef4\u6570\uff0c  \u662f\u4fe1\u53f7\u7ef4\u6570  \u7684\u5927\u5c0f\u3002</p> <p>\u8be5\u65b9\u6cd5\u652f\u63011D\uff0c2D\u548c3D\u590d\u6742\u5230\u590d\u5408\u53d8\u6362\uff0c\u7531<code>signal_ndim</code>\u8868\u793a\u3002 <code>input</code>\u5fc5\u987b\u662f\u6700\u540e\u4e00\u4e2a\u5c3a\u5bf8\u4e3a2\u7684\u5f20\u91cf\uff0c\u8868\u793a\u590d\u6570\u7684\u5b9e\u90e8\u548c\u865a\u90e8\uff0c\u5e76\u4e14\u81f3\u5c11\u5e94\u5177\u6709<code>signal_ndim + 1</code>\u5c3a\u5bf8\u548c\u4efb\u610f\u6570\u91cf\u7684\u524d\u5bfc\u6279\u91cf\u5c3a\u5bf8\u3002\u5982\u679c<code>normalized</code>\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u5219\u901a\u8fc7\u5c06\u5176\u9664\u4ee5  \u6765\u5c06\u7ed3\u679c\u6807\u51c6\u5316\uff0c\u4ee5\u4fbf\u64cd\u4f5c\u7b26\u662f\u5355\u4e00\u7684\u3002</p> <p>\u5c06\u5b9e\u90e8\u548c\u865a\u90e8\u4e00\u8d77\u4f5c\u4e3a<code>input</code>\u7684\u76f8\u540c\u5f62\u72b6\u7684\u4e00\u4e2a\u5f20\u91cf\u8fd4\u56de\u3002</p> <p>\u8be5\u51fd\u6570\u7684\u53cd\u51fd\u6570\u662f <code>ifft()</code> \u3002</p> <p>\u6ce8\u610f</p> <p>\u5bf9\u4e8eCUDA\u5f20\u91cf\uff0cLRU\u9ad8\u901f\u7f13\u5b58\u7528\u4e8ecuFFT\u8ba1\u5212\uff0c\u4ee5\u52a0\u901f\u5728\u5177\u6709\u76f8\u540c\u914d\u7f6e\u7684\u76f8\u540c\u51e0\u4f55\u7684\u5f20\u91cf\u4e0a\u91cd\u590d\u8fd0\u884cFFT\u65b9\u6cd5\u3002</p> <p>\u66f4\u6539<code>torch.backends.cuda.cufft_plan_cache.max_size</code>(CUDA 10\u53ca\u66f4\u9ad8\u7248\u672c\u4e0a\u7684\u9ed8\u8ba4\u503c\u4e3a4096\uff0c\u65e7\u7248\u672c\u7684CUDA\u4e0a\u4e3a1023\uff09\u63a7\u5236\u6b64\u7f13\u5b58\u7684\u5bb9\u91cf\u3002\u4e00\u4e9bcuFFT\u8ba1\u5212\u53ef\u80fd\u4f1a\u5206\u914dGPU\u5185\u5b58\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528<code>torch.backends.cuda.cufft_plan_cache.size</code>\u67e5\u8be2\u5f53\u524d\u7f13\u5b58\u4e2d\u7684\u8ba1\u5212\u6570\u91cf\uff0c\u4f7f\u7528<code>torch.backends.cuda.cufft_plan_cache.clear()</code>\u6e05\u9664\u7f13\u5b58\u3002</p> <p>\u8b66\u544a</p> <p>\u5bf9\u4e8eCPU\u5f20\u91cf\uff0c\u6b64\u65b9\u6cd5\u76ee\u524d\u4ec5\u9002\u7528\u4e8eMKL\u3002\u4f7f\u7528<code>torch.backends.mkl.is_available()</code>\u68c0\u67e5\u662f\u5426\u5b89\u88c5\u4e86MKL\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u81f3\u5c11<code>signal_ndim</code> <code>+ 1</code>\u7ef4\u5ea6\u7684\u8f93\u5165\u5f20\u91cf</li> <li>signal_ndim  (int\uff09 - \u6bcf\u4e2a\u4fe1\u53f7\u4e2d\u7684\u7ef4\u6570\u3002 <code>signal_ndim</code>\u53ea\u80fd\u662f1,2\u62163</li> <li>\u5f52\u4e00\u5316 (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u5f52\u4e00\u5316\u7ed3\u679c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> </ul> \u8fd4\u56de\uff1a \u5305\u542b\u590d\u6570\u5230\u590d\u6570\u5085\u7acb\u53f6\u53d8\u6362\u7ed3\u679c\u7684\u5f20\u91cf \u8fd4\u56de\u7c7b\u578b\uff1a Tensor <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; # unbatched 2D FFT\n&gt;&gt;&gt; x = torch.randn(4, 3, 2)\n&gt;&gt;&gt; torch.fft(x, 2)\ntensor([[[-0.0876,  1.7835],\n [-2.0399, -2.9754],\n [ 4.4773, -5.0119]],\n\n [[-1.5716,  2.7631],\n [-3.8846,  5.2652],\n [ 0.2046, -0.7088]],\n\n [[ 1.9938, -0.5901],\n [ 6.5637,  6.4556],\n [ 2.9865,  4.9318]],\n\n [[ 7.0193,  1.1742],\n [-1.3717, -2.1084],\n [ 2.0289,  2.9357]]])\n&gt;&gt;&gt; # batched 1D FFT\n&gt;&gt;&gt; torch.fft(x, 1)\ntensor([[[ 1.8385,  1.2827],\n [-0.1831,  1.6593],\n [ 2.4243,  0.5367]],\n\n [[-0.9176, -1.5543],\n [-3.9943, -2.9860],\n [ 1.2838, -2.9420]],\n\n [[-0.8854, -0.6860],\n [ 2.4450,  0.0808],\n [ 1.3076, -0.5768]],\n\n [[-0.1231,  2.7411],\n [-0.3075, -1.7295],\n [-0.5384, -2.0299]]])\n&gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT\n&gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2)\n&gt;&gt;&gt; y = torch.fft(x, 2)\n&gt;&gt;&gt; y.shape\ntorch.Size([3, 3, 5, 5, 2])\n\n</code></pre> <pre><code>torch.ifft(input, signal_ndim, normalized=False) \u2192 Tensor\n</code></pre> <p>\u590d\u6570\u5230\u590d\u6570\u7684\u9006\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362</p> <p>\u8be5\u65b9\u6cd5\u8ba1\u7b97\u590d\u6570\u5230\u590d\u6570\u7684\u79bb\u6563\u5085\u91cc\u53f6\u9006\u53d8\u6362\u3002\u5ffd\u7565\u6279\u91cf\u7ef4\u5ea6\uff0c\u5b83\u8ba1\u7b97\u4ee5\u4e0b\u8868\u8fbe\u5f0f\uff1a</p> <p></p> <p>where  = <code>signal_ndim</code> is number of dimensions for the signal, and  is the size of signal dimension .</p> <p>\u53c2\u6570\u89c4\u8303\u4e0e <code>fft()</code> \u51e0\u4e4e\u76f8\u540c\u3002\u4f46\u662f\uff0c\u5982\u679c<code>normalized</code>\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u5219\u8fd4\u56de\u7ed3\u679c\u4e58\u4ee5  \uff0c\u6210\u4e3a\u5355\u4e00\u8fd0\u7b97\u7b26\u3002\u56e0\u6b64\uff0c\u8981\u53cd\u8f6c <code>fft()</code> \uff0c<code>normalized</code>\u53c2\u6570\u5e94\u8bbe\u7f6e\u4e3a <code>fft()</code> \u76f8\u540c\u3002</p> <p>Returns the real and the imaginary parts together as one tensor of the same shape of <code>input</code>.</p> <p>\u8be5\u51fd\u6570\u7684\u53cd\u51fd\u6570\u662f <code>fft()</code> \u3002</p> <p>Note</p> <p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration.</p> <p>Changing <code>torch.backends.cuda.cufft_plan_cache.max_size</code> (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use <code>torch.backends.cuda.cufft_plan_cache.size</code> to query the number of plans currently in cache, and <code>torch.backends.cuda.cufft_plan_cache.clear()</code> to clear the cache.</p> <p>Warning</p> <p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u81f3\u5c11<code>signal_ndim</code> <code>+ 1</code>\u7ef4\u5ea6\u7684\u8f93\u5165\u5f20\u91cf</li> <li>signal_ndim  (int\uff09 - \u6bcf\u4e2a\u4fe1\u53f7\u4e2d\u7684\u7ef4\u6570\u3002 <code>signal_ndim</code>\u53ea\u80fd\u662f1,2\u62163</li> <li>\u5f52\u4e00\u5316 (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u5f52\u4e00\u5316\u7ed3\u679c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> </ul> Returns: \u5305\u542b\u590d\u6570\u5230\u590d\u6570\u9006\u5085\u7acb\u53f6\u53d8\u6362\u7ed3\u679c\u7684\u5f20\u91cf Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 3, 2)\n&gt;&gt;&gt; x\ntensor([[[ 1.2766,  1.3680],\n [-0.8337,  2.0251],\n [ 0.9465, -1.4390]],\n\n [[-0.1890,  1.6010],\n [ 1.1034, -1.9230],\n [-0.9482,  1.0775]],\n\n [[-0.7708, -0.8176],\n [-0.1843, -0.2287],\n [-1.9034, -0.2196]]])\n&gt;&gt;&gt; y = torch.fft(x, 2)\n&gt;&gt;&gt; torch.ifft(y, 2)  # recover x\ntensor([[[ 1.2766,  1.3680],\n [-0.8337,  2.0251],\n [ 0.9465, -1.4390]],\n\n [[-0.1890,  1.6010],\n [ 1.1034, -1.9230],\n [-0.9482,  1.0775]],\n\n [[-0.7708, -0.8176],\n [-0.1843, -0.2287],\n [-1.9034, -0.2196]]])\n\n</code></pre> <pre><code>torch.rfft(input, signal_ndim, normalized=False, onesided=True) \u2192 Tensor\n</code></pre> <p>\u5b9e\u5bf9\u590d\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362</p> <p>\u8be5\u65b9\u6cd5\u8ba1\u7b97\u5b9e\u6570\u5230\u590d\u6570\u7684\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362\u3002\u5b83\u5728\u6570\u5b66\u4e0a\u7b49\u540c\u4e8e <code>fft()</code> \uff0c\u4ec5\u5728\u8f93\u5165\u548c\u8f93\u51fa\u7684\u683c\u5f0f\u4e0a\u6709\u6240\u4e0d\u540c\u3002</p> <p>\u8be5\u65b9\u6cd5\u652f\u63011D\uff0c2D\u548c3D\u5b9e\u5bf9\u590d\u53d8\u6362\uff0c\u7531<code>signal_ndim</code>\u8868\u793a\u3002 <code>input</code>\u5fc5\u987b\u662f\u5177\u6709\u81f3\u5c11<code>signal_ndim</code>\u5c3a\u5bf8\u7684\u5f20\u91cf\uff0c\u53ef\u9009\u62e9\u4efb\u610f\u6570\u91cf\u7684\u524d\u5bfc\u6279\u91cf\u3002\u5982\u679c<code>normalized</code>\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u5219\u901a\u8fc7\u5c06\u5176\u9664\u4ee5  \u6765\u5c06\u7ed3\u679c\u6807\u51c6\u5316\uff0c\u4ee5\u4fbf\u64cd\u4f5c\u7b26\u662f\u5355\u4e00\u7684\uff0c\u5176\u4e2d  \u662f\u4fe1\u53f7\u7684\u5927\u5c0f\u7ef4  \u3002</p> <p>\u5b9e\u5bf9\u590d\u5085\u91cc\u53f6\u53d8\u6362\u7ed3\u679c\u9075\u5faa\u5171\u8f6d\u5bf9\u79f0\uff1a</p> <p></p> <p>\u8ba1\u7b97\u6307\u6570\u7b97\u672f\u7684\u6a21\u6570\u662f\u76f8\u5e94\u7ef4\u6570\u7684\u5927\u5c0f\uff0c  \u662f\u5171\u8f6d\u7b97\u5b50\uff0c  = <code>signal_ndim</code>\u3002 <code>onesided</code>\u6807\u5fd7\u63a7\u5236\u662f\u5426\u907f\u514d\u8f93\u51fa\u7ed3\u679c\u4e2d\u7684\u5197\u4f59\u3002\u5982\u679c\u8bbe\u7f6e\u4e3a<code>True</code>(\u9ed8\u8ba4\uff09\uff0c\u8f93\u51fa\u5c06\u4e0d\u662f\u5f62\u72b6  \u7684\u5b8c\u6574\u590d\u6742\u7ed3\u679c\uff0c\u5176\u4e2d  \u662f<code>input</code>\u7684\u5f62\u72b6\uff0c\u800c\u662f\u6700\u540e\u4e00\u4e2a\u5c3a\u5bf8\u5c06\u662f\u5927\u5c0f  \u7684\u4e00\u534a\u3002</p> <p>\u8be5\u51fd\u6570\u7684\u53cd\u51fd\u6570\u662f <code>irfft()</code> \u3002</p> <p>Note</p> <p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration.</p> <p>Changing <code>torch.backends.cuda.cufft_plan_cache.max_size</code> (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use <code>torch.backends.cuda.cufft_plan_cache.size</code> to query the number of plans currently in cache, and <code>torch.backends.cuda.cufft_plan_cache.clear()</code> to clear the cache.</p> <p>Warning</p> <p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u81f3\u5c11<code>signal_ndim</code>\u7ef4\u5ea6\u7684\u8f93\u5165\u5f20\u91cf</li> <li>signal_ndim  (int\uff09 - \u6bcf\u4e2a\u4fe1\u53f7\u4e2d\u7684\u7ef4\u6570\u3002 <code>signal_ndim</code>\u53ea\u80fd\u662f1,2\u62163</li> <li>\u5f52\u4e00\u5316 (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u5f52\u4e00\u5316\u7ed3\u679c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> <li>\u5355\u72ec (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u4e00\u534a\u7ed3\u679c\u4ee5\u907f\u514d\u5197\u4f59\u3002\u9ed8\u8ba4\u503c\uff1a<code>True</code></li> </ul> Returns: \u5305\u542b\u5b9e\u6570\u5230\u590d\u6570\u5085\u7acb\u53f6\u53d8\u6362\u7ed3\u679c\u7684\u5f20\u91cf Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(5, 5)\n&gt;&gt;&gt; torch.rfft(x, 2).shape\ntorch.Size([5, 3, 2])\n&gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape\ntorch.Size([5, 5, 2])\n\n</code></pre> <pre><code>torch.irfft(input, signal_ndim, normalized=False, onesided=True, signal_sizes=None) \u2192 Tensor\n</code></pre> <p>\u590d\u6570\u5230\u5b9e\u6570\u7684\u9006\u79bb\u6563\u5085\u7acb\u53f6\u53d8\u6362</p> <p>\u8be5\u65b9\u6cd5\u8ba1\u7b97\u590d\u6570\u5230\u5b9e\u6570\u7684\u9006\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u3002\u5b83\u5728\u6570\u5b66\u4e0a\u7b49\u540c\u4e8e <code>ifft()</code> \uff0c\u4ec5\u5728\u8f93\u5165\u548c\u8f93\u51fa\u7684\u683c\u5f0f\u4e0a\u6709\u6240\u4e0d\u540c\u3002</p> <p>\u53c2\u6570\u89c4\u8303\u4e0e <code>ifft()</code> \u51e0\u4e4e\u76f8\u540c\u3002\u7c7b\u4f3c\u4e8e <code>ifft()</code> \uff0c\u5982\u679c<code>normalized</code>\u8bbe\u7f6e\u4e3a<code>True</code>\uff0c\u5219\u901a\u8fc7\u5c06\u5176\u4e0e  \u76f8\u4e58\u6765\u4f7f\u7ed3\u679c\u5f52\u4e00\u5316\uff0c\u4ee5\u4fbf\u8fd0\u7b97\u7b26\u662f\u5355\u4e00\u7684\uff0c\u5176\u4e2d [] ](/apachecn/pytorch-doc-zh/blob/master/docs/1.0/img/4236d8cccece7d17f3a004865adbf94d.jpg) \u662f\u4fe1\u53f7\u7ef4  \u7684\u5927\u5c0f\u3002</p> <p>\u7531\u4e8e\u5171\u8f6d\u5bf9\u79f0\u6027\uff0c<code>input</code>\u4e0d\u9700\u8981\u5305\u542b\u5b8c\u6574\u7684\u590d\u9891\u7387\u503c\u3002\u5927\u7ea6\u4e00\u534a\u7684\u503c\u5c31\u8db3\u591f\u4e86\uff0c <code>rfft()</code> <code>rfft(signal, onesided=True)</code>\u7ed9\u51fa<code>input</code>\u7684\u60c5\u51b5\u5c31\u8db3\u591f\u4e86\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c06\u6b64\u65b9\u6cd5\u7684<code>onesided</code>\u53c2\u6570\u8bbe\u7f6e\u4e3a<code>True</code>\u3002\u6b64\u5916\uff0c\u539f\u59cb\u4fe1\u53f7\u5f62\u72b6\u4fe1\u606f\u6709\u65f6\u4f1a\u4e22\u5931\uff0c\u53ef\u9009\u5730\u5c06<code>signal_sizes</code>\u8bbe\u7f6e\u4e3a\u539f\u59cb\u4fe1\u53f7\u7684\u5927\u5c0f(\u5982\u679c\u5904\u4e8e\u6279\u5904\u7406\u6a21\u5f0f\uff0c\u5219\u6ca1\u6709\u6279\u91cf\u7ef4\u5ea6\uff09\u4ee5\u6b63\u786e\u7684\u5f62\u72b6\u6062\u590d\u5b83\u3002</p> <p>\u56e0\u6b64\uff0c\u8981\u53cd\u8f6c <code>rfft()</code> \uff0c<code>normalized</code>\u548c<code>onesided</code>\u53c2\u6570\u5e94\u8bbe\u7f6e\u4e3a <code>irfft()</code> \u76f8\u540c\uff0c\u5e76\u4e14\u6700\u597d\u7ed9\u51fa<code>signal_sizes</code>\u4ee5\u907f\u514d\u5927\u5c0f\u4e0d\u5339\u914d\u3002\u6709\u5173\u5c3a\u5bf8\u4e0d\u5339\u914d\u7684\u60c5\u51b5\uff0c\u8bf7\u53c2\u9605\u4e0b\u9762\u7684\u793a\u4f8b\u3002</p> <p>\u6709\u5173\u5171\u8f6d\u5bf9\u79f0\u6027\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 <code>rfft()</code> \u3002</p> <p>\u8be5\u51fd\u6570\u7684\u53cd\u51fd\u6570\u662f <code>rfft()</code> \u3002</p> <p>Warning</p> <p>\u4e00\u822c\u800c\u8a00\uff0c\u6b64\u51fd\u6570\u7684\u8f93\u5165\u5e94\u5305\u542b\u5171\u8f6d\u5bf9\u79f0\u540e\u7684\u503c\u3002\u8bf7\u6ce8\u610f\uff0c\u5373\u4f7f<code>onesided</code>\u4e3a<code>True</code>\uff0c\u4ecd\u7136\u9700\u8981\u5bf9\u67d0\u4e9b\u90e8\u5206\u8fdb\u884c\u5bf9\u79f0\u3002\u5f53\u4e0d\u6ee1\u8db3\u6b64\u8981\u6c42\u65f6\uff0c <code>irfft()</code> \u7684\u884c\u4e3a\u672a\u5b9a\u4e49\u3002\u7531\u4e8e <code>torch.autograd.gradcheck()</code> \u4f30\u8ba1\u5177\u6709\u70b9\u6270\u52a8\u7684\u6570\u503c\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff0c <code>irfft()</code> \u51e0\u4e4e\u80af\u5b9a\u4f1a\u5931\u8d25\u3002</p> <p>Note</p> <p>For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same same configuration.</p> <p>Changing <code>torch.backends.cuda.cufft_plan_cache.max_size</code> (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions) controls the capacity of this cache. Some cuFFT plans may allocate GPU memory. You can use <code>torch.backends.cuda.cufft_plan_cache.size</code> to query the number of plans currently in cache, and <code>torch.backends.cuda.cufft_plan_cache.clear()</code> to clear the cache.</p> <p>Warning</p> <p>For CPU tensors, this method is currently only available with MKL. Use <code>torch.backends.mkl.is_available()</code> to check if MKL is installed.</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u81f3\u5c11<code>signal_ndim</code> <code>+ 1</code>\u7ef4\u5ea6\u7684\u8f93\u5165\u5f20\u91cf</li> <li>signal_ndim  (int\uff09 - \u6bcf\u4e2a\u4fe1\u53f7\u4e2d\u7684\u7ef4\u6570\u3002 <code>signal_ndim</code>\u53ea\u80fd\u662f1,2\u62163</li> <li>\u5f52\u4e00\u5316 (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u5f52\u4e00\u5316\u7ed3\u679c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> <li>\u5355\u72ec (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236<code>input</code>\u662f\u5426\u4e3a\u534a\u6570\u4ee5\u907f\u514d\u5197\u4f59\uff0c\u4f8b\u5982\uff0c <code>rfft()</code> \u3002\u9ed8\u8ba4\u503c\uff1a<code>True</code></li> <li>signal_sizes (\u5217\u8868\u6216<code>torch.Size</code>\uff0c\u53ef\u9009\uff09 - \u539f\u59cb\u4fe1\u53f7\u7684\u5927\u5c0f(\u65e0\u6279\u91cf\u7ef4\u5ea6\uff09\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code></li> </ul> Returns: \u5305\u542b\u590d\u6570\u5230\u5b9e\u6570\u9006\u5085\u7acb\u53f6\u53d8\u6362\u7ed3\u679c\u7684\u5f20\u91cf Return type: Tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(4, 4)\n&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape\ntorch.Size([4, 3, 2])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size\n&gt;&gt;&gt; x = torch.randn(4, 5)\n\n&gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape\ntorch.Size([4, 3, 2])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # now we use the original shape to recover x\n&gt;&gt;&gt; x\ntensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],\n [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],\n [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],\n [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])\n&gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True)\n&gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x\ntensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],\n [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],\n [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],\n [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])\n\n</code></pre> <pre><code>torch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=True)\n</code></pre> <p>\u77ed\u65f6\u5085\u7acb\u53f6\u53d8\u6362(STFT\uff09\u3002</p> <p>\u5ffd\u7565\u53ef\u9009\u6279\u5904\u7406\u7ef4\u5ea6\uff0c\u6b64\u65b9\u6cd5\u8ba1\u7b97\u4ee5\u4e0b\u8868\u8fbe\u5f0f\uff1a</p> <p></p> <p>\u5176\u4e2d  \u662f\u6ed1\u52a8\u7a97\u53e3\u7684\u7d22\u5f15\uff0c  \u662f  \u7684\u9891\u7387\u3002\u5f53<code>onesided</code>\u662f\u9ed8\u8ba4\u503c<code>True</code>\u65f6\uff0c</p> <ul> <li><code>input</code>\u5fc5\u987b\u662f1-D\u65f6\u95f4\u5e8f\u5217\u62162-D\u6279\u65f6\u95f4\u5e8f\u5217\u3002</li> <li>\u5982\u679c<code>hop_length</code>\u4e3a<code>None</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u89c6\u4e3a\u7b49\u4e8e<code>floor(n_fft / 4)</code>\u3002</li> <li>\u5982\u679c<code>win_length</code>\u4e3a<code>None</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u89c6\u4e3a\u7b49\u4e8e<code>n_fft</code>\u3002</li> <li><code>window</code>\u53ef\u4ee5\u662f\u5c3a\u5bf8<code>win_length</code>\u76841-D\u5f20\u91cf\uff0c\u4f8b\u5982\u6765\u81ea <code>torch.hann_window()</code> \u3002\u5982\u679c<code>window</code>\u662f<code>None</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u89c6\u4e3a\u5728\u7a97\u53e3\u4e2d\u7684\u4efb\u4f55\u5730\u65b9\u90fd\u6709  \u3002\u5982\u679c  \uff0c<code>window</code>\u5c06\u5728\u65bd\u52a0\u4e4b\u524d\u5728\u957f\u5ea6<code>n_fft</code>\u7684\u4e24\u4fa7\u586b\u5145\u3002</li> <li>\u5982\u679c<code>center</code>\u4e3a<code>True</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219<code>input</code>\u5c06\u5728\u4e24\u4fa7\u586b\u5145\uff0c\u4ee5\u4fbf  \u5e27\u5728  \u65f6\u95f4\u5c45\u4e2d\u3002\u5426\u5219\uff0c  - \u5e27\u5728\u65f6\u95f4  \u5f00\u59cb\u3002</li> <li><code>pad_mode</code>\u786e\u5b9a<code>center</code>\u4e3a<code>True</code>\u65f6<code>input</code>\u4e0a\u4f7f\u7528\u7684\u586b\u5145\u65b9\u6cd5\u3002\u6709\u5173\u6240\u6709\u53ef\u7528\u9009\u9879\uff0c\u8bf7\u53c2\u9605 <code>torch.nn.functional.pad()</code> \u3002\u9ed8\u8ba4\u4e3a<code>\"reflect\"</code>\u3002</li> <li>\u5982\u679c<code>onesided</code>\u662f<code>True</code>(\u9ed8\u8ba4\u503c\uff09\uff0c\u5219\u4ec5\u8fd4\u56de  \u4e2d  \u7684\u503c\uff0c\u56e0\u4e3a\u5b9e\u6570\u5230\u590d\u6570\u7684\u5085\u91cc\u53f6\u53d8\u6362\u6ee1\u8db3\u5171\u8f6d\u5bf9\u79f0\u6027\uff0c\u5373\uff0c  \u3002</li> <li>\u5982\u679c<code>normalized</code>\u662f<code>True</code>(\u9ed8\u8ba4\u4e3a<code>False</code>\uff09\uff0c\u5219\u8be5\u51fd\u6570\u8fd4\u56de\u6807\u51c6\u5316\u7684STFT\u7ed3\u679c\uff0c\u5373\u4e58\u4ee5  \u3002</li> </ul> <p>\u5c06\u5b9e\u90e8\u548c\u865a\u90e8\u4e00\u8d77\u4f5c\u4e3a\u4e00\u4e2a\u5c3a\u5bf8  \u8fd4\u56de\uff0c\u5176\u4e2d  \u662f<code>input</code>\uff0c \u7684\u53ef\u9009\u6279\u91cf\u5927\u5c0f\u662f\u5e94\u7528STFT\u7684\u9891\u7387\u7684\u6570\u91cf\uff0c  \u662f\u4f7f\u7528\u7684\u5e27\u7684\u603b\u6570\uff0c\u5e76\u4e14\u6700\u540e\u7ef4\u5ea6\u4e2d\u7684\u6bcf\u5bf9\u8868\u793a\u4f5c\u4e3a\u5b9e\u90e8\u548c\u865a\u90e8\u7684\u590d\u6570\u3002</p> <p>Warning</p> <p>\u6b64\u529f\u80fd\u57280.4.1\u7248\u672c\u4e0a\u66f4\u6539\u4e86\u7b7e\u540d\u3002\u4f7f\u7528\u5148\u524d\u7684\u7b7e\u540d\u8c03\u7528\u53ef\u80fd\u4f1a\u5bfc\u81f4\u9519\u8bef\u6216\u8fd4\u56de\u9519\u8bef\u7684\u7ed3\u679c\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u8f93\u5165\u5f20\u91cf</li> <li>n_fft  (int\uff09 - \u5085\u7acb\u53f6\u53d8\u6362\u7684\u5927\u5c0f</li> <li>hop_length  (int\uff0c \u53ef\u9009\uff09 - \u76f8\u90bb\u6ed1\u52a8\u7a97\u53e3\u5e27\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code>(\u89c6\u4e3a\u7b49\u4e8e<code>floor(n_fft / 4)</code>\uff09</li> <li>win_length  (int\uff0c \u4efb\u9009\uff09 - \u7a97\u53e3\u6846\u67b6\u548cSTFT\u8fc7\u6ee4\u5668\u7684\u5927\u5c0f\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code>(\u89c6\u4e3a\u7b49\u4e8e<code>n_fft</code>\uff09</li> <li>\u7a97\u53e3 (Tensor\uff0c \u53ef\u9009\uff09 - \u53ef\u9009\u7a97\u51fd\u6570\u3002\u9ed8\u8ba4\u503c\uff1a<code>None</code>(\u88ab\u89c6\u4e3a\u6240\u6709  s\u7684\u7a97\u53e3\uff09</li> <li>\u4e2d\u5fc3 (bool\uff0c \u4efb\u9009\uff09 - \u662f\u5426\u5728\u4e24\u4fa7\u57ab<code>input</code>\u4f7f  \u7b2c\u4e00\u5e27\u4ee5\u65f6\u95f4  \u4e3a\u4e2d\u5fc3\u3002\u9ed8\u8ba4\u503c\uff1a<code>True</code></li> <li>pad_mode  (string \uff0c \u53ef\u9009\uff09 - \u63a7\u5236<code>center</code>\u4e3a<code>True</code>\u65f6\u4f7f\u7528\u7684\u586b\u5145\u65b9\u6cd5\u3002\u9ed8\u8ba4\u503c\uff1a<code>\"reflect\"</code></li> <li>\u5f52\u4e00\u5316 (bool\uff0c \u4efb\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u5f52\u4e00\u5316STFT\u7ed3\u679c\u9ed8\u8ba4\u503c\uff1a<code>False</code></li> <li>\u5355\u72ec (bool\uff0c \u53ef\u9009\uff09 - \u63a7\u5236\u662f\u5426\u8fd4\u56de\u4e00\u534a\u7ed3\u679c\u4ee5\u907f\u514d\u5197\u4f59\u9ed8\u8ba4\uff1a<code>True</code></li> </ul> Returns: \u5305\u542b\u5177\u6709\u4e0a\u8ff0\u5f62\u72b6\u7684STFT\u7ed3\u679c\u7684\u5f20\u91cf Return type: Tensor <pre><code>torch.bartlett_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u5df4\u7279\u5229\u7279\u7684\u7a97\u53e3\u529f\u80fd\u3002</p> <p></p> <p>\u5176\u4e2d  \u662f\u5b8c\u6574\u7684\u7a97\u53e3\u5927\u5c0f\u3002</p> <p>\u8f93\u5165<code>window_length</code>\u662f\u63a7\u5236\u8fd4\u56de\u7a97\u53e3\u5927\u5c0f\u7684\u6b63\u6574\u6570\u3002 <code>periodic</code>\u6807\u5fd7\u786e\u5b9a\u8fd4\u56de\u7684\u7a97\u53e3\u662f\u5426\u4ece\u5bf9\u79f0\u7a97\u53e3\u4e2d\u5220\u9664\u6700\u540e\u4e00\u4e2a\u91cd\u590d\u503c\uff0c\u5e76\u51c6\u5907\u7528\u4f5c\u5177\u6709 <code>torch.stft()</code> \u7b49\u529f\u80fd\u7684\u5468\u671f\u7a97\u53e3\u3002\u56e0\u6b64\uff0c\u5982\u679c<code>periodic</code>\u4e3a\u771f\uff0c\u5219\u4e0a\u5f0f\u4e2d\u7684  \u5b9e\u9645\u4e0a\u662f  \u3002\u6b64\u5916\uff0c\u6211\u4eec\u603b\u662f<code>torch.bartlett_window(L, periodic=True)</code>\u7b49\u4e8e<code>torch.bartlett_window(L + 1, periodic=False)[:-1])</code>\u3002</p> <p>Note</p> <p>\u5982\u679c<code>window_length</code>  \uff0c\u5219\u8fd4\u56de\u7684\u7a97\u53e3\u5305\u542b\u5355\u4e2a\u503c1\u3002</p> <p>Parameters:</p> <ul> <li>window_length  (int\uff09 - \u8fd4\u56de\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>\u5468\u671f\u6027 (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679c\u4e3aTrue\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u7a97\u53e3\u4f5c\u4e3a\u5468\u671f\u51fd\u6570\u3002\u5982\u679c\u4e3aFalse\uff0c\u5219\u8fd4\u56de\u5bf9\u79f0\u7a97\u53e3\u3002</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002\u4ec5\u652f\u6301\u6d6e\u70b9\u7c7b\u578b\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7a97\u53e3\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u4ec5\u652f\u6301<code>torch.strided</code>(\u5bc6\u96c6\u5e03\u5c40\uff09\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> Returns: \u542b\u6709\u7a97\u53e3\u76841-D\u5f20\u91cf  Return type: Tensor <pre><code>torch.blackman_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u5e03\u83b1\u514b\u66fc\u7a97\u53e3\u529f\u80fd\u3002</p> <p></p> <p>where  is the full window size.</p> <p>\u8f93\u5165<code>window_length</code>\u662f\u63a7\u5236\u8fd4\u56de\u7a97\u53e3\u5927\u5c0f\u7684\u6b63\u6574\u6570\u3002 <code>periodic</code>\u6807\u5fd7\u786e\u5b9a\u8fd4\u56de\u7684\u7a97\u53e3\u662f\u5426\u4ece\u5bf9\u79f0\u7a97\u53e3\u4e2d\u5220\u9664\u6700\u540e\u4e00\u4e2a\u91cd\u590d\u503c\uff0c\u5e76\u51c6\u5907\u7528\u4f5c\u5177\u6709 <code>torch.stft()</code> \u7b49\u529f\u80fd\u7684\u5468\u671f\u7a97\u53e3\u3002\u56e0\u6b64\uff0c\u5982\u679c<code>periodic</code>\u4e3a\u771f\uff0c\u5219\u4e0a\u5f0f\u4e2d\u7684  \u5b9e\u9645\u4e0a\u662f  \u3002\u6b64\u5916\uff0c\u6211\u4eec\u603b\u662f<code>torch.blackman_window(L, periodic=True)</code>\u7b49\u4e8e<code>torch.blackman_window(L + 1, periodic=False)[:-1])</code>\u3002</p> <p>Note</p> <p>If <code>window_length</code> , the returned window contains a single value 1.</p> <p>Parameters:</p> <ul> <li>window_length  (int\uff09 - \u8fd4\u56de\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>\u5468\u671f\u6027 (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679c\u4e3aTrue\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u7a97\u53e3\u4f5c\u4e3a\u5468\u671f\u51fd\u6570\u3002\u5982\u679c\u4e3aFalse\uff0c\u5219\u8fd4\u56de\u5bf9\u79f0\u7a97\u53e3\u3002</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002\u4ec5\u652f\u6301\u6d6e\u70b9\u7c7b\u578b\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7a97\u53e3\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u4ec5\u652f\u6301<code>torch.strided</code>(\u5bc6\u96c6\u5e03\u5c40\uff09\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> Returns: A 1-D tensor of size  containing the window Return type: Tensor <pre><code>torch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u6c49\u660e\u7a97\u529f\u80fd\u3002</p> <p></p> <p>where  is the full window size.</p> <p>\u8f93\u5165<code>window_length</code>\u662f\u63a7\u5236\u8fd4\u56de\u7a97\u53e3\u5927\u5c0f\u7684\u6b63\u6574\u6570\u3002 <code>periodic</code>\u6807\u5fd7\u786e\u5b9a\u8fd4\u56de\u7684\u7a97\u53e3\u662f\u5426\u4ece\u5bf9\u79f0\u7a97\u53e3\u4e2d\u5220\u9664\u6700\u540e\u4e00\u4e2a\u91cd\u590d\u503c\uff0c\u5e76\u51c6\u5907\u7528\u4f5c\u5177\u6709 <code>torch.stft()</code> \u7b49\u529f\u80fd\u7684\u5468\u671f\u7a97\u53e3\u3002\u56e0\u6b64\uff0c\u5982\u679c<code>periodic</code>\u4e3a\u771f\uff0c\u5219\u4e0a\u5f0f\u4e2d\u7684  \u5b9e\u9645\u4e0a\u662f  \u3002\u6b64\u5916\uff0c\u6211\u4eec\u603b\u662f<code>torch.hamming_window(L, periodic=True)</code>\u7b49\u4e8e<code>torch.hamming_window(L + 1, periodic=False)[:-1])</code>\u3002</p> <p>Note</p> <p>If <code>window_length</code> , the returned window contains a single value 1.</p> <p>Note</p> <p>\u8fd9\u662f <code>torch.hann_window()</code> \u7684\u901a\u7528\u7248\u672c\u3002</p> <p>Parameters:</p> <ul> <li>window_length  (int\uff09 - \u8fd4\u56de\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>\u5468\u671f\u6027 (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679c\u4e3aTrue\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u7a97\u53e3\u4f5c\u4e3a\u5468\u671f\u51fd\u6570\u3002\u5982\u679c\u4e3aFalse\uff0c\u5219\u8fd4\u56de\u5bf9\u79f0\u7a97\u53e3\u3002</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002\u4ec5\u652f\u6301\u6d6e\u70b9\u7c7b\u578b\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7a97\u53e3\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u4ec5\u652f\u6301<code>torch.strided</code>(\u5bc6\u96c6\u5e03\u5c40\uff09\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> Returns: A 1-D tensor of size  containing the window Return type: Tensor <pre><code>torch.hann_window(window_length, periodic=True, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u6c49\u6069\u7a97\u529f\u80fd\u3002</p> <p></p> <p>where  is the full window size.</p> <p>\u8f93\u5165<code>window_length</code>\u662f\u63a7\u5236\u8fd4\u56de\u7a97\u53e3\u5927\u5c0f\u7684\u6b63\u6574\u6570\u3002 <code>periodic</code>\u6807\u5fd7\u786e\u5b9a\u8fd4\u56de\u7684\u7a97\u53e3\u662f\u5426\u4ece\u5bf9\u79f0\u7a97\u53e3\u4e2d\u5220\u9664\u6700\u540e\u4e00\u4e2a\u91cd\u590d\u503c\uff0c\u5e76\u51c6\u5907\u7528\u4f5c\u5177\u6709 <code>torch.stft()</code> \u7b49\u529f\u80fd\u7684\u5468\u671f\u7a97\u53e3\u3002\u56e0\u6b64\uff0c\u5982\u679c<code>periodic</code>\u4e3a\u771f\uff0c\u5219\u4e0a\u5f0f\u4e2d\u7684  \u5b9e\u9645\u4e0a\u662f  \u3002\u6b64\u5916\uff0c\u6211\u4eec\u603b\u662f<code>torch.hann_window(L, periodic=True)</code>\u7b49\u4e8e<code>torch.hann_window(L + 1, periodic=False)[:-1])</code>\u3002</p> <p>Note</p> <p>If <code>window_length</code> , the returned window contains a single value 1.</p> <p>Parameters:</p> <ul> <li>window_length  (int\uff09 - \u8fd4\u56de\u7a97\u53e3\u7684\u5927\u5c0f</li> <li>\u5468\u671f\u6027 (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679c\u4e3aTrue\uff0c\u5219\u8fd4\u56de\u4e00\u4e2a\u7a97\u53e3\u4f5c\u4e3a\u5468\u671f\u51fd\u6570\u3002\u5982\u679c\u4e3aFalse\uff0c\u5219\u8fd4\u56de\u5bf9\u79f0\u7a97\u53e3\u3002</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002\u4ec5\u652f\u6301\u6d6e\u70b9\u7c7b\u578b\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7a97\u53e3\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u4ec5\u652f\u6301<code>torch.strided</code>(\u5bc6\u96c6\u5e03\u5c40\uff09\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> Returns: A 1-D tensor of size  containing the window Return type: Tensor"},{"location":"1.0/torch_random_sampling/","title":"Random sampling","text":""},{"location":"1.0/torch_random_sampling/#_1","title":"\u968f\u673a\u62bd\u6837","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.manual_seed(seed)\n</code></pre> <p>\u8bbe\u7f6e\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u79cd\u5b50\u3002\u8fd4\u56de<code>torch._C.Generator</code>\u5bf9\u8c61\u3002</p> \u53c2\u6570\uff1a \u79cd\u5b50 (int\uff09 - \u6240\u9700\u79cd\u5b50\u3002 <pre><code>torch.initial_seed()\n</code></pre> <p>\u8fd4\u56de\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\u7684\u521d\u59cb\u79cd\u5b50\uff0c\u5982Python <code>long</code>\u3002</p> <pre><code>torch.get_rng_state()\n</code></pre> <p>\u5c06\u968f\u673a\u6570\u751f\u6210\u5668\u72b6\u6001\u8fd4\u56de\u4e3a<code>torch.ByteTensor</code>\u3002</p> <pre><code>torch.set_rng_state(new_state)\n</code></pre> <p>\u8bbe\u7f6e\u968f\u673a\u6570\u751f\u6210\u5668\u72b6\u6001\u3002</p> Parameters: new_state  (torch.ByteTensor\uff09 - \u7406\u60f3\u72b6\u6001 <pre><code>torch.default_generator = &lt;torch._C.Generator object&gt;\n</code></pre> <pre><code>torch.bernoulli(input, *, generator=None, out=None) \u2192 Tensor\n</code></pre> <p>\u4ece\u4f2f\u52aa\u5229\u5206\u5e03\u4e2d\u7ed8\u5236\u4e8c\u8fdb\u5236\u968f\u673a\u6570(0\u62161\uff09\u3002</p> <p><code>input</code>\u5f20\u91cf\u5e94\u8be5\u662f\u5305\u542b\u7528\u4e8e\u7ed8\u5236\u4e8c\u8fdb\u5236\u968f\u673a\u6570\u7684\u6982\u7387\u7684\u5f20\u91cf\u3002\u56e0\u6b64\uff0c<code>input</code>\u4e2d\u7684\u6240\u6709\u503c\u5fc5\u987b\u5728\u4ee5\u4e0b\u8303\u56f4\u5185\uff1a  \u3002</p> <p>\u8f93\u51fa\u5f20\u91cf\u7684  \u5143\u7d20\u5c06\u6839\u636e<code>input</code>\u4e2d\u7ed9\u51fa\u7684  \u6982\u7387\u503c\u7ed8\u5236\u503c  \u3002</p> <p></p> <p>\u8fd4\u56de\u7684<code>out</code>\u5f20\u91cf\u4ec5\u5177\u6709\u503c0\u62161\uff0c\u5e76\u4e14\u4e0e<code>input</code>\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\u3002</p> <p><code>out</code>\u53ef\u4ee5\u6709\u6574\u6570<code>dtype</code>\uff0c\u4f46\u662f\uff1aattr <code>input</code>\u5fc5\u987b\u6709\u6d6e\u70b9<code>dtype</code>\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u4f2f\u52aa\u5229\u5206\u5e03\u7684\u6982\u7387\u503c\u7684\u8f93\u5165\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n&gt;&gt;&gt; a\ntensor([[ 0.1737,  0.0950,  0.3609],\n [ 0.7148,  0.0289,  0.2676],\n [ 0.9456,  0.8937,  0.7202]])\n&gt;&gt;&gt; torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n [ 0.,  0.,  0.],\n [ 1.,  1.,  1.]])\n\n&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n&gt;&gt;&gt; torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n [ 1.,  1.,  1.],\n [ 1.,  1.,  1.]])\n&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n&gt;&gt;&gt; torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n [ 0.,  0.,  0.],\n [ 0.,  0.,  0.]])\n\n</code></pre> <pre><code>torch.multinomial(input, num_samples, replacement=False, out=None) \u2192 LongTensor\n</code></pre> <p>\u8fd4\u56de\u5f20\u91cf\uff0c\u5176\u4e2d\u6bcf\u884c\u5305\u542b\u4ece\u4f4d\u4e8e\u5f20\u91cf<code>input</code>\u7684\u76f8\u5e94\u884c\u4e2d\u7684\u591a\u9879\u6982\u7387\u5206\u5e03\u4e2d\u91c7\u6837\u7684<code>num_samples</code>\u7d22\u5f15\u3002</p> <p>\u6ce8\u610f</p> <p><code>input</code>\u7684\u884c\u4e0d\u9700\u8981\u6c42\u548c\u4e3a\u4e00(\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6211\u4eec\u4f7f\u7528\u503c\u4f5c\u4e3a\u6743\u91cd\uff09\uff0c\u4f46\u5fc5\u987b\u662f\u975e\u8d1f\u7684\uff0c\u6709\u9650\u7684\u5e76\u4e14\u5177\u6709\u975e\u96f6\u548c\u3002</p> <p>\u6839\u636e\u6bcf\u4e2a\u6837\u672c\u7684\u65f6\u95f4(\u7b2c\u4e00\u4e2a\u6837\u672c\u653e\u5728\u7b2c\u4e00\u5217\u4e2d\uff09\u4ece\u5de6\u5230\u53f3\u6392\u5e8f\u6307\u6570\u3002</p> <p>\u5982\u679c<code>input</code>\u662f\u77e2\u91cf\uff0c<code>out</code>\u662f\u5927\u5c0f\u4e3a<code>num_samples</code>\u7684\u77e2\u91cf\u3002</p> <p>\u5982\u679c<code>input</code>\u662f\u5177\u6709<code>m</code>\u884c\u7684\u77e9\u9635\uff0c\u5219<code>out</code>\u662f\u5f62\u72b6\u77e9\u9635  \u3002</p> <p>\u5982\u679c\u66f4\u6362\u4e3a<code>True</code>\uff0c\u5219\u66f4\u6362\u6837\u54c1\u3002</p> <p>\u5982\u679c\u6ca1\u6709\uff0c\u5219\u7ed8\u5236\u5b83\u4eec\u800c\u4e0d\u66ff\u6362\u5b83\u4eec\uff0c\u8fd9\u610f\u5473\u7740\u5f53\u4e3a\u4e00\u884c\u7ed8\u5236\u6837\u672c\u7d22\u5f15\u65f6\uff0c\u4e0d\u80fd\u518d\u4e3a\u8be5\u884c\u7ed8\u5236\u5b83\u3002</p> <p>\u8fd9\u610f\u5473\u7740<code>num_samples</code>\u5fc5\u987b\u4f4e\u4e8e<code>input</code>\u957f\u5ea6(\u6216\u8005<code>input</code>\u7684\u5217\u6570\uff0c\u5982\u679c\u5b83\u662f\u77e9\u9635\uff09\u7684\u7ea6\u675f\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - \u5305\u542b\u6982\u7387\u7684\u8f93\u5165\u5f20\u91cf</li> <li>num_samples  (int\uff09 - \u8981\u62bd\u53d6\u7684\u6837\u672c\u6570\u91cf</li> <li>\u66ff\u4ee3 (bool\uff0c \u53ef\u9009\uff09 - \u662f\u5426\u4e0e\u66ff\u6362\u6709\u5173</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n&gt;&gt;&gt; torch.multinomial(weights, 4)\ntensor([ 1,  2,  0,  0])\n&gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])\n\n</code></pre> <pre><code>torch.normal()\n</code></pre> <pre><code>torch.normal(mean, std, out=None) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4ece\u5355\u72ec\u7684\u6b63\u6001\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u968f\u673a\u6570\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u7ed9\u51fa\u4e86\u5747\u503c\u548c\u6807\u51c6\u5dee\u3002</p> <p><code>mean</code> \u662f\u4e00\u4e2a\u5f20\u91cf\uff0c\u5177\u6709\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u6b63\u6001\u5206\u5e03\u7684\u5747\u503c</p> <p><code>std</code> \u662f\u4e00\u4e2a\u5f20\u91cf\uff0c\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u7684\u6b63\u6001\u5206\u5e03\u7684\u6807\u51c6\u5dee</p> <p><code>mean</code> \u548c <code>std</code> \u7684\u5f62\u72b6\u4e0d\u9700\u8981\u5339\u914d\uff0c\u4f46\u6bcf\u4e2a\u5f20\u91cf\u4e2d\u7684\u5143\u7d20\u603b\u6570\u9700\u8981\u76f8\u540c\u3002</p> <p>Note</p> <p>\u5f53\u5f62\u72b6\u4e0d\u5339\u914d\u65f6\uff0c <code>mean</code> \u7684\u5f62\u72b6\u7528\u4f5c\u8fd4\u56de\u8f93\u51fa\u5f20\u91cf\u7684\u5f62\u72b6</p> <p>Parameters:</p> <ul> <li>\u610f\u5473\u7740 (tensor) - \u6bcf\u4e2a\u5143\u7d20\u7684\u5f20\u91cf\u610f\u5473\u7740</li> <li>std  (Tensor\uff09 - \u6bcf\u5143\u7d20\u6807\u51c6\u5dee\u7684\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n 8.0505,   8.1408,   9.0563,  10.0566])\n\n</code></pre> <pre><code>torch.normal(mean=0.0, std, out=None) \u2192 Tensor\n</code></pre> <p>\u4e0e\u4e0a\u9762\u7684\u51fd\u6570\u7c7b\u4f3c\uff0c\u4f46\u662f\u6240\u6709\u7ed8\u5236\u5143\u7d20\u4e4b\u95f4\u5171\u4eab\u5747\u503c\u3002</p> <p>Parameters:</p> <ul> <li>\u610f\u5473\u7740 (\u6f02\u6d6e \uff0c \u4efb\u9009\uff09 - \u6240\u6709\u5206\u5e03\u7684\u5747\u503c</li> <li>std  (Tensor\uff09 - \u6bcf\u5143\u7d20\u6807\u51c6\u5dee\u7684\u5f20\u91cf</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n\n</code></pre> <pre><code>torch.normal(mean, std=1.0, out=None) \u2192 Tensor\n</code></pre> <p>\u4e0e\u4e0a\u8ff0\u529f\u80fd\u7c7b\u4f3c\uff0c\u4f46\u6807\u51c6\u504f\u5dee\u5728\u6240\u6709\u7ed8\u5236\u5143\u7d20\u4e4b\u95f4\u5171\u4eab\u3002</p> <p>Parameters:</p> <ul> <li>\u610f\u5473\u7740 (tensor) - \u6bcf\u4e2a\u5143\u7d20\u7684\u5f20\u91cf\u610f\u5473\u7740</li> <li>std  (float\uff0c \u53ef\u9009\uff09 - \u6240\u6709\u5206\u5e03\u7684\u6807\u51c6\u5dee</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n\n</code></pre> <pre><code>torch.rand(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u4ece\u533a\u95f4  \u4e0a\u7684\u5747\u5300\u5206\u5e03\u8fd4\u56de\u586b\u5145\u968f\u673a\u6570\u7684\u5f20\u91cf</p> <p>\u5f20\u91cf\u7684\u5f62\u72b6\u7531\u53d8\u91cf\u53c2\u6570<code>sizes</code>\u5b9a\u4e49\u3002</p> <p>Parameters:</p> <ul> <li>sizes  (int ... ) - \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u5e8f\u5217\u3002\u53ef\u4ee5\u662f\u53ef\u53d8\u6570\u91cf\u7684\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u5217\u8868\u6216\u5143\u7ec4\u4e4b\u7c7b\u7684\u96c6\u5408\u3002</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56deTensor\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1a<code>torch.strided</code>\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n&gt;&gt;&gt; torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n [ 0.3816,  0.7249,  0.0998]])\n\n</code></pre> <pre><code>torch.rand_like(input, dtype=None, layout=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e0e<code>input</code>\u5927\u5c0f\u76f8\u540c\u7684\u5f20\u91cf\uff0c\u8be5\u5f20\u91cf\u7528\u95f4\u9694  \u4e0a\u7684\u5747\u5300\u5206\u5e03\u586b\u5145\u968f\u673a\u6570\u3002 <code>torch.rand_like(input)</code>\u76f8\u5f53\u4e8e<code>torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - <code>input</code>\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7684Tensor\u7684\u7406\u60f3\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684dtype\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u5e03\u5c40\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <pre><code>torch.randint(low=0, high, size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u586b\u5145\u4e86\u5728<code>low</code>(\u5305\u62ec\uff09\u548c<code>high</code>(\u4e0d\u5305\u62ec\uff09\u4e4b\u95f4\u7edf\u4e00\u751f\u6210\u7684\u968f\u673a\u6574\u6570\u7684\u5f20\u91cf\u3002</p> <p>\u5f20\u91cf\u7684\u5f62\u72b6\u7531\u53d8\u91cf\u53c2\u6570<code>size</code>\u5b9a\u4e49\u3002</p> <p>Parameters:</p> <ul> <li>\u4f4e (int\uff0c \u4efb\u9009\uff09 - \u4ece\u5206\u5e03\u4e2d\u5f97\u51fa\u7684\u6700\u5c0f\u6574\u6570\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>\u9ad8 (int\uff09 - \u9ad8\u4e8e\u4ece\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u6700\u9ad8\u6574\u6570\u3002</li> <li>\u5927\u5c0f (\u5143\u7ec4) - \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u5143\u7ec4\u3002</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56deTensor\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1a<code>torch.strided</code>\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n&gt;&gt;&gt; torch.randint(10, (2, 2))\ntensor([[0, 2],\n [5, 5]])\n\n&gt;&gt;&gt; torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n [6, 7]])\n\n</code></pre> <pre><code>torch.randint_like(input, low=0, high, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e0eTensor <code>input</code>\u5177\u6709\u76f8\u540c\u5f62\u72b6\u7684\u5f20\u91cf\uff0c\u586b\u5145\u5728<code>low</code>(\u5305\u62ec\uff09\u548c<code>high</code>(\u4e0d\u5305\u62ec\uff09\u4e4b\u95f4\u5747\u5300\u751f\u6210\u7684\u968f\u673a\u6574\u6570\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - <code>input</code>\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f</li> <li>\u4f4e (int\uff0c \u4efb\u9009\uff09 - \u4ece\u5206\u5e03\u4e2d\u5f97\u51fa\u7684\u6700\u5c0f\u6574\u6570\u3002\u9ed8\u8ba4\u503c\uff1a0\u3002</li> <li>\u9ad8 (int\uff09 - \u9ad8\u4e8e\u4ece\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u6700\u9ad8\u6574\u6570\u3002</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7684Tensor\u7684\u7406\u60f3\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684dtype\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u5e03\u5c40\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <pre><code>torch.randn(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u586b\u5145\u4e86\u6b63\u6001\u5206\u5e03\u4e2d\u968f\u673a\u6570\u7684\u5f20\u91cf\uff0c\u5176\u5747\u503c\u4e3a<code>0</code>\u548c\u65b9\u5dee<code>1</code>(\u4e5f\u79f0\u4e3a\u6807\u51c6\u6b63\u6001\u5206\u5e03\uff09\u3002</p> <p></p> <p>The shape of the tensor is defined by the variable argument <code>sizes</code>.</p> <p>Parameters:</p> <ul> <li>sizes  (int ... ) - \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u5e8f\u5217\u3002\u53ef\u4ee5\u662f\u53ef\u53d8\u6570\u91cf\u7684\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u662f\u5217\u8868\u6216\u5143\u7ec4\u4e4b\u7c7b\u7684\u96c6\u5408\u3002</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56deTensor\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1a<code>torch.strided</code>\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n&gt;&gt;&gt; torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n [ 1.1719, -0.4709, -0.1996]])\n\n</code></pre> <pre><code>torch.randn_like(input, dtype=None, layout=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e0e<code>input</code>\u5177\u6709\u76f8\u540c\u5927\u5c0f\u7684\u5f20\u91cf\uff0c\u8be5\u5f20\u91cf\u7528\u6b63\u6001\u5206\u5e03\u4e2d\u7684\u968f\u673a\u6570\u586b\u5145\uff0c\u5747\u503c\u4e3a0\u4e14\u65b9\u5dee\u4e3a1. <code>torch.randn_like(input)</code>\u7b49\u6548\u4e8e<code>torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>\u3002</p> <p>Parameters:</p> <ul> <li>\u8f93\u5165 (Tensor\uff09 - <code>input</code>\u7684\u5927\u5c0f\u5c06\u51b3\u5b9a\u8f93\u51fa\u5f20\u91cf\u7684\u5927\u5c0f</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u7684Tensor\u7684\u7406\u60f3\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684dtype\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1aif <code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u5e03\u5c40\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u9ed8\u8ba4\u4e3a<code>input</code>\u7684\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <pre><code>torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) \u2192 LongTensor\n</code></pre> <p>\u8fd4\u56de\u4ece<code>0</code>\u5230<code>n - 1</code>\u7684\u6574\u6570\u7684\u968f\u673a\u6392\u5217\u3002</p> <p>Parameters:</p> <ul> <li>n  (int\uff09 - \u4e0a\u9650(\u4e0d\u5305\u62ec\uff09</li> <li>out  (Tensor\uff0c \u4efb\u9009\uff09 - \u8f93\u51fa\u5f20\u91cf</li> <li>dtype  (<code>torch.dtype</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u6570\u636e\u7c7b\u578b\u3002\u9ed8\u8ba4\u503c\uff1a<code>torch.int64</code>\u3002</li> <li>\u5e03\u5c40 (<code>torch.layout</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56deTensor\u7684\u7406\u60f3\u5e03\u5c40\u3002\u9ed8\u8ba4\u503c\uff1a<code>torch.strided</code>\u3002</li> <li>\u8bbe\u5907 (<code>torch.device</code> \uff0c\u53ef\u9009\uff09 - \u8fd4\u56de\u5f20\u91cf\u7684\u6240\u9700\u8bbe\u5907\u3002\u9ed8\u8ba4\u503c\uff1a\u5982\u679c<code>None</code>\uff0c\u5219\u4f7f\u7528\u5f53\u524d\u8bbe\u5907\u4f5c\u4e3a\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(\u53c2\u89c1 <code>torch.set_default_tensor_type()</code>)\u3002 <code>device</code>\u5c06\u662fCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548cCUDA\u5f20\u91cf\u7c7b\u578b\u7684\u5f53\u524dCUDA\u8bbe\u5907\u3002</li> <li>requires_grad  (bool\uff0c \u53ef\u9009\uff09 - \u5982\u679cautograd\u5e94\u8be5\u8bb0\u5f55\u5bf9\u8fd4\u56de\u5f20\u91cf\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4\u503c\uff1a<code>False</code>\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.randperm(4)\ntensor([2, 1, 0, 3])\n\n</code></pre>"},{"location":"1.0/torch_random_sampling/#_2","title":"\u5c31\u5730\u968f\u673a\u62bd\u6837","text":"<p>Tensors\u8fd8\u5b9a\u4e49\u4e86\u4e00\u4e9b\u66f4\u591a\u7684\u5c31\u5730\u968f\u673a\u62bd\u6837\u51fd\u6570\u3002\u70b9\u51fb\u67e5\u770b\u4ed6\u4eec\u7684\u6587\u6863\uff1a</p> <ul> <li><code>torch.Tensor.bernoulli_()</code> - <code>torch.bernoulli()</code> \u7684\u539f\u4f4d\u7248\u672c</li> <li><code>torch.Tensor.cauchy_()</code> - \u4eceCauchy\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u6570\u5b57</li> <li><code>torch.Tensor.exponential_()</code> - \u4ece\u6307\u6570\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u6570\u5b57</li> <li><code>torch.Tensor.geometric_()</code> - \u4ece\u51e0\u4f55\u5206\u5e03\u4e2d\u63d0\u53d6\u7684\u5143\u7d20</li> <li><code>torch.Tensor.log_normal_()</code> - \u6765\u81ea\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u7684\u6837\u672c</li> <li><code>torch.Tensor.normal_()</code> - <code>torch.normal()</code> \u7684\u539f\u4f4d\u7248\u672c</li> <li><code>torch.Tensor.random_()</code> - \u4ece\u79bb\u6563\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57</li> <li><code>torch.Tensor.uniform_()</code> - \u4ece\u8fde\u7eed\u5747\u5300\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6570\u5b57</li> </ul>"},{"location":"1.0/torch_script_custom_ops/","title":"Extending TorchScript with Custom C-- Operators","text":"<p>\uff03\u4f7f\u7528\u81ea\u5b9a\u4e49C++ \u8fd0\u7b97\u7b26\u6269\u5c55 TorchScript</p> <p>\u4f5c\u8005\uff1aPyTorch</p> <p>\u8bd1\u8005\uff1aApacheCN</p> <p>PyTorch1.0\u7248\u672c\u5411 PyTorch \u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a [TorchScript](https://pytorch.org/docs/master/jit.html\uff09\u7684\u65b0\u7f16\u7a0b\u6a21\u578b\u3002TorchScript\u662fPython\u7f16\u7a0b\u8bed\u8a00\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u53ef\u4ee5\u901a\u8fc7TorchScript\u7f16\u8bd1\u5668\u8fdb\u884c\u89e3\u6790\uff0c\u7f16\u8bd1\u548c\u4f18\u5316\u3002\u6b64\u5916\uff0c\u5df2\u7f16\u8bd1\u7684TorchScript\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\u5e8f\u5217\u5316\u4e3a\u78c1\u76d8\u6587\u4ef6\u683c\u5f0f\uff0c\u60a8\u53ef\u4ee5\u968f\u540e\u4ece\u7eafC++(\u4ee5\u53caPython\uff09\u7a0b\u5e8f\u8fdb\u884c\u52a0\u8f7d\u548c\u8fd0\u884c\u4ee5\u8fdb\u884c\u63a8\u7406\u3002</p> <p>TorchScript\u652f\u6301\u7531<code>torch</code>\u5305\u63d0\u4f9b\u7684\u5927\u91cf\u64cd\u4f5c\uff0c\u5141\u8bb8\u60a8\u5c06\u591a\u79cd\u590d\u6742\u6a21\u578b\u7eaf\u7cb9\u8868\u793a\u4e3aPyTorch\u7684\u201c\u6807\u51c6\u5e93\u201d\u4e2d\u7684\u4e00\u7cfb\u5217\u5f20\u91cf\u8fd0\u7b97\u3002\u7136\u800c\uff0c\u6709\u65f6\u60a8\u53ef\u80fd\u4f1a\u53d1\u73b0\u9700\u8981\u4f7f\u7528\u81ea\u5b9a\u4e49C ++\u6216CUDA\u51fd\u6570\u6269\u5c55TorchScript\u3002\u867d\u7136\u6211\u4eec\u5efa\u8bae\u60a8\u4f7f\u7528\u6b64\u8fd9\u4e2a\u9009\u9879\u65f6\u53ea\u5728\u60a8\u7684\u60f3\u6cd5\u65e0\u6cd5(\u8db3\u591f\u6709\u6548\uff09\u8868\u8fbe\u4e3a\u4e00\u4e2a\u7b80\u5355\u7684Python\u51fd\u6570\u65f6\uff0c\u6211\u4eec\u786e\u5b9e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u53cb\u597d\u548c\u7b80\u5355\u7684\u754c\u9762\uff0c\u4f7f\u7528ATen \u5b9a\u4e49\u81ea\u5b9a\u4e49C++ \u548c CUDA \u5185\u6838\uff0cPyTorch \u7684\u9ad8\u6027\u80fdC ++\u5f20\u91cf\u5e93\u3002\u4e00\u65e6\u7ed1\u5b9a\u5230TorchScript\uff0c\u60a8\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u81ea\u5b9a\u4e49\u5185\u6838(\u6216\u201cops\u201d)\u5d4c\u5165\u5230\u60a8\u7684TorchScript\u6a21\u578b\u4e2d\uff0c\u5e76\u4ee5Python\u548cc++\u7684\u5e8f\u5217\u5316\u5f62\u5f0f\u76f4\u63a5\u6267\u884c\u5b83\u4eec\u3002</p> <p>\u4ee5\u4e0b\u6bb5\u843d\u7ed9\u51fa\u4e86\u4e00\u4e2a\u7f16\u5199TorchScript\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7684\u793a\u4f8b\uff0c\u4ee5\u8c03\u7528[OpenCV](https://www.opencv.org\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528C ++\u7f16\u5199\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e93\u3002\u6211\u4eec\u5c06\u8ba8\u8bba\u5982\u4f55\u5728C ++\u4e2d\u4f7f\u7528\u5f20\u91cf\uff0c\u5982\u4f55\u6709\u6548\u5730\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u7b2c\u4e09\u65b9\u5f20\u91cf\u683c\u5f0f(\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0cOpenCV [<code>](\uff03id1\uff09Mat</code>s\uff09\uff0c\u5982\u4f55\u5728TorchScript\u8fd0\u884c\u65f6\u6ce8\u518c\u8fd0\u7b97\u7b26\uff0c\u6700\u540e\u5982\u4f55\u7f16\u8bd1\u8fd0\u7b97\u7b26\u5e76\u5728Python\u548cC ++\u4e2d\u4f7f\u7528\u5b83\u3002</p> <p>\u672c\u6559\u7a0b\u5047\u8bbe\u60a8\u901a\u8fc7<code>pip</code>\u6216<code>conda</code>\u5b89\u88c5\u4e86PyTorch 1.0\u7684_preview release_\u3002\u6709\u5173\u83b7\u53d6\u6700\u65b0\u7248PyTorch 1.0 \\\u7684\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605 [https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally\uff09\u3002\u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u4ece\u6e90\u4ee3\u7801\u7f16\u8bd1PyTorch\u3002[\u6b64\u6587\u4ef6](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md\uff09\u4e2d\u7684\u6587\u6863\u5c06\u4e3a\u60a8\u63d0\u4f9b\u5e2e\u52a9\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#c","title":"\u5728C ++\u4e2d\u5b9e\u73b0\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>\u5bf9\u4e8e\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u5c06\u516c\u5f00[warpPerspective](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective\uff09\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u900f\u89c6\u53d8\u6362\u5e94\u7528\u4e8e\u56fe\u50cf\uff0c OpenCV to TorchScript\u4f5c\u4e3a\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u3002\u7b2c\u4e00\u6b65\u662f\u7528C ++\u7f16\u5199\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u5b9e\u73b0\u3002\u8ba9\u6211\u4eec\u8c03\u7528\u8fd9\u4e2a\u5b9e\u73b0<code>op.cpp</code>\u7684\u6587\u4ef6\uff0c\u5e76\u4f7f\u5b83\u770b\u8d77\u6765\u50cf\u8fd9\u6837\uff1a</p> <pre><code>#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;torch/script.h&gt;\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data&lt;float&gt;());\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data&lt;float&gt;());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});\n\n  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8});\n  return output.clone();\n}\n\n</code></pre> <p>\u8be5\u8fd0\u7b97\u7b26\u7684\u4ee3\u7801\u5f88\u77ed\u3002\u5728\u6587\u4ef6\u7684\u9876\u90e8\uff0c\u6211\u4eec\u5305\u542bOpenCV\u5934\u6587\u4ef6 <code>opencv2/opencv.hpp</code>\uff0c\u4ee5\u53ca <code>torch/script.h</code> \u5934\u6587\u4ef6\uff0c\u5b83\u5c55\u793a\u4e86\u6211\u4eec\u9700\u8981\u7f16\u5199\u81ea\u5b9a\u4e49TorchScript\u8fd0\u7b97\u7b26\u7684 PyTorch C++ API\u6240\u9700\u7684\u6240\u6709\u597d\u4e1c\u897f.\u6211\u4eec\u7684\u51fd\u6570 <code>warp_perspective</code> \u6709\u4e24\u4e2a\u53c2\u6570\uff1a\u8f93\u5165<code>image</code>\u548c\u6211\u4eec\u5e0c\u671b\u5e94\u7528\u4e8e\u56fe\u50cf\u7684<code>warp</code>\u53d8\u6362\u77e9\u9635\u3002\u8fd9\u4e9b\u8f93\u5165\u7684\u7c7b\u578b\u662f<code>torch::Tensor</code>\uff0cPyTorch\u5728C++\u4e2d\u7684\u5f20\u91cf\u7c7b\u578b(\u5b83\u4e5f\u662fPython\u4e2d\u6240\u6709\u5f20\u91cf\u7684\u57fa\u7840\u7c7b\u578b\uff09\u3002\u6211\u4eec\u7684<code>warp_perspective</code>\u51fd\u6570\u7684\u8fd4\u56de\u7c7b\u578b\u4e5f\u5c06\u662f<code>torch::Tensor</code>\u3002</p> <p>\u63d0\u793a</p> <p>\u6709\u5173ATen\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605[\u672c\u8bf4\u660e](https://pytorch.org/cppdocs/notes/tensor_basics.html\uff09\uff0cATen\u662f\u4e3aPyTorch\u63d0\u4f9b<code>Tensor</code>\u7c7b\u7684\u5e93\u3002\u6b64\u5916\uff0c[\u672c\u6559\u7a0b](https://pytorch.org/cppdocs/notes/tensor_creation.html\uff09\u63cf\u8ff0\u4e86\u5982\u4f55\u5728C ++\u4e2d\u5206\u914d\u548c\u521d\u59cb\u5316\u65b0\u7684\u5f20\u91cf\u5bf9\u8c61(\u6b64\u8fd0\u7b97\u7b26\u4e0d\u9700\u8981\uff09\u3002</p> <p>\u6ce8\u610f</p> <p>TorchScript\u7f16\u8bd1\u5668\u4e86\u89e3\u56fa\u5b9a\u6570\u91cf\u7684\u7c7b\u578b\u3002\u53ea\u6709\u8fd9\u4e9b\u7c7b\u578b\u53ef\u4ee5\u7528\u4f5c\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u53c2\u6570\u3002\u76ee\u524d\u8fd9\u4e9b\u7c7b\u578b\u662f\uff1a<code>torch::Tensor</code>\uff0c<code>torch::Scalar</code>\uff0c<code>double</code>\uff0c<code>int64_t</code>\u548c<code>std::vector</code>\u8fd9\u4e9b\u7c7b\u578b\u3002\u6ce8\u610f__only__<code>double</code>\u548c__not__<code>float</code>\u548c__only__<code>int64_t</code>\u548c__not__\u652f\u6301\u5176\u4ed6\u6574\u6570\u7c7b\u578b\uff0c\u5982<code>int</code>\uff0c<code>short</code>\u6216<code>long</code>\u3002</p> <p>\u5728\u6211\u4eec\u7684\u51fd\u6570\u5185\u90e8\uff0c\u6211\u4eec\u9700\u8981\u505a\u7684\u7b2c\u4e00\u4ef6\u4e8b\u662f\u5c06PyTorch\u5f20\u91cf\u8f6c\u6362\u4e3aOpenCV\u77e9\u9635\uff0c\u56e0\u4e3aOpenCV\u7684 <code>warpPerspective</code> \u671f\u671b<code>cv::Mat</code>\u5bf9\u8c61\u4f5c\u4e3a\u8f93\u5165\u3002\u5e78\u8fd0\u7684\u662f\uff0c\u6709\u4e00\u79cd\u65b9\u6cd5\u53ef\u4ee5\u505a\u5230\u8fd9\u4e00\u70b9\u800c\u65e0\u9700\u590d\u5236\u4efb\u4f55\u6570\u636e\u3002\u5728\u524d\u51e0\u884c\u4e2d\uff0c</p> <pre><code>cv::Mat image_mat(/*rows=*/image.size(0),\n                  /*cols=*/image.size(1),\n                  /*type=*/CV_32FC1,\n                  /*data=*/image.data&lt;float&gt;());\n\n</code></pre> <p>\u6211\u4eec\u6b63\u5728\u8c03\u7528OpenCV <code>Mat</code> \u7c7b\u7684 \u8fd9\u4e2a\u6784\u9020\u51fd\u6570 \u5c06\u6211\u4eec\u7684\u5f20\u91cf\u8f6c\u6362\u4e3a<code>Mat</code>\u5bf9\u8c61\u3002\u6211\u4eec\u4f20\u9012\u539f\u59cb<code>image</code> tensor\u7684\u884c\u6570\u548c\u5217\u6570\uff0c\u6570\u636e\u7c7b\u578b(\u6211\u4eec\u5c06\u5728\u672c\u4f8b\u4e2d\u5c06\u5176\u5b9a\u4e49\u4e3a<code>float32</code>\uff09\uff0c\u6700\u540e\u662f\u4e00\u4e2a\u6307\u5411\u5e95\u5c42\u6570\u636e\u7684\u539f\u59cb\u6307\u9488 - \u4e00\u4e2a<code>float *</code>\u3002<code>Mat</code>\u7c7b\u7684\u8fd9\u4e2a\u6784\u9020\u51fd\u6570\u7684\u7279\u6b8a\u4e4b\u5904\u5728\u4e8e\u5b83\u4e0d\u590d\u5236\u8f93\u5165\u6570\u636e\u3002\u76f8\u53cd\uff0c\u5b83\u5c06\u7b80\u5355\u5730\u4e3a\u5728\u201cMat\u201d\u4e0a\u6267\u884c\u7684\u6240\u6709\u64cd\u4f5c\u5f15\u7528\u8be5\u5185\u5b58\u3002\u5982\u679c\u5bf9<code>image_mat</code>\u6267\u884cin-place\u64cd\u4f5c\uff0c\u5219\u8fd9\u5c06\u53cd\u6620\u5728\u539f\u59cb<code>image</code>\u5f20\u91cf\u4e2d(\u53cd\u4e4b\u4ea6\u7136\uff09\u3002\u8fd9\u5141\u8bb8\u6211\u4eec\u4f7f\u7528\u5e93\u7684\u672c\u673a\u77e9\u9635\u7c7b\u578b\u8c03\u7528\u540e\u7eed\u7684OpenCV\u4f8b\u7a0b\uff0c\u5373\u4f7f\u6211\u4eec\u5b9e\u9645\u4e0a\u5c06\u6570\u636e\u5b58\u50a8\u5728PyTorch\u5f20\u91cf\u4e2d\u3002\u6211\u4eec\u91cd\u590d\u6b64\u8fc7\u7a0b\u5c06<code>warp</code> PyTorch\u5f20\u91cf\u8f6c\u6362\u4e3a<code>warp_mat</code> OpenCV\u77e9\u9635\uff1a</p> <pre><code>cv::Mat warp_mat(/*rows=*/warp.size(0),\n                 /*cols=*/warp.size(1),\n                 /*type=*/CV_32FC1,\n                 /*data=*/warp.data&lt;float&gt;());\n\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u51c6\u5907\u8c03\u7528\u6211\u4eec\u975e\u5e38\u6e34\u671b\u5728TorchScript\u4e2d\u4f7f\u7528\u7684OpenCV\u51fd\u6570\uff1a<code>warpPerspective</code>\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u4f20\u9012OpenCV\u51fd\u6570<code>image_mat</code>\u548c<code>warp_mat</code>\u77e9\u9635\uff0c\u4ee5\u53ca\u4e00\u4e2a\u540d\u4e3a<code>output_mat</code>\u7684\u7a7a\u8f93\u51fa\u77e9\u9635\u3002\u6211\u4eec\u8fd8\u6307\u5b9a\u4e86\u6211\u4eec\u60f3\u8981\u8f93\u51fa\u77e9\u9635(\u56fe\u50cf\uff09\u7684\u5927\u5c0f<code>dsize</code>\u3002\u5bf9\u4e8e\u6b64\u793a\u4f8b\uff0c\u5b83\u88ab\u786c\u7f16\u7801\u4e3a<code>8 x 8</code>\uff1a</p> <pre><code>cv::Mat output_mat;\ncv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});\n\n</code></pre> <p>\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5b9e\u73b0\u7684\u6700\u540e\u4e00\u6b65\u662f\u5c06<code>output_mat</code>\u8f6c\u6362\u56dePyTorch\u5f20\u91cf\uff0c\u4ee5\u4fbf\u6211\u4eec\u53ef\u4ee5\u5728PyTorch\u4e2d\u8fdb\u4e00\u6b65\u4f7f\u7528\u5b83\u3002\u8fd9\u4e0e\u6211\u4eec\u4e4b\u524d\u5728\u53e6\u4e00\u4e2a\u65b9\u5411\u8f6c\u6362\u65f6\u6240\u505a\u7684\u60ca\u4eba\u76f8\u4f3c\u3002PyTorch\u63d0\u4f9b\u4e86\u4e00\u4e2a<code>torch::from_blob</code>\u65b9\u6cd5\u3002\u5728\u672c\u4f8b\u4e2d\uff0c_blob_\u610f\u4e3a\u6307\u5411\u5185\u5b58\u7684\u4e00\u4e9b\u4e0d\u900f\u660e\u7684\u5e73\u9762\u6307\u9488\uff0c\u6211\u4eec\u60f3\u5c06\u5176\u89e3\u91ca\u4e3aPyTorch\u5f20\u91cf\u3002\u5bf9<code>torch::from_blob</code>\u7684\u8c03\u7528\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8})\n\n</code></pre> <p>\u6211\u4eec\u5728OpenCV <code>Mat</code>\u7c7b\u4e0a\u4f7f\u7528<code>.ptr&amp;lt;float&amp;gt;()</code>\u65b9\u6cd5\u6765\u83b7\u53d6\u6307\u5411\u5e95\u5c42\u6570\u636e\u7684\u539f\u59cb\u6307\u9488(\u5c31\u50cf\u4e4b\u524d\u7684PyTorch\u5f20\u91cf\u7684<code>.data&amp;lt;float&amp;gt;()</code>\u4e00\u6837\uff09\u3002\u6211\u4eec\u8fd8\u6307\u5b9a\u4e86\u5f20\u91cf\u7684\u8f93\u51fa\u5f62\u72b6\uff0c\u6211\u4eec\u5c06\u5176\u786c\u7f16\u7801\u4e3a<code>8 x 8</code>\u3002\u7136\u540e<code>torch::from_blob</code>\u7684\u8f93\u51fa\u4e3a<code>torch::Tensor</code>\uff0c\u6307\u5411OpenCV\u77e9\u9635\u62e5\u6709\u7684\u5185\u5b58\u3002</p> <p>\u5728\u4ece\u8fd0\u7b97\u7b26\u5b9e\u73b0\u8fd4\u56de\u6b64\u5f20\u91cf\u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u5728\u5f20\u91cf\u4e0a\u8c03\u7528<code>.clone()</code>\u6765\u6267\u884c\u57fa\u7840\u6570\u636e\u7684\u5185\u5b58\u590d\u5236\u3002\u539f\u56e0\u662f<code>torch::from_blob</code>\u8fd4\u56de\u4e0d\u62e5\u6709\u5176\u6570\u636e\u7684\u5f20\u91cf\u3002\u6b64\u65f6\uff0c\u6570\u636e\u4ecd\u5f52OpenCV\u77e9\u9635\u6240\u6709\u3002\u4f46\u662f\uff0c\u6b64OpenCV\u77e9\u9635\u5c06\u8d85\u51fa\u8303\u56f4\u5e76\u5728\u51fd\u6570\u672b\u5c3e\u53d6\u6d88\u5206\u914d\u3002\u5982\u679c\u6211\u4eec\u6309\u539f\u6837\u8fd4\u56de<code>output</code>\u5f20\u91cf\uff0c\u90a3\u4e48\u5f53\u6211\u4eec\u5728\u51fd\u6570\u5916\u90e8\u4f7f\u7528\u5b83\u65f6\uff0c\u5b83\u5c06\u6307\u5411\u65e0\u6548\u7684\u5185\u5b58\u3002\u8c03\u7528<code>.clone()</code>\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u5305\u542b\u65b0\u5f20\u91cf\u6240\u62e5\u6709\u7684\u539f\u59cb\u6570\u636e\u7684\u526f\u672c\u3002\u56e0\u6b64\u8fd4\u56de\u5916\u90e8\u4e16\u754c\u662f\u5b89\u5168\u7684\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#torchscript","title":"\u4f7f\u7528TorchScript\u6ce8\u518c\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>\u73b0\u5728\u5df2\u7ecf\u5728C ++\u4e2d\u5b9e\u73b0\u4e86\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\uff0c\u6211\u4eec\u9700\u8981_\u4f7f\u7528TorchScript\u8fd0\u884c\u65f6\u548c\u7f16\u8bd1\u5668\u6ce8\u518c\u5b83_\u3002\u8fd9\u5c06\u5141\u8bb8TorchScript\u7f16\u8bd1\u5668\u5728TorchScript\u4ee3\u7801\u4e2d\u89e3\u6790\u5bf9\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u5f15\u7528\u3002\u6ce8\u518c\u975e\u5e38\u7b80\u5355\u3002\u5bf9\u4e8e\u6211\u4eec\u7684\u60c5\u51b5\uff0c\u6211\u4eec\u9700\u8981\u5199\uff1a</p> <pre><code>static auto registry =\n  torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &amp;warp_perspective);\n\n</code></pre> <p>\u5728\u6211\u4eec\u7684<code>op.cpp</code>\u6587\u4ef6\u7684\u5168\u5c40\u8303\u56f4\u5185\u7684\u67d0\u4e2a\u5730\u65b9\u3002\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2a\u5168\u5c40\u53d8\u91cf<code>registry</code>\uff0c\u5b83\u5c06\u5728\u5176\u6784\u9020\u51fd\u6570\u4e2d\u4f7f\u7528TorchScript\u6ce8\u518c\u6211\u4eec\u7684\u8fd0\u7b97\u7b26(\u5373\u6bcf\u4e2a\u7a0b\u5e8f\u53ea\u6ce8\u518c\u4e00\u6b21\uff09\u3002\u6211\u4eec\u6307\u5b9a\u8fd0\u7b97\u7b26\u7684\u540d\u79f0\uff0c\u4ee5\u53ca\u6307\u5411\u5176\u5b9e\u73b0\u7684\u6307\u9488(\u6211\u4eec\u4e4b\u524d\u7f16\u5199\u7684\u51fd\u6570\uff09\u3002\u8be5\u540d\u79f0\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1anamespace(<code>my_ops</code>\uff09\u548c\u6211\u4eec\u6b63\u5728\u6ce8\u518c\u7684\u7279\u5b9a\u8fd0\u7b97\u7b26\u7684\u540d\u79f0(<code>warp_perspective</code>\uff09\u3002\u547d\u540d\u7a7a\u95f4\u548c\u8fd0\u7b97\u7b26\u540d\u79f0\u7531\u4e24\u4e2a\u5192\u53f7(<code>::</code>\uff09\u5206\u9694\u3002</p> <p>\u6ce8\u610f</p> <p>\u5982\u679c\u8981\u6ce8\u518c\u591a\u4e2a\u8fd0\u7b97\u7b26\uff0c\u53ef\u4ee5\u5728\u6784\u9020\u51fd\u6570\u4e4b\u540e\u5c06\u8c03\u7528\u94fe\u63a5\u5230<code>.op()</code>\uff1a</p> <pre><code>static auto registry =\n  torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &amp;warp_perspective)\n  .op(\"my_ops::another_op\", &amp;another_op)\n  .op(\"my_ops::and_another_op\", &amp;and_another_op);\n\n</code></pre> <p>\u5728\u540e\u53f0\uff0c<code>RegisterOperators</code>\u5c06\u6267\u884c\u4e00\u4e9b\u76f8\u5f53\u590d\u6742\u7684C++\u6a21\u677f\u5143\u7f16\u7a0b\u9b54\u672f\u6280\u5de7\u6765\u63a8\u65ad\u6211\u4eec\u4f20\u9012\u5b83\u7684\u51fd\u6570\u6307\u9488\u7684\u53c2\u6570\u548c\u8fd4\u56de\u503c\u7c7b\u578b(<code>&amp;warp_perspective</code>\uff09\u3002\u8be5\u4fe1\u606f\u7528\u4e8e\u4e3a\u6211\u4eec\u7684\u8fd0\u8425\u5546\u5f62\u6210 function schema\u3002\u51fd\u6570\u6a21\u5f0f\u662f\u8fd0\u7b97\u7b26\u7684\u7ed3\u6784\u5316\u8868\u793a - \u4e00\u79cd\u201c\u7b7e\u540d\u201d\u6216\u201c\u539f\u578b\u201d - \u7531 TorchScript \u7f16\u8bd1\u5668\u7528\u4e8e\u9a8c\u8bc1 TorchScript\u7a0b\u5e8f\u4e2d\u7684\u6b63\u786e\u6027\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#_1","title":"\u6784\u5efa\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u7528C++ \u5b9e\u73b0\u4e86\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5e76\u7f16\u5199\u4e86\u5b83\u7684\u6ce8\u518c\u4ee3\u7801\uff0c\u73b0\u5728\u662f\u65f6\u5019\u5c06\u8fd0\u7b97\u7b26\u6784\u5efa\u5230\u4e00\u4e2a(\u5171\u4eab\uff09\u5e93\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u52a0\u8f7d\u5230Python\u4e2d\u8fdb\u884c\u7814\u7a76\u548c\u5b9e\u9a8c\uff0c\u6216\u8005\u52a0\u8f7d\u5230C ++\u4e2d\u4ee5\u4fbf\u5728\u975ePython\u4e2d\u8fdb\u884c\u63a8\u7406\u73af\u5883\u3002\u4f7f\u7528\u7eafCMake\u6216\u50cf<code>setuptools</code>\u8fd9\u6837\u7684Python\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5b58\u5728\u591a\u79cd\u6784\u5efa\u8fd0\u7b97\u7b26\u7684\u65b9\u6cd5\u3002\u4e3a\u7b80\u6d01\u8d77\u89c1\uff0c\u4ee5\u4e0b\u6bb5\u843d\u4ec5\u8ba8\u8bbaCMake\u65b9\u6cd5\u3002\u672c\u6559\u7a0b\u7684\u9644\u5f55\u6df1\u5165\u7814\u7a76\u4e86\u57fa\u4e8ePython\u7684\u66ff\u4ee3\u65b9\u6848\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#cmake","title":"\u7528CMake\u5efa\u8bbe","text":"<p>\u8981\u4f7f\u7528 CMake \u6784\u5efa\u7cfb\u7edf\u5c06\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u6784\u5efa\u5230\u5171\u4eab\u5e93\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u7f16\u5199\u4e00\u4e2a\u7b80\u77ed\u7684<code>CMakeLists.txt</code>\u6587\u4ef6\u5e76\u5c06\u5176\u4e0e\u6211\u4eec\u4e4b\u524d\u7684<code>op.cpp</code>\u6587\u4ef6\u653e\u5728\u4e00\u8d77\u3002\u4e3a\u6b64\uff0c\u8ba9\u6211\u4eec\u5546\u5b9a\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u76ee\u5f55\u7ed3\u6784\uff1a</p> <pre><code>warp-perspective/\n  op.cpp\n  CMakeLists.txt\n\n</code></pre> <p>\u6b64\u5916\uff0c\u8bf7\u786e\u4fdd\u4ece pytorch.org \u83b7\u53d6\u6700\u65b0\u7248\u672c\u7684LibTorch\u53d1\u884c\u7248\uff0c\u8be5\u53d1\u884c\u7248\u5305\u542bPyTorch\u7684C++ \u5e93\u548c CMake \u6784\u5efa\u6587\u4ef6\u3002\u5c06\u89e3\u538b\u7f29\u7684\u5206\u53d1\u653e\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u53ef\u8bbf\u95ee\u7684\u4f4d\u7f6e\u3002\u4ee5\u4e0b\u6bb5\u843d\u5c06\u8be5\u4f4d\u7f6e\u79f0\u4e3a<code>/path/to/libtorch</code>\u3002\u6211\u4eec\u7684<code>CMakeLists.txt</code>\u6587\u4ef6\u7684\u5185\u5bb9\u5e94\u8be5\u5982\u4e0b\uff1a</p> <pre><code>cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(warp_perspective)\n\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\n\n# Define our library target\nadd_library(warp_perspective SHARED op.cpp)\n# Enable C++11\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\n# Link against LibTorch\ntarget_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\")\n# Link against OpenCV\ntarget_link_libraries(warp_perspective opencv_core opencv_imgproc)\n\n</code></pre> <p>\u8b66\u544a</p> <p>\u6b64\u8bbe\u7f6e\u5bf9\u6784\u5efa\u73af\u5883\u505a\u51fa\u4e00\u4e9b\u5047\u8bbe\uff0c\u7279\u522b\u662f\u4e0eOpenCV\u7684\u5b89\u88c5\u6709\u5173\u7684\u5185\u5bb9\u3002\u4e0a\u9762\u7684<code>CMakeLists.txt</code>\u6587\u4ef6\u5728\u8fd0\u884cUbuntu Xenial\u7684Docker\u5bb9\u5668\u5185\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7<code>apt</code>\u5b89\u88c5\u4e86<code>libopencv-dev</code>\u3002\u5982\u679c\u5b83\u4e0d\u9002\u5408\u60a8\u5e76\u4e14\u60a8\u611f\u5230\u5361\u4f4f\uff0c\u8bf7\u4f7f\u7528\u968f\u9644\u6559\u7a0b\u5e93\u4e2d\u7684<code>Dockerfile</code>\u6784\u5efa\u4e00\u4e2a\u9694\u79bb\u7684\uff0c\u53ef\u91cd\u73b0\u7684\u73af\u5883\uff0c\u5728\u5176\u4e2d\u4f7f\u7528\u672c\u6559\u7a0b\u4e2d\u7684\u4ee3\u7801\u3002\u5982\u679c\u60a8\u9047\u5230\u8fdb\u4e00\u6b65\u7684\u9ebb\u70e6\uff0c\u8bf7\u5728\u6559\u7a0b\u5e93\u4e2d\u63d0\u51fa\u95ee\u9898\u6216\u5728\u6211\u4eec\u7684\u8bba\u575b\u4e2d\u53d1\u5e16\u63d0\u95ee\u3002</p> <p>\u8981\u73b0\u5728\u6784\u5efa\u6211\u4eec\u7684\u8fd0\u7b97\u7b26\uff0c\u6211\u4eec\u53ef\u4ee5\u4ece<code>warp_perspective</code>\u6587\u4ef6\u5939\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o\n[100%] Linking CXX shared library libwarp_perspective.so\n[100%] Built target warp_perspective\n\n</code></pre> <p>\u8fd9\u5c06\u5728<code>build</code>\u6587\u4ef6\u5939\u4e2d\u653e\u7f6e\u4e00\u4e2a<code>libwarp_perspective.so</code>\u5171\u4eab\u5e93\u6587\u4ef6\u3002\u5728\u4e0a\u9762\u7684<code>cmake</code>\u547d\u4ee4\u4e2d\uff0c\u60a8\u5e94\u8be5\u5c06<code>/path/to/libtorch</code>\u66ff\u6362\u4e3a\u89e3\u538b\u7f29\u7684LibTorch\u5206\u53d1\u7684\u8def\u5f84\u3002</p> <p>\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u8be6\u7ec6\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u548c\u8c03\u7528\u6211\u4eec\u7684\u8fd0\u7b97\u7b26\uff0c\u4f46\u4e3a\u4e86\u5c3d\u65e9\u83b7\u5f97\u6210\u529f\u611f\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u5728Python\u4e2d\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.ops.load_library(\"/path/to/libwarp_perspective.so\")\n&gt;&gt;&gt; print(torch.ops.my_ops.warp_perspective)\n\n</code></pre> <p>\u8fd9\u91cc\uff0c<code>/path/to/libwarp_perspective.so</code>\u5e94\u8be5\u662f\u6211\u4eec\u521a\u521a\u6784\u5efa\u7684<code>libwarp_perspective.so</code>\u5171\u4eab\u5e93\u7684\u76f8\u5bf9\u8def\u5f84\u6216\u7edd\u5bf9\u8def\u5f84\u3002\u5982\u679c\u4e00\u5207\u987a\u5229\uff0c\u8fd9\u5e94\u8be5\u6253\u5370\u51fa\u7c7b\u4f3c\u7684\u4e1c\u897f</p> <pre><code>&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f618fc6fa50&gt;\n\n</code></pre> <p>\u8fd9\u662f\u6211\u4eec\u7a0d\u540e\u7528\u6765\u8c03\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684Python\u51fd\u6570\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#pythontorchscript","title":"\u5728Python\u4e2d\u4f7f\u7528TorchScript\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>\u4e00\u65e6\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5185\u7f6e\u5230\u5171\u4eab\u5e93\u4e2d\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5728Python\u7684TorchScript\u6a21\u578b\u4e2d\u4f7f\u7528\u6b64\u8fd0\u7b97\u7b26\u3002\u8fd9\u6709\u4e24\u4e2a\u90e8\u5206\uff1a\u9996\u5148\u5c06\u8fd0\u7b97\u7b26\u52a0\u8f7d\u5230Python\u4e2d\uff0c\u7136\u540e\u5728TorchScript\u4ee3\u7801\u4e2d\u4f7f\u7528\u8fd0\u7b97\u7b26\u3002</p> <p>\u60a8\u5df2\u7ecf\u4e86\u89e3\u4e86\u5982\u4f55\u5c06\u8fd0\u7b97\u7b26\u5bfc\u5165Python\uff1a<code>torch.ops.load_library()</code>\u3002\u6b64\u51fd\u6570\u83b7\u53d6\u5305\u542b\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u5171\u4eab\u5e93\u7684\u8def\u5f84\uff0c\u5e76\u5c06\u5176\u52a0\u8f7d\u5230\u5f53\u524d\u8fdb\u7a0b\u4e2d\u3002\u52a0\u8f7d\u5171\u4eab\u5e93\u8fd8\u5c06\u6267\u884c\u6211\u4eec\u653e\u5165\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5b9e\u73b0\u6587\u4ef6\u7684\u5168\u5c40<code>RegisterOperators</code>\u5bf9\u8c61\u7684\u6784\u9020\u51fd\u6570\u3002\u8fd9\u5c06\u4f7f\u7528TorchScript\u7f16\u8bd1\u5668\u6ce8\u518c\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\uff0c\u5e76\u5141\u8bb8\u6211\u4eec\u5728TorchScript\u4ee3\u7801\u4e2d\u4f7f\u7528\u8be5\u8fd0\u7b97\u7b26\u3002</p> <p>\u60a8\u53ef\u4ee5\u5c06\u52a0\u8f7d\u7684\u8fd0\u7b97\u7b26\u79f0\u4e3a<code>torch.ops.&amp;lt;namespace&amp;gt;.&amp;lt;function&amp;gt;</code>\uff0c\u5176\u4e2d<code>&amp;lt;namespace&amp;gt;</code>\u662f\u8fd0\u7b97\u7b26\u540d\u79f0\u7684\u540d\u79f0\u7a7a\u95f4\u90e8\u5206\uff0c<code>&amp;lt;function&amp;gt;</code>\u662f\u8fd0\u7b97\u7b26\u7684\u51fd\u6570\u540d\u79f0\u3002\u5bf9\u4e8e\u6211\u4eec\u4e0a\u9762\u5199\u7684\u8fd0\u7b97\u7b26\uff0c\u547d\u540d\u7a7a\u95f4\u662f<code>my_ops</code>\u548c\u51fd\u6570\u540d<code>warp_perspective</code>\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u7684\u8fd0\u7b97\u7b26\u53ef\u7528\u4f5c<code>torch.ops.my_ops.warp_perspective</code>\u3002\u867d\u7136\u6b64\u51fd\u6570\u53ef\u7528\u4e8e\u811a\u672c\u6216\u8ddf\u8e2a\u7684TorchScript\u6a21\u5757\uff0c\u4f46\u6211\u4eec\u4e5f\u53ef\u4ee5\u5728vanilla eager PyTorch\u4e2d\u4f7f\u7528\u5b83\u5e76\u5c06\u5176\u4f20\u9012\u7ed9\u5e38\u89c4\u7684PyTorch\u5f20\u91cf\uff1a</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.ops.load_library(\"libwarp_perspective.so\")\n&gt;&gt;&gt; torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3))\ntensor([[0.0000, 0.3218, 0.4611,  ..., 0.4636, 0.4636, 0.4636],\n [0.3746, 0.0978, 0.5005,  ..., 0.4636, 0.4636, 0.4636],\n [0.3245, 0.0169, 0.0000,  ..., 0.4458, 0.4458, 0.4458],\n ...,\n [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000]])\n\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5e55\u540e\u53d1\u751f\u7684\u4e8b\u60c5\u662f\uff0c\u7b2c\u4e00\u6b21\u5728Python\u4e2d\u8bbf\u95ee<code>torch.ops.namespace.function</code>\u65f6\uff0cTorchScript\u7f16\u8bd1\u5668(\u5728C ++\u7248\u672c\u4e2d\uff09\u5c06\u67e5\u770b\u662f\u5426\u5df2\u6ce8\u518c\u51fd\u6570<code>namespace::function</code>\uff0c\u5982\u679c\u5df2\u6ce8\u518c\uff0c\u5219\u8fd4\u56de\u6b64\u51fd\u6570\u7684Python\u53e5\u67c4\uff0c\u6211\u4eec\u968f\u540e\u53ef\u4ee5\u4f7f\u7528\u4ecePython\u8c03\u7528\u6211\u4eec\u7684C ++\u8fd0\u7b97\u7b26\u5b9e\u73b0\u3002\u8fd9\u662fTorchScript\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u548cC ++\u6269\u5c55\u4e4b\u95f4\u7684\u4e00\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u533a\u522b\uff1aC ++\u6269\u5c55\u4f7f\u7528pybind11\u624b\u52a8\u7ed1\u5b9a\uff0c\u800cTorchScript\u81ea\u5b9a\u4e49ops\u7531PyTorch\u672c\u8eab\u7ed1\u5b9a\u3002 Pybind11\u4e3a\u60a8\u63d0\u4f9b\u4e86\u66f4\u591a\u5173\u4e8e\u53ef\u4ee5\u7ed1\u5b9a\u5230Python\u7684\u7c7b\u578b\u548c\u7c7b\u7684\u7075\u6d3b\u6027\uff0c\u56e0\u6b64\u5efa\u8bae\u7528\u4e8e\u7eaf\u7cb9\u7684\u70ed\u5207\u4ee3\u7801\uff0c\u4f46TorchScript\u64cd\u4f5c\u4e0d\u652f\u6301\u5b83\u3002</p> <p>\u4ece\u8fd9\u91cc\u5f00\u59cb\uff0c\u60a8\u53ef\u4ee5\u5728\u811a\u672c\u6216\u8ddf\u8e2a\u4ee3\u7801\u4e2d\u4f7f\u7528\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\uff0c\u5c31\u50cf\u4f7f\u7528<code>torch</code>\u5305\u4e2d\u7684\u5176\u4ed6\u51fd\u6570\u4e00\u6837\u3002\u5b9e\u9645\u4e0a\uff0c\u50cf<code>torch.matmul</code>\u8fd9\u6837\u7684\u201c\u6807\u51c6\u5e93\u201d\u51fd\u6570\u4e0e\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u6ce8\u518c\u8def\u5f84\u5927\u81f4\u76f8\u540c\uff0c\u8fd9\u4f7f\u5f97\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5728TorchScript\u4e2d\u7684\u4f7f\u7528\u65b9\u5f0f\u548c\u4f4d\u7f6e\u65b9\u9762\u786e\u5b9e\u662f\u4e00\u6d41\u516c\u6c11\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#_2","title":"\u4f7f\u7528\u5e26\u6709\u8ddf\u8e2a\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>\u8ba9\u6211\u4eec\u9996\u5148\u5c06\u8fd0\u7b97\u7b26\u5d4c\u5165\u5230\u8ddf\u8e2a\u51fd\u6570\u4e2d\u3002\u56de\u60f3\u4e00\u4e0b\uff0c\u5bf9\u4e8e\u8ddf\u8e2a\uff0c\u6211\u4eec\u4ece\u4e00\u4e9b\u9999\u8349Pytorch\u4ee3\u7801\u5f00\u59cb\uff1a</p> <pre><code>def compute(x, y, z):\n    return x.matmul(y) + torch.relu(z)\n\n</code></pre> <p>\u7136\u540e\u5728\u5176\u4e0a\u8c03\u7528<code>torch.jit.trace</code>\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u4f20\u9012<code>torch.jit.trace</code>\u4e00\u4e9b\u793a\u4f8b\u8f93\u5165\uff0c\u5b83\u5c06\u8f6c\u53d1\u5230\u6211\u4eec\u7684\u5b9e\u73b0\uff0c\u4ee5\u8bb0\u5f55\u8f93\u5165\u6d41\u7ecf\u5b83\u65f6\u53d1\u751f\u7684\u64cd\u4f5c\u5e8f\u5217\u3002\u8fd9\u6837\u505a\u7684\u7ed3\u679c\u5b9e\u9645\u4e0a\u662f\u70ed\u5207\u7684PyTorch\u7a0b\u5e8f\u7684\u201c\u51bb\u7ed3\u201d\u7248\u672c\uff0cTorchScript\u7f16\u8bd1\u5668\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5206\u6790\uff0c\u4f18\u5316\u548c\u5e8f\u5217\u5316\uff1a</p> <pre><code>&gt;&gt;&gt; inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)]\n&gt;&gt;&gt; trace = torch.jit.trace(compute, inputs)\n&gt;&gt;&gt; print(trace.graph)\ngraph(%x : Float(4, 8)\n %y : Float(8, 5)\n %z : Float(4, 5)) {\n %3 : Float(4, 5) = aten::matmul(%x, %y)\n %4 : Float(4, 5) = aten::relu(%z)\n %5 : int = prim::Constant[value=1]()\n %6 : Float(4, 5) = aten::add(%3, %4, %5)\n return (%6);\n}\n\n</code></pre> <p>\u73b0\u5728\uff0c\u4ee4\u4eba\u5174\u594b\u7684\u542f\u793a\u662f\u6211\u4eec\u53ef\u4ee5\u7b80\u5355\u5730\u5c06\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u653e\u5165\u6211\u4eec\u7684PyTorch\u8ddf\u8e2a\u4e2d\uff0c\u5c31\u50cf\u5b83\u662f<code>torch.relu</code>\u6216\u4efb\u4f55\u5176\u4ed6<code>torch</code>\u51fd\u6570\u4e00\u6837\uff1a</p> <pre><code>torch.ops.load_library(\"libwarp_perspective.so\")\n\ndef compute(x, y, z):\n    x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n    return x.matmul(y) + torch.relu(z)\n\n</code></pre> <p>\u7136\u540e\u8ddf\u8e2a\u5b83\uff1a</p> <pre><code>&gt;&gt;&gt; inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)]\n&gt;&gt;&gt; trace = torch.jit.trace(compute, inputs)\n&gt;&gt;&gt; print(trace.graph)\ngraph(%x.1 : Float(4, 8)\n %y : Float(8, 5)\n %z : Float(8, 5)) {\n %3 : int = prim::Constant[value=3]()\n %4 : int = prim::Constant[value=6]()\n %5 : int = prim::Constant[value=0]()\n %6 : int[] = prim::Constant[value=[0, -1]]()\n %7 : Float(3, 3) = aten::eye(%3, %4, %5, %6)\n %x : Float(8, 8) = my_ops::warp_perspective(%x.1, %7)\n %11 : Float(8, 5) = aten::matmul(%x, %y)\n %12 : Float(8, 5) = aten::relu(%z)\n %13 : int = prim::Constant[value=1]()\n %14 : Float(8, 5) = aten::add(%11, %12, %13)\n return (%14);\n }\n\n</code></pre> <p>\u5c06TorchScript\u81ea\u5b9a\u4e49\u64cd\u4f5c\u96c6\u6210\u5230\u8ddf\u8e2a\u7684PyTorch\u4ee3\u7801\u5c31\u50cf\u8fd9\u6837\u7b80\u5355\uff01</p>"},{"location":"1.0/torch_script_custom_ops/#_3","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7b26\u548c\u811a\u672c","text":"<p>\u9664\u4e86\u8ddf\u8e2a\u4e4b\u5916\uff0c\u53e6\u4e00\u79cd\u83b7\u5f97PyTorch\u7a0b\u5e8f\u7684TorchScript\u8868\u793a\u7684\u65b9\u6cd5\u662f\u76f4\u63a5\u5728 TorchScript\u4e2d\u7f16\u5199\u4ee3\u7801_\u3002 TorchScript\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662fPython\u8bed\u8a00\u7684\u4e00\u4e2a\u5b50\u96c6\uff0c\u4f46\u6709\u4e00\u4e9b\u9650\u5236\u4f7f\u5f97TorchScript\u7f16\u8bd1\u5668\u66f4\u5bb9\u6613\u63a8\u7406\u7a0b\u5e8f\u3002\u901a\u8fc7\u4f7f\u7528<code>@torch.jit.script</code>\u4e3a\u81ea\u7531\u51fd\u6570\u548c<code>@torch.jit.script_method</code>\u4e3a\u7c7b\u4e2d\u7684\u65b9\u6cd5(\u5fc5\u987b\u4e5f\u4ece<code>torch.jit.ScriptModule</code>\u6d3e\u751f\uff09\u6ce8\u91ca\uff0c\u5c06\u5e38\u89c4PyTorch\u4ee3\u7801\u8f6c\u6362\u4e3aTorchScript\u3002\u6709\u5173TorchScript\u6ce8\u91ca\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6b64\u5904</p> <p>\u4f7f\u7528TorchScript\u800c\u4e0d\u662f\u8ddf\u8e2a\u7684\u4e00\u4e2a\u7279\u6b8a\u539f\u56e0\u662f\u8ddf\u8e2a\u65e0\u6cd5\u6355\u83b7PyTorch\u4ee3\u7801\u4e2d\u7684\u63a7\u5236\u6d41\u3002\u56e0\u6b64\uff0c\u8ba9\u6211\u4eec\u8003\u8651\u4e00\u4e0b\u8fd9\u4e2a\u4f7f\u7528\u63a7\u5236\u6d41\u7a0b\u7684\u529f\u80fd\uff1a</p> <pre><code>def compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z\n\n</code></pre> <p>\u8981\u5c06\u6b64\u51fd\u6570\u4ecevanilla PyTorch\u8f6c\u6362\u4e3aTorchScript\uff0c\u6211\u4eec\u4f7f\u7528<code>@torch.jit.script</code>\u5bf9\u5176\u8fdb\u884c\u6ce8\u91ca\uff1a</p> <pre><code>@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z\n\n</code></pre> <p>\u8fd9\u5c06\u53ca\u65f6\u5c06<code>compute</code>\u51fd\u6570\u7f16\u8bd1\u6210\u56fe\u8868\u793a\uff0c\u6211\u4eec\u53ef\u4ee5\u5728<code>compute.graph</code>\u5c5e\u6027\u4e2d\u68c0\u67e5\u5b83\uff1a</p> <pre><code>&gt;&gt;&gt; compute.graph\ngraph(%x : Dynamic\n %y : Dynamic) {\n %14 : int = prim::Constant[value=1]()\n %2 : int = prim::Constant[value=0]()\n %7 : int = prim::Constant[value=42]()\n %z.1 : int = prim::Constant[value=5]()\n %z.2 : int = prim::Constant[value=10]()\n %4 : Dynamic = aten::select(%x, %2, %2)\n %6 : Dynamic = aten::select(%4, %2, %2)\n %8 : Dynamic = aten::eq(%6, %7)\n %9 : bool = prim::TensorToBool(%8)\n %z : int = prim::If(%9)\n block0() {\n -&gt; (%z.1)\n }\n block1() {\n -&gt; (%z.2)\n }\n %13 : Dynamic = aten::matmul(%x, %y)\n %15 : Dynamic = aten::add(%13, %z, %14)\n return (%15);\n}\n\n</code></pre> <p>\u73b0\u5728\uff0c\u5c31\u50cf\u4ee5\u524d\u4e00\u6837\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\uff0c\u5c31\u50cf\u6211\u4eec\u811a\u672c\u4ee3\u7801\u4e2d\u7684\u4efb\u4f55\u5176\u4ed6\u51fd\u6570\u4e00\u6837\uff1a</p> <pre><code>torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\n</code></pre> <p>\u5f53TorchScript\u7f16\u8bd1\u5668\u770b\u5230\u5bf9 <code>torch.ops.my_ops.warp_perspective</code> \u7684\u5f15\u7528\u65f6\uff0c\u5b83\u5c06\u627e\u5230\u6211\u4eec\u901a\u8fc7C++ \u4e2d\u7684 <code>RegisterOperators</code> \u5bf9\u8c61\u6ce8\u518c\u7684\u5b9e\u73b0\uff0c\u5e76\u5c06\u5176\u7f16\u8bd1\u4e3a\u5176\u56fe\u8868\u793a\uff1a</p> <pre><code>&gt;&gt;&gt; compute.graph\ngraph(%x.1 : Dynamic\n %y : Dynamic) {\n %20 : int = prim::Constant[value=1]()\n %16 : int[] = prim::Constant[value=[0, -1]]()\n %14 : int = prim::Constant[value=6]()\n %2 : int = prim::Constant[value=0]()\n %7 : int = prim::Constant[value=42]()\n %z.1 : int = prim::Constant[value=5]()\n %z.2 : int = prim::Constant[value=10]()\n %13 : int = prim::Constant[value=3]()\n %4 : Dynamic = aten::select(%x.1, %2, %2)\n %6 : Dynamic = aten::select(%4, %2, %2)\n %8 : Dynamic = aten::eq(%6, %7)\n %9 : bool = prim::TensorToBool(%8)\n %z : int = prim::If(%9)\n block0() {\n -&gt; (%z.1)\n }\n block1() {\n -&gt; (%z.2)\n }\n %17 : Dynamic = aten::eye(%13, %14, %2, %16)\n %x : Dynamic = my_ops::warp_perspective(%x.1, %17)\n %19 : Dynamic = aten::matmul(%x, %y)\n %21 : Dynamic = aten::add(%19, %z, %20)\n return (%21);\n }\n\n</code></pre> <p>\u8bf7\u7279\u522b\u6ce8\u610f\u56fe\u8868\u672b\u5c3e\u5bf9<code>my_ops::warp_perspective</code>\u7684\u5f15\u7528\u3002</p> <p>\u6ce8\u610f TorchScript\u56fe\u8868\u8868\u793a\u4ecd\u6709\u53ef\u80fd\u53d1\u751f\u53d8\u5316\u3002\u4e0d\u8981\u4f9d\u8d56\u5b83\u770b\u8d77\u6765\u50cf\u8fd9\u6837\u3002</p> <p>\u5f53\u5728Python\u4e2d\u4f7f\u7528\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u65f6\uff0c\u8fd9\u5c31\u662f\u5b83\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u60a8\u4f7f\u7528<code>torch.ops.load_library</code>\u5bfc\u5165\u5305\u542b\u64cd\u4f5c\u7b26\u7684\u5e93\uff0c\u5e76\u50cf\u8ddf\u8e2a\u6216\u811a\u672c\u5316\u7684TorchScript\u4ee3\u7801\u4e00\u6837\u8c03\u7528\u60a8\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u5c31\u50cf\u4efb\u4f55\u5176\u4ed6<code>torch</code>\u64cd\u4f5c\u7b26\u4e00\u6837\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#c-torchscript","title":"\u5728C ++\u4e2d\u4f7f\u7528TorchScript\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26","text":"<p>TorchScript\u7684\u4e00\u4e2a\u6709\u7528\u529f\u80fd\u662f\u80fd\u591f\u5c06\u6a21\u578b\u5e8f\u5217\u5316\u4e3a\u78c1\u76d8\u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u53ef\u4ee5\u901a\u8fc7\u7ebf\u8def\u53d1\u9001\uff0c\u5b58\u50a8\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u66f4\u91cd\u8981\u7684\u662f\uff0c\u53ef\u4ee5\u52a8\u6001\u53cd\u5e8f\u5217\u5316\u548c\u6267\u884c\uff0c\u800c\u65e0\u9700\u4fdd\u7559\u539f\u59cb\u6e90\u4ee3\u7801\u3002\u8fd9\u53ef\u4ee5\u5728Python\u4e2d\u5b9e\u73b0\uff0c\u4e5f\u53ef\u4ee5\u5728C ++\u4e2d\u5b9e\u73b0\u3002\u4e3a\u6b64\uff0cPyTorch\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7eafC ++ API \uff0c\u7528\u4e8e\u53cd\u5e8f\u5217\u5316\u4ee5\u53ca\u6267\u884cTorchScript\u6a21\u578b\u3002\u5982\u679c\u60a8\u8fd8\u6ca1\u6709\uff0c\u8bf7\u9605\u8bfb\u5173\u4e8e\u5728C ++ \u4e2d\u52a0\u8f7d\u548c\u8fd0\u884c\u5e8f\u5217\u5316TorchScript\u6a21\u578b\u7684\u6559\u7a0b\uff0c\u63a5\u4e0b\u6765\u7684\u51e0\u6bb5\u5c06\u6784\u5efa\u3002</p> <p>\u7b80\u800c\u8a00\u4e4b\uff0c\u5373\u4f7f\u4ece\u6587\u4ef6\u53cd\u5e8f\u5217\u5316\u5e76\u5728C ++\u4e2d\u8fd0\u884c\uff0c\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u4e5f\u53ef\u4ee5\u50cf\u5e38\u89c4<code>torch</code>\u8fd0\u7b97\u7b26\u4e00\u6837\u6267\u884c\u3002\u5bf9\u6b64\u7684\u552f\u4e00\u8981\u6c42\u662f\u5c06\u6211\u4eec\u4e4b\u524d\u6784\u5efa\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7b26\u5171\u4eab\u5e93\u4e0e\u6211\u4eec\u6267\u884c\u6a21\u578b\u7684C ++\u5e94\u7528\u7a0b\u5e8f\u94fe\u63a5\u8d77\u6765\u3002\u5728Python\u4e2d\uff0c\u8fd9\u53ea\u662f\u8c03\u7528<code>torch.ops.load_library</code>\u3002\u5728C ++\u4e2d\uff0c\u60a8\u9700\u8981\u5728\u60a8\u4f7f\u7528\u7684\u4efb\u4f55\u6784\u5efa\u7cfb\u7edf\u4e2d\u5c06\u5171\u4eab\u5e93\u4e0e\u4e3b\u5e94\u7528\u7a0b\u5e8f\u94fe\u63a5\u3002\u4ee5\u4e0b\u793a\u4f8b\u5c06\u4f7f\u7528CMake\u5c55\u793a\u6b64\u5185\u5bb9\u3002</p> <p>\u6ce8\u610f</p> <p>\u4ece\u6280\u672f\u4e0a\u8bb2\uff0c\u60a8\u4e5f\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u5c06\u5171\u4eab\u5e93\u52a8\u6001\u52a0\u8f7d\u5230C ++\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u5c31\u50cf\u5728Python\u4e2d\u4e00\u6837\u3002\u5728Linux\u4e0a\uff0c\u4f60\u53ef\u4ee5\u7528dlopen \u6765\u505a\u5230\u8fd9\u4e00\u70b9\u3002\u5728\u5176\u4ed6\u5e73\u53f0\u4e0a\u5b58\u5728\u7b49\u4ef7\u7269\u3002</p> <p>\u5728\u4e0a\u9762\u94fe\u63a5\u7684C ++\u6267\u884c\u6559\u7a0b\u7684\u57fa\u7840\u4e0a\uff0c\u8ba9\u6211\u4eec\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u7684\u6700\u5c0fC ++\u5e94\u7528\u7a0b\u5e8f\u5f00\u59cb\uff0c<code>main.cpp</code>\u5728\u6211\u4eec\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7b26\u7684\u4e0d\u540c\u6587\u4ef6\u5939\u4e2d\uff0c\u52a0\u8f7d\u5e76\u6267\u884c\u5e8f\u5217\u5316\u7684TorchScript\u6a21\u578b\uff1a</p> <pre><code>#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n//\u4f7f\u7528torch::jit::load()\u4ece\u6587\u4ef6\u53cd\u5e8f\u5217\u5316ScriptModule\u3002\n  std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]);\n  std::vector&lt;torch::jit::IValue&gt; inputs;\n  inputs.push_back(torch::randn({4, 8}));\n  inputs.push_back(torch::randn({8, 5}));\n\n  torch::Tensor output = module-&gt;forward(std::move(inputs)).toTensor();\n\n  std::cout &lt;&lt; output &lt;&lt; std::endl;\n}\n\n</code></pre> <p>\u8fde\u540c\u4e00\u4e2a\u5c0f\u7684<code>CMakeLists.txt</code>\u6587\u4ef6\uff1a</p> <pre><code>cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_compile_features(example_app PRIVATE cxx_range_for)\n\n</code></pre> <p>\u6b64\u65f6\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u591f\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\uff1a</p> <pre><code>$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app\n\n</code></pre> <p>\u5e76\u4e14\u5728\u6ca1\u6709\u901a\u8fc7\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\u5b83\uff1a</p> <pre><code>$ ./example_app\nusage: example_app &lt;path-to-exported-script-module&gt;\n\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u5e8f\u5217\u5316\u6211\u4eec\u4e4b\u524d\u7f16\u5199\u7684\u4f7f\u7528\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u811a\u672c\u51fd\u6570\uff1a</p> <pre><code>torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\ncompute.save(\"example.pt\")\n\n</code></pre> <p>\u6700\u540e\u4e00\u884c\u5c06\u811a\u672c\u51fd\u6570\u5e8f\u5217\u5316\u4e3a\u540d\u4e3a\u201cexample.pt\u201d\u7684\u6587\u4ef6\u3002\u5982\u679c\u6211\u4eec\u5c06\u8fd9\u4e2a\u5e8f\u5217\u5316\u6a21\u578b\u4f20\u9012\u7ed9\u6211\u4eec\u7684C ++\u5e94\u7528\u7a0b\u5e8f\uff0c\u6211\u4eec\u53ef\u4ee5\u7acb\u5373\u8fd0\u884c\u5b83\uff1a</p> <pre><code>$ ./example_app example.pt\nterminate called after throwing an instance of 'torch::jit::script::ErrorReport'\nwhat():\nSchema not found for node. File a bug report.\nNode: %16 : Dynamic = my_ops::warp_perspective(%0, %19)\n\n</code></pre> <p>\u6216\u8005\u53ef\u80fd\u4e0d\u662f\u3002\u4e5f\u8bb8\u8fd8\u6ca1\u6709\u3002\u5f53\u7136\uff01\u6211\u4eec\u5c1a\u672a\u5c06\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5e93\u4e0e\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u76f8\u5173\u8054\u3002\u6211\u4eec\u73b0\u5728\u5c31\u8fd9\u6837\u505a\uff0c\u4e3a\u4e86\u6b63\u786e\u5730\u6267\u884c\u6b64\u64cd\u4f5c\uff0c\u8ba9\u6211\u4eec\u7a0d\u5fae\u66f4\u65b0\u4e00\u4e0b\u6211\u4eec\u7684\u6587\u4ef6\u7ec4\u7ec7\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>example_app/\n  CMakeLists.txt\n  main.cpp\n  warp_perspective/\n    CMakeLists.txt\n    op.cpp\n\n</code></pre> <p>\u8fd9\u5c06\u5141\u8bb8\u6211\u4eec\u5c06<code>warp_perspective</code>\u5e93CMake\u76ee\u6807\u6dfb\u52a0\u4e3a\u6211\u4eec\u7684\u5e94\u7528\u7a0b\u5e8f\u76ee\u6807\u7684\u5b50\u76ee\u5f55\u3002 <code>example_app</code>\u6587\u4ef6\u5939\u4e2d\u7684\u6700\u4e0a\u5c42<code>CMakeLists.txt</code>\u5e94\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(warp_perspective)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(example_app -Wl,--no-as-needed warp_perspective)\ntarget_compile_features(example_app PRIVATE cxx_range_for)\n\n</code></pre> <p>\u8fd9\u4e2a\u57fa\u672c\u7684CMake\u914d\u7f6e\u770b\u8d77\u6765\u5f88\u50cf\u4ee5\u524d\uff0c\u9664\u4e86\u6211\u4eec\u5c06<code>warp_perspective</code> CMake\u6784\u5efa\u6dfb\u52a0\u4e3a\u5b50\u76ee\u5f55\u3002\u4e00\u65e6\u5176CMake\u4ee3\u7801\u8fd0\u884c\uff0c\u6211\u4eec\u5c06<code>example_app</code>\u5e94\u7528\u7a0b\u5e8f\u4e0e<code>warp_perspective</code>\u5171\u4eab\u5e93\u94fe\u63a5\u3002</p> <p>\u6ce8\u610f</p> <p>\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\u5d4c\u5165\u4e86\u4e00\u4e2a\u5173\u952e\u7ec6\u8282\uff1a<code>warp_perspective</code>\u94fe\u63a5\u7ebf\u7684<code>-Wl,--no-as-needed</code>\u524d\u7f00\u3002\u8fd9\u662f\u5fc5\u9700\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u5b9e\u9645\u4e0a\u4e0d\u4f1a\u5728\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u4e2d\u4ece<code>warp_perspective</code>\u5171\u4eab\u5e93\u4e2d\u8c03\u7528\u4efb\u4f55\u51fd\u6570\u3002\u6211\u4eec\u53ea\u9700\u8981\u8fd0\u884c\u5168\u5c40<code>RegisterOperators</code>\u5bf9\u8c61\u7684\u6784\u9020\u51fd\u6570\u3002\u4e0d\u65b9\u4fbf\u7684\u662f\uff0c\u8fd9\u4f1a\u4f7f\u94fe\u63a5\u5668\u6df7\u4e71\u5e76\u4f7f\u5176\u8ba4\u4e3a\u5b83\u53ef\u4ee5\u5b8c\u5168\u8df3\u8fc7\u4e0e\u5e93\u7684\u94fe\u63a5\u3002\u5728Linux\u4e0a\uff0c<code>-Wl,--no-as-needed</code>\u6807\u5fd7\u5f3a\u5236\u94fe\u63a5\u53d1\u751f(\u6ce8\u610f\uff1a\u6b64\u6807\u5fd7\u7279\u5b9a\u4e8eLinux\uff01\uff09\u3002\u8fd8\u6709\u5176\u4ed6\u89e3\u51b3\u65b9\u6cd5\u3002\u6700\u7b80\u5355\u7684\u662f\u5728\u8fd0\u7b97\u7b26\u5e93\u4e2d\u5b9a\u4e49_\u67d0\u4e9b\u51fd\u6570_\uff0c\u60a8\u9700\u8981\u4ece\u4e3b\u5e94\u7528\u7a0b\u5e8f\u8c03\u7528\u5b83\u3002\u8fd9\u53ef\u4ee5\u50cf\u5728\u67d0\u4e2a\u5934\u4e2d\u58f0\u660e\u7684\u51fd\u6570<code>void init();</code>\u4e00\u6837\u7b80\u5355\uff0c\u7136\u540e\u5728\u8fd0\u7b97\u7b26\u5e93\u4e2d\u5c06\u5176\u5b9a\u4e49\u4e3a<code>void init() { }</code>\u3002\u5728\u4e3b\u5e94\u7528\u7a0b\u5e8f\u4e2d\u8c03\u7528\u6b64<code>init()</code>\u51fd\u6570\u5c06\u4f7f\u94fe\u63a5\u5668\u611f\u89c9\u8fd9\u662f\u4e00\u4e2a\u503c\u5f97\u94fe\u63a5\u7684\u5e93\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u8d85\u51fa\u4e86\u6211\u4eec\u7684\u63a7\u5236\u8303\u56f4\uff0c\u6211\u4eec\u5b81\u613f\u8ba9\u60a8\u77e5\u9053\u539f\u56e0\u548c\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u5c06\u4e00\u4e9b\u4e0d\u900f\u660e\u7684\u5b8f\u4ea4\u7ed9\u60a8\u7684\u4ee3\u7801\u4e2d\u7684plop\u3002</p> <p>\u73b0\u5728\uff0c\u7531\u4e8e\u6211\u4eec\u73b0\u5728\u5728\u6700\u4e0a\u5c42\u627e\u5230<code>Torch</code>\u5305\uff0c<code>warp_perspective</code>\u5b50\u76ee\u5f55\u4e2d\u7684<code>CMakeLists.txt</code>\u6587\u4ef6\u53ef\u4ee5\u7f29\u77ed\u4e00\u70b9\u3002\u5b83\u5e94\u8be5\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)\n\n</code></pre> <p>\u8ba9\u6211\u4eec\u91cd\u65b0\u6784\u5efa\u6211\u4eec\u7684\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u5b83\u8fd8\u5c06\u4e0e\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u5e93\u94fe\u63a5\u3002\u5728\u6700\u4e0a\u5c42 <code>example_app</code> \u76ee\u5f55\u4e2d\uff1a</p> <pre><code>$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app\n\n</code></pre> <p>\u5982\u679c\u6211\u4eec\u73b0\u5728\u8fd0\u884c<code>example_app</code>\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u4ea4\u7ed9\u6211\u4eec\u7684\u5e8f\u5217\u5316\u6a21\u578b\uff0c\u6211\u4eec\u5e94\u8be5\u5f97\u5230\u4e00\u4e2a\u5706\u6ee1\u7684\u7ed3\u5c40\uff1a</p> <pre><code>$ ./example_app example.pt\n11.4125   5.8262   9.5345   8.6111  12.3997\n 7.4683  13.5969   9.0850  11.0698   9.4008\n 7.4597  15.0926  12.5727   8.9319   9.0666\n 9.4834  11.1747   9.0162  10.9521   8.6269\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n[ Variable[CPUFloatType]{8,5} ]\n\n</code></pre> <p>\u6210\u529f\uff01\u4f60\u73b0\u5728\u51c6\u5907\u63a8\u65ad\u4e86\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#_4","title":"\u7ed3\u8bba","text":"<p>\u672c\u6559\u7a0b\u5411\u60a8\u4ecb\u7ecd\u4e86\u5982\u4f55\u5728C ++\u4e2d\u5b9e\u73b0\u81ea\u5b9a\u4e49TorchScript\u8fd0\u7b97\u7b26\uff0c\u5982\u4f55\u5c06\u5176\u6784\u5efa\u5230\u5171\u4eab\u5e93\u4e2d\uff0c\u5982\u4f55\u5728Python\u4e2d\u4f7f\u7528\u5b83\u6765\u5b9a\u4e49TorchScript\u6a21\u578b\uff0c\u4ee5\u53ca\u6700\u540e\u5982\u4f55\u5c06\u5176\u52a0\u8f7d\u5230C ++\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4ee5\u8fdb\u884c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u3002\u73b0\u5728\uff0c\u60a8\u5df2\u51c6\u5907\u597d\u4f7f\u7528\u4e0e\u7b2c\u4e09\u65b9C ++\u5e93\u8fde\u63a5\u7684C ++\u8fd0\u7b97\u7b26\u6269\u5c55TorchScript\u6a21\u578b\uff0c\u7f16\u5199\u81ea\u5b9a\u4e49\u9ad8\u6027\u80fdCUDA\u5185\u6838\uff0c\u6216\u5b9e\u73b0\u9700\u8981Python\uff0cTorchScript\u548cC ++\u4e4b\u95f4\u7684\u754c\u7ebf\u5e73\u6ed1\u6df7\u5408\u7684\u4efb\u4f55\u5176\u4ed6\u7528\u4f8b\u3002</p> <p>\u4e0e\u5f80\u5e38\u4e00\u6837\uff0c\u5982\u679c\u60a8\u9047\u5230\u4efb\u4f55\u95ee\u9898\u6216\u6709\u7591\u95ee\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684\u8bba\u575b\u6216 GitHub\u95ee\u9898\u53d6\u5f97\u8054\u7cfb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u5e38\u89c1\u95ee\u9898\u89e3\u7b54(FAQ\uff09\u9875\u9762\u53ef\u80fd\u4f1a\u63d0\u4f9b\u6709\u7528\u7684\u4fe1\u606f\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#a","title":"\u9644\u5f55A\uff1a\u6784\u5efa\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u66f4\u591a\u65b9\u6cd5","text":"<p>\u201c\u6784\u5efa\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u201d\u90e8\u5206\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528CMake\u5c06\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u6784\u5efa\u5230\u5171\u4eab\u5e93\u4e2d\u3002\u672c\u9644\u5f55\u6982\u8ff0\u4e86\u53e6\u5916\u4e24\u79cd\u7f16\u8bd1\u65b9\u6cd5\u3002\u5b83\u4eec\u90fd\u4f7f\u7528Python\u4f5c\u4e3a\u7f16\u8bd1\u8fc7\u7a0b\u7684\u201c\u9a71\u52a8\u7a0b\u5e8f\u201d\u6216\u201c\u63a5\u53e3\u201d\u3002\u6b64\u5916\uff0c\u4e24\u8005\u90fd\u91cd\u590d\u4f7f\u7528\uff0c\u8fd9\u662f\u76f8\u5f53\u4e8eTorchScript\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u9999\u8349(\u6e34\u671b\uff09PyTorch\uff0c\u5b83\u4f9d\u8d56\u4e8e pybind11 \u6765\u5b9e\u73b0\u4eceC ++\u5230Python\u7684\u201c\u663e\u5f0f\u201d\u7ed1\u5b9a\u3002</p> <p>\u7b2c\u4e00\u79cd\u65b9\u6cd5\u4f7f\u7528C ++\u6269\u5c55'\u65b9\u4fbf\u7684\u5373\u65f6(JIT\uff09\u7f16\u8bd1\u63a5\u53e3\uff0c\u5728\u7b2c\u4e00\u6b21\u8fd0\u884c\u65f6\u5728PyTorch\u811a\u672c\u7684\u540e\u53f0\u7f16\u8bd1\u4ee3\u7801\u3002\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53e4\u8001\u7684<code>setuptools</code>\u5305\uff0c\u5e76\u6d89\u53ca\u7f16\u5199\u5355\u72ec\u7684<code>setup.py</code>\u6587\u4ef6\u3002\u8fd9\u5141\u8bb8\u66f4\u9ad8\u7ea7\u7684\u914d\u7f6e\u4ee5\u53ca\u4e0e\u5176\u4ed6\u57fa\u4e8e<code>setuptools</code>\u7684\u9879\u76ee\u7684\u96c6\u6210\u3002\u6211\u4eec\u5c06\u5728\u4e0b\u9762\u8be6\u7ec6\u63a2\u8ba8\u8fd9\u4e24\u79cd\u65b9\u6cd5\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#jit","title":"\u4f7f\u7528JIT\u7f16\u8bd1\u6784\u5efa","text":"<p>PyTorch C ++\u6269\u5c55\u5de5\u5177\u5305\u63d0\u4f9b\u7684JIT\u7f16\u8bd1\u529f\u80fd\u5141\u8bb8\u5c06\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u7f16\u8bd1\u76f4\u63a5\u5d4c\u5165\u5230Python\u4ee3\u7801\u4e2d\uff0c\u4f8b\u5982\uff1a\u5728\u8bad\u7ec3\u811a\u672c\u7684\u9876\u90e8\u3002</p> <p>\u6ce8\u610f</p> <p>\u8fd9\u91cc\u7684\u201cJIT\u7f16\u8bd1\u201d\u4e0eTorchScript\u7f16\u8bd1\u5668\u4e2d\u7684JIT\u7f16\u8bd1\u65e0\u5173\uff0c\u4ee5\u4f18\u5316\u60a8\u7684\u7a0b\u5e8f\u3002\u5b83\u53ea\u662f\u610f\u5473\u7740\u60a8\u7684\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7b26C ++\u4ee3\u7801\u5c06\u5728\u60a8\u7b2c\u4e00\u6b21\u5bfc\u5165\u65f6\u5728\u7cfb\u7edf\u7684<code>/tmp</code>\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u5939\u4e2d\u7f16\u8bd1\uff0c\u5c31\u50cf\u60a8\u4e8b\u5148\u5df2\u7ecf\u81ea\u5df1\u7f16\u8bd1\u5b83\u4e00\u6837\u3002</p> <p>\u8fd9\u4e2aJIT\u7f16\u8bd1\u529f\u80fd\u6709\u4e24\u79cd\u5f62\u5f0f\u3002\u9996\u5148\uff0c\u60a8\u4ecd\u7136\u5c06\u8fd0\u7b97\u7b26\u5b9e\u73b0\u4fdd\u5b58\u5728\u5355\u72ec\u7684\u6587\u4ef6(<code>op.cpp</code>\uff09\u4e2d\uff0c\u7136\u540e\u4f7f\u7528<code>torch.utils.cpp_extension.load()</code>\u7f16\u8bd1\u6269\u5c55\u3002\u901a\u5e38\uff0c\u6b64\u51fd\u6570\u5c06\u8fd4\u56de\u516c\u5f00C ++\u6269\u5c55\u7684Python\u6a21\u5757\u3002\u4f46\u662f\uff0c\u7531\u4e8e\u6211\u4eec\u6ca1\u6709\u5c06\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7f16\u8bd1\u5230\u81ea\u5df1\u7684Python\u6a21\u5757\u4e2d\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u60f3\u7f16\u8bd1\u666e\u901a\u7684\u5171\u4eab\u5e93\u3002\u5e78\u8fd0\u7684\u662f\uff0c<code>torch.utils.cpp_extension.load()</code>\u6709\u4e00\u4e2a\u53c2\u6570<code>is_python_module</code>\u6211\u4eec\u53ef\u4ee5\u8bbe\u7f6e\u4e3a<code>False</code>\u6765\u8868\u793a\u6211\u4eec\u53ea\u5bf9\u6784\u5efa\u5171\u4eab\u5e93\u800c\u4e0d\u662fPython\u6a21\u5757\u611f\u5174\u8da3\u3002\u7136\u540e<code>torch.utils.cpp_extension.load()</code>\u5c06\u7f16\u8bd1\u5e76\u5c06\u5171\u4eab\u5e93\u52a0\u8f7d\u5230\u5f53\u524d\u8fdb\u7a0b\u4e2d\uff0c\u5c31\u50cf\u4e4b\u524d<code>torch.ops.load_library</code>\u6240\u505a\u7684\u90a3\u6837\uff1a</p> <pre><code>import torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)\n\n</code></pre> <p>\u8fd9\u5e94\u8be5\u5927\u81f4\u6253\u5370\uff1a</p> <pre><code>&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f3e0f840b10&gt;\n\n</code></pre> <p>JIT\u7f16\u8bd1\u7684\u7b2c\u4e8c\u79cd\u98ce\u683c\u5141\u8bb8\u60a8\u5c06\u81ea\u5b9a\u4e49TorchScript\u8fd0\u7b97\u7b26\u7684\u6e90\u4ee3\u7801\u4f5c\u4e3a\u5b57\u7b26\u4e32\u4f20\u9012\u3002\u4e3a\u6b64\uff0c\u8bf7\u4f7f\u7528<code>torch.utils.cpp_extension.load_inline</code>\uff1a</p> <pre><code>import torch\nimport torch.utils.cpp_extension\n\nop_source = \"\"\"\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;torch/script.h&gt;\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n cv::Mat image_mat(/*rows=*/image.size(0),\n /*cols=*/image.size(1),\n /*type=*/CV_32FC1,\n /*data=*/image.data&lt;float&gt;());\n cv::Mat warp_mat(/*rows=*/warp.size(0),\n /*cols=*/warp.size(1),\n /*type=*/CV_32FC1,\n /*data=*/warp.data&lt;float&gt;());\n\n cv::Mat output_mat;\n cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});\n\n torch::Tensor output =\n torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{64, 64});\n return output.clone();\n}\n\nstatic auto registry =\n torch::jit::RegisterOperators(\"my_ops::warp_perspective\", &amp;warp_perspective);\n\"\"\"\ntorch.utils.cpp_extension.load_inline(\n    name=\"warp_perspective\",\n    cpp_sources=op_source,\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True,\n)\n\nprint(torch.ops.my_ops.warp_perspective)\n\n</code></pre> <p>\u5f53\u7136\uff0c\u5982\u679c\u6e90\u4ee3\u7801\u76f8\u5f53\u77ed\uff0c\u6700\u597d\u53ea\u4f7f\u7528<code>torch.utils.cpp_extension.load_inline</code>\u3002</p>"},{"location":"1.0/torch_script_custom_ops/#setuptools","title":"\u4f7f\u7528Setuptools\u6784\u5efa","text":"<p>\u4ecePython\u72ec\u5bb6\u6784\u5efa\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u7684\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662f\u4f7f\u7528<code>setuptools</code>\u3002\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f<code>setuptools</code>\u5177\u6709\u975e\u5e38\u5f3a\u5927\u548c\u5e7f\u6cdb\u7684\u63a5\u53e3\uff0c\u7528\u4e8e\u6784\u5efa\u7528C ++\u7f16\u5199\u7684Python\u6a21\u5757\u3002\u4f46\u662f\uff0c\u7531\u4e8e<code>setuptools</code>\u5b9e\u9645\u4e0a\u662f\u7528\u4e8e\u6784\u5efaPython\u6a21\u5757\u800c\u4e0d\u662f\u666e\u901a\u7684\u5171\u4eab\u5e93(\u5b83\u6ca1\u6709Python\u671f\u671b\u4ece\u6a21\u5757\u4e2d\u83b7\u5f97\u7684\u5fc5\u8981\u5165\u53e3\u70b9\uff09\uff0c\u56e0\u6b64\u8fd9\u6761\u8def\u7ebf\u53ef\u80fd\u6709\u70b9\u53e4\u602a\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u4f60\u53ea\u9700\u8981\u4e00\u4e2a<code>setup.py</code>\u6587\u4ef6\u4ee3\u66ff<code>CMakeLists.txt</code>\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <p>\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u5728\u5e95\u90e8\u7684<code>BuildExtension</code>\u4e2d\u542f\u7528\u4e86<code>no_python_abi_suffix</code>\u9009\u9879\u3002\u8fd9\u6307\u793a<code>setuptools</code>\u5728\u751f\u6210\u7684\u5171\u4eab\u5e93\u7684\u540d\u79f0\u4e2d\u7701\u7565\u4efb\u4f55Python-3\u7279\u5b9a\u7684ABI\u540e\u7f00\u3002\u5426\u5219\uff0c\u5728Python 3.7\u4e0a\uff0c\u4f8b\u5982\uff0c\u5e93\u53ef\u80fd\u88ab\u79f0\u4e3a<code>warp_perspective.cpython-37m-x86_64-linux-gnu.so</code>\uff0c\u5176\u4e2d<code>cpython-37m-x86_64-linux-gnu</code>\u662fABI\u6807\u7b7e\uff0c\u4f46\u6211\u4eec\u771f\u7684\u53ea\u662f\u60f3\u8981\u5b83\u88ab\u79f0\u4e3a<code>warp_perspective.so</code></p> <p>\u5982\u679c\u6211\u4eec\u73b0\u5728\u5728<code>setup.py</code>\u6240\u5728\u7684\u6587\u4ef6\u5939\u4e2d\u7684\u7ec8\u7aef\u4e2d\u8fd0\u884c<code>python setup.py build develop</code>\uff0c\u6211\u4eec\u5e94\u8be5\u770b\u5230\u7c7b\u4f3c\u7684\u5185\u5bb9\uff1a</p> <pre><code>$ python setup.py build develop\nrunning build\nrunning build_ext\nbuilding 'warp_perspective' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\ncc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so\nrunning develop\nrunning egg_info\ncreating warp_perspective.egg-info\nwriting warp_perspective.egg-info/PKG-INFO\nwriting dependency_links to warp_perspective.egg-info/dependency_links.txt\nwriting top-level names to warp_perspective.egg-info/top_level.txt\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nreading manifest file 'warp_perspective.egg-info/SOURCES.txt'\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nrunning build_ext\ncopying build/lib.linux-x86_64-3.7/warp_perspective.so -&gt;\nCreating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .)\nAdding warp-perspective 0.0.0 to easy-install.pth file\n\nInstalled /warp_perspective\nProcessing dependencies for warp-perspective==0.0.0\nFinished processing dependencies for warp-perspective==0.0.0\n\n</code></pre> <p>\u8fd9\u5c06\u751f\u6210\u4e00\u4e2a\u540d\u4e3a<code>warp_perspective.so</code>\u7684\u5171\u4eab\u5e93\uff0c\u6211\u4eec\u53ef\u4ee5\u50cf\u4e4b\u524d\u90a3\u6837\u5c06\u5176\u4f20\u9012\u7ed9<code>torch.ops.load_library</code>\uff0c\u4ee5\u4f7f\u6211\u4eec\u7684\u64cd\u4f5c\u7b26\u5bf9TorchScript\u53ef\u89c1\uff1a</p>"},{"location":"1.0/torch_serialization_parallelism_utilities/","title":"Serialization, Parallelism, Utilities","text":""},{"location":"1.0/torch_serialization_parallelism_utilities/#_1","title":"\u5e8f\u5217\u5316","text":"<p>\u8bd1\u8005\uff1aApacheCN</p> <pre><code>torch.save(obj, f, pickle_module=&lt;module 'pickle' from '/scratch/rzou/pt/release-env/lib/python3.7/pickle.py'&gt;, pickle_protocol=2)\n</code></pre> <p>\u5c06\u5bf9\u8c61\u4fdd\u5b58\u5230\u78c1\u76d8\u6587\u4ef6\u3002</p> <p>\u53e6\u8bf7\u53c2\u9605\uff1a\u4fdd\u5b58\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>obj - \u4fdd\u5b58\u5bf9\u8c61</li> <li>f - \u7c7b\u4f3c\u6587\u4ef6\u7684\u5bf9\u8c61(\u5fc5\u987b\u5b9e\u73b0\u5199\u5165\u548c\u5237\u65b0\uff09\u6216\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32</li> <li>pickle_module - \u7528\u4e8e\u814c\u5236\u5143\u6570\u636e\u548c\u5bf9\u8c61\u7684\u6a21\u5757</li> <li>pickle_protocol - \u53ef\u4ee5\u6307\u5b9a\u8986\u76d6\u9ed8\u8ba4\u534f\u8bae</li> </ul> <p>\u8b66\u544a</p> <p>\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662fPython 2\uff0c\u5219torch.save\u4e0d\u652f\u6301\u5c06StringIO.StringIO\u4f5c\u4e3a\u6709\u6548\u7684\u7c7b\u6587\u4ef6\u5bf9\u8c61\u3002\u8fd9\u662f\u56e0\u4e3awrite\u65b9\u6cd5\u5e94\u8be5\u8fd4\u56de\u5199\u5165\u7684\u5b57\u8282\u6570; StringIO.write(\uff09\u4e0d\u4f1a\u8fd9\u6837\u505a\u3002</p> <p>\u8bf7\u4f7f\u7528\u50cfio.BytesIO\u8fd9\u6837\u7684\u4e1c\u897f\u3002</p> <p>\u4f8b</p> <pre><code>&gt;&gt;&gt; # Save to file\n&gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4])\n&gt;&gt;&gt; torch.save(x, 'tensor.pt')\n&gt;&gt;&gt; # Save to io.BytesIO buffer\n&gt;&gt;&gt; buffer = io.BytesIO()\n&gt;&gt;&gt; torch.save(x, buffer)\n\n</code></pre> <pre><code>torch.load(f, map_location=None, pickle_module=&lt;module 'pickle' from '/scratch/rzou/pt/release-env/lib/python3.7/pickle.py'&gt;)\n</code></pre> <p>\u4ece\u6587\u4ef6\u52a0\u8f7d\u7528 <code>torch.save()</code> \u4fdd\u5b58\u7684\u5bf9\u8c61\u3002</p> <p><code>torch.load()</code> \u4f7f\u7528Python\u7684unpickling\u8bbe\u65bd\uff0c\u4f46\u7279\u522b\u662f\u5904\u7406\u4f5c\u4e3a\u5f20\u91cf\u4f20\u611f\u5668\u7684\u5b58\u50a8\u5668\u3002\u5b83\u4eec\u9996\u5148\u5728CPU\u4e0a\u53cd\u5e8f\u5217\u5316\uff0c\u7136\u540e\u79fb\u52a8\u5230\u5b83\u4eec\u4fdd\u5b58\u7684\u8bbe\u5907\u4e0a\u3002\u5982\u679c\u6b64\u64cd\u4f5c\u5931\u8d25(\u4f8b\u5982\uff0c\u56e0\u4e3a\u8fd0\u884c\u65f6\u7cfb\u7edf\u6ca1\u6709\u67d0\u4e9b\u8bbe\u5907\uff09\uff0c\u5219\u4f1a\u5f15\u53d1\u5f02\u5e38\u3002\u4f46\u662f\uff0c\u53ef\u4ee5\u4f7f\u7528<code>map_location</code>\u53c2\u6570\u5c06\u5b58\u50a8\u91cd\u65b0\u6620\u5c04\u5230\u53e6\u4e00\u7ec4\u8bbe\u5907\u3002</p> <p>\u5982\u679c<code>map_location</code>\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5219\u6bcf\u4e2a\u5e8f\u5217\u5316\u5b58\u50a8\u5668\u5c06\u8c03\u7528\u4e00\u6b21\uff0c\u5176\u4e2d\u5305\u542b\u4e24\u4e2a\u53c2\u6570\uff1a\u5b58\u50a8\u548c\u4f4d\u7f6e\u3002\u5b58\u50a8\u53c2\u6570\u5c06\u662f\u9a7b\u7559\u5728CPU\u4e0a\u7684\u5b58\u50a8\u7684\u521d\u59cb\u53cd\u5e8f\u5217\u5316\u3002\u6bcf\u4e2a\u5e8f\u5217\u5316\u5b58\u50a8\u90fd\u6709\u4e00\u4e2a\u4e0e\u4e4b\u5173\u8054\u7684\u4f4d\u7f6e\u6807\u8bb0\uff0c\u7528\u4e8e\u6807\u8bc6\u4ece\u4e2d\u4fdd\u5b58\u7684\u8bbe\u5907\uff0c\u6b64\u6807\u8bb0\u662f\u4f20\u9012\u7ed9map_location\u7684\u7b2c\u4e8c\u4e2a\u53c2\u6570\u3002\u5185\u7f6e\u4f4d\u7f6e\u6807\u7b7e\u662fCPU\u5f20\u91cf\u7684<code>'cpu'</code>\u548cCUDA\u5f20\u91cf\u7684<code>'cuda:device_id'</code>(\u4f8b\u5982<code>'cuda:2'</code>\uff09\u3002 <code>map_location</code>\u5e94\u8fd4\u56deNone\u6216\u5b58\u50a8\u3002\u5982\u679c<code>map_location</code>\u8fd4\u56de\u5b58\u50a8\uff0c\u5b83\u5c06\u7528\u4f5c\u6700\u7ec8\u53cd\u5e8f\u5217\u5316\u5bf9\u8c61\uff0c\u5df2\u79fb\u52a8\u5230\u6b63\u786e\u7684\u8bbe\u5907\u3002\u5426\u5219\uff0c  \u5c06\u56de\u9000\u5230\u9ed8\u8ba4\u884c\u4e3a\uff0c\u5c31\u50cf\u672a\u6307\u5b9a<code>map_location</code>\u4e00\u6837\u3002</p> <p>\u5982\u679c<code>map_location</code>\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5b83\u5e94\u8be5\u662f\u4e00\u4e2a\u8bbe\u5907\u6807\u7b7e\uff0c\u5e94\u8be5\u52a0\u8f7d\u6240\u6709\u5f20\u91cf\u3002</p> <p>\u5426\u5219\uff0c\u5982\u679c<code>map_location</code>\u662f\u4e00\u4e2adict\uff0c\u5b83\u5c06\u7528\u4e8e\u5c06\u6587\u4ef6(\u952e\uff09\u4e2d\u51fa\u73b0\u7684\u4f4d\u7f6e\u6807\u8bb0\u91cd\u65b0\u6620\u5c04\u5230\u6307\u5b9a\u5b58\u50a8\u4f4d\u7f6e(\u503c\uff09\u7684\u4f4d\u7f6e\u6807\u8bb0\u3002</p> <p>\u7528\u6237\u6269\u5c55\u53ef\u4ee5\u4f7f\u7528<code>register_package</code>\u6ce8\u518c\u81ea\u5df1\u7684\u4f4d\u7f6e\u6807\u8bb0\u4ee5\u53ca\u6807\u8bb0\u548c\u53cd\u5e8f\u5217\u5316\u65b9\u6cd5\u3002</p> <p>Parameters:</p> <ul> <li>f - \u7c7b\u6587\u4ef6\u5bf9\u8c61(\u5fc5\u987b\u5b9e\u73b0read\uff0creadline\uff0ctell\u548cseek\uff09\uff0c\u6216\u5305\u542b\u6587\u4ef6\u540d\u7684\u5b57\u7b26\u4e32</li> <li>map_location - \u4e00\u4e2a\u51fd\u6570\uff0ctorch.device\uff0cstring\u6216dict\uff0c\u6307\u5b9a\u5982\u4f55\u91cd\u65b0\u6620\u5c04\u5b58\u50a8\u4f4d\u7f6e</li> <li>pickle_module - \u7528\u4e8e\u53d6\u6d88\u5143\u6570\u636e\u548c\u5bf9\u8c61\u53d6\u6d88\u7684\u6a21\u5757(\u5fc5\u987b\u4e0e\u7528\u4e8e\u5e8f\u5217\u5316\u6587\u4ef6\u7684pickle_module\u76f8\u5339\u914d\uff09</li> </ul> <p>\u6ce8\u610f</p> <p>\u5f53\u60a8\u5728\u5305\u542bGPU\u5f20\u91cf\u7684\u6587\u4ef6\u4e0a\u8c03\u7528 <code>torch.load()</code> \u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u8fd9\u4e9b\u5f20\u91cf\u5c06\u88ab\u52a0\u8f7d\u5230GPU\u3002\u5728\u52a0\u8f7d\u6a21\u578b\u68c0\u67e5\u70b9\u65f6\uff0c\u53ef\u4ee5\u8c03\u7528<code>torch.load(.., map_location='cpu')</code>\u7136\u540e\u8c03\u7528<code>load_state_dict()</code>\u4ee5\u907f\u514dGPU RAM\u6fc0\u589e\u3002</p> <p>Example</p> <pre><code>&gt;&gt;&gt; torch.load('tensors.pt')\n# Load all tensors onto the CPU\n&gt;&gt;&gt; torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n&gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n&gt;&gt;&gt; torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n&gt;&gt;&gt; with open('tensor.pt') as f:\n buffer = io.BytesIO(f.read())\n&gt;&gt;&gt; torch.load(buffer)\n\n</code></pre>"},{"location":"1.0/torch_serialization_parallelism_utilities/#_2","title":"\u6392\u6bd4","text":"<pre><code>torch.get_num_threads() \u2192 int\n</code></pre> <p>\u83b7\u53d6\u7528\u4e8e\u5e76\u884c\u5316CPU\u64cd\u4f5c\u7684OpenMP\u7ebf\u7a0b\u6570</p> <pre><code>torch.set_num_threads(int)\n</code></pre> <p>\u8bbe\u7f6e\u7528\u4e8e\u5e76\u884c\u5316CPU\u64cd\u4f5c\u7684OpenMP\u7ebf\u7a0b\u6570</p>"},{"location":"1.0/torch_serialization_parallelism_utilities/#_3","title":"\u5728\u672c\u5730\u7981\u7528\u6e10\u53d8\u8ba1\u7b97","text":"<p>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668<code>torch.no_grad()</code>\uff0c<code>torch.enable_grad()</code>\u548c<code>torch.set_grad_enabled()</code>\u6709\u52a9\u4e8e\u672c\u5730\u7981\u7528\u548c\u542f\u7528\u68af\u5ea6\u8ba1\u7b97\u3002\u6709\u5173\u5176\u7528\u6cd5\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u672c\u5730\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\u3002</p> <p>\u4f8b\u5b50\uff1a</p> <pre><code>&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; is_train = False\n&gt;&gt;&gt; with torch.set_grad_enabled(is_train):\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n\n&gt;&gt;&gt; torch.set_grad_enabled(False)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n</code></pre>"},{"location":"1.0/torch_serialization_parallelism_utilities/#_4","title":"\u516c\u7528\u4e8b\u4e1a","text":"<pre><code>torch.compiled_with_cxx11_abi()\n</code></pre> <p>\u8fd4\u56de\u662f\u5426\u4f7f\u7528_GLIBCXX_USE_CXX11_ABI = 1\u6784\u5efaPyTorch</p>"},{"location":"1.0/torch_tensors/","title":"Tensors","text":""},{"location":"1.0/torch_tensors/#tensors","title":"Tensors","text":"<p>\u8bd1\u8005\uff1adyywinner cluster</p> <pre><code>torch.is_tensor(obj)\n</code></pre> <p>\u5982\u679c<code>obj</code>\u662f\u4e00\u4e2aPyTorch\u5f20\u91cf\uff0c\u5219\u8fd4\u56deTrue.</p> Parameters: obj (Object) \u2013 Object to test <pre><code>torch.is_storage(obj)\n</code></pre> <p>\u5982\u679c<code>obj</code>\u662f\u4e00\u4e2aPyTorch \u5b58\u50a8\u5bf9\u8c61\uff0c\u5219\u8fd4\u56deTrue.</p> Parameters: obj (Object) \u2013 Object to test <pre><code>torch.set_default_dtype(d)\n</code></pre> <p>\u5c06<code>d</code>\u8bbe\u7f6e\u4e3a\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b(dtype). \u8be5\u7c7b\u578b\u5c06\u5728 <code>torch.tensor()</code> \u4e2d\u4f5c\u4e3a\u7c7b\u578b\u63a8\u65ad\u7684\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u3002\u521d\u59cb\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u4e3a<code>torch.float32</code>\u3002</p> Parameters: d (<code>torch.dtype</code>) \u2013 \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # \u521d\u59cb\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u4e3a torch.float32\nfloating point is torch.float32\ntorch.float32\n&gt;&gt;&gt; torch.set_default_dtype(torch.float64)\n&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # \u4e00\u4e2a\u65b0\u7684\u6d6e\u70b9\u7c7b\u578b\u7684\u5f20\u91cf\ntorch.float64\n\n</code></pre> <pre><code>torch.get_default_dtype() \u2192 torch.dtype\n</code></pre> <p>\u83b7\u53d6\u5f53\u524d\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b <code>torch.dtype</code>.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.get_default_dtype()  # \u521d\u59cb\u7684\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u662f torch.float32\ntorch.float32\n&gt;&gt;&gt; torch.set_default_dtype(torch.float64)\n&gt;&gt;&gt; torch.get_default_dtype()  # \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u4e3a torch.float64\ntorch.float64\n&gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # \u8bbe\u7f6e\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b\u4e5f\u4f1a\u5f71\u54cd\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\n&gt;&gt;&gt; torch.get_default_dtype()  # \u53d8\u5316\u5230 torch.float32( \u6b64\u7c7b\u578b(dtype)\u6765\u81ea\u4e8etorch.FloatTensor )\ntorch.float32\n\n</code></pre> <pre><code>torch.set_default_tensor_type(t)\n</code></pre> <p>\u8bbe\u7f6e\u9ed8\u8ba4\u7684 <code>torch.Tensor</code> \u7c7b\u578b\u5230\u6d6e\u70b9\u5f20\u91cf\u7c7b\u578b <code>t</code>. \u8be5\u7c7b\u578b\u5c06\u5728 <code>torch.tensor()</code> \u4e2d\u4f5c\u4e3a\u7c7b\u578b\u63a8\u65ad\u7684\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u3002 \u521d\u59cb\u9ed8\u8ba4\u6d6e\u70b9\u5f20\u91cf\u7c7b\u578b\u4e3a <code>torch.FloatTensor</code>.</p> Parameters: t (type or string) \u2013 \u6d6e\u70b9\u5f20\u91cf\u7684\u7c7b\u578b\u6216\u8005\u5b83\u7684\u540d\u79f0 <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # \u521d\u59cb\u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\u4e3a torch.float32\ntorch.float32\n&gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)\n&gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # \u4e00\u4e2a\u65b0\u7684\u6d6e\u70b9\u5f20\u91cf\ntorch.float64\n\n</code></pre> <pre><code>torch.numel(input) \u2192 int\n</code></pre> <p>\u8fd4\u56de <code>input</code> \u5f20\u91cf\u4e2d\u5143\u7d20\u603b\u6570.</p> Parameters: input (Tensor) \u2013 the input tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)\n&gt;&gt;&gt; torch.numel(a)\n120\n&gt;&gt;&gt; a = torch.zeros(4,4)\n&gt;&gt;&gt; torch.numel(a)\n16\n\n</code></pre> <pre><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)\n</code></pre> <p>\u8bbe\u7f6e\u6253\u5370\u9009\u9879. \u4ece NumPy \u527d\u7a83\u8fc7\u6765\u7684(\u6ed1\u7a3d\uff09</p> <p>Parameters: </p> <ul> <li>precision \u2013 \u6d6e\u70b9\u8f93\u51fa\u7684\u6709\u6548\u4f4d\u6570 (\u9ed8\u8ba4\u4e3a 4).</li> <li>threshold \u2013 \u8f93\u51fa\u65f6\u7684\u9608\u503c\uff0c\u5f53\u6570\u7ec4\u5143\u7d20\u603b\u548c\u8d85\u8fc7\u9608\u503c\uff0c\u4f1a\u88ab\u622a\u65ad\u8f93\u51fa (\u9ed8\u8ba4\u4e3a 1000).</li> <li>edgeitems \u2013 \u6bcf\u4e2a\u7ef4\u5ea6\u6240\u7edf\u8ba1\u7684\u6570\u7ec4\u6761\u76ee\u6570\u91cf(\u9ed8\u8ba4\uff1a3).</li> <li>linewidth \u2013 \u6bcf\u4e00\u884c\u8f93\u51fa\u7684\u5b57\u7b26\u957f\u5ea6 (\u9ed8\u8ba4\u4e3a80). \u9608\u503c\u77e9\u9635\u5c06\u5ffd\u7565\u8be5\u53c2\u6570.</li> <li>profile \u2013 \u6253\u5370\u8f93\u51fa\u7684\u7f8e\u89c2\u7a0b\u5ea6 \u9ed8\u8ba4\u503c\u4e3aSane. \u53ef\u4ee5\u7528\u540e\u9762\u62ec\u53f7\u4e2d\u7684\u9009\u9879\u8986\u76d6( <code>default</code>, <code>short</code>, <code>full</code>).</li> </ul> <pre><code>torch.set_flush_denormal(mode) \u2192 bool\n</code></pre> <p>CPU\u4e0d\u652f\u6301\u975e\u89c4\u683c\u5316\u6d6e\u70b9\u6570 .</p> <p>\u5982\u679c\u4f60\u7684\u7cfb\u7edf\u652f\u6301\u975e\u89c4\u683c\u5316\u6570\u5b57\u6a21\u5f0f(flush denormal mode)\u5e76\u4e14\u53ef\u6210\u529f\u914d\u7f6e\u8be5\u6a21\u5f0f\u5219\u8fd4\u56de <code>True</code> . <code>set_flush_denormal()</code> \u53ef\u4ee5\u4f7f\u7528\u5728\u652f\u6301SSE3\u7684x86\u67b6\u6784.</p> Parameters: mode (bool) \u2013 \u662f\u5426\u5f00\u542f  flush denormal mode <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.set_flush_denormal(True)\nTrue\n&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n&gt;&gt;&gt; torch.set_flush_denormal(False)\nTrue\n&gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n [ 1.0000], dtype=torch.float64)\n\n</code></pre>"},{"location":"1.0/torch_tensors/#creation-ops","title":"Creation Ops","text":"<p>Note</p> <p>\u968f\u673a\u91c7\u6837\u521b\u9020\u968f\u673a\u6570\u7684\u65b9\u5f0f\u5728 Random sampling \u5217\u4e3e\u3002\u5176\u4e2d\u5305\u62ec <code>torch.rand()</code> <code>torch.rand_like()</code> <code>torch.randn()</code> <code>torch.randn_like()</code> <code>torch.randint()</code> <code>torch.randint_like()</code> <code>torch.randperm()</code> . \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>torch.empty()</code> \uff0c\u5e76\u4f7f\u7528 In-place random sampling \u65b9\u6cd5\u53bb\u4ece\u66f4\u5bbd\u6cdb\u7684\u8303\u56f4\u91c7\u6837,\u751f\u6210 <code>torch.Tensor</code> .</p> <pre><code>torch.tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u7528 <code>data</code> \u6784\u5efa\u5f20\u91cf.</p> <p>Warning</p> <p><code>torch.tensor()</code> \u4f1a\u62f7\u8d1d <code>data</code>. \u5982\u679c\u4f60\u6709\u4e00\u4e2a\u5f20\u91cf( <code>data</code> )\uff0c\u5e76\u4e14\u60f3\u8981\u907f\u514d\u62f7\u8d1d, \u8bf7\u4f7f\u7528 <code>torch.Tensor.requires_grad_()</code> \u6216\u8005 <code>torch.Tensor.detach()</code>. \u5982\u679c\u4f60\u6709\u4e00\u4e2aNumPy\u6570\u7ec4(<code>ndarray</code>) \u5e76\u4e14\u60f3\u8981\u907f\u514d\u62f7\u8d1d, \u8bf7\u4f7f\u7528 <code>torch.from_numpy()</code>.</p> <p>Warning</p> <p>\u5f53 data \u4e3a\u4e00\u4e2a\u540d\u4e3a <code>x</code> \u7684\u5f20\u91cf\uff0c <code>torch.tensor()</code> \u8bfb\u53d6 'the data' (\u65e0\u8bba\u4f20\u8f93\u4e86\u4ec0\u4e48), \u90fd\u4f1a\u6784\u5efa\u4e00\u4e2a leaf variable(\u8ba1\u7b97\u56fe\u6a21\u578b\u4e2d\u4e8b\u5148\u521b\u5efa\u7684\u3001\u800c\u975e\u8fd0\u7b97\u5f97\u5230\u7684\u53d8\u91cf). \u56e0\u6b64 <code>torch.tensor(x)</code> \u7b49\u4ef7\u4e8e <code>x.clone().detach()</code> \uff0c\u540c\u65f6 <code>torch.tensor(x, requires_grad=True)</code> \u7b49\u4ef7\u4e8e <code>x.clone().detach().requires_grad_(True)</code>. \u6211\u4eec\u63a8\u8350\u8fd9\u79cd\u4f7f\u7528 <code>clone()</code> and <code>detach()</code> \u7684\u5199\u6cd5.</p> <p>Parameters: </p> <ul> <li>data (array_like) \u2013 \u521d\u59cb\u5316\u5f20\u91cf\u7684\u6570\u636e. \u5141\u8bb8\u7684\u7c7b\u578b\u6709 list, tuple, NumPy <code>ndarray</code>, scalar(\u6807\u91cf), \u4ee5\u53ca\u5176\u4ed6\u7c7b\u578b.</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u7684\u5f20\u91cf\u6240\u8981\u6c42\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u6b64\u53c2\u6570\u4e3a <code>None</code>,\u4ece <code>data</code>\u4e2d\u63a8\u65ad\u6570\u636e\u7c7b\u578b.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u7684\u5f20\u91cf\u6240\u8981\u6c42\u7684\u786c\u4ef6. \u9ed8\u8ba4: \u5982\u679c\u6b64\u53c2\u6570\u4e3a <code>None</code>,\u5bf9\u5f53\u524d\u5f20\u91cf\u7c7b\u578b\u4f7f\u7528\u5f53\u524d\u786c\u4ef6(\u53c2\u8003 <code>torch.set_default_tensor_type()</code>). <code>device</code> \u53ef\u4ee5\u4e3a \u63d0\u4f9bCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548c \u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907.</li> <li>requires_grad (bool, optional) \u2013 \u5bf9\u8fd4\u56de\u7684\u5f20\u91cf\u81ea\u52a8\u6c42\u5bfc\u65f6\u662f\u5426\u9700\u8981\u8bb0\u5f55\u64cd\u4f5c. \u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n       [ 2.2000,  3.1000],\n       [ 4.9000,  5.2000]])\n\n&gt;&gt;&gt; torch.tensor([0, 1])  # \u8f93\u5165\u6570\u636e\u63a8\u65ad\ntensor([ 0,  1])\n\n&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],\n                 dtype=torch.float64,\n                 device=torch.device('cuda:0'))  # \u521b\u5efa\u4e00\u4e2a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, \ndevice='cuda:0')\n\n&gt;&gt;&gt; torch.tensor(3.14159)  # \u521b\u5efa\u4e00\u4e2a\u6807\u91cf (\u96f6\u7ef4\u5f20\u91cf)\ntensor(3.1416)\n\n&gt;&gt;&gt; torch.tensor([])  # \u521b\u5efa\u4e00\u4e2a\u7a7a\u5f20\u91cf (\u5f62\u72b6\u662f (0,))\ntensor([])\n\n</code></pre> <pre><code>torch.sparse_coo_tensor(indices, values, size=None, dtype=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u7528\u975e0\u5143\u7d20\u503c<code>values</code>\u548c\u4e0b\u6807<code>indices</code>\u5728COO(\u987a\u5e8f\u6807\u6ce8)\u6784\u5efa\u4e00\u4e2a\u7a00\u758f\u77e9\u9635\u3002\u4e00\u4e2a\u7a00\u758f\u5411\u91cf\u53ef\u4ee5\u662f<code>\u672a\u5408\u5e76\u7684</code>(<code>uncoalesced</code>), \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b,\u5728\u7d22\u5f15\u4e2d\u4f1a\u5b58\u5728\u6709\u91cd\u590d\u5750\u6807 ,\u8fd9\u4e2a\u7d22\u5f15\u7684\u503c\u662f\u6240\u6709\u91cd\u590d\u503c\u6570\u91cf\u7684\u548c: torch.sparse.</p> <p>Parameters: </p> <ul> <li>indices (array_like) \u2013 \u7ed9\u5f20\u91cf\u521d\u59cb\u5316\u6570\u636e\u3002\u53ef\u4ee5\u662f\u5217\u8868,\u5143\u7ec4,Numpy\u77e9\u9635(<code>ndarry</code>\u7c7b\u578b),\u6807\u91cf\u548c\u5176\u4ed6\u7c7b\u578b\u3002\u4e4b\u540e\u5c06\u5728\u5185\u90e8\u88ab\u6620\u5c04\u6210<code>torch.LongTensor</code>\u3002 \u56e0\u6b64\u77e9\u9635\u4e2d\u7684\u975e\u96f6\u5143\u7d20\u4e0b\u6807\u7684\u5750\u6807,\u9700\u8981\u662f\u4e8c\u7ef4\u7684\uff0c\u5e76\u4e14\u7b2c\u4e00\u7ef4\u662f\u5f20\u91cf\u7684\u7ef4\u5ea6\uff0c\u7b2c\u4e8c\u7ef4\u662f\u975e\u96f6\u5143\u7d20\u6570\u91cf\u3002</li> <li>values (array_like) \u2013 \u521d\u59cb\u5316\u5f20\u91cf\u7684\u503c\u3002\u53ef\u4ee5\u662f\u5217\u8868,\u5143\u7ec4,Numpy\u77e9\u9635(<code>ndarry</code>\u7c7b\u578b),\u6807\u91cf\u548c\u5176\u4ed6\u7c7b\u578b\u3002</li> <li>size (list, tuple, or <code>torch.Size</code>, optional) \u2013 \u7a00\u758f\u77e9\u9635\u7684\u5f62\u72b6\u3002\u5982\u679c\u4e0d\u63d0\u4f9bsize\u5f62\u72b6\uff0c\u5c06\u4f1a\u88ab\u81ea\u52a8\u4f18\u5316\u4e3a\u53ef\u4ee5\u88c5\u4e0b\u6240\u6709\u975e\u96f6\u5143\u7d20\u7684\u6700\u5c0f\u5927\u5c0f\u3002</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u5f20\u91cf\u8fd4\u56de\u503c\u7684\u671f\u671b\u6570\u636e\u7c7b\u578b\u3002\u5982\u679c\u6ca1\u6709\uff0c\u5219\u9ed8\u8ba4\u4e3a<code>values</code>\u3002</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u7684\u5f20\u91cf\u6240\u8981\u6c42\u7684\u786c\u4ef6. \u9ed8\u8ba4: \u5982\u679c\u6b64\u53c2\u6570\u4e3a <code>None</code>,\u5bf9\u5f53\u524d\u5f20\u91cf\u7c7b\u578b\u4f7f\u7528\u5f53\u524d\u786c\u4ef6(\u53c2\u8003 <code>torch.set_default_tensor_type()</code>). <code>device</code> \u53ef\u4ee5\u4e3a \u63d0\u4f9bCPU\u5f20\u91cf\u7c7b\u578b\u7684CPU\u548c \u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907.torch.set_default_tensor_type \"torch.set_default_tensor_type\")). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>requires_grad (bool, optional) \u2013 \u5bf9\u8fd4\u56de\u7684\u5f20\u91cf\u81ea\u52a8\u6c42\u5bfc\u65f6\u662f\u5426\u9700\u8981\u8bb0\u5f55\u64cd\u4f5c. \u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; i = torch.tensor([[0, 1, 1],\n                      [2, 0, 2]])\n&gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32)\n&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n&gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n&gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],\n                            dtype=torch.float64,\n                            device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# \u4f7f\u7528\u4e0b\u5217\u5e38\u91cf\u521b\u5efa\u4e00\u4e2a\u7a7a\u7a00\u758f\u5f20\u91cf:\n#   1\\. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2\\. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3\\. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# \u6bd4\u5982\uff0c\u4f7f\u7528nnz = 0, dense_dim = 0\u548c\n# sparse_dim = 1 (\u8fd9\u91cc\u7684\u4e0b\u6807\u662f\u4e00\u4e2a\u4e8c\u7ef4\u5f20\u91cf\u5f62\u72b6shape = (1, 0)) \u6765\u521b\u5efa\u4e00\u4e2a\u7a7a\u7a00\u758f\u77e9\u9635\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# \u7136\u540e\u4f7f\u7528nnz = 0, dense_dim = 1 \u548c sparse_dim = 1\n# \u6765\u521b\u5efa\u4e00\u4e2a\u7a7a\u7a00\u758f\u77e9\u9635\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n\n</code></pre> <pre><code>torch.as_tensor(data, dtype=None, device=None) \u2192 Tensor\n</code></pre> <p>\u8f6c\u6362 <code>data</code> \u5230 <code>torch.Tensor</code> \u7c7b\u578b. \u5982\u679c <code>data</code> \u5df2\u7ecf\u662f\u76f8\u540c <code>dtype</code> \u548c <code>device</code> \u7684\u5f20\u91cf\uff0c \u90a3\u5c31\u4e0d\u4f1a\u6267\u884c\u62f7\u8d1d\uff0c\u5426\u5219\u5c06\u4f1a\u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u5f20\u91cf(\u5982\u679c data <code>Tensor</code> \u4e2d <code>requires_grad=True</code>,\u5219\u8fd4\u56de\u7684\u5f20\u91cf\u8ba1\u7b97\u56fe\u4f1a\u88ab\u4fdd\u7559)\u76f8\u4f3c\u5730\uff0c\u5982\u679c data <code>dtype</code> \u4e3a <code>ndarry</code> \u5e76\u4e14 <code>device</code> \u4e3aCPU\uff0c\u5219\u62f7\u8d1d\u4e0d\u4f1a\u53d1\u751f\u3002</p> <p>Parameters:</p> <ul> <li>data (array_like) \u2013 \u63d0\u4f9b\u5f20\u91cf\u521d\u59cb\u5316\u7684\u6570\u636e\u7ed3\u6784\u3002\u53ef\u80fd\u662fist, tuple, NumPy <code>ndarray</code>, scalar \u6216\u5176\u4ed6\u7c7b\u578b\u3002</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u63d0\u4f9b\u5f20\u91cf\u521d\u59cb\u5316\u7684\u503c\u3002\u53ef\u80fd\u662fist, tuple, NumPy <code>ndarray</code>, scalar \u6216\u5176\u4ed6\u7c7b\u578b\u3002</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u8981\u7684\u8bbe\u5907\u3002\u9ed8\u8ba4\uff1a\u82e5\u4e3a\u7a7a\uff0c\u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])\n&gt;&gt;&gt; t = torch.as_tensor(a)\n&gt;&gt;&gt; t\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; t[0] = -1\n&gt;&gt;&gt; a\narray([-1,  2,  3])\n\n&gt;&gt;&gt; a = numpy.array([1, 2, 3])\n&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda'))\n&gt;&gt;&gt; t\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; t[0] = -1\n&gt;&gt;&gt; a\narray([1,  2,  3])\n\n</code></pre> <pre><code>torch.from_numpy(ndarray) \u2192 Tensor\n</code></pre> <p>\u4ece\u4e00\u4e2a <code>numpy.ndarray</code> \u521b\u5efa\u4e00\u4e2a <code>Tensor</code>.</p> <p>\u8fd4\u56de\u7684\u5f20\u91cf\u548c <code>ndarry</code> \u5171\u4eab\u76f8\u540c\u7684\u5185\u5b58\u3002 \u5bf9\u5f20\u91cf\u7684\u4fee\u9970\u5c06\u4f1a\u53cd\u6620\u5728 <code>ndarry</code>\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u8fd4\u56de\u7684\u5f20\u91cf \u5927\u5c0f\u4e0d\u53ef\u53d8\u3002</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = numpy.array([1, 2, 3])\n&gt;&gt;&gt; t = torch.from_numpy(a)\n&gt;&gt;&gt; t\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; t[0] = -1\n&gt;&gt;&gt; a\narray([-1,  2,  3])\n\n</code></pre> <pre><code>torch.zeros(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf <code>0</code> \u586b\u5145\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u53ef\u53d8\u957f\u53c2\u6570 <code>sizes</code> \u5b9a\u4e49\u4e86\u8be5\u5f20\u91cf\u5f62\u72b6(shape).</p> <p>Parameters:</p> <ul> <li>sizes (int...) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u5e8f\u5217. \u53ef\u4ee5\u662f\u53ef\u53d8\u957f\u7684\u53c2\u6570 \u6216\u8005\u662f\u50cf \u5217\u8868\u5143\u7ec4\u8fd9\u6837\u7684\u96c6\u5408\u3002</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li> </li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n [ 0.,  0.,  0.]])\n\n&gt;&gt;&gt; torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])\n\n</code></pre> <pre><code>torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7528\u6807\u91cf<code>0</code>\u586b\u5145\u7684\u5f20\u91cf\uff0c\u5927\u5c0f\u548c<code>input</code>\u7684<code>size</code>\u4e00\u6837. <code>torch.zeros_like(input)</code> \u7b49\u4ef7\u4e8e <code>torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p> <p>Warning</p> <p>\u622a\u6b62\u5230 0.4, \u8be5\u51fd\u6570\u4e0d\u518d\u652f\u6301<code>out</code>\u5173\u952e\u5b57. \u540c\u65f6\uff0c\u8001\u7248\u7684 <code>torch.zeros_like(input, out=output)</code> \u7b49\u4ef7\u4e8e <code>torch.zeros(input.size(), out=output)</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 <code>input</code>\u7684<code>size</code>\u5c5e\u6027\u51b3\u5b9a\u8f93\u51fa\u5f20\u91cf\u5927\u5c0f</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528<code>input</code>\u7684<code>dtype</code>\u5c5e\u6027 .</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: \u9ed8\u8ba4\u4e3a<code>input</code>\u7684<code>layout</code>\u5c5e\u6027.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u4e3a<code>input</code>\u7684<code>device</code>\u5c5e\u6027.</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)\n&gt;&gt;&gt; torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n [ 0.,  0.,  0.]])\n\n</code></pre> <pre><code>torch.ones(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u7528\u6807\u91cf <code>1</code> \u586b\u5145\u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u53ef\u53d8\u957f\u53c2\u6570 <code>sizes</code> \u5b9a\u4e49\u4e86\u8be5\u5f20\u91cf\u5f62\u72b6(shape).</p> <p>Parameters:</p> <ul> <li>sizes (int...) \u2013 \u5b9a\u4e49\u8f93\u51fa\u5f20\u91cf\u5f62\u72b6\u7684\u6574\u6570\u5e8f\u5217. \u53ef\u4ee5\u662f\u53ef\u53d8\u957f\u7684\u53c2\u6570 \u6216\u8005\u662f\u50cf \u5217\u8868\u5143\u7ec4\u8fd9\u6837\u7684\u96c6\u5408\u3002</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n [ 1.,  1.,  1.]])\n\n&gt;&gt;&gt; torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])\n\n</code></pre> <pre><code>torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u7528\u6807\u91cf<code>1</code>\u586b\u5145\u7684\u5f20\u91cf\uff0c\u5927\u5c0f\u548c<code>input</code>\u7684<code>size</code>\u4e00\u6837. <code>torch.ones_like(input)</code> \u7b49\u4ef7\u4e8e <code>torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code></p> <p>Warning</p> <p>\u622a\u6b62\u5230 0.4, \u8be5\u51fd\u6570\u4e0d\u518d\u652f\u6301<code>out</code>\u5173\u952e\u5b57. \u540c\u65f6\uff0c\u8001\u7248\u7684 <code>torch.ones_like(input, out=output)</code> \u7b49\u4ef7\u4e8e <code>torch.ones(input.size(), out=output)</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 <code>input</code>\u7684<code>size</code>\u5c5e\u6027\u51b3\u5b9a\u8f93\u51fa\u5f20\u91cf\u5927\u5c0f</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528<code>input</code>\u7684<code>dtype</code>\u5c5e\u6027 .</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: \u9ed8\u8ba4\u4e3a<code>input</code>\u7684<code>layout</code>\u5c5e\u6027.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u4e3a<code>input</code>\u7684<code>device</code>\u5c5e\u6027.</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; input = torch.empty(2, 3)\n&gt;&gt;&gt; torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n [ 1.,  1.,  1.]])\n\n</code></pre> <pre><code>torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u4e00\u7ef4\u5f20\u91cf\uff0c\u5927\u5c0f\u4e3a\uff0c\u503c\u4e3a\u533a\u95f4 <code>[start,end)</code>\u5185\uff0c\u4ee5<code>step</code>\u4e3a\u6b65\u8ddd,\u4ece<code>start</code>\u5f00\u59cb\u7684\u6570\u5217.</p> <p>\u6ce8\u610f: \u975e\u6574\u578b\u6570 <code>step</code> \u548c <code>end</code> \u6bd4\u8f83\u65f6\u5b58\u5728\u6d6e\u70b9\u56db\u820d\u4e94\u5165\u8bef\u5dee;\u4e3a\u907f\u514d\u4e0d\u4e00\u81f4\uff0c\u5efa\u8bae\u5728<code>end</code>\u540e\u9762\u52a0\u4e0a\u4e00\u4e2a\u5c0f\u7684epsilon.</p> <p></p> <p>Parameters: </p> <ul> <li>start (Number) \u2013 \u70b9\u96c6\u7684\u8d77\u59cb\u503c. \u9ed8\u8ba4\u4e3a<code>0</code>.</li> <li>end (Number) \u2013 \u70b9\u96c6\u7684\u7ec8\u503c.</li> <li>step (Number) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb . \u9ed8\u8ba4\u4e3a <code>1</code>.</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c. (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>). \u82e5 <code>dtype</code> \u672a\u63d0\u4f9b, \u5219\u4ece\u5176\u4ed6\u8f93\u5165\u53c2\u6570\u63a8\u65ad\u6570\u636e\u7c7b\u578b. \u5982\u679c <code>start</code>, <code>end</code>, <code>stop</code> \u4e2d\u5b58\u5728\u6d6e\u70b9\u6570, \u5219 <code>dtype</code> \u4f1a\u4f7f\u7528\u9ed8\u8ba4\u6570\u636e\u7c7b\u578b, \u8bf7\u67e5\u770b <code>get_default_dtype()</code>. \u5426\u5219,  <code>dtype</code> \u4f1a\u4f7f\u7528 <code>torch.int64</code>.</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n&gt;&gt;&gt; torch.arange(1, 4)\ntensor([ 1,  2,  3])\n&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n\n</code></pre> <pre><code>torch.range(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e00\u4e2a\u4e00\u7ef4\u5f20\u91cf\uff0c\u5927\u5c0f\u4e3a\uff0c\u503c\u4ece<code>start</code>\u5230<code>end</code>\uff0c\u4ee5<code>step</code>\u4e3a\u6b65\u8ddd\u7684\u6570\u5217.</p> <p></p> <p>Warning</p> <p>\u8fd9\u4e2a\u51fd\u6570\u88ab\u5f03\u7528\uff0c\u6539\u4e3a <code>torch.arange()</code>.</p> <p>Parameters: </p> <ul> <li>start (Number) \u2013 \u70b9\u96c6\u7684\u8d77\u59cb\u503c. \u9ed8\u8ba4\u4e3a<code>0</code>.</li> <li>end (Number) \u2013 \u70b9\u96c6\u7684\u7ec8\u503c.</li> <li>step (Number) \u2013 \u6bcf\u5bf9\u76f8\u90bb\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb . \u9ed8\u8ba4\u4e3a <code>1</code>.</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u7684\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c. (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>). \u82e5 <code>dtype</code> \u672a\u63d0\u4f9b, \u5219\u4ece\u5176\u4ed6\u8f93\u5165\u53c2\u6570\u63a8\u65ad\u6570\u636e\u7c7b\u578b. \u5982\u679c <code>start</code>, <code>end</code>, <code>stop</code> \u4e2d\u5b58\u5728\u6d6e\u70b9\u6570, \u5219 <code>dtype</code> \u4f1a\u4f7f\u7528\u9ed8\u8ba4\u6570\u636e\u7c7b\u578b, \u8bf7\u67e5\u770b <code>get_default_dtype()</code>. \u5426\u5219,  <code>dtype</code> \u4f1a\u4f7f\u7528 <code>torch.int64</code>.</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n&gt;&gt;&gt; torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n\n</code></pre> <pre><code>torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5171<code>steps</code>\u6570\u91cf\u5728<code>start</code> \u548c <code>end</code>\u4e4b\u95f4\u7684\u7b49\u8ddd\u70b9\uff0c\u4ece\u800c\u7ec4\u6210\u7684\u4e00\u7ef4\u5f20\u91cf.</p> <p>\u8f93\u51fa\u5f20\u91cf\u5927\u5c0f\u4e3a<code>steps</code>\uff0c\u7ef4\u5ea6\u4e3a\u4e00\u7ef4.</p> <p>Parameters: </p> <ul> <li>start (float) \u2013 \u70b9\u96c6\u7684\u8d77\u59cb\u503c. </li> <li>end (float) \u2013\u70b9\u96c6\u7684\u7ec8\u503c.</li> <li>steps (int) \u2013  <code>start</code> \u548c <code>end</code>\u4e4b\u95f4\u7684\u6837\u672c\u70b9\u6570\u76ee. \u9ed8\u8ba4: <code>100</code>.</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c. (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n\n</code></pre> <pre><code>torch.logspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u5171\u6709<code>steps</code>\u6570\u91cf\u7684\u4e00\u7ef4\u5f20\u91cf\uff0c\u70b9\u96c6\u7531 \u548c \u4e4b\u95f4\u5bf9\u6570\u5206\u5e03\u7684\u70b9\u7ec4\u6210.</p> <p>\u8f93\u51fa\u5f20\u91cf\u5927\u5c0f\u4e3a<code>steps</code>\uff0c\u7ef4\u5ea6\u4e3a\u4e00\u7ef4.</p> <p>Parameters: </p> <ul> <li>start (float) \u2013 \u70b9\u96c6\u7684\u8d77\u59cb\u503c. </li> <li>end (float) \u2013\u70b9\u96c6\u7684\u7ec8\u503c.</li> <li>steps (int) \u2013  <code>start</code> \u548c <code>end</code>\u4e4b\u95f4\u7684\u6837\u672c\u70b9\u6570\u76ee. \u9ed8\u8ba4: <code>100</code>.</li> <li>out (Tensor, optional) \u2013 \u8f93\u51fa\u5f20\u91cf</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u4f7f\u7528\u5168\u5c40\u9ed8\u8ba4\u503c. (\u53c2\u8003 <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u7684\u5c42\u6570. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 \u8fd4\u56de\u5f20\u91cf\u6240\u9700\u7684\u8bbe\u5907. \u9ed8\u8ba4: \u5982\u679c\u4e3a <code>None</code>, \u5219\u5f53\u524d\u7684\u8bbe\u5907\u63d0\u4f9b\u7ed9\u9ed8\u8ba4\u5f20\u91cf\u7c7b\u578b(see <code>torch.set_default_tensor_type()</code>). <code>device</code> \u5c06\u4e3a\u652f\u6301CPU\u5f20\u91cf\u7684CPU\u548c\u652f\u6301CUDA\u5f20\u91cf\u7c7b\u578b\u7684CUDA\u8bbe\u5907\u3002</li> <li>requires_grad (bool, optional) \u2013 \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u662f\u5426\u9700\u8981\u8bb0\u5f55\u5728\u8fd4\u56de\u5f20\u91cf\u4e0a\u7684\u64cd\u4f5c\u3002\u9ed8\u8ba4: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n\n</code></pre> <pre><code>torch.eye(n, m=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>\u8fd4\u56de\u4e8c\u7ef4\u5f20\u91cf\uff0c\u5bf9\u89d2\u7ebf\u4e0a\u662f1\uff0c\u5176\u5b83\u5730\u65b9\u662f0.</p> <p>Parameters: </p> <ul> <li>n (int) \u2013 the number of rows</li> <li>m (int, optional) \u2013 the number of columns with default being <code>n</code></li> <li>out (Tensor, optional) \u2013 the output tensor</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> Returns: A 2-D tensor with ones on the diagonal and zeros elsewhere Return type: Tensor --- --- <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n [ 0.,  1.,  0.],\n [ 0.,  0.,  1.]])\n\n</code></pre> <pre><code>torch.empty(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument <code>sizes</code>.</p> <p>Parameters: </p> <ul> <li>sizes (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.</li> <li>out (Tensor, optional) \u2013 the output tensor</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.empty(2, 3)\ntensor(1.00000e-08 *\n [[ 6.3984,  0.0000,  0.0000],\n [ 0.0000,  0.0000,  0.0000]])\n\n</code></pre> <pre><code>torch.empty_like(input, dtype=None, layout=None, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>Returns an uninitialized tensor with the same size as <code>input</code>. <code>torch.empty_like(input)</code> is equivalent to <code>torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the size of <code>input</code> will determine size of the output tensor</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</li> <li>layout (<code>torch.layout</code>, optional) \u2013 the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</li> <li>requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n\n</code></pre> <pre><code>torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>Returns a tensor of size <code>size</code> filled with <code>fill_value</code>.</p> <p>Parameters: </p> <ul> <li>size (int...) \u2013 a list, tuple, or <code>torch.Size</code> of integers defining the shape of the output tensor.</li> <li>fill_value \u2013 the number to fill the output tensor with.</li> <li>out (Tensor, optional) \u2013 the output tensor</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 the desired data type of returned tensor. Default: if <code>None</code>, uses a global default (see <code>torch.set_default_tensor_type()</code>).</li> <li>layout (<code>torch.layout</code>, optional) \u2013 the desired layout of returned Tensor. Default: <code>torch.strided</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <code>torch.set_default_tensor_type()</code>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n [ 3.1416,  3.1416,  3.1416]])\n\n</code></pre> <pre><code>torch.full_like(input, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor\n</code></pre> <p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>. <code>torch.full_like(input, fill_value)</code> is equivalent to <code>torch.full_like(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the size of <code>input</code> will determine size of the output tensor</li> <li>fill_value \u2013 the number to fill the output tensor with.</li> <li>dtype (<code>torch.dtype</code>, optional) \u2013 the desired data type of returned Tensor. Default: if <code>None</code>, defaults to the dtype of <code>input</code>.</li> <li>layout (<code>torch.layout</code>, optional) \u2013 the desired layout of returned tensor. Default: if <code>None</code>, defaults to the layout of <code>input</code>.</li> <li>device (<code>torch.device</code>, optional) \u2013 the desired device of returned tensor. Default: if <code>None</code>, defaults to the device of <code>input</code>.</li> <li>requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul>"},{"location":"1.0/torch_tensors/#todo","title":"TODO","text":""},{"location":"1.0/torch_tensors/#indexing-slicing-joining-mutating-ops","title":"Indexing, Slicing, Joining, Mutating Ops","text":"<pre><code>torch.cat(tensors, dim=0, out=None) \u2192 Tensor\n</code></pre> <p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p> <p><code>torch.cat()</code> can be seen as an inverse operation for <code>torch.split()</code> and <code>torch.chunk()</code>.</p> <p><code>torch.cat()</code> can be best understood via examples.</p> <p>Parameters: </p> <ul> <li>tensors (sequence of Tensors) \u2013 any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.</li> <li>dim (int, optional) \u2013 the dimension over which the tensors are concatenated</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)\n&gt;&gt;&gt; x\ntensor([[ 0.6580, -1.0969, -0.4614],\n [-0.1034, -0.5790,  0.1497]])\n&gt;&gt;&gt; torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n [-0.1034, -0.5790,  0.1497],\n [ 0.6580, -1.0969, -0.4614],\n [-0.1034, -0.5790,  0.1497],\n [ 0.6580, -1.0969, -0.4614],\n [-0.1034, -0.5790,  0.1497]])\n&gt;&gt;&gt; torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n -1.0969, -0.4614],\n [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n -0.5790,  0.1497]])\n\n</code></pre> <pre><code>torch.chunk(tensor, chunks, dim=0) \u2192 List of Tensors\n</code></pre> <p>Splits a tensor into a specific number of chunks.</p> <p>Last chunk will be smaller if the tensor size along the given dimension <code>dim</code> is not divisible by <code>chunks</code>.</p> <p>Parameters: </p> <ul> <li>tensor (Tensor) \u2013 the tensor to split</li> <li>chunks (int) \u2013 number of chunks to return</li> <li>dim (int) \u2013 dimension along which to split the tensor</li> </ul> <pre><code>torch.gather(input, dim, index, out=None) \u2192 Tensor\n</code></pre> <p>Gathers values along an axis specified by <code>dim</code>.</p> <p>For a 3-D tensor the output is specified by:</p> <pre><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n\n</code></pre> <p>If <code>input</code> is an n-dimensional tensor with size  and <code>dim = i</code>, then <code>index</code> must be an -dimensional tensor with size  where  and <code>out</code> will have the same size as <code>index</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the source tensor</li> <li>dim (int) \u2013 the axis along which to index</li> <li>index (LongTensor) \u2013 the indices of elements to gather</li> <li>out (Tensor, optional) \u2013 the destination tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]])\n&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\ntensor([[ 1,  1],\n [ 4,  3]])\n\n</code></pre> <pre><code>torch.index_select(input, dim, index, out=None) \u2192 Tensor\n</code></pre> <p>Returns a new tensor which indexes the <code>input</code> tensor along dimension <code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.</p> <p>The returned tensor has the same number of dimensions as the original tensor (<code>input</code>). The <code>dim</code>th dimension has the same size as the length of <code>index</code>; other dimensions have the same size as in the original tensor.</p> <p>Note</p> <p>The returned tensor does not use the same storage as the original tensor. If <code>out</code> has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>dim (int) \u2013 the dimension in which we index</li> <li>index (LongTensor) \u2013 the 1-D tensor containing the indices to index</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)\n&gt;&gt;&gt; x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n [-0.4664,  0.2647, -0.1228, -1.1068],\n [-1.1734, -0.6571,  0.7230, -0.6004]])\n&gt;&gt;&gt; indices = torch.tensor([0, 2])\n&gt;&gt;&gt; torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n [-1.1734, -0.6571,  0.7230, -0.6004]])\n&gt;&gt;&gt; torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n [-0.4664, -0.1228],\n [-1.1734,  0.7230]])\n\n</code></pre> <pre><code>torch.masked_select(input, mask, out=None) \u2192 Tensor\n</code></pre> <p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the binary mask <code>mask</code> which is a <code>ByteTensor</code>.</p> <p>The shapes of the <code>mask</code> tensor and the <code>input</code> tensor don't need to match, but they must be broadcastable.</p> <p>Note</p> <p>The returned tensor does not use the same storage as the original tensor</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input data</li> <li>mask (ByteTensor) \u2013 the tensor containing the binary mask to index with</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 4)\n&gt;&gt;&gt; x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n [-1.2035,  1.2252,  0.5002,  0.6248],\n [ 0.1307, -2.0608,  0.1244,  2.0139]])\n&gt;&gt;&gt; mask = x.ge(0.5)\n&gt;&gt;&gt; mask\ntensor([[ 0,  0,  0,  0],\n [ 0,  1,  1,  1],\n [ 0,  0,  0,  1]], dtype=torch.uint8)\n&gt;&gt;&gt; torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n\n</code></pre> <pre><code>torch.narrow(input, dimension, start, length) \u2192 Tensor\n</code></pre> <p>Returns a new tensor that is a narrowed version of <code>input</code> tensor. The dimension <code>dim</code> is input from <code>start</code> to <code>start + length</code>. The returned tensor and <code>input</code> tensor share the same underlying storage.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the tensor to narrow</li> <li>dimension (int) \u2013 the dimension along which to narrow</li> <li>start (int) \u2013 the starting dimension</li> <li>length (int) \u2013 the distance to the ending dimension</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n [ 4,  5,  6]])\n&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n [ 5,  6],\n [ 8,  9]])\n\n</code></pre> <pre><code>torch.nonzero(input, out=None) \u2192 LongTensor\n</code></pre> <p>Returns a tensor containing the indices of all non-zero elements of <code>input</code>. Each row in the result contains the indices of a non-zero element in <code>input</code>.</p> <p>If <code>input</code> has <code>n</code> dimensions, then the resulting indices tensor <code>out</code> is of size , where  is the total number of non-zero elements in the <code>input</code> tensor.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>out (LongTensor__, optional) \u2013 the output tensor containing indices</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n [ 1],\n [ 2],\n [ 4]])\n&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n [0.0, 0.4, 0.0, 0.0],\n [0.0, 0.0, 1.2, 0.0],\n [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n [ 1,  1],\n [ 2,  2],\n [ 3,  3]])\n\n</code></pre> <pre><code>torch.reshape(input, shape) \u2192 Tensor\n</code></pre> <p>Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape. When possible, the returned tensor will be a view of <code>input</code>. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.</p> <p>See <code>torch.Tensor.view()</code> on when it is possible to return a view.</p> <p>A single dimension may be -1, in which case it's inferred from the remaining dimensions and the number of elements in <code>input</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the tensor to be reshaped</li> <li>shape (tuple of python:ints) \u2013 the new shape</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; a = torch.arange(4.)\n&gt;&gt;&gt; torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n [ 2.,  3.]])\n&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])\n&gt;&gt;&gt; torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])\n\n</code></pre> <pre><code>torch.split(tensor, split_size_or_sections, dim=0)\n</code></pre> <p>Splits the tensor into chunks.</p> <p>If <code>split_size_or_sections</code> is an integer type, then <code>tensor</code> will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension <code>dim</code> is not divisible by <code>split_size</code>.</p> <p>If <code>split_size_or_sections</code> is a list, then <code>tensor</code> will be split into <code>len(split_size_or_sections)</code> chunks with sizes in <code>dim</code> according to <code>split_size_or_sections</code>.</p> <p>Parameters: </p> <ul> <li>tensor (Tensor) \u2013 tensor to split.</li> <li>split_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or list of sizes for each chunk</li> <li>dim (int) \u2013 dimension along which to split the tensor.</li> </ul> <pre><code>torch.squeeze(input, dim=None, out=None) \u2192 Tensor\n</code></pre> <p>Returns a tensor with all the dimensions of <code>input</code> of size <code>1</code> removed.</p> <p>For example, if <code>input</code> is of shape:  then the <code>out</code> tensor will be of shape: .</p> <p>When <code>dim</code> is given, a squeeze operation is done only in the given dimension. If <code>input</code> is of shape: , <code>squeeze(input, 0)</code> leaves the tensor unchanged, but <code>squeeze(input, 1)</code> will squeeze the tensor to the shape .</p> <p>Note</p> <p>The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>dim (int, optional) \u2013 if given, the input will be squeezed only in this dimension</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)\n&gt;&gt;&gt; x.size()\ntorch.Size([2, 1, 2, 1, 2])\n&gt;&gt;&gt; y = torch.squeeze(x)\n&gt;&gt;&gt; y.size()\ntorch.Size([2, 2, 2])\n&gt;&gt;&gt; y = torch.squeeze(x, 0)\n&gt;&gt;&gt; y.size()\ntorch.Size([2, 1, 2, 1, 2])\n&gt;&gt;&gt; y = torch.squeeze(x, 1)\n&gt;&gt;&gt; y.size()\ntorch.Size([2, 2, 1, 2])\n\n</code></pre> <pre><code>torch.stack(seq, dim=0, out=None) \u2192 Tensor\n</code></pre> <p>Concatenates sequence of tensors along a new dimension.</p> <p>All tensors need to be of the same size.</p> <p>Parameters: </p> <ul> <li>seq (sequence of Tensors) \u2013 sequence of tensors to concatenate</li> <li>dim (int) \u2013 dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <pre><code>torch.t(input) \u2192 Tensor\n</code></pre> <p>Expects <code>input</code> to be a matrix (2-D tensor) and transposes dimensions 0 and 1.</p> <p>Can be seen as a short-hand function for <code>transpose(input, 0, 1)</code>.</p> Parameters: input (Tensor) \u2013 the input tensor <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)\n&gt;&gt;&gt; x\ntensor([[ 0.4875,  0.9158, -0.5872],\n [ 0.3938, -0.6929,  0.6932]])\n&gt;&gt;&gt; torch.t(x)\ntensor([[ 0.4875,  0.3938],\n [ 0.9158, -0.6929],\n [-0.5872,  0.6932]])\n\n</code></pre> <pre><code>torch.take(input, indices) \u2192 Tensor\n</code></pre> <p>Returns a new tensor with the elements of <code>input</code> at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>indices (LongTensor) \u2013 the indices into tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],\n [6, 7, 8]])\n&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])\n\n</code></pre> <pre><code>torch.transpose(input, dim0, dim1) \u2192 Tensor\n</code></pre> <p>Returns a tensor that is a transposed version of <code>input</code>. The given dimensions <code>dim0</code> and <code>dim1</code> are swapped.</p> <p>The resulting <code>out</code> tensor shares it's underlying storage with the <code>input</code> tensor, so changing the content of one would change the content of the other.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>dim0 (int) \u2013 the first dimension to be transposed</li> <li>dim1 (int) \u2013 the second dimension to be transposed</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(2, 3)\n&gt;&gt;&gt; x\ntensor([[ 1.0028, -0.9893,  0.5809],\n [-0.1669,  0.7299,  0.4942]])\n&gt;&gt;&gt; torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n [-0.9893,  0.7299],\n [ 0.5809,  0.4942]])\n\n</code></pre> <pre><code>torch.unbind(tensor, dim=0) \u2192 seq\n</code></pre> <p>Removes a tensor dimension.</p> <p>Returns a tuple of all slices along a given dimension, already without it.</p> <p>Parameters: </p> <ul> <li>tensor (Tensor) \u2013 the tensor to unbind</li> <li>dim (int) \u2013 dimension to remove</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],\n&gt;&gt;&gt;                            [4, 5, 6],\n&gt;&gt;&gt;                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n\n</code></pre> <pre><code>torch.unsqueeze(input, dim, out=None) \u2192 Tensor\n</code></pre> <p>Returns a new tensor with a dimension of size one inserted at the specified position.</p> <p>The returned tensor shares the same underlying data with this tensor.</p> <p>A <code>dim</code> value within the range <code>[-input.dim() - 1, input.dim() + 1)</code> can be used. Negative <code>dim</code> will correspond to <code>unsqueeze()</code> applied at <code>dim</code> = <code>dim + input.dim() + 1</code>.</p> <p>Parameters: </p> <ul> <li>input (Tensor) \u2013 the input tensor</li> <li>dim (int) \u2013 the index at which to insert the singleton dimension</li> <li>out (Tensor, optional) \u2013 the output tensor</li> </ul> <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])\n&gt;&gt;&gt; torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n&gt;&gt;&gt; torch.unsqueeze(x, 1)\ntensor([[ 1],\n [ 2],\n [ 3],\n [ 4]])\n\n</code></pre> <pre><code>torch.where(condition, x, y) \u2192 Tensor\n</code></pre> <p>Return a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>condition</code>.</p> <p>The operation is defined as:</p> <p></p> <p>Note</p> <p>The tensors <code>condition</code>, <code>x</code>, <code>y</code> must be broadcastable.</p> <p>Parameters: </p> <ul> <li>condition (ByteTensor) \u2013 When True (nonzero), yield x, otherwise yield y</li> <li>x (Tensor) \u2013 values selected at indices where <code>condition</code> is <code>True</code></li> <li>y (Tensor) \u2013 values selected at indices where <code>condition</code> is <code>False</code></li> </ul> Returns: A tensor of shape equal to the broadcasted shape of <code>condition</code>, <code>x</code>, <code>y</code> Return type: Tensor --- --- <p>Example:</p> <pre><code>&gt;&gt;&gt; x = torch.randn(3, 2)\n&gt;&gt;&gt; y = torch.ones(3, 2)\n&gt;&gt;&gt; x\ntensor([[-0.4620,  0.3139],\n [ 0.3898, -0.7197],\n [ 0.0478, -0.1657]])\n&gt;&gt;&gt; torch.where(x &gt; 0, x, y)\ntensor([[ 1.0000,  0.3139],\n [ 0.3898,  1.0000],\n [ 0.0478,  1.0000]])\n\n</code></pre>"},{"location":"1.0/torchvision_datasets/","title":"torchvision.datasets","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <p>\u6240\u6709\u7684\u6570\u636e\u96c6\u90fd\u662f<code>torch.utils.data.Dataset</code>\u7684\u5b50\u7c7b\uff0c \u5373\uff1a\u5b83\u4eec\u5b9e\u73b0\u4e86<code>__getitem__</code>\u548c<code>__len__</code>\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u90fd\u53ef\u4ee5\u4f20\u9012\u7ed9<code>torch.utils.data.DataLoader</code>\uff0c\u8fdb\u800c\u901a\u8fc7<code>torch.multiprocessing</code>\u5b9e\u73b0\u6279\u6570\u636e\u7684\u5e76\u884c\u5316\u52a0\u8f7d\u3002\u4f8b\u5982\uff1a</p> <pre><code>imagenet_data = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\ndata_loader = torch.utils.data.DataLoader(imagenet_data,\n                                          batch_size=4,\n                                          shuffle=True,\n                                          num_workers=args.nThreads)\n\n</code></pre> <p>\u76ee\u524d\u4e3a\u6b62\uff0c\u6536\u5f55\u7684\u6570\u636e\u96c6\u5305\u62ec\uff1a</p> <p>\u6570\u636e\u96c6</p> <ul> <li>MNIST</li> <li>Fashion-MNIST</li> <li>EMNIST</li> <li>COCO<ul> <li>Captions</li> <li>Detection</li> </ul> </li> <li>LSUN</li> <li>ImageFolder</li> <li>DatasetFolder</li> <li>Imagenet-12</li> <li>CIFAR</li> <li>STL10</li> <li>SVHN</li> <li>PhotoTour</li> <li>SBU</li> <li>Flickr</li> <li>VOC</li> </ul> <p>\u4ee5\u4e0a\u6570\u636e\u96c6\u7684\u63a5\u53e3\u57fa\u672c\u4e0a\u5f88\u76f8\u8fd1\u3002\u5b83\u4eec\u81f3\u5c11\u5305\u62ec\u4e24\u4e2a\u516c\u5171\u7684\u53c2\u6570<code>transform</code>\u548c<code>target_transform</code>\uff0c\u4ee5\u4fbf\u5206\u522b\u5bf9\u8f93\u5165\u548c\u548c\u76ee\u6807\u505a\u53d8\u6362\u3002</p> <pre><code>class torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)\n</code></pre> <p>MNIST\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u7684\u6839\u76ee\u5f55\uff0c\u5176\u4e2d\u5b58\u653e<code>processed/training.pt</code>\u548c<code>processed/test.pt</code>\u6587\u4ef6\u3002</li> <li>train(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece<code>training.pt</code>\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5426\u5219\u4ece<code>test.pt</code>\u521b\u5efa\u3002</li> <li>download(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue, \u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform (\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <pre><code>class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)\n</code></pre> <p>Fashion-MNIST\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u7684\u6839\u76ee\u5f55\uff0c\u5176\u4e2d\u5b58\u653e<code>processed/training.pt</code>\u548c<code>processed/test.pt</code>\u6587\u4ef6\u3002</li> <li>train(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece<code>training.pt</code>\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5426\u5219\u4ece<code>test.pt</code>\u521b\u5efa\u3002</li> <li>download(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <pre><code>class torchvision.datasets.EMNIST(root, split, **kwargs)\n</code></pre> <p>EMNIST\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570: </p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u7684\u6839\u76ee\u5f55\uff0c\u5176\u4e2d\u5b58\u653e<code>processed/training.pt</code>\u548c<code>processed/test.pt</code>\u6587\u4ef6\u3002</li> <li>split(string\uff09\u2013 \u8be5\u6570\u636e\u96c6\u5206\u62106\u79cd\uff1a<code>byclass</code>\uff0c<code>bymerge</code>\uff0c<code>balanced</code>\uff0c<code>letters</code>\uff0c<code>digits</code>\u548c<code>mnist</code>\u3002\u8fd9\u4e2a\u53c2\u6570\u6307\u5b9a\u4e86\u9009\u62e9\u5176\u4e2d\u7684\u54ea\u4e00\u79cd\u3002</li> <li>train(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece<code>training.pt</code>\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5426\u5219\u4ece<code>test.pt</code>\u521b\u5efa\u3002</li> <li>download(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue, \u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009) \u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <p>\u6ce8\u610f\uff1a</p> <p>\u4ee5\u4e0b\u8981\u6c42\u9884\u5148\u5b89\u88c5COCO API\u3002</p> <pre><code>class torchvision.datasets.CocoCaptions(root, annFile, transform=None, target_transform=None)\n</code></pre> <p>MS Coco Captions\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>root(string\uff09\u2013 \u4e0b\u8f7d\u6570\u636e\u7684\u76ee\u6807\u76ee\u5f55\u3002</li> <li>annFile(string\uff09\u2013 json\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.ToTensor</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <p>\u793a\u4f8b</p> <pre><code>import torchvision.datasets as dset\nimport torchvision.transforms as transforms\ncap = dset.CocoCaptions(root = 'dir where images are',\n                        annFile = 'json annotation file',\n                        transform=transforms.ToTensor())\n\nprint('Number of samples: ', len(cap))\nimg, target = cap[3] # load 4th sample\n\nprint(\"Image Size: \", img.size())\nprint(target)\n\n</code></pre> <p>\u8f93\u51fa\uff1a</p> <pre><code>Number of samples: 82783\nImage Size: (3L, 427L, 640L)\n[u'A plane emitting smoke stream flying over a mountain.',\nu'A plane darts across a bright blue sky behind a mountain covered in snow',\nu'A plane leaves a contrail above the snowy mountain top.',\nu'A mountain that has a plane flying overheard in the distance.',\nu'A mountain view with a plume of smoke in the background']\n\n</code></pre> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a \u5143\u7ec4(image, target)\uff0c\u5176\u4e2dtarget\u662f\u5217\u8868\u7c7b\u578b\uff0c\u5305\u542b\u4e86\u5bf9\u56fe\u7247image\u7684\u63cf\u8ff0\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.CocoDetection(root, annFile, transform=None, target_transform=None)\n</code></pre> <p>MS Coco Detection\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>root(string\uff09\u2013 \u4e0b\u8f7d\u6570\u636e\u7684\u76ee\u6807\u76ee\u5f55\u3002</li> <li>annFile(string\uff09\u2013 json\u6807\u6ce8\u6587\u4ef6\u7684\u8def\u5f84\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.ToTensor</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570: index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a \u5143\u7ec4(image, target)\uff0c\u5176\u4e2dtarget\u662f<code>coco.loadAnns</code>\u8fd4\u56de\u7684\u5bf9\u8c61\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.LSUN(root, classes='train', transform=None, target_transform=None)\n</code></pre> <p>LSUN\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>root(string\uff09\u2013 \u5b58\u653e\u6570\u636e\u6587\u4ef6\u7684\u6839\u76ee\u5f55\u3002</li> <li>classes(string \u6216 list\uff09\u2013 {'train', 'val', 'test'}\u4e4b\u4e00\uff0c\u6216\u8981\u52a0\u8f7d\u7c7b\u522b\u7684\u5217\u8868\uff0c\u5982['bedroom_train', 'church_train']\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009) \u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a \u5143\u7ec4(image, target)\uff0c\u5176\u4e2dtarget\u662f\u76ee\u6807\u7c7b\u522b\u7684\u7d22\u5f15\u3002 --- --- Return type: tuple --- --- <pre><code>class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=&lt;function default_loader&gt;)\n</code></pre> <p>\u4e00\u79cd\u901a\u7528\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5176\u56fe\u7247\u5e94\u8be5\u6309\u7167\u5982\u4e0b\u7684\u5f62\u5f0f\u4fdd\u5b58\uff1a</p> <pre><code>root/dog/xxx.png\nroot/dog/xxy.png\nroot/dog/xxz.png\n\nroot/cat/123.png\nroot/cat/nsdf3.png\nroot/cat/asd932_.png\n\n</code></pre> <p>\u53c2\u6570\uff1a </p> <ul> <li>root(string\uff09\u2013 \u6839\u76ee\u5f55\u8def\u5f84\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> <li>loader \u2013 \u4e00\u79cd\u51fd\u6570\uff0c\u53ef\u4ee5\u7531\u7ed9\u5b9a\u7684\u8def\u5f84\u52a0\u8f7d\u56fe\u7247\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (sample, target)\uff0c\u5176\u4e2dtarget\u662f\u76ee\u6807\u7c7b\u7684\u7c7b\u7d22\u5f15\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.DatasetFolder(root, loader, extensions, transform=None, target_transform=None)\n</code></pre> <p>\u4e00\u79cd\u901a\u7528\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5176\u6570\u636e\u5e94\u8be5\u6309\u7167\u5982\u4e0b\u7684\u5f62\u5f0f\u4fdd\u5b58\uff1a</p> <pre><code>root/class_x/xxx.ext\nroot/class_x/xxy.ext\nroot/class_x/xxz.ext\n\nroot/class_y/123.ext\nroot/class_y/nsdf3.ext\nroot/class_y/asd932_.ext\n\n</code></pre> <p>\u53c2\u6570: </p> <ul> <li>root(string\uff09\u2013 \u6839\u76ee\u5f55\u8def\u5f84\u3002</li> <li>loader(\u53ef\u88ab\u8c03\u7528\uff09\u2013 \u4e00\u79cd\u51fd\u6570\uff0c\u53ef\u4ee5\u7531\u7ed9\u5b9a\u7684\u8def\u5f84\u52a0\u8f7d\u6570\u636e\u3002</li> <li>extensions(list[__string__]\uff09\u2013 \u5217\u8868\uff0c\u5305\u542b\u5141\u8bb8\u7684\u6269\u5c55\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u6570\u636e\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a\u5bf9\u4e8e\u56fe\u7247\u6709<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform \u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (sample, target)\uff0c\u5176\u4e2dtarget\u662f\u76ee\u6807\u7c7b\u7684\u7c7b\u7d22\u5f15. --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <p>\u8fd9\u4e2a\u7c7b\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u5b9e\u73b0<code>ImageFolder</code>\u6570\u636e\u96c6\u3002\u6570\u636e\u9884\u5904\u7406\u89c1\u6b64\u5904\u3002</p> <p>\u793a\u4f8b\u3002</p> <pre><code>class torchvision.datasets.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)\n</code></pre> <p>CIFAR10\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u6839\u76ee\u5f55\uff0c\u8981\u4e48\u5176\u4e2d\u5e94\u5b58\u5728<code>cifar-10-batches-py</code>\u6587\u4ef6\u5939\uff0c\u8981\u4e48\u5f53download\u8bbe\u7f6e\u4e3aTrue\u65f6<code>cifar-10-batches-py</code>\u6587\u4ef6\u5939\u4fdd\u5b58\u5728\u6b64\u5904\u3002</li> <li>train(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue, \u4ece\u8bad\u7ec3\u96c6\u4e2d\u521b\u5efa\uff0c\u5426\u5219\u4ece\u6d4b\u8bd5\u96c6\u4e2d\u521b\u5efa\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> <li>download(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (image, target)\uff0c\u5176\u4e2dtarget\u662f\u76ee\u6807\u7c7b\u7684\u7c7b\u7d22\u5f15\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.CIFAR100(root, train=True, transform=None, target_transform=None, download=False)\n</code></pre> <p>CIFAR100\u6570\u636e\u96c6\u3002</p> <p>\u8fd9\u662f<code>CIFAR10</code>\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b50\u96c6\u3002</p> <pre><code>class torchvision.datasets.STL10(root, split='train', transform=None, target_transform=None, download=False)\n</code></pre> <p>STL10\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u6839\u76ee\u5f55\uff0c\u5e94\u8be5\u5305\u542b<code>stl10_binary</code>\u6587\u4ef6\u5939\u3002</li> <li>split(string\uff09\u2013 {'train', 'test', 'unlabeled', 'train+unlabeled'}\u4e4b\u4e00\uff0c\u9009\u62e9\u76f8\u5e94\u7684\u6570\u636e\u96c6\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> <li>download(bool, optional\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (image, target)\uff0c\u5176\u4e2dtarget\u5e94\u662f\u76ee\u6807\u7c7b\u7684\u7c7b\u7d22\u5f15\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=False)\n</code></pre> <p>SVHN\u6570\u636e\u96c6\u3002\u6ce8\u610f\uff1aSVHN\u6570\u636e\u96c6\u5c06<code>10</code>\u6307\u5b9a\u4e3a\u6570\u5b57<code>0</code>\u7684\u6807\u7b7e\u3002\u7136\u800c\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06<code>0</code>\u6307\u5b9a\u4e3a\u6570\u5b57<code>0</code>\u7684\u6807\u7b7e\u4ee5\u517c\u5bb9PyTorch\u7684\u635f\u5931\u51fd\u6570\uff0c\u56e0\u4e3a\u635f\u5931\u51fd\u6570\u8981\u6c42\u7c7b\u6807\u7b7e\u5728<code>[0, C-1]</code>\u7684\u8303\u56f4\u5185\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>root(string\uff09\u2013 \u6570\u636e\u96c6\u6839\u76ee\u5f55\uff0c\u5e94\u5305\u542b<code>SVHN</code>\u6587\u4ef6\u5939\u3002</li> <li>split(string\uff09\u2013 {'train', 'test', 'extra'}\u4e4b\u4e00\uff0c\u76f8\u5e94\u7684\u6570\u636e\u96c6\u4f1a\u88ab\u9009\u62e9\u3002'extra'\u662fextra\u8bad\u7ec3\u96c6\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002\u5982\uff1a<code>transforms.RandomCrop</code>\u3002</li> <li>target_transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165\u76ee\u6807\uff0c\u8fdb\u884c\u53d8\u6362\u3002</li> <li>download(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (image, target)\uff0c\u5176\u4e2dtarget\u662f\u76ee\u6807\u7c7b\u7684\u7c7b\u7d22\u5f15\u3002 --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- --- <pre><code>class torchvision.datasets.PhotoTour(root, name, train=True, transform=None, download=False)\n</code></pre> <p>Learning Local Image Descriptors Data\u6570\u636e\u96c6\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>root(string\uff09\u2013 \u4fdd\u5b58\u56fe\u7247\u7684\u6839\u76ee\u5f55\u3002</li> <li>name(string\uff09\u2013 \u8981\u52a0\u8f7d\u7684\u6570\u636e\u96c6\u3002</li> <li>transform(\u53ef\u88ab\u8c03\u7528 , \u53ef\u9009\uff09\u2013 \u4e00\u79cd\u51fd\u6570\u6216\u53d8\u6362\uff0c\u8f93\u5165PIL\u56fe\u7247\uff0c\u8fd4\u56de\u53d8\u6362\u4e4b\u540e\u7684\u6570\u636e\u3002</li> <li>download (bool, optional) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4ece\u4e92\u8054\u7f51\u4e0b\u8f7d\u6570\u636e\u5e76\u653e\u5230root\u6587\u4ef6\u5939\u4e0b\u3002\u5982\u679croot\u76ee\u5f55\u4e0b\u5df2\u7ecf\u5b58\u5728\u6570\u636e\uff0c\u4e0d\u4f1a\u518d\u6b21\u4e0b\u8f7d\u3002</li> </ul> <pre><code>__getitem__(index)\n</code></pre> \u53c2\u6570\uff1a index (int) \u2013 \u7d22\u5f15 \u8fd4\u56de\uff1a (data1, data2, matches) --- --- \u8fd4\u56de\u7c7b\u578b\uff1a tuple --- ---"},{"location":"1.0/torchvision_models/","title":"torchvision.models","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <p>models\u5b50\u5305\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u6a21\u578b\u67b6\u6784\uff1a</p> <ul> <li>AlexNet</li> <li>VGG</li> <li>ResNet</li> <li>SqueezeNet</li> <li>DenseNet</li> <li>Inception v3</li> </ul> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528\u4ee5\u4e0b\u6784\u9020\u51fd\u6570\u6784\u9020\u968f\u673a\u6743\u91cd\u7684\u6a21\u578b\uff1a</p> <pre><code>import torchvision.models as models\nresnet18 = models.resnet18()\nalexnet = models.alexnet()\nvgg16 = models.vgg16()\nsqueezenet = models.squeezenet1_0()\ndensenet = models.densenet161()\ninception = models.inception_v3()\n\n</code></pre> <p>\u6211\u4eec\u5728<code>torch.utils.model_zoo</code>\u4e2d\u63d0\u4f9b\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u53c2\u6570<code>pretrained=True</code>\u6784\u9020\uff1a</p> <pre><code>import torchvision.models as models\nresnet18 = models.resnet18(pretrained=True)\nalexnet = models.alexnet(pretrained=True)\nsqueezenet = models.squeezenet1_0(pretrained=True)\nvgg16 = models.vgg16(pretrained=True)\ndensenet = models.densenet161(pretrained=True)\ninception = models.inception_v3(pretrained=True)\n\n</code></pre> <p>\u5b9a\u4e49\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4f1a\u628a\u6743\u503c\u4e0b\u8f7d\u5230\u4e00\u4e2a\u7f13\u5b58\u6587\u4ef6\u5939\u4e2d\uff0c\u8fd9\u4e2a\u7f13\u5b58\u6587\u4ef6\u53ef\u4ee5\u901a\u8fc7\u73af\u5883\u53d8\u91cf<code>TORCH_MODEL_ZOO</code>\u6765\u6307\u5b9a\u3002\u66f4\u591a\u7ec6\u8282\u89c1<code>torch.utils.model_zoo.load_url()</code>\u3002</p> <p>\u6709\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u7528\u5230\u4e86\u4e0d\u540c\u7684\u6a21\u5757\uff0c\u4f8b\u5982\u6279\u6807\u51c6\u5316(batch normalization\uff09\u3002\u4f7f\u7528<code>model.train()</code>\u6216<code>model.eval()</code>\u53ef\u4ee5\u5207\u6362\u5230\u76f8\u5e94\u7684\u6a21\u5f0f\u3002\u66f4\u591a\u7ec6\u8282\u89c1<code>train()</code>\u6216<code>eval()</code>\u3002</p> <p>\u6240\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u90fd\u8981\u6c42\u8f93\u5165\u56fe\u7247\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u5373\uff1a\u5c0f\u6279(mini-batch\uff09\u4e09\u901a\u9053RGB\u683c\u5f0f(3 x H x W\uff09\uff0c\u5176\u4e2dH\u548cW\u4e0d\u5f97\u5c0f\u4e8e224\u3002\u56fe\u7247\u52a0\u8f7d\u65f6\u50cf\u7d20\u503c\u7684\u8303\u56f4\u5e94\u5728[0, 1]\u5185\uff0c\u7136\u540e\u901a\u8fc7\u6307\u5b9a<code>mean = [0.485, 0.456, 0.406]</code>\u548c<code>std = [0.229, 0.224, 0.225]</code>\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u4f8b\u5982\uff1a</p> <pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n</code></pre> <p>\u5728imagenet\u7684\u793a\u4f8b\u4e2d\u53ef\u4ee5\u770b\u5230\u6807\u51c6\u5316\u7684\u4e00\u4e2a\u5e94\u7528\u3002</p> <p>\u4e0b\u8868\u662fImageNet\u5355\u6b21224x224\u4e2d\u5fc3\u88c1\u526a\u7684\u9519\u8bef\u7387\u3002</p> \u7f51\u7edc Top-1\u9519\u8bef\u7387(%\uff09 Top-5\u9519\u8bef\u7387(%\uff09 AlexNet 43.45 20.91 VGG-11 30.98 11.37 VGG-13 30.07 10.75 VGG-16 28.41 9.62 VGG-19 27.62 9.12 \u5e26\u6709\u6279\u6807\u51c6\u5316\u7684VGG-11 29.62 10.19 \u5e26\u6709\u6279\u6807\u51c6\u5316\u7684VGG-13 28.45 9.63 \u5e26\u6709\u6279\u6807\u51c6\u5316\u7684VGG-16 26.63 8.50 \u5e26\u6709\u6279\u6807\u51c6\u5316\u7684VGG-19 25.76 8.15 ResNet-18 30.24 10.92 ResNet-34 26.70 8.58 ResNet-50 23.85 7.13 ResNet-101 22.63 6.44 ResNet-152 21.69 5.94 SqueezeNet 1.0 41.90 19.58 SqueezeNet 1.1 41.81 19.38 Densenet-121 25.35 7.83 Densenet-169 24.00 7.00 Densenet-201 22.80 6.43 Densenet-161 22.35 6.20 Inception v3 22.55 6.44"},{"location":"1.0/torchvision_models/#alexnet","title":"Alexnet","text":"<pre><code>torchvision.models.alexnet(pretrained=False, **kwargs)\n</code></pre> <p>AlexNet\u6a21\u578b\uff0c\u53c2\u89c1\u8bba\u6587\u300aOne weird trick\u2026\u300b \u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_models/#vgg","title":"VGG","text":"<pre><code>torchvision.models.vgg11(pretrained=False, **kwargs)\n</code></pre> <p>VGG11\u6a21\u578b\u3002(\u8bba\u6587\u4e2d\u7684\u201cA\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg11_bn(pretrained=False, **kwargs)\n</code></pre> <p>VGG11\u6a21\u578b\uff0c\u5e26\u6709\u6279\u6807\u51c6\u5316\u3002(\u8bba\u6587\u4e2d\u7684\u201cA\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg13(pretrained=False, **kwargs)\n</code></pre> <p>VGG13\u6a21\u578b\u3002(\u8bba\u6587\u4e2d\u7684\u201cB\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg13_bn(pretrained=False, **kwargs)\n</code></pre> <p>VGG13\u6a21\u578b\uff0c\u5e26\u6709\u6279\u6807\u51c6\u5316\u3002(\u8bba\u6587\u4e2d\u7684\u201cB\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg16(pretrained=False, **kwargs)\n</code></pre> <p>VGG16\u6a21\u578b\u3002(\u8bba\u6587\u4e2d\u7684\u201cD\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg16_bn(pretrained=False, **kwargs)\n</code></pre> <p>VGG16\u6a21\u578b\uff0c\u5e26\u6709\u6279\u6807\u51c6\u5316\u3002(\u8bba\u6587\u4e2d\u7684\u201cD\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg19(pretrained=False, **kwargs)\n</code></pre> <p>VGG19\u6a21\u578b\u3002(\u8bba\u6587\u4e2d\u7684\u201cE\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.vgg19_bn(pretrained=False, **kwargs)\n</code></pre> <p>VGG19\u6a21\u578b\uff0c\u5e26\u6709\u6279\u6807\u51c6\u5316\u3002(\u8bba\u6587\u4e2d\u7684\u201cE\u201d\u6a21\u578b\uff09</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_models/#resnet","title":"ResNet","text":"<pre><code>torchvision.models.resnet18(pretrained=False, **kwargs)\n</code></pre> <p>\u6784\u9020ResNet-18\u6a21\u578b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.resnet34(pretrained=False, **kwargs)\n</code></pre> <p>\u6784\u9020ResNet-34\u6a21\u578b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.resnet50(pretrained=False, **kwargs)\n</code></pre> <p>\u6784\u9020ResNet-50\u6a21\u578b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.resnet101(pretrained=False, **kwargs)\n</code></pre> <p>\u6784\u9020ResNet-101\u6a21\u578b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.resnet152(pretrained=False, **kwargs)\n</code></pre> <p>\u6784\u9020ResNet-152\u6a21\u578b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_models/#squeezenet","title":"SqueezeNet","text":"<pre><code>torchvision.models.squeezenet1_0(pretrained=False, **kwargs)\n</code></pre> <p>SqueezeNet\u6a21\u578b\uff0c\u53c2\u89c1\u8bba\u6587\u300aSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.squeezenet1_1(pretrained=False, **kwargs)\n</code></pre> <p>SqueezeNet 1.1\u6a21\u578b\uff0c\u53c2\u89c1SqueezeNet\u5b98\u65b9\u4ed3\u5e93\u3002SqueezeNet 1.1\u6bd4SqueezeNet 1.0\u8282\u7ea62.4\u500d\u7684\u8ba1\u7b97\u91cf\uff0c\u53c2\u6570\u4e5f\u7565\u5c11\uff0c\u7136\u800c\u7cbe\u5ea6\u672a\u505a\u727a\u7272\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_models/#densenet","title":"DenseNet","text":"<pre><code>torchvision.models.densenet121(pretrained=False, **kwargs)\n</code></pre> <p>Densenet-121\u6a21\u578b\uff0c\u53c2\u89c1\u300aDensely Connected Convolutional Networks\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.densenet169(pretrained=False, **kwargs)\n</code></pre> <p>Densenet-169\u6a21\u578b\uff0c\u53c2\u89c1\u300aDensely Connected Convolutional Networks\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.densenet161(pretrained=False, **kwargs)\n</code></pre> <p>Densenet-161\u6a21\u578b\uff0c\u53c2\u89c1\u300aDensely Connected Convolutional Networks\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b <pre><code>torchvision.models.densenet201(pretrained=False, **kwargs)\n</code></pre> <p>Densenet-201\u6a21\u578b\uff0c\u53c2\u89c1\u300aDensely Connected Convolutional Networks\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_models/#inception-v3","title":"Inception v3","text":"<pre><code>torchvision.models.inception_v3(pretrained=False, **kwargs)\n</code></pre> <p>Inception v3\u6a21\u578b\uff0c\u53c2\u89c1\u300aRethinking the Inception Architecture for Computer Vision\u300b\u3002</p> \u53c2\u6570\uff1a pretrained (bool) \u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u8fd4\u56deImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"1.0/torchvision_transforms/","title":"torchvision.transforms","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <p>transforms\u5305\u542b\u4e86\u4e00\u4e9b\u5e38\u7528\u7684\u56fe\u50cf\u53d8\u6362\uff0c\u8fd9\u4e9b\u53d8\u6362\u80fd\u591f\u7528<code>Compose</code>\u4e32\u8054\u7ec4\u5408\u8d77\u6765\u3002\u53e6\u5916\uff0ctorchvision\u63d0\u4f9b\u4e86<code>torchvision.transforms.functional</code>\u6a21\u5757\u3002functional\u53ef\u4ee5\u63d0\u4f9b\u4e86\u4e00\u4e9b\u66f4\u52a0\u7cbe\u7ec6\u7684\u53d8\u6362\uff0c\u7528\u4e8e\u642d\u5efa\u590d\u6742\u7684\u53d8\u6362\u6d41\u6c34\u7ebf(\u4f8b\u5982\u5206\u5272\u4efb\u52a1\uff09\u3002</p> <pre><code>class torchvision.transforms.Compose(transforms)\n</code></pre> <p>\u7528\u4e8e\u628a\u4e00\u7cfb\u5217\u53d8\u6362\u7ec4\u5408\u5230\u4e00\u8d77\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>transforms(list\u6216<code>Transform</code>\u5bf9\u8c61\uff09- \u4e00\u7cfb\u5217\u9700\u8981\u8fdb\u884c\u7ec4\u5408\u7684\u53d8\u6362\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; transforms.Compose([\n&gt;&gt;&gt;     transforms.CenterCrop(10),\n&gt;&gt;&gt;     transforms.ToTensor(),\n&gt;&gt;&gt; ])\n</code></pre>"},{"location":"1.0/torchvision_transforms/#pil","title":"\u5bf9PIL\u56fe\u7247\u7684\u53d8\u6362","text":"<pre><code>class torchvision.transforms.CenterCrop(size)\n</code></pre> <p>\u5728\u4e2d\u5fc3\u5904\u88c1\u526aPIL\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u9700\u8981\u88c1\u526a\u51fa\u7684\u5f62\u72b6\u3002\u5982\u679csize\u662fint\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u6b63\u65b9\u5f62\uff1b\u5982\u679c\u662f\u5f62\u5982(h, w)\u7684\u5e8f\u5217\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u77e9\u5f62\u3002 </li> </ul> <pre><code>class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)\n</code></pre> <p>\u968f\u673a\u6539\u53d8\u56fe\u7247\u7684\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u548c\u9971\u548c\u5ea6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>brightness(float\u6216 float\u7c7b\u578b\u5143\u7ec4(min, max)\uff09\u2013 \u4eae\u5ea6\u7684\u6270\u52a8\u5e45\u5ea6\u3002brightness_factor\u4ece[max(0, 1 - brightness), 1 + brightness]\u4e2d\u968f\u673a\u91c7\u6837\u4ea7\u751f\u3002\u5e94\u5f53\u662f\u975e\u8d1f\u6570\u3002</li> <li>contrast(float\u6216 float\u7c7b\u578b\u5143\u7ec4(min, max)\uff09\u2013 \u5bf9\u6bd4\u5ea6\u6270\u52a8\u5e45\u5ea6\u3002contrast_factor\u4ece[max(0, 1 - contrast), 1 + contrast]\u4e2d\u968f\u673a\u91c7\u6837\u4ea7\u751f\u3002\u5e94\u5f53\u662f\u975e\u8d1f\u6570\u3002</li> <li>saturation(float\u6216 float\u7c7b\u578b\u5143\u7ec4(min, max)\uff09\u2013 \u9971\u548c\u5ea6\u6270\u52a8\u5e45\u5ea6\u3002saturation_factor\u4ece[max(0, 1 - saturation), 1 + saturation]\u4e2d\u968f\u673a\u91c7\u6837\u4ea7\u751f\u3002\u5e94\u5f53\u662f\u975e\u8d1f\u6570\u3002</li> <li>hue(float\u6216 float\u7c7b\u578b\u5143\u7ec4(min, max)\uff09\u2013 \u8272\u76f8\u6270\u52a8\u5e45\u5ea6\u3002hue_factor\u4ece[-hue, hue]\u4e2d\u968f\u673a\u91c7\u6837\u4ea7\u751f\uff0c\u5176\u503c\u5e94\u5f53\u6ee1\u8db30&lt;= hue &lt;= 0.5\u6216-0.5 &lt;= min &lt;= max &lt;= 0.5</li> </ul> <pre><code>class torchvision.transforms.FiveCrop(size)\n</code></pre> <p>\u4ece\u56db\u89d2\u548c\u4e2d\u5fc3\u88c1\u526aPIL\u56fe\u7247\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u8be5\u53d8\u6362\u8fd4\u56de\u56fe\u50cf\u5143\u7ec4\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u56fe\u7247\u5728\u7f51\u7edc\u4e2d\u4f20\u5bfc\u540e\u548c\u6570\u636e\u96c6\u7ed9\u51fa\u7684\u6807\u7b7e\u7b49\u4fe1\u606f\u4e0d\u80fd\u5339\u914d\u3002\u5904\u7406\u65b9\u6cd5\u89c1\u4e0b\u9762\u7684\u793a\u4f8b\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u9700\u8981\u88c1\u526a\u51fa\u7684\u5f62\u72b6\u3002\u5982\u679csize\u662fint\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u6b63\u65b9\u5f62\uff1b\u5982\u679c\u662f\u5e8f\u5217\uff0c\u5982(h, w)\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u77e9\u5f62\u3002 </li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; transform = Compose([\n&gt;&gt;&gt;    FiveCrop(size), # \u8fd9\u91cc\u4ea7\u751f\u4e86\u4e00\u4e2aPIL\u56fe\u50cf\u5217\u8868\n&gt;&gt;&gt;    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # \u8fd4\u56de4\u7ef4\u5f20\u91cf\n&gt;&gt;&gt; ])\n&gt;&gt;&gt; # \u5728\u6d4b\u8bd5\u9636\u6bb5\u4f60\u53ef\u4ee5\u8fd9\u6837\u505a\uff1a\n&gt;&gt;&gt; input, target = batch # input\u662f5\u7ef4\u5f20\u91cf\uff0ctarget\u662f2\u7ef4\u5f20\u91cf\u3002\n&gt;&gt;&gt; bs, ncrops, c, h, w = input.size()\n&gt;&gt;&gt; result = model(input.view(-1, c, h, w)) # \u628abatch size\u548cncrops\u878d\u5408\u5728\u4e00\u8d77\n&gt;&gt;&gt; result_avg = result.view(bs, ncrops, -1).mean(1) # crops\u4e0a\u7684\u5e73\u5747\u503c\n\n</code></pre> <pre><code>class torchvision.transforms.Grayscale(num_output_channels=1)\n</code></pre> <p>\u628a\u56fe\u7247\u8f6c\u6362\u4e3a\u7070\u9636\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>num_output_channels(int\uff0c1\u62163\uff09\u2013 \u5e0c\u671b\u5f97\u5230\u7684\u56fe\u7247\u901a\u9053\u6570\u3002</li> </ul> <p>\u8fd4\u56de\uff1a *   \u8f93\u5165\u56fe\u7247\u7684\u7070\u9636\u7248\u672c\u3002 - \u5982\u679cnum_output_channels == 1\uff1a\u8fd4\u56de\u5355\u901a\u9053\u56fe\u50cf\uff1b- \u5982\u679cnum_output_channels == 3\uff1a\u8fd4\u56de3\u901a\u9053\u56fe\u50cf\uff0c\u5176\u4e2dr == g == b\u3002</p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a *   PIL\u56fe\u50cf\u3002</p> <pre><code>class torchvision.transforms.Pad(padding, fill=0, padding_mode='constant')\n</code></pre> <p>\u5bf9PIL\u56fe\u50cf\u7684\u5404\u6761\u8fb9\u7f18\u8fdb\u884c\u6269\u5c55\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>padding(int \u6216 tuple\uff09\u2013 \u5728\u6bcf\u6761\u8fb9\u4e0a\u5c55\u5f00\u7684\u5bbd\u5ea6\u3002\u5982\u679c\u4f20\u5165\u7684\u662f\u5355\u4e2aint\uff0c\u5c31\u5728\u6240\u6709\u8fb9\u5c55\u5f00\u3002\u5982\u679c\u4f20\u5165\u957f\u4e3a2\u7684\u5143\u7ec4\uff0c\u5219\u6307\u5b9a\u5de6\u53f3\u548c\u4e0a\u4e0b\u7684\u5c55\u5f00\u5bbd\u5ea6\u3002\u5982\u679c\u4f20\u5165\u957f\u4e3a4\u7684\u5143\u7ec4\uff0c\u5219\u4f9d\u6b21\u6307\u5b9a\u4e3a\u5de6\u3001\u4e0a\u3001\u53f3\u3001\u4e0b\u7684\u5c55\u5f00\u5bbd\u5ea6\u3002</li> <li>fill(int \u6216 tuple\uff09 \u2013 \u50cf\u7d20\u586b\u5145\u503c\u3002\u9ed8\u8ba4\u662f0\u3002\u5982\u679c\u6307\u5b9a\u957f\u5ea6\u4e3a3\u7684\u5143\u7ec4\uff0c\u8868\u793a\u5206\u522b\u586b\u5145R, G, B\u901a\u9053\u3002\u8fd9\u4e2a\u53c2\u6570\u4ec5\u5728padding_mode\u662f'constant'\u65f6\u6307\u5b9a\u6709\u6548\u3002</li> <li>padding_mode(str\uff09\u2013 \u5c55\u5f00\u7c7b\u578b\u3002\u5e94\u5f53\u662f'constant'\uff0c'edge'\uff0c'reflect'\u6216'symmetric'\u4e4b\u4e00\u3002\u9ed8\u8ba4\u4e3a'constant'\u3002</li> <li>constant\uff1a\u7528\u5e38\u6570\u6269\u5c55\uff0c\u8fd9\u4e2a\u503c\u7531fill\u53c2\u6570\u6307\u5b9a\u3002</li> <li>edge\uff1a\u7528\u56fe\u50cf\u8fb9\u7f18\u4e0a\u7684\u6307\u586b\u5145\u3002</li> <li>reflect\uff1a\u4ee5\u8fb9\u7f18\u4e3a\u5bf9\u79f0\u8f74\u8fdb\u884c\u8f74\u5bf9\u79f0\u586b\u5145(\u8fb9\u7f18\u503c\u4e0d\u91cd\u590d\uff09\u3002    &gt; \u4f8b\u5982\uff0c\u5728[1, 2, 3, 4]\u7684\u4e24\u8fb9\u586b\u51452\u4e2a\u5143\u7d20\u4f1a\u5f97\u5230[3, 2, 1, 2, 3, 4, 3, 2]\u3002</li> <li>symmetric\uff1a\u7528\u56fe\u50cf\u8fb9\u7f18\u7684\u53cd\u8f6c\u8fdb\u884c\u586b\u5145(\u56fe\u50cf\u7684\u8fb9\u7f18\u503c\u9700\u8981\u91cd\u590d\uff09\u3002    &gt; \u4f8b\u5982\uff0c\u5728[1, 2, 3, 4]\u7684\u4e24\u8fb9\u586b\u51452\u4e2a\u5143\u7d20\u4f1a\u5f97\u5230[2, 1, 1, 2, 3, 4, 4, 3]\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n</code></pre> <p>\u4fdd\u6301\u50cf\u7d20\u7684\u5206\u5e03\u4e2d\u5fc3\u4e0d\u53d8\uff0c\u5bf9\u56fe\u50cf\u505a\u968f\u673a\u4eff\u5c04\u53d8\u6362\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>degrees(\u5e8f\u5217 \u6216 float \u6216 int\uff09\u2013 \u65cb\u8f6c\u89d2\u5ea6\u7684\u7b5b\u9009\u8303\u56f4\u3002\u5982\u679c\u662f\u5e8f\u5217(min, max\uff09\uff0c\u4ece\u4e2d\u968f\u673a\u5747\u5300\u91c7\u6837\uff1b\u5982\u679c\u662f\u6570\u5b57\uff0c\u5219\u4ece(-degrees, +degrees\uff09\u4e2d\u91c7\u6837\u3002\u5982\u679c\u4e0d\u9700\u8981\u65cb\u8f6c\uff0c\u90a3\u4e48\u8bbe\u7f6e\u4e3a0\u3002</li> <li>translate(tuple, \u53ef\u9009\uff09\u2013 \u5143\u7ec4\uff0c\u5143\u7d20\u503c\u662f\u6c34\u5e73\u548c\u5782\u76f4\u5e73\u79fb\u53d8\u6362\u7684\u6700\u5927\u7edd\u5bf9\u503c\u3002\u4f8b\u5982translate=(a, b)\u65f6\uff0c\u6c34\u5e73\u4f4d\u79fb\u503c\u4ece -img_width * a &lt; dx &lt; img_width * a\u4e2d\u968f\u673a\u91c7\u6837\u5f97\u5230\uff0c\u5782\u76f4\u4f4d\u79fb\u503c\u4ece-img_height * b &lt; dy &lt; img_height * b\u4e2d\u968f\u673a\u91c7\u6837\u5f97\u5230\u3002\u9ed8\u8ba4\u4e0d\u505a\u5e73\u79fb\u53d8\u6362\u3002</li> <li>scale(tuple, \u53ef\u9009\uff09\u2013 \u5c3a\u5ea6\u653e\u7f29\u56e0\u5b50\u7684\u5185\u533a\u95f4\uff0c\u5982[a, b]\uff0c\u653e\u7f29\u56e0\u5b50scale\u7684\u968f\u673a\u91c7\u6837\u533a\u95f4\u4e3a\uff1aa &lt;= scale &lt;= b\u3002\u9ed8\u8ba4\u4e0d\u8fdb\u884c\u5c3a\u5ea6\u653e\u7f29\u53d8\u6362\u3002</li> <li>shear(\u5e8f\u5217 \u6216 float \u6216 int, \u53ef\u9009\uff09\u2013 \u626d\u66f2\u89d2\u5ea6\u7684\u7b5b\u9009\u8303\u56f4\u3002\u5982\u679c\u662f\u5e8f\u5217(min, max\uff09\uff0c\u4ece\u4e2d\u968f\u673a\u5747\u5300\u91c7\u6837\uff1b\u5982\u679c\u662f\u6570\u5b57\uff0c\u5219\u4ece(-degrees, +degrees\uff09\u4e2d\u91c7\u6837\u3002\u9ed8\u8ba4\u4e0d\u4f1a\u8fdb\u884c\u626d\u66f2\u64cd\u4f5c\u3002</li> <li>resample({PIL.Image.NEAREST , PIL.Image.BILINEAR , PIL.Image.BICUBIC} , \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u91cd\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u89c1filters\u3002\u5982\u679c\u6ca1\u6709\u8be5\u9009\u9879\uff0c\u6216\u8005\u56fe\u7247\u6a21\u5f0f\u662f\u201c1\u201d\u6216\u201cP\u201d\uff0c\u8bbe\u7f6e\u4e3aPIL.Image.NEAREST\u3002</li> <li>fillcolor(int\uff09\u2013 \u5728\u8f93\u51fa\u56fe\u7247\u7684\u53d8\u6362\u5916\u533a\u57df\u53ef\u9009\u5730\u586b\u5145\u989c\u8272\u3002(Pillow&gt;=5.0.0\uff09\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomApply(transforms, p=0.5)\n</code></pre> <p>\u5bf9transforms\u4e2d\u7684\u5404\u53d8\u6362\u4ee5\u6307\u5b9a\u7684\u6982\u7387\u51b3\u5b9a\u662f\u5426\u9009\u62e9\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>transforms(list or tuple\uff09\u2013 \u53d8\u6362\u7684\u96c6\u5408\u3002</li> <li>p(float\uff09\u2013 \u6982\u7387\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomChoice(transforms)\n</code></pre> <p>\u4ece\u5217\u8868\u4e2d\u968f\u673a\u9009\u62e9\u4e00\u79cd\u53d8\u6362\u3002</p> <pre><code>class torchvision.transforms.RandomCrop(size, padding=0, pad_if_needed=False)\n</code></pre> <p>\u5bf9\u7ed9\u51fa\u7684PIL\u56fe\u7247\u5728\u968f\u673a\u4f4d\u7f6e\u5904\u8fdb\u884c\u88c1\u526a\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u60f3\u8981\u88c1\u526a\u51fa\u7684\u56fe\u7247\u7684\u5f62\u72b6\u3002\u5982\u679csize\u662fint\uff0c\u6309\u7167\u6b63\u65b9\u5f62(size, size\uff09\u88c1\u526a\uff1b \u5982\u679csize\u662f\u5e8f\u5217(h, w\uff09\uff0c\u88c1\u526a\u4e3a\u77e9\u5f62\u3002</li> <li>padding(int \u6216 \u5e8f\u5217 , \u53ef\u9009\uff09\u2013 \u5728\u56fe\u50cf\u7684\u8fb9\u7f18\u8fdb\u884c\u586b\u5145\uff0c\u9ed8\u8ba40\uff0c\u5373\u4e0d\u505a\u586b\u5145\u3002\u5982\u679c\u6307\u5b9a\u957f\u4e3a4\u7684\u5e8f\u5217\uff0c\u5219\u5206\u522b\u6307\u5b9a\u5de6\u3001\u4e0a\u3001\u53f3\u3001\u4e0b\u7684\u586b\u5145\u5bbd\u5ea6\u3002</li> <li>pad_if_needed(boolean\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u82e5\u56fe\u7247\u5c0f\u4e8e\u76ee\u6807\u5f62\u72b6\uff0c\u5c06\u8fdb\u884c\u586b\u5145\u4ee5\u907f\u514d\u62a5\u5f02\u5e38\u3002</li> <li>fill(int \u6216 tuple\uff09 \u2013 \u50cf\u7d20\u586b\u5145\u503c\u3002\u9ed8\u8ba4\u662f0\u3002\u5982\u679c\u6307\u5b9a\u957f\u5ea6\u4e3a3\u7684\u5143\u7ec4\uff0c\u8868\u793a\u5206\u522b\u586b\u5145R, G, B\u901a\u9053\u3002\u8fd9\u4e2a\u53c2\u6570\u4ec5\u5728padding_mode\u662f'constant'\u65f6\u6307\u5b9a\u6709\u6548\u3002</li> <li>padding_mode(str\uff09\u2013 \u5c55\u5f00\u7c7b\u578b\u3002\u5e94\u5f53\u662f'constant'\uff0c'edge'\uff0c'reflect'\u6216'symmetric'\u4e4b\u4e00\u3002\u9ed8\u8ba4\u4e3a'constant'\u3002</li> <li>constant\uff1a\u7528\u5e38\u6570\u6269\u5c55\uff0c\u8fd9\u4e2a\u503c\u7531fill\u53c2\u6570\u6307\u5b9a\u3002</li> <li>edge\uff1a\u7528\u56fe\u50cf\u8fb9\u7f18\u4e0a\u7684\u6307\u586b\u5145\u3002</li> <li>reflect\uff1a\u4ee5\u8fb9\u7f18\u4e3a\u5bf9\u79f0\u8f74\u8fdb\u884c\u8f74\u5bf9\u79f0\u586b\u5145(\u8fb9\u7f18\u503c\u4e0d\u91cd\u590d\uff09\u3002    &gt; \u4f8b\u5982\uff0c\u5728[1, 2, 3, 4]\u7684\u4e24\u8fb9\u586b\u51452\u4e2a\u5143\u7d20\u4f1a\u5f97\u5230[3, 2, 1, 2, 3, 4, 3, 2]\u3002</li> <li>symmetric\uff1a\u7528\u56fe\u50cf\u8fb9\u7f18\u7684\u53cd\u8f6c\u8fdb\u884c\u586b\u5145(\u56fe\u50cf\u7684\u8fb9\u7f18\u503c\u9700\u8981\u91cd\u590d\uff09\u3002    &gt; \u4f8b\u5982\uff0c\u5728[1, 2, 3, 4]\u7684\u4e24\u8fb9\u586b\u51452\u4e2a\u5143\u7d20\u4f1a\u5f97\u5230[2, 1, 1, 2, 3, 4, 4, 3]\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomGrayscale(p=0.1)\n</code></pre> <p>\u4ee5\u6982\u7387p(\u9ed8\u8ba40.1\uff09\u5c06\u56fe\u7247\u968f\u673a\u8f6c\u5316\u4e3a\u7070\u9636\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>p(float\uff09\u2013\u56fe\u50cf\u8f6c\u5316\u4e3a\u7070\u9636\u7684\u6982\u7387\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u4ee5\u6982\u7387p\u8f6c\u6362\u4e3a\u7070\u9636\uff0c\u4ee5\u6982\u7387(1-p\uff09\u4e0d\u505a\u53d8\u6362\u3002\u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a1\u901a\u9053\uff0c\u5219\u7070\u9636\u7248\u672c\u4e5f\u662f1\u901a\u9053\u3002\u5982\u679c\u8f93\u5165\u56fe\u50cf\u4e3a3\u901a\u9053\uff0c\u5219\u7070\u9636\u7248\u672c\u662f3\u901a\u9053\uff0cr == g == b\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomHorizontalFlip(p=0.5)\n</code></pre> <p>\u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u968f\u673a\u6c34\u5e73\u7ffb\u6298PIL\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>p(float\uff09\u2013 \u7ffb\u6298\u56fe\u7247\u7684\u6982\u7387\u3002\u9ed8\u8ba40.5\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomOrder(transforms)\n</code></pre> <p>\u4ee5\u968f\u673a\u7684\u987a\u5e8f\u5bf9\u56fe\u7247\u505a\u53d8\u6362\u3002</p> <pre><code>class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n</code></pre> <p>\u4ee5\u968f\u673a\u7684\u5f62\u72b6\u548c\u957f\u5bbd\u6bd4\u88c1\u526a\u56fe\u7247\u3002</p> <p>\u4ee5\u968f\u673a\u7684\u5f62\u72b6(\u9ed8\u8ba4\u4ece\u539f\u59cb\u56fe\u7247\u76840.08\u52301.0) \u548c\u968f\u673a\u957f\u5bbd\u6bd4(\u9ed8\u8ba4\u4ece3/4\u52304/3\uff09\u88c1\u526a\u56fe\u7247\u3002\u7136\u540e\u8c03\u6574\u5230\u6307\u5b9a\u5f62\u72b6\u3002\u8fd9\u4e00\u53d8\u6362\u901a\u5e38\u7528\u4e8e\u8bad\u7ec3Inception\u7f51\u7edc\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>size \u2013 \u6bcf\u6761\u8fb9\u7684\u671f\u671b\u8f93\u51fa\u5f62\u72b6\u3002</li> <li>scale \u2013 \u88c1\u526a\u539f\u59cb\u56fe\u7247\u51fa\u7684\u5f62\u72b6\u8303\u56f4\u3002</li> <li>ratio \u2013 \u539f\u59cb\u957f\u5bbd\u6bd4\u88c1\u526a\u51fa\u7684\u76ee\u6807\u8303\u56f4\u3002</li> <li>interpolation \u2013 \u9ed8\u8ba4\uff1aPIL.Image.BILINEAR\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None)\n</code></pre> <p>\u4ee5\u6307\u5b9a\u7684\u89d2\u5ea6\u9009\u88c5\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>degrees(\u5e8f\u5217 \u6216 float or int\uff09\u2013 \u65cb\u8f6c\u89d2\u5ea6\u7684\u968f\u673a\u9009\u53d6\u8303\u56f4\u3002\u5982\u679cdegrees\u662f\u5e8f\u5217(min, max\uff09\uff0c\u5219\u4ece\u4e2d\u968f\u673a\u9009\u53d6\uff1b\u5982\u679c\u662f\u6570\u5b57\uff0c\u5219\u9009\u62e9\u8303\u56f4\u662f(-degrees, +degrees\uff09\u3002</li> <li>resample({PIL.Image.NEAREST , PIL.Image.BILINEAR , PIL.Image.BICUBIC} , \u53ef\u9009) \u2013 \u53ef\u9009\u7684\u91cd\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u89c1filters\u3002\u5982\u679c\u8be5\u9009\u9879\u5ffd\u7565\uff0c\u6216\u56fe\u7247\u6a21\u5f0f\u662f\u201c1\u201d\u6216\u8005\u201cP\u201d\u5219\u8bbe\u7f6e\u4e3aPIL.Image.NEAREST\u3002</li> <li>expand(bool, \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u6269\u5c55\u6807\u5fd7\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aTrue, \u5c06\u8f93\u51fa\u6269\u5c55\u5230\u8db3\u591f\u5927\u4ece\u800c\u80fd\u5bb9\u7eb3\u5168\u56fe\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aFalse\u6216\u4e0d\u8bbe\u7f6e\uff0c\u8f93\u51fa\u56fe\u7247\u5c06\u548c\u8f93\u5165\u540c\u6837\u5927\u3002\u6ce8\u610fexpand\u6807\u5fd7\u8981\u6c42 flag assumes rotation around the center and no translation\u3002</li> <li>center(2-tuple , \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u65cb\u8f6c\u4e2d\u5fc3\u5750\u6807\u3002\u4ee5\u5de6\u4e0a\u89d2\u4e3a\u539f\u70b9\u8ba1\u7b97\u3002\u9ed8\u8ba4\u662f\u56fe\u50cf\u4e2d\u5fc3\u3002</li> </ul> <pre><code>class torchvision.transforms.RandomSizedCrop(*args, **kwargs)\n</code></pre> <p>\u6ce8\u610f\uff1a\u8be5\u53d8\u6362\u5df2\u88ab\u5f03\u7528\uff0c\u53ef\u7528RandomResizedCrop\u4ee3\u66ff\u3002</p> <pre><code>class torchvision.transforms.RandomVerticalFlip(p=0.5)\n</code></pre> <p>\u4ee5\u7ed9\u5b9a\u7684\u6982\u7387\u968f\u673a\u5782\u76f4\u7ffb\u6298PIL\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>p (float) \u2013 \u7ffb\u6298\u56fe\u7247\u7684\u6982\u7387\u3002\u9ed8\u8ba40.5\u3002</li> </ul> <pre><code>class torchvision.transforms.Resize(size, interpolation=2)\n</code></pre> <p>\u5c06\u8f93\u5165PIL\u56fe\u7247\u8c03\u6574\u5927\u5c0f\u5230\u7ed9\u5b9a\u5f62\u72b6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u671f\u671b\u8f93\u51fa\u5f62\u72b6\u3002\u5982\u679csize\u5f62\u5982(h, w\uff09\uff0c\u8f93\u51fa\u5c31\u4ee5\u8be5\u5f62\u72b6\u3002 \u5982\u679csize\u662fint\u66f4\u77ed\u7684\u8fb9\u5c06\u8c03\u6574\u4e3aint\uff0c\u5373\u5982\u679c\u9ad8&gt;\u5bbd\uff0c\u90a3\u4e48\u56fe\u7247\u5c06\u8c03\u6574\u4e3a(size * \u9ad8 / \u5bbd\uff0csize\uff09\u3002</li> <li>interpolation(int, \u53ef\u9009\uff09\u2013 \u63d2\u503c\u65b9\u5f0f\u3002\u9ed8\u8ba4\u91c7\u7528<code>PIL.Image.BILINEAR</code>\u3002</li> </ul> <pre><code>class torchvision.transforms.Scale(*args, **kwargs)\n</code></pre> <p>\u6ce8\u610f\uff1a\u8be5\u53d8\u6362\u5df2\u88ab\u5f03\u7528\uff0c\u53ef\u7528Resize\u4ee3\u66ff\u3002</p> <pre><code>class torchvision.transforms.TenCrop(size, vertical_flip=False)\n</code></pre> <p>\u5c06PIL\u56fe\u7247\u4ee5\u56db\u89d2\u548c\u4e2d\u5fc3\u88c1\u526a\uff0c\u540c\u65f6\u52a0\u5165\u7ffb\u6298\u7248\u672c\u3002(\u9ed8\u8ba4\u4ee5\u6c34\u5e73\u7684\u65b9\u5f0f\u7ffb\u6298\uff09</p> <p>\u6ce8\u610f\uff1a</p> <p>\u8be5\u53d8\u6362\u8fd4\u56de\u56fe\u50cf\u5143\u7ec4\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u56fe\u7247\u5728\u7f51\u7edc\u4e2d\u4f20\u5bfc\u540e\u548c\u6570\u636e\u96c6\u7ed9\u51fa\u7684\u6807\u7b7e\u7b49\u4fe1\u606f\u4e0d\u80fd\u5339\u914d\u3002\u5904\u7406\u65b9\u6cd5\u89c1\u4e0b\u9762\u7684\u793a\u4f8b\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u671f\u671b\u88c1\u526a\u8f93\u51fa\u7684\u5f62\u72b6\u3002\u9700\u8981\u88c1\u526a\u51fa\u7684\u5f62\u72b6\u3002\u5982\u679csize\u662fint\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u6b63\u65b9\u5f62\uff1b\u5982\u679c\u662f\u5e8f\u5217\uff0c\u5982(h, w)\uff0c\u5c06\u4f1a\u88c1\u526a\u6210\u77e9\u5f62\u3002</li> <li>vertical_flip(bool\uff09\u2013 \u662f\u5426\u7528\u5782\u76f4\u7ffb\u6298\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>&gt;&gt;&gt; transform = Compose([\n&gt;&gt;&gt;    TenCrop(size), # \u8fd9\u91cc\u4ea7\u751f\u4e86\u4e00\u4e2aPIL\u56fe\u50cf\u5217\u8868\n&gt;&gt;&gt;    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) \u8fd4\u56de4\u7ef4\u5f20\u91cf\n&gt;&gt;&gt; ])\n&gt;&gt;&gt; # \u5728\u6d4b\u8bd5\u9636\u6bb5\u4f60\u53ef\u4ee5\u8fd9\u6837\u505a\uff1a\n&gt;&gt;&gt; input, target = batch # input\u662f5\u7ef4\u5f20\u91cf, target\u662f2\u7ef4\u5f20\u91cf\n&gt;&gt;&gt; bs, ncrops, c, h, w = input.size()\n&gt;&gt;&gt; result = model(input.view(-1, c, h, w)) # \u628abatch size\u548cncrops\u878d\u5408\u5728\u4e00\u8d77\n&gt;&gt;&gt; result_avg = result.view(bs, ncrops, -1).mean(1) # crops\u4e0a\u7684\u5e73\u5747\u503c\n\n</code></pre>"},{"location":"1.0/torchvision_transforms/#torchtensor","title":"torch.*Tensor\u4e0a\u7684\u53d8\u6362","text":"<pre><code>class torchvision.transforms.LinearTransformation(transformation_matrix)\n</code></pre> <p>\u7528\u4e00\u4e2a\u9884\u5148\u51c6\u5907\u597d\u7684\u53d8\u6362\u65b9\u9635\u5bf9\u56fe\u7247\u5f20\u91cf\u505a\u53d8\u6362\u3002</p> <p>torch.*Tensor\u4f1a\u88abtransformation_matrix\u62c9\u5e73\uff0c\u548c\u53d8\u6362\u77e9\u9635\u505a\u70b9\u79ef\u540e\u8c03\u6574\u5230\u539f\u59cb\u5f20\u91cf\u7684\u5f62\u72b6\u3002</p> <p>\u5e94\u7528\uff1a</p> <ul> <li>\u767d\u5316\uff1a\u5c06\u6570\u636e\u7684\u5206\u5e03\u4e2d\u5fc3\u5904\u7406\u52300\uff0c\u8ba1\u7b97\u6570\u636e\u7684\u534f\u65b9\u5dee\u77e9\u9635\u3002</li> <li>\u7528np.dot(X.T, X)\u53ef\u4ee5\u5904\u7406\u5230[D x D]\u7684\u5f62\u72b6\uff0c\u5bf9\u6b64\u505a\u5947\u5f02\u503c\u5206\u89e3\u7136\u540e\u4f20\u7ed9transformation_matrix\u5373\u53ef\u3002</li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li>transformation_matrix(Tensor\uff09\u2013 [D x D]\u7684\u5f20\u91cf\uff0cD = C x H x W\u3002</li> </ul> <pre><code>class torchvision.transforms.Normalize(mean, std)\n</code></pre> <p>\u7528\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u6807\u51c6\u5316\u8f93\u5165\u56fe\u7247\u3002\u7ed9\u5b9a<code>n</code>\u4e2a\u901a\u9053\u7684\u5e73\u5747\u503c<code>(M1,...,Mn)</code>\u548c\u6807\u51c6\u5dee<code>(S1,..,Sn)</code>\uff0c\u8fd9\u4e00\u53d8\u6362\u4f1a\u5728<code>torch.*Tensor</code>\u7684\u6bcf\u4e00\u4e2a\u901a\u9053\u4e0a\u8fdb\u884c\u6807\u51c6\u5316\uff0c\u5373<code>input[channel] = (input[channel] - mean[channel]) / std[channel]</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>mean(\u5e8f\u5217\uff09\u2013 \u5e8f\u5217\uff0c\u5305\u542b\u5404\u901a\u9053\u7684\u5e73\u5747\u503c\u3002</li> <li>std(\u5e8f\u5217\uff09\u2013 \u5e8f\u5217\uff0c\u5305\u542b\u5404\u901a\u9053\u7684\u6807\u51c6\u5dee\u3002</li> </ul> <pre><code>__call__(tensor)\n</code></pre> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor (Tensor) \u2013 \u9700\u8981\u6807\u51c6\u5316\u7684\u56fe\u50cfTensor\uff0c\u5f62\u72b6\u987b\u4e3a(C, H, W)\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u6807\u51c6\u5316\u4e4b\u540e\u7684\u56fe\u7247Tensor</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>Tensor\u3002</li> </ul>"},{"location":"1.0/torchvision_transforms/#_1","title":"\u683c\u5f0f\u53d8\u6362","text":"<pre><code>class torchvision.transforms.ToPILImage(mode=None)\n</code></pre> <p>\u628a\u5f20\u91cf\u6216ndarray\u8f6c\u5316\u4e3aPIL\u56fe\u50cf\u3002</p> <p>\u628a\u5f62\u72b6\u4e3aC x H x W\u7684torch.*Tensor\u6216\u8005\u5f62\u72b6\u4e3aH x W x C\u7684numpy\u77e9\u9635ndarray\u8f6c\u5316\u4e3aPIL\u56fe\u50cf\uff0c\u4fdd\u7559\u503c\u7684\u4e0a\u4e0b\u754c\u3002</p> <p>\u53c2\u6570 \uff1a</p> <ul> <li>mode (PIL.Image mode) \u2013 \u8f93\u5165\u6570\u636e\u7684\u989c\u8272\u7a7a\u95f4\u6216\u8005\u50cf\u7d20\u6df1\u5ea6(\u53ef\u9009\uff09\u3002 \u5982\u679c<code>mode</code>\u8bbe\u7f6e\u4e3a<code>None</code>(\u9ed8\u8ba4\uff09\uff0c\u6309\u7167\u4e0b\u9762\u7684\u89c4\u5219\u8fdb\u884c\u5904\u7406\uff1a</li> <li>\u5982\u679c\u8f93\u51653\u901a\u9053\uff0c<code>mode</code>\u4f1a\u8bbe\u7f6e\u4e3a<code>RGB</code>\u3002</li> <li>\u5982\u679c\u8f93\u51654\u901a\u9053\uff0c<code>mode</code>\u4f1a\u8bbe\u7f6e\u4e3a<code>RGBA</code>\u3002</li> <li>\u5982\u679c\u8f93\u51651\u901a\u9053\uff0c<code>mode</code>\u7531\u6570\u636e\u7c7b\u578b\u51b3\u5b9a(\u5373<code>int</code>\uff0c<code>float</code>\uff0c<code>short</code>)\u3002</li> </ul> <pre><code>__call__(pic)\n</code></pre> <p>\u53c2\u6570\uff1a</p> <ul> <li>pic (Tensor \u6216 numpy.ndarray\uff09\u2013 \u8981\u8f6c\u5316\u6210PIL\u56fe\u50cf\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8f6c\u5316\u540e\u7684PIL\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>class torchvision.transforms.ToTensor\n</code></pre> <p>\u5c06<code>PIL Image</code>\u6216<code>numpy.ndarray</code>\u8f6c\u5316\u6210\u5f20\u91cf\u3002</p> <p>\u628aPIL\u56fe\u50cf\u6216[0, 255]\u8303\u56f4\u5185\u7684numpy.ndarray(\u5f62\u72b6(H x W x C)\uff09\u8f6c\u5316\u6210torch.FloatTensor\uff0c\u5f20\u91cf\u5f62\u72b6(C x H x W)\uff0c\u8303\u56f4\u5728[0.0, 1.0]\u4e2d\u3002\u8f93\u5165\u5e94\u662f\u662fPIL\u56fe\u50cf\u4e14\u662f\u6a21\u5f0f(L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1\uff09\u4e2d\u7684\u4e00\u79cd\uff0c\u6216\u8f93\u5165\u662fnumpy.ndarray\u4e14\u7c7b\u578b\u4e3anp.uint8\u3002</p> <pre><code>__call__(pic)\n</code></pre> <p>\u53c2\u6570\uff1a *   pic (PIL\u56fe\u50cf \u6216 numpy.ndarray) \u2013 \u8981\u8f6c\u5316\u6210\u5f20\u91cf\u7684\u56fe\u7247\u3002</p> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8f6c\u5316\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>Tensor \u3002</li> </ul>"},{"location":"1.0/torchvision_transforms/#_2","title":"\u901a\u7528\u53d8\u6362","text":"<pre><code>class torchvision.transforms.Lambda(lambd)\n</code></pre> <p>\u5c06\u7528\u6237\u5b9a\u4e49\u7684\u51fd\u6570\u7528\u4f5c\u53d8\u6362\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>lambd (\u51fd\u6570) \u2013 \u7528\u4e8e\u53d8\u6362\u7684Lambda\u51fd\u6570\u6216\u51fd\u6570\u540d\u3002</li> </ul>"},{"location":"1.0/torchvision_transforms/#functional","title":"Functional\u53d8\u6362","text":"<p>functional\u53ef\u4ee5\u63d0\u4f9b\u4e86\u4e00\u4e9b\u66f4\u52a0\u7cbe\u7ec6\u7684\u53d8\u6362\uff0c\u7528\u4e8e\u642d\u5efa\u590d\u6742\u7684\u53d8\u6362\u6d41\u6c34\u7ebf\u3002\u548c\u524d\u9762\u7684\u53d8\u6362\u76f8\u53cd\uff0c\u51fd\u6570\u53d8\u6362\u7684\u53c2\u6570\u4e0d\u5305\u542b\u968f\u673a\u6570\u79cd\u5b50\u751f\u6210\u5668\u3002\u8fd9\u610f\u5473\u7740\u4f60\u5fc5\u987b\u6307\u5b9a\u6240\u6709\u53c2\u6570\u7684\u503c\uff0c\u4f46\u662f\u4f60\u53ef\u4ee5\u81ea\u5df1\u5f15\u5165\u968f\u673a\u6570\u3002\u6bd4\u5982\uff0c\u5bf9\u4e00\u7ec4\u56fe\u7247\u4f7f\u7528\u5982\u4e0b\u7684functional\u53d8\u6362\uff1a</p> <pre><code>import torchvision.transforms.functional as TF\nimport random\n\ndef my_segmentation_transforms(image, segmentation):\n    if random.random() &gt; 5:\n        angle = random.randint(-30, 30)\n        image = TF.rotate(image, angle)\n        segmentation = TF.rotate(segmentation, angle)\n    # \u66f4\u591a\u53d8\u6362 ...\n    return image, segmentation\n\n</code></pre> <pre><code>torchvision.transforms.functional.adjust_brightness(img, brightness_factor)\n</code></pre> <p>\u8c03\u6574\u56fe\u50cf\u4eae\u5ea6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8c03\u6574\u7684PIL\u56fe\u50cf\u3002</li> <li>brightness_factor(float\uff09\u2013 \u4eae\u5ea6\u7684\u8c03\u6574\u503c\uff0c\u53ef\u4ee5\u662f\u4efb\u610f\u975e\u8d1f\u6574\u6570\u30020\u8868\u793a\u9ed1\u8272\u56fe\u50cf\uff0c1\u8868\u793a\u539f\u59cb\u56fe\u50cf\uff0c2\u8868\u793a\u589e\u52a0\u52302\u500d\u4eae\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8c03\u6574\u4eae\u5ea6\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.adjust_contrast(img, contrast_factor)\n</code></pre> <p>\u8c03\u6574\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8c03\u6574\u7684PIL\u56fe\u50cf\u3002</li> <li>contrast_factor(float\uff09\u2013 \u5bf9\u6bd4\u5ea6\u7684\u8c03\u6574\u5e45\u5ea6\uff0c\u53ef\u4ee5\u662f\u4efb\u610f\u975e\u8d1f\u6570\u30020\u8868\u793a\u7070\u9636\u56fe\u7247\uff0c1\u8868\u793a\u539f\u59cb\u56fe\u7247\uff0c2\u8868\u793a\u5bf9\u6bd4\u5ea6\u589e\u52a0\u52302\u500d\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8c03\u6574\u5bf9\u6bd4\u5ea6\u4e4b\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.adjust_gamma(img, gamma, gain=1)\n</code></pre> <p>\u5bf9\u56fe\u50cf\u8fdb\u884c\u4f3d\u9a6c\u77eb\u6b63\u3002</p> <p>\u53c8\u79f0\u5e42\u7387\u53d8\u6362\u3002RGB\u6a21\u5f0f\u4e0b\u7684\u5f3a\u5ea6\u6309\u7167\u4e0b\u9762\u7684\u7b49\u5f0f\u8fdb\u884c\u8c03\u6574\uff1a</p> \\[I_{out} = 255 \\times gain \\times (\\dfrac{I_{in}}{255})^\\gamma\\] <p>\u66f4\u591a\u7ec6\u8282\u89c1\u4f3d\u9a6c\u77eb\u6b63\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 PIL\u8981\u8c03\u6574\u7684PIL\u56fe\u50cf\u3002</li> <li>gamma(float\uff09\u2013 \u975e\u8d1f\u5b9e\u6570\uff0c\u516c\u5f0f\u4e2d\u7684 \\(\\(\\gamma\\)\\)\u3002gamma\u5927\u4e8e1\u65f6\u4ee4\u6697\u533a\u66f4\u6697\uff0cgamma\u5c0f\u4e8e1\u65f6\u4f7f\u5f97\u6697\u533a\u66f4\u4eae\u3002</li> <li>gain(float\uff09\u2013 \u5e38\u6570\u4e58\u6570\u3002</li> </ul> <pre><code>torchvision.transforms.functional.adjust_hue(img, hue_factor)\n</code></pre> <p>\u8c03\u6574\u56fe\u50cf\u8272\u76f8\u3002</p> <p>\u8c03\u6574\u65f6\uff0c\u5148\u628a\u56fe\u50cf\u8f6c\u6362\u5230HSV\u7a7a\u95f4\uff0c\u7136\u540e\u6cbf\u7740\u8272\u76f8\u8f74(H\u8f74\uff09\u5faa\u73af\u79fb\u52a8\u3002\u6700\u540e\u5207\u6362\u56de\u56fe\u50cf\u539f\u59cb\u6a21\u5f0f\u3002</p> <p><code>hue_factor</code>\u662fH\u901a\u9053\u7684\u504f\u79fb\u91cf\uff0c\u5fc5\u987b\u5728<code>[-0.5, 0.5]</code>\u7684\u8303\u56f4\u5185\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u89c1\u8272\u76f8\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8c03\u6574\u7684PIL\u56fe\u50cf\u3002</li> <li>hue_factor(float\uff09\u2013 H\u901a\u9053\u7684\u504f\u79fb\u91cf\u5e94\u8be5\u5728[-0.5, 0.5]\u7684\u8303\u56f4\u5185\u30020.5\u548c-0.5\u5206\u522b\u8868\u793a\u5728HSV\u7a7a\u95f4\u7684H\u8f74\u4e0a\u6cbf\u6b63\u3001\u8d1f\u65b9\u5411\u8fdb\u884c\u79fb\u52a8\uff0c0\u8868\u793a\u4e0d\u504f\u79fb\u3002\u56e0\u6b64\uff0c-0.5\u548c0.5\u90fd\u80fd\u8868\u793a\u8865\u8272\uff0c0\u8868\u793a\u539f\u56fe\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8272\u76f8\u8c03\u6574\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.adjust_saturation(img, saturation_factor)\n</code></pre> <p>\u8c03\u6574\u56fe\u50cf\u7684\u989c\u8272\u9971\u548c\u5ea6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8c03\u6574\u7684PIL\u56fe\u50cf\u3002</li> <li>saturation_factor(float\uff09\u2013 \u9971\u548c\u5ea6\u8c03\u6574\u503c\u30020\u8868\u793a\u7eaf\u9ed1\u767d\u56fe\u50cf\uff0c1\u8868\u793a\u539f\u59cb\u56fe\u50cf\uff0c2\u8868\u793a\u589e\u52a0\u5230\u539f\u6765\u76842\u500d\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8c03\u6574\u9971\u548c\u5ea6\u4e4b\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.affine(img, angle, translate, scale, shear, resample=0, fillcolor=None)\n</code></pre> <p>\u4fdd\u6301\u56fe\u7247\u50cf\u7d20\u5206\u5e03\u4e2d\u5fc3\u4e0d\u53d8\uff0c\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u65cb\u8f6c\u7684PIL\u56fe\u50cf\u3002</li> <li>angle({python:float \u6216 int}\uff09\u2013 \u65cb\u8f6c\u89d2\u5ea6\uff0c\u5e94\u5728\u65f6\u949f\u65b9\u5411\u7684-180\u5230180\u5ea6\u4e4b\u95f4\u3002</li> <li>translate(list \u6216 \u6574\u5f62\u6570\u5143\u7ec4\uff09\u2013 \u6c34\u5e73\u548c\u5782\u76f4\u53d8\u6362(\u65cb\u8f6c\u4e4b\u540e\uff09</li> <li>scale(float\uff09\u2013 \u5c3a\u5ea6\u53d8\u6362\u3002</li> <li>shear(float\uff09\u2013 \u626d\u66f2\u89d2\u5ea6\uff0c\u5e94\u5728\u65f6\u949f\u65b9\u5411\u7684-180\u5230180\u5ea6\u4e4b\u95f4\u3002</li> <li>resample(<code>PIL.Image.NEAREST</code> \u6216 <code>PIL.Image.BILINEAR</code> \u6216 <code>PIL.Image.BICUBIC</code> , \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u91cd\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u89c1\u6ee4\u6ce2\u5668\u3002\u5982\u679c\u4e0d\u8bbe\u7f6e\u8be5\u9009\u9879\uff0c\u6216\u8005\u56fe\u50cf\u6a21\u5f0f\u662f\u201c1\u201d\u6216\u201cP\u201d\uff0c\u8bbe\u7f6e\u4e3a<code>PIL.Image.NEAREST</code>\u3002</li> <li>fillcolor(int\uff09\u2013 \u53ef\u9009\u5728\u8f93\u51fa\u56fe\u7247\u7684\u53d8\u6362\u5916\u533a\u57df\u53ef\u9009\u5730\u586b\u5145\u989c\u8272\u3002(Pillow&gt;=5.0.0\uff09</li> </ul> <pre><code>torchvision.transforms.functional.crop(img, i, j, h, w)\n</code></pre> <p>\u88c1\u526a\u6307\u5b9aPIL\u56fe\u50cf\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u88c1\u526a\u7684\u56fe\u50cf\u3002</li> <li>i \u2013 \u6700\u4e0a\u4fa7\u50cf\u7d20\u7684\u5750\u6807\u3002</li> <li>j \u2013 \u6700\u5de6\u4fa7\u50cf\u7d20\u7684\u5750\u6807\u3002</li> <li>h \u2013 \u8981\u88c1\u526a\u51fa\u7684\u9ad8\u5ea6\u3002</li> <li>w \u2013 \u8981\u88c1\u526a\u51fa\u7684\u5bbd\u5ea6\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u88c1\u526a\u51fa\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.five_crop(img, size)\n</code></pre> <p>\u5728\u56db\u89d2\u548c\u4e2d\u5fc3\u5904\u88c1\u526a\u56fe\u7247\u3002</p> <p>\u6ce8\u610f\uff1a</p> <p>\u8be5\u53d8\u6362\u8fd4\u56de\u56fe\u50cf\u5143\u7ec4\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u56fe\u7247\u5728\u7f51\u7edc\u4e2d\u4f20\u5bfc\u540e\u548c\u4f60\u7684<code>Dataset</code>\u7ed9\u51fa\u7684\u6807\u7b7e\u7b49\u4fe1\u606f\u4e0d\u80fd\u5339\u914d\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u5e0c\u671b\u5f97\u5230\u7684\u88c1\u526a\u8f93\u51fa\u3002\u5982\u679csize\u662f\u5e8f\u5217(h, w)\uff0c\u8f93\u51fa\u77e9\u5f62\uff1b\u5982\u679c\u662fint \uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a(size, size)\u7684\u6b63\u65b9\u5f62\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u5143\u7ec4(tl, tr, bl, br, center)\uff0c\u5206\u522b\u8868\u793a\u5de6\u4e0a\u3001\u53f3\u4e0a\u3001\u5de6\u4e0b\u3001\u53f3\u4e0b\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>tuple </li> </ul> <pre><code>torchvision.transforms.functional.hflip(img)\n</code></pre> <p>\u5c06\u6307\u5b9a\u56fe\u50cf\u6c34\u5e73\u7ffb\u6298\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u7ffb\u6298\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u6c34\u5e73\u7ffb\u6298\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.normalize(tensor, mean, std)\n</code></pre> <p>\u7528\u5747\u503c\u548c\u65b9\u5dee\u5c06\u56fe\u50cf\u6807\u51c6\u5316\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u89c1<code>Normalize</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor(Tensor\uff09\u2013 \u9700\u8981\u6807\u51c6\u5316\u7684\u56fe\u50cfTensor\uff0c\u5f62\u72b6\u5e94\u662f(C, H, W)\u3002</li> <li>mean(\u5e8f\u5217\uff09\u2013 \u5404\u901a\u9053\u7684\u5747\u503c\u3002</li> <li>std(\u5e8f\u5217\uff09\u2013 \u5404\u901a\u9053\u7684\u6807\u51c6\u5dee\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u6807\u51c6\u5316\u4e4b\u540e\u7684\u56fe\u50cfTensor\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>Tensor\u3002</li> </ul> <pre><code>torchvision.transforms.functional.pad(img, padding, fill=0, padding_mode='constant')\n</code></pre> <p>\u7528\u6307\u5b9a\u7684\u586b\u5145\u6a21\u5f0f\u548c\u586b\u5145\u503c\u586b\u5145PIL\u56fe\u50cf\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u586b\u5145\u7684\u56fe\u50cf\u3002</li> <li>padding(int \u6216 tuple\uff09\u2013 \u5404\u8fb9\u7684\u586b\u5145\u53ef\u5bbd\u5ea6\u3002\u5982\u679c\u6307\u5b9a\u4e3aint\uff0c\u8868\u793a\u6240\u6709\u8fb9\u90fd\u6309\u7167\u6b64\u5bbd\u5ea6\u586b\u5145\u3002\u5982\u679c\u6307\u5b9a\u4e3a\u957f\u4e3a2\u7684\u5143\u7ec4\uff0c\u8868\u793a\u5de6\u53f3\u548c\u4e0a\u4e0b\u8fb9\u7684\u586b\u5145\u5bbd\u5ea6\u3002\u5982\u679c\u6307\u5b9a\u4e3a\u957f\u4e3a4\u7684\u5143\u7ec4\uff0c\u5206\u522b\u8868\u793a\u5de6\u3001\u4e0a\u3001\u53f3\u3001\u4e0b\u7684\u586b\u5145\u5bbd\u5ea6\u3002</li> <li>fill \u2013 \u8981\u586b\u5145\u7684\u50cf\u7d20\u503c\uff0c\u9ed8\u8ba4\u662f0\u3002\u5982\u679c\u6307\u5b9a\u4e3a\u957f\u4e3a3\u7684\u5143\u7ec4\uff0c\u8868\u793aRGB\u4e09\u901a\u9053\u7684\u586b\u5145\u503c\u3002\u8fd9\u4e2a\u9009\u9879\u4ec5\u5728padding_mode\u662fconstant\u65f6\u6709\u7528\u3002</li> <li>padding_mode \u2013 \u586b\u5145\u7c7b\u578b\uff0c\u5e94\u5f53\u4e3a\uff1aconstant\uff0cedge\uff0creflect\u6216symmetric\u3002\u9ed8\u8ba4\u662fconstant\u3002</li> <li>constant\uff1a\u7528\u5e38\u6570\u586b\u5145\uff0c\u8be5\u5e38\u6570\u503c\u7531fill\u6307\u5b9a\u3002 </li> <li>edge\uff1a\u7528\u8fb9\u4e0a\u7684\u503c\u586b\u5145\u3002</li> <li> <p>reflect\uff1a \u4ee5\u8fb9\u4e3a\u5bf9\u79f0\u8f74\u8fdb\u884c\u586b\u5145\u3002(\u4e0d\u91cd\u590d\u8fb9\u4e0a\u7684\u503c\uff09</p> <ul> <li>\u5728reflect\u6a21\u5f0f\u4e2d\uff0c\u5728\u4e24\u8fb9\u5206\u522b\u75282\u4e2a\u5143\u7d20\u586b\u5145[1, 2, 3, 4]\u5c06\u4f1a\u5f97\u5230[3, 2, 1, 2, 3, 4, 3, 2]\u3002</li> <li> <p>symmetric\uff1a\u4ee5\u8fb9\u4e3a\u5bf9\u79f0\u8f74\u8fdb\u884c\u586b\u5145\u3002(\u91cd\u590d\u8fb9\u4e0a\u7684\u503c\uff09</p> </li> <li> <p>\u5728symmetric\u6a21\u5f0f\u4e2d\uff0c\u5728\u4e24\u8fb9\u5206\u522b\u75282\u4e2a\u5143\u7d20\u586b\u5145[1, 2, 3, 4]\u5c06\u4f1a\u5f97\u5230[2, 1, 1, 2, 3, 4, 4, 3]\u3002</p> </li> </ul> </li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u586b\u5145\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf</li> </ul> <pre><code>torchvision.transforms.functional.resize(img, size, interpolation=2)\n</code></pre> <p>\u5c06\u539f\u662fPIL\u56fe\u50cf\u91cd\u65b0\u8c03\u6574\u5230\u6307\u5b9a\u5f62\u72b6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8c03\u6574\u5f62\u72b6\u7684\u56fe\u50cf\u3002</li> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u8f93\u51fa\u56fe\u50cf\u7684\u5f62\u72b6\u3002\u5982\u679csize\u6307\u5b9a\u4e3a\u5e8f\u5217(h, w)\uff0c\u8f93\u51fa\u77e9\u5f62\u3002\u5982\u679csize\u6307\u5b9a\u4e3aint\u56fe\u7247\u7684\u77ed\u8fb9\u5c06\u8c03\u6574\u4e3a\u8fd9\u4e2a\u6570\uff0c\u957f\u8fb9\u6309\u7167\u76f8\u540c\u7684\u957f\u5bbd\u6bd4\u8fdb\u884c\u8c03\u6574\u3002\u5373\uff0c\u5982\u679c\u9ad8\u5ea6&gt;\u5bbd\u5ea6\uff0c\u5219\u56fe\u7247\u5f62\u72b6\u5c06\u8c03\u6574\u4e3a \\( (size\\times\\frac{ \u9ad8\u5ea6 }{ \u5bbd\u5ea6 }, size) \\)</li> <li>interpolation(int, \u53ef\u9009\uff09\u2013 \u63d2\u503c\u65b9\u5f0f\uff0c\u9ed8\u8ba4\u662f<code>PIL.Image.BILINEAR</code>\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8c03\u6574\u5927\u5c0f\u4e4b\u540e\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.resized_crop(img, i, j, h, w, size, interpolation=2)\n</code></pre> <p>\u88c1\u526aPIL\u5e76\u8c03\u6574\u5230\u76ee\u6807\u5f62\u72b6\u3002</p> <p>\u6ce8\u610f\uff1a\u5728RandomResizedCrop\u88ab\u8c03\u7528\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u88c1\u526a\u7684\u56fe\u50cf\u3002</li> <li>i \u2013 \u6700\u4e0a\u4fa7\u7684\u50cf\u7d20\u5750\u6807\u3002</li> <li>j \u2013 \u6700\u5de6\u4fa7\u7684\u50cf\u7d20\u5750\u6807\u3002</li> <li>h \u2013 \u88c1\u526a\u51fa\u7684\u56fe\u50cf\u9ad8\u5ea6\u3002</li> <li>w \u2013 \u88c1\u526a\u51fa\u7684\u56fe\u50cf\u5bbd\u5ea6\u3002</li> <li>size(\u5e8f\u5217 \u6216 int\uff09\u2013 \u8981\u8f93\u51fa\u7684\u56fe\u50cf\u5f62\u72b6\uff0c\u540c<code>scale</code>\u3002</li> <li>interpolation(int, \u53ef\u9009\uff09\u2013 \u63d2\u503c\u65b9\u5f0f\uff0c\u9ed8\u8ba4\u662f <code>PIL.Image.BILINEAR</code>\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u88c1\u526a\u540e\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.rotate(img, angle, resample=False, expand=False, center=None)\n</code></pre> <p>\u65cb\u8f6c\u56fe\u7247\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u65cb\u8f6c\u7684PIL\u56fe\u50cf\u3002</li> <li>angle(float \u6216 int}\uff09\u2013 \u987a\u65f6\u9488\u65cb\u8f6c\u89d2\u5ea6\u3002</li> <li>resample(<code>PIL.Image.NEAREST</code> \u6216 <code>PIL.Image.BILINEAR</code> \u6216 <code>PIL.Image.BICUBIC</code> , \u53ef\u9009\uff09 \u2013 \u53ef\u9009\u7684\u91cd\u91c7\u6837\u6ee4\u6ce2\u5668\uff0c\u89c1\u6ee4\u6ce2\u5668\u3002\u5982\u679c\u8be5\u9009\u9879\u4e0d\u8bbe\u7f6e\uff0c\u6216\u8005\u56fe\u50cf\u6a21\u5f0f\u662f\u201c1\u201d\u6216\u201cP\u201d\uff0c\u5c06\u88ab\u8bbe\u7f6e\u4e3aPIL.Image.NEAREST\u3002</li> <li>expand(bool, \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u6269\u5c55\u9009\u9879\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u4f7f\u8f93\u51fa\u8db3\u591f\u5927\uff0c\u4ece\u800c\u5305\u542b\u4e86\u6240\u6709\u50cf\u7d20\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aFalse\u6216\u4e0d\u8bbe\u7f6e\uff0c\u5219\u8f93\u51fa\u5e94\u548c\u8f93\u5165\u5f62\u72b6\u76f8\u540c\u3002\u6ce8\u610fexpand\u9009\u9879\u5047\u5b9a\u65cb\u8f6c\u4e2d\u5fc3\u662fcenter\u4e14\u4e0d\u505a\u5e73\u79fb\u3002</li> <li>center(2-tuple , \u53ef\u9009\uff09\u2013 \u53ef\u9009\u7684\u65cb\u8f6c\u4e2d\u5fc3\u3002\u539f\u70b9\u5728\u5de6\u4e0a\u89d2\u3002\u9ed8\u8ba4\u4ee5\u56fe\u7247\u4e2d\u5fc3\u4e3a\u65cb\u8f6c\u4e2d\u5fc3\u3002</li> </ul> <pre><code>torchvision.transforms.functional.ten_crop(img, size, vertical_flip=False)\n</code></pre> <p>\u5c06\u56fe\u7247\u5728\u56db\u89d2\u548c\u4e2d\u5fc3\u5904\u88c1\u526a\uff0c\u540c\u65f6\u8fd4\u56de\u5b83\u4eec\u7ffb\u6298\u540e\u7684\u56fe\u7247\u3002(\u9ed8\u8ba4\u6c34\u5e73\u7ffb\u6298\uff09</p> <p>\u6ce8\u610f\uff1a</p> <ul> <li>\u8be5\u53d8\u6362\u8fd4\u56de\u56fe\u50cf\u5143\u7ec4\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u56fe\u7247\u5728\u7f51\u7edc\u4e2d\u4f20\u5bfc\u540e\u548c\u4f60\u7684<code>Dataset</code>\u7ed9\u51fa\u7684\u6807\u7b7e\u7b49\u4fe1\u606f\u4e0d\u80fd\u5339\u914d\u3002</li> </ul> <p>\u53c2\u6570\uff1a</p> <ul> <li>size(\u5e8f\u5217 \u6216 int\uff09- \u88c1\u526a\u540e\u8f93\u51fa\u7684\u5f62\u72b6\u3002\u5982\u679csize\u662fint\uff0c\u8f93\u51fa(size, size)\u7684\u6b63\u65b9\u5f62\uff1b\u5982\u679csize\u662f\u5e8f\u5217\uff0c\u8f93\u51fa\u77e9\u5f62\u3002</li> <li>vertical_flip(bool\uff09- \u4f7f\u7528\u5782\u76f4\u7ffb\u6298\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u5143\u7ec4(tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip\uff09 - \u5bf9\u5e94\u7684\u5de6\u4e0a\u3001\u53f3\u4e0a\u3001\u5de6\u4e0b\u3001\u53f3\u4e0b\u3001\u4e2d\u5fc3\u88c1\u526a\u56fe\u7247\u548c\u6c34\u5e73\u7ffb\u6298\u540e\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>\u5143\u7ec4</li> </ul> <pre><code>torchvision.transforms.functional.to_grayscale(img, num_output_channels=1)\n</code></pre> <p>\u5c06\u56fe\u50cf\u8f93\u51fa\u6210\u7070\u9636\u7248\u672c\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u8f6c\u5316\u6210\u7070\u9636\u56fe\u50cf\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u7070\u9636\u7248\u672c\u7684\u56fe\u50cf\u3002\u5982\u679cnum_output_channels == 1\uff1a\u8fd4\u56de\u5355\u901a\u9053\u56fe\u50cf\uff1b\u5982\u679cnum_output_channels == 3\uff1a\u8fd4\u56de\u4e09\u901a\u9053\u56fe\u50cf\uff0c\u5176\u4e2dr == g == b\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.to_pil_image(pic, mode=None)\n</code></pre> <p>\u5c06\u5f20\u91cf\u6216ndarray\u8f6c\u5316\u4e3aPIL\u56fe\u50cf\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u89c1<code>ToPIlImage</code>\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>pic(Tensor \u6216 numpy.ndarray\uff09\u2013 \u8981\u8f6c\u5316\u6210PIL\u7684\u56fe\u7247\u3002</li> <li>mode(PIL.Image mode\uff09\u2013 \u8f93\u5165\u6570\u636e\u7684\u8272\u5f69\u7a7a\u95f4\u548c\u50cf\u7d20\u6df1\u5ea6\u3002(\u53ef\u9009\uff09</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8981\u8f6c\u5316\u6210PIL\u56fe\u50cf\u7684\u6570\u636e\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul> <pre><code>torchvision.transforms.functional.to_tensor(pic)\n</code></pre> <p>\u5c06<code>PIL Image</code>\u6216<code>numpy.ndarray</code>\u8f6c\u5316\u6210\u5f20\u91cf\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u89c1<code>ToTensor</code>\u3002</p> <p>\u53c2\u6570\uff1a  *   pic (PIL\u56fe\u50cf \u6216 numpy.ndarray) \u2013 \u8981\u8f6c\u5316\u6210\u5f20\u91cf\u7684\u56fe\u7247\u3002</p> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u8f6c\u5316\u540e\u7684\u56fe\u7247\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>Tensor\u3002</li> </ul> <pre><code>torchvision.transforms.functional.vflip(img)\n</code></pre> <p>\u5782\u76f4\u7ffb\u6298PIL\u56fe\u50cf\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>img(PIL\u56fe\u50cf\uff09\u2013 \u8981\u7ffb\u6298\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\uff1a</p> <ul> <li>\u5782\u76f4\u7ffb\u6298\u540e\u7684\u56fe\u50cf\u3002</li> </ul> <p>\u8fd4\u56de\u7c7b\u578b\uff1a</p> <ul> <li>PIL\u56fe\u50cf\u3002</li> </ul>"},{"location":"1.0/torchvision_utils/","title":"torchvision.utils","text":"<p>\u8bd1\u8005\uff1aBXuan694</p> <pre><code>torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n</code></pre> <p>\u628a\u56fe\u7247\u6392\u5217\u6210\u7f51\u683c\u5f62\u72b6\u3002</p> <p>\u53c2\u6570\uff1a </p> <ul> <li>tensor(Tensor \u6216 list\uff09\u2013 \u56db\u7ef4\u6279(batch\uff09Tensor\u6216\u5217\u8868\u3002\u5982\u679c\u662fTensor\uff0c\u5176\u5f62\u72b6\u5e94\u662f(B x C x H x W\uff09\uff1b\u5982\u679c\u662f\u5217\u8868\uff0c\u5143\u7d20\u5e94\u4e3a\u76f8\u540c\u5927\u5c0f\u7684\u56fe\u7247\u3002</li> <li>nrow(int, \u53ef\u9009\uff09\u2013 \u6700\u7ec8\u5c55\u793a\u7684\u56fe\u7247\u7f51\u683c\u4e2d\u6bcf\u884c\u6446\u653e\u7684\u56fe\u7247\u6570\u91cf\u3002\u7f51\u683c\u7684\u957f\u5bbd\u5e94\u8be5\u662f(B / nrow, nrow\uff09\u3002\u9ed8\u8ba4\u662f8\u3002</li> <li>padding(int, \u53ef\u9009\uff09\u2013 \u6269\u5c55\u586b\u5145\u7684\u50cf\u7d20\u5bbd\u5ea6\u3002\u9ed8\u8ba4\u662f2\u3002</li> <li>normalize(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u901a\u8fc7\u51cf\u53bb\u6700\u5c0f\u50cf\u7d20\u503c\u7136\u540e\u9664\u4ee5\u6700\u5927\u50cf\u7d20\u503c\uff0c\u628a\u56fe\u7247\u79fb\u5230(0\uff0c1\uff09\u7684\u8303\u56f4\u5185\u3002</li> <li>range(tuple, \u53ef\u9009\uff09\u2013 \u5143\u7ec4(min, max\uff09\uff0cmin\u548cmax\u7528\u4e8e\u5bf9\u56fe\u7247\u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406\u3002\u9ed8\u8ba4\u7684\uff0cmin\u548cmax\u7531\u8f93\u5165\u7684\u5f20\u91cf\u8ba1\u7b97\u5f97\u5230\u3002</li> <li>scale_each(bool, \u53ef\u9009\uff09\u2013 \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u5c06\u6279\u4e2d\u7684\u6bcf\u5f20\u56fe\u7247\u6309\u7167\u5404\u81ea\u7684\u6700\u503c\u5206\u522b\u7f29\u653e\uff0c\u5426\u5219\u4f7f\u7528\u5f53\u524d\u6279\u4e2d\u6240\u6709\u56fe\u7247\u7684\u6700\u503c(min, max)\u8fdb\u884c\u7edf\u4e00\u7f29\u653e\u3002</li> <li>pad_value(float, \u53ef\u9009\uff09\u2013 \u6269\u5c55\u586b\u5145\u7684\u50cf\u7d20\u503c\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> <p>\u8bf7\u770b \u8fd9\u91cc</p> <pre><code>torchvision.utils.save_image(tensor, filename, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n</code></pre> <p>\u7528\u4e8e\u628a\u6307\u5b9a\u7684Tensor\u4fdd\u5b58\u6210\u56fe\u7247\u6587\u4ef6\u3002</p> <p>\u53c2\u6570\uff1a</p> <ul> <li>tensor(Tensor \u6216 list\uff09\u2013 \u9700\u8981\u4fdd\u5b58\u6210\u56fe\u7247\u7684Tensor\u3002\u5982\u679cTensor\u4ee5\u6279\u7684\u5f62\u5f0f\u7ed9\u51fa\uff0c\u5219\u4f1a\u8c03\u7528<code>make_grid</code>\u5c06\u8fd9\u4e9b\u56fe\u7247\u4fdd\u5b58\u6210\u7f51\u683c\u7684\u5f62\u5f0f\u3002</li> <li>**kwargs \u2013 \u5176\u4ed6\u53c2\u6570\u540c<code>make_grid</code>\u3002</li> </ul>"},{"location":"1.0/transfer_learning_tutorial/","title":"\u8fc1\u79fb\u5b66\u4e60\u6559\u7a0b","text":"<p>\u8bd1\u8005\uff1a\u7247\u523b</p> <p>\u6821\u5bf9\u8005\uff1acluster</p> <p>\u4f5c\u8005: Sasank Chilamkurthy</p> <p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u6765\u8bad\u7ec3\u60a8\u7684\u7f51\u7edc\u3002\u60a8\u53ef\u4ee5\u5728 cs231n \u7b14\u8bb0 \u4e0a\u9605\u8bfb\u66f4\u591a\u5173\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u4fe1\u606f</p> <p>\u5f15\u7528\u8fd9\u4e9b\u7b14\u8bb0\uff1a</p> <p>\u5728\u5b9e\u8df5\u4e2d\uff0c\u5f88\u5c11\u6709\u4eba\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6574\u4e2a\u5377\u79ef\u7f51\u7edc(\u968f\u673a\u521d\u59cb\u5316\uff09\uff0c\u56e0\u4e3a\u62e5\u6709\u8db3\u591f\u5927\u5c0f\u7684\u6570\u636e\u96c6\u662f\u76f8\u5bf9\u7f55\u89c1\u7684\u3002\u76f8\u53cd\uff0c\u901a\u5e38\u5728\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6(\u4f8b\u5982 ImageNet\uff0c\u5176\u5305\u542b\u5177\u67091000\u4e2a\u7c7b\u522b\u7684120\u4e07\u4e2a\u56fe\u50cf\uff09\u4e0a\u9884\u5148\u8bad\u7ec3 ConvNet\uff0c\u7136\u540e\u4f7f\u7528 ConvNet \u5bf9\u611f\u5174\u8da3\u7684\u4efb\u52a1\u8fdb\u884c\u521d\u59cb\u5316\u6216\u7528\u4f5c\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3002</p> <p>\u5982\u4e0b\u662f\u4e24\u4e2a\u4e3b\u8981\u7684\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\uff1a</p> <ul> <li>Finetuning the convnet: \u6211\u4eec\u4f7f\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u800c\u4e0d\u662f\u968f\u673a\u521d\u59cb\u5316\uff0c\u5c31\u50cf\u5728imagenet 1000\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u7f51\u7edc\u4e00\u6837\u3002\u5176\u4f59\u8bad\u7ec3\u770b\u8d77\u6765\u50cf\u5f80\u5e38\u4e00\u6837\u3002(\u6b64\u5fae\u8c03\u8fc7\u7a0b\u5bf9\u5e94\u5f15\u7528\u4e2d\u6240\u8bf4\u7684\u521d\u59cb\u5316)</li> <li>ConvNet as fixed feature extractor: \u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u51bb\u7ed3\u9664\u6700\u7ec8\u5b8c\u5168\u8fde\u63a5\u5c42\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u7684\u6743\u91cd\u3002\u6700\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\u88ab\u66ff\u6362\u4e3a\u5177\u6709\u968f\u673a\u6743\u91cd\u7684\u65b0\u5c42\uff0c\u5e76\u4e14\u4ec5\u8bad\u7ec3\u8be5\u5c42\u3002(\u6b64\u6b65\u5bf9\u5e94\u5f15\u7528\u4e2d\u7684\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668)</li> </ul> <pre><code># License: BSD\n# Author: Sasank Chilamkurthy\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()   # interactive mode\n\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_2","title":"\u52a0\u8f7d\u6570\u636e","text":"<p>\u6211\u4eec\u5c06\u4f7f\u7528 torchvision \u548c torch.utils.data \u5305\u6765\u52a0\u8f7d\u6570\u636e\u3002</p> <p>\u6211\u4eec\u4eca\u5929\u8981\u89e3\u51b3\u7684\u95ee\u9898\u662f\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u6765\u5bf9 \u8682\u8681 \u548c \u871c\u8702 \u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u6709\u5927\u7ea6120\u4e2a\u8bad\u7ec3\u56fe\u50cf\uff0c\u6bcf\u4e2a\u56fe\u50cf\u7528\u4e8e \u8682\u8681 \u548c \u871c\u8702\u3002\u6bcf\u4e2a\u7c7b\u670975\u4e2a\u9a8c\u8bc1\u56fe\u50cf\u3002\u901a\u5e38\uff0c\u5982\u679c\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u6570\u636e\u96c6\u3002\u7531\u4e8e\u6211\u4eec\u6b63\u5728\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u591f\u5408\u7406\u5730\u6cdb\u5316\u3002</p> <p>\u8be5\u6570\u636e\u96c6\u662f imagenet \u7684\u4e00\u4e2a\u975e\u5e38\u5c0f\u7684\u5b50\u96c6\u3002</p> <p>\u6ce8\u610f</p> <p>\u4ece \u6b64\u5904 \u4e0b\u8f7d\u6570\u636e\u5e76\u5c06\u5176\u89e3\u538b\u7f29\u5230\u5f53\u524d\u76ee\u5f55\u3002</p> <pre><code># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_3","title":"\u53ef\u89c6\u5316\u4e00\u4e9b\u56fe\u50cf","text":"<p>\u8ba9\u6211\u4eec\u53ef\u89c6\u5316\u4e00\u4e9b\u8bad\u7ec3\u56fe\u50cf\uff0c\u4ee5\u4fbf\u4e86\u89e3\u6570\u636e\u589e\u5f3a\u3002</p> <pre><code>def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n</code></pre> <p></p>"},{"location":"1.0/transfer_learning_tutorial/#_4","title":"\u8bad\u7ec3\u6a21\u578b","text":"<p>\u73b0\u5728, \u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u901a\u7528\u51fd\u6570\u6765\u8bad\u7ec3\u6a21\u578b. \u8fd9\u91cc, \u6211\u4eec\u5c06\u4f1a\u4e3e\u4f8b\u8bf4\u660e:</p> <ul> <li>\u8c03\u5ea6\u5b66\u4e60\u7387</li> <li>\u4fdd\u5b58\u6700\u4f73\u7684\u5b66\u4e60\u6a21\u578b</li> </ul> <p>\u4e0b\u9762\u51fd\u6570\u4e2d, <code>scheduler</code> \u53c2\u6570\u662f <code>torch.optim.lr_scheduler</code> \u4e2d\u7684 LR scheduler \u5bf9\u8c61.</p> <pre><code>def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_5","title":"\u53ef\u89c6\u5316\u6a21\u578b\u9884\u6d4b","text":"<p>\u7528\u4e8e\u663e\u793a\u5c11\u91cf\u56fe\u50cf\u9884\u6d4b\u7684\u901a\u7528\u529f\u80fd</p> <pre><code>def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_6","title":"\u5fae\u8c03\u5377\u79ef\u7f51\u7edc","text":"<p>\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u91cd\u7f6e\u6700\u7ec8\u7684\u5168\u8fde\u63a5\u5c42\u3002</p> <pre><code>model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_7","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30","text":"<p>CPU\u4e0a\u9700\u8981\u5927\u7ea615-25\u5206\u949f\u3002\u4f46\u662f\u5728GPU\u4e0a\uff0c\u5b83\u53ea\u9700\u4e0d\u5230\u4e00\u5206\u949f\u3002</p> <pre><code>model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n\n</code></pre> <p>Out:</p> <pre><code>Epoch 0/24\n----------\ntrain Loss: 0.6022 Acc: 0.6844\nval Loss: 0.1765 Acc: 0.9412\n\nEpoch 1/24\n----------\ntrain Loss: 0.4156 Acc: 0.8238\nval Loss: 0.2380 Acc: 0.9216\n\nEpoch 2/24\n----------\ntrain Loss: 0.5010 Acc: 0.7951\nval Loss: 0.2571 Acc: 0.8954\n\nEpoch 3/24\n----------\ntrain Loss: 0.7152 Acc: 0.7705\nval Loss: 0.2060 Acc: 0.9346\n\nEpoch 4/24\n----------\ntrain Loss: 0.5779 Acc: 0.8033\nval Loss: 0.4542 Acc: 0.8889\n\nEpoch 5/24\n----------\ntrain Loss: 0.5653 Acc: 0.7951\nval Loss: 0.3167 Acc: 0.8824\n\nEpoch 6/24\n----------\ntrain Loss: 0.4948 Acc: 0.8074\nval Loss: 0.3238 Acc: 0.8758\n\nEpoch 7/24\n----------\ntrain Loss: 0.3712 Acc: 0.8361\nval Loss: 0.2284 Acc: 0.9020\n\nEpoch 8/24\n----------\ntrain Loss: 0.2982 Acc: 0.8730\nval Loss: 0.3488 Acc: 0.8497\n\nEpoch 9/24\n----------\ntrain Loss: 0.2491 Acc: 0.8934\nval Loss: 0.2405 Acc: 0.8889\n\nEpoch 10/24\n----------\ntrain Loss: 0.3498 Acc: 0.8238\nval Loss: 0.2435 Acc: 0.8889\n\nEpoch 11/24\n----------\ntrain Loss: 0.3042 Acc: 0.8648\nval Loss: 0.3021 Acc: 0.8627\n\nEpoch 12/24\n----------\ntrain Loss: 0.2500 Acc: 0.8852\nval Loss: 0.2340 Acc: 0.8954\n\nEpoch 13/24\n----------\ntrain Loss: 0.3246 Acc: 0.8730\nval Loss: 0.2236 Acc: 0.9020\n\nEpoch 14/24\n----------\ntrain Loss: 0.2976 Acc: 0.8566\nval Loss: 0.2928 Acc: 0.8562\n\nEpoch 15/24\n----------\ntrain Loss: 0.2733 Acc: 0.8934\nval Loss: 0.2370 Acc: 0.8954\n\nEpoch 16/24\n----------\ntrain Loss: 0.3502 Acc: 0.8361\nval Loss: 0.2792 Acc: 0.8824\n\nEpoch 17/24\n----------\ntrain Loss: 0.2215 Acc: 0.8975\nval Loss: 0.2790 Acc: 0.8497\n\nEpoch 18/24\n----------\ntrain Loss: 0.3929 Acc: 0.8484\nval Loss: 0.2648 Acc: 0.8824\n\nEpoch 19/24\n----------\ntrain Loss: 0.3227 Acc: 0.8607\nval Loss: 0.2643 Acc: 0.8693\n\nEpoch 20/24\n----------\ntrain Loss: 0.3816 Acc: 0.8484\nval Loss: 0.2395 Acc: 0.9085\n\nEpoch 21/24\n----------\ntrain Loss: 0.2904 Acc: 0.8975\nval Loss: 0.2399 Acc: 0.8889\n\nEpoch 22/24\n----------\ntrain Loss: 0.3375 Acc: 0.8648\nval Loss: 0.2380 Acc: 0.9020\n\nEpoch 23/24\n----------\ntrain Loss: 0.2107 Acc: 0.9139\nval Loss: 0.2251 Acc: 0.9085\n\nEpoch 24/24\n----------\ntrain Loss: 0.3243 Acc: 0.8525\nval Loss: 0.2545 Acc: 0.8824\n\nTraining complete in 1m 7s\nBest val Acc: 0.941176\n\n</code></pre> <pre><code>visualize_model(model_ft)\n\n\n</code></pre> <p></p>"},{"location":"1.0/transfer_learning_tutorial/#convnet","title":"ConvNet \u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668","text":"<p>\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u9700\u8981\u51bb\u7ed3\u9664\u6700\u540e\u4e00\u5c42\u4e4b\u5916\u7684\u6240\u6709\u7f51\u7edc\u3002\u6211\u4eec\u9700\u8981\u8bbe\u7f6e <code>requires_grad == False</code> \u51bb\u7ed3\u53c2\u6570\uff0c\u4ee5\u4fbf\u5728 <code>backward()</code> \u4e2d\u4e0d\u8ba1\u7b97\u68af\u5ea6\u3002</p> <p>\u60a8\u53ef\u4ee5\u5728 \u6b64\u5904 \u7684\u6587\u6863\u4e2d\u9605\u8bfb\u66f4\u591a\u76f8\u5173\u4fe1\u606f\u3002</p> <pre><code>model_conv = torchvision.models.resnet18(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n\n</code></pre>"},{"location":"1.0/transfer_learning_tutorial/#_8","title":"\u8bad\u7ec3\u548c\u8bc4\u4f30","text":"<p>\u5728CPU\u4e0a\uff0c\u4e0e\u524d\u4e00\u4e2a\u573a\u666f\u76f8\u6bd4\uff0c\u8fd9\u5c06\u82b1\u8d39\u5927\u7ea6\u4e00\u534a\u7684\u65f6\u95f4\u3002\u8fd9\u662f\u9884\u671f\u7684\uff0c\u56e0\u4e3a\u4e0d\u9700\u8981\u4e3a\u5927\u591a\u6570\u7f51\u7edc\u8ba1\u7b97\u68af\u5ea6\u3002\u4f46\u662f\uff0c\u524d\u5411\u4f20\u9012\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\u3002</p> <pre><code>model_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)\n\n\n</code></pre> <p>Out:</p> <pre><code>Epoch 0/24\n----------\ntrain Loss: 0.5666 Acc: 0.6967\nval Loss: 0.2794 Acc: 0.8824\n\nEpoch 1/24\n----------\ntrain Loss: 0.5590 Acc: 0.7582\nval Loss: 0.1473 Acc: 0.9477\n\nEpoch 2/24\n----------\ntrain Loss: 0.4187 Acc: 0.8156\nval Loss: 0.3534 Acc: 0.8693\n\nEpoch 3/24\n----------\ntrain Loss: 0.5248 Acc: 0.7459\nval Loss: 0.1848 Acc: 0.9477\n\nEpoch 4/24\n----------\ntrain Loss: 0.4315 Acc: 0.8115\nval Loss: 0.1640 Acc: 0.9477\n\nEpoch 5/24\n----------\ntrain Loss: 0.3948 Acc: 0.8238\nval Loss: 0.1609 Acc: 0.9542\n\nEpoch 6/24\n----------\ntrain Loss: 0.3359 Acc: 0.8648\nval Loss: 0.1734 Acc: 0.9608\n\nEpoch 7/24\n----------\ntrain Loss: 0.3681 Acc: 0.8443\nval Loss: 0.1715 Acc: 0.9477\n\nEpoch 8/24\n----------\ntrain Loss: 0.4034 Acc: 0.8361\nval Loss: 0.1602 Acc: 0.9477\n\nEpoch 9/24\n----------\ntrain Loss: 0.2983 Acc: 0.8811\nval Loss: 0.1561 Acc: 0.9542\n\nEpoch 10/24\n----------\ntrain Loss: 0.4516 Acc: 0.7992\nval Loss: 0.1660 Acc: 0.9477\n\nEpoch 11/24\n----------\ntrain Loss: 0.3516 Acc: 0.8484\nval Loss: 0.1551 Acc: 0.9542\n\nEpoch 12/24\n----------\ntrain Loss: 0.3592 Acc: 0.8238\nval Loss: 0.1525 Acc: 0.9477\n\nEpoch 13/24\n----------\ntrain Loss: 0.2982 Acc: 0.8648\nval Loss: 0.1772 Acc: 0.9542\n\nEpoch 14/24\n----------\ntrain Loss: 0.3352 Acc: 0.8484\nval Loss: 0.1583 Acc: 0.9542\n\nEpoch 15/24\n----------\ntrain Loss: 0.2981 Acc: 0.8770\nval Loss: 0.2133 Acc: 0.9412\n\nEpoch 16/24\n----------\ntrain Loss: 0.2778 Acc: 0.8811\nval Loss: 0.1934 Acc: 0.9542\n\nEpoch 17/24\n----------\ntrain Loss: 0.3678 Acc: 0.8156\nval Loss: 0.1846 Acc: 0.9477\n\nEpoch 18/24\n----------\ntrain Loss: 0.3520 Acc: 0.8197\nval Loss: 0.1577 Acc: 0.9542\n\nEpoch 19/24\n----------\ntrain Loss: 0.3342 Acc: 0.8402\nval Loss: 0.1734 Acc: 0.9542\n\nEpoch 20/24\n----------\ntrain Loss: 0.3649 Acc: 0.8361\nval Loss: 0.1554 Acc: 0.9412\n\nEpoch 21/24\n----------\ntrain Loss: 0.2948 Acc: 0.8566\nval Loss: 0.1878 Acc: 0.9542\n\nEpoch 22/24\n----------\ntrain Loss: 0.3047 Acc: 0.8811\nval Loss: 0.1760 Acc: 0.9477\n\nEpoch 23/24\n----------\ntrain Loss: 0.3363 Acc: 0.8648\nval Loss: 0.1660 Acc: 0.9542\n\nEpoch 24/24\n----------\ntrain Loss: 0.2745 Acc: 0.8770\nval Loss: 0.1853 Acc: 0.9542\n\nTraining complete in 0m 34s\nBest val Acc: 0.960784\n\n\n</code></pre> <pre><code>visualize_model(model_conv)\n\nplt.ioff()\nplt.show()\n\n</code></pre> <p></p> <p>\u811a\u672c\u603b\u8fd0\u884c\u65f6\u95f4: (1\u520654.087\u79d2)</p> <p><code>Download Python source code: transfer_learning_tutorial.py</code><code>Download Jupyter notebook: transfer_learning_tutorial.ipynb</code></p> <p>\u7531Sphinx-Gallery\u751f\u6210\u7684\u56fe\u5e93</p>"},{"location":"1.0/tut_extending_pytorch/","title":"\u6269\u5c55 PyTorch","text":""},{"location":"1.0/tut_generative/","title":"\u751f\u6210","text":""},{"location":"1.0/tut_getting_started/","title":"\u8d77\u6b65","text":""},{"location":"1.0/tut_image/","title":"\u56fe\u50cf","text":""},{"location":"1.0/tut_other_language/","title":"\u5176\u5b83\u8bed\u8a00\u4e2d\u7684 PyTorch","text":""},{"location":"1.0/tut_production_usage/","title":"\u751f\u4ea7\u6027\u4f7f\u7528","text":""},{"location":"1.0/tut_reinforcement_learning/","title":"\u5f3a\u5316\u5b66\u4e60","text":""},{"location":"1.0/tut_text/","title":"\u6587\u672c","text":""},{"location":"1.0/type_info/","title":"\u6570\u636e\u7c7b\u578b\u4fe1\u606f","text":"<p>\u8bd1\u8005\uff1a\u51af\u5b9d\u5b9d </p> <p>\u53ef\u4ee5\u901a\u8fc7<code>torch.finfo</code> \u6216 <code>torch.iinfo</code>\u8bbf\u95ee<code>torch.dtype</code>\u7684\u6570\u5b57\u5c5e\u6027\u3002  </p>"},{"location":"1.0/type_info/#torchfinfo","title":"torch.finfo","text":"<pre><code>class torch.finfo\n</code></pre> <p><code>torch.finfo</code> \u662f\u4e00\u4e2a\u7528\u6765\u8868\u793a\u6d6e\u70b9<code>torch.dtype</code>\u7684\u6570\u5b57\u5c5e\u6027\u7684\u5bf9\u8c61(\u5373<code>torch.float32</code>\uff0c<code>torch.float64</code>\u548c<code>torch.float16</code>\uff09\u3002 \u8fd9\u7c7b\u4f3c\u4e8e numpy.finfo\u3002  </p> <p><code>torch.finfo</code> \u63d0\u4f9b\u4ee5\u4e0b\u5c5e\u6027:  </p> \u540d\u79f0 \u7c7b\u578b \u63cf\u8ff0 bits \u6574\u578b\u3000int \u6570\u636e\u7c7b\u578b\u5360\u7528\u7684\u4f4d\u6570 eps \u6d6e\u70b9\u578bfloat \u53ef\u8868\u793a\u7684\u6700\u5c0f\u6570\u5b57\uff0c\u4f7f\u5f971.0 + eps\uff01= 1.0 max \u6d6e\u70b9\u578bfloat \u53ef\u8868\u793a\u7684\u6700\u5927\u6570\u5b57 tiny \u6d6e\u70b9\u578bfloat \u53ef\u8868\u793a\u7684\u6700\u5c0f\u6b63\u6570 <p>\u6ce8\u610f  </p> <p>\u5728\u4f7f\u7528pytorch\u9ed8\u8ba4dtype\u521b\u5efa\u7c7b(\u7531<code>torch.get_default_dtype(\uff09</code>\u8fd4\u56de\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u6784\u9020\u7684 <code>torch.finfo</code> \u51fd\u6570\u53ef\u4ee5\u4e0d\u5e26\u53c2\u6570\u88ab\u8c03\u7528\u3002  </p>"},{"location":"1.0/type_info/#torchiinfo","title":"torch.iinfo","text":"<pre><code>class torch.iinfo\n</code></pre> <p><code>torch.iinfo</code>\u662f\u4e00\u4e2a\u7528\u6765\u8868\u793a\u6574\u6570<code>torch.dtype</code> \u7684\u6570\u5b57\u5c5e\u6027\u7684\u5bf9\u8c61\uff0c(\u5373<code>torch.uint8</code>\uff0c<code>torch.int8</code>\uff0c<code>torch.int16</code>\uff0c<code>torch.int32</code>\u548c<code>torch.int64</code>\uff09\u3002 \u8fd9\u4e0enumpy.iinfo\u7c7b\u4f3c\u3002  </p> <p><code>torch.iinfo</code> \u63d0\u4f9b\u4ee5\u4e0b\u5c5e\u6027\uff1a   </p> \u540d\u79f0 \u7c7b\u578b \u63cf\u8ff0 bits \u6574\u578b \u6570\u636e\u7c7b\u578b\u5360\u7528\u7684\u4f4d\u6570 max \u6574\u578b \u53ef\u8868\u793a\u7684\u6700\u5927\u6570\u5b57"},{"location":"LatestChanges/","title":"\u7248\u672c\u7279\u6027","text":"<ul> <li>PyTorch V1.2 \u65b0\u7279\u6027</li> </ul>"},{"location":"LatestChanges/PyTorch_V1.2/","title":"\u65b0\u7248\u672c: PyTorch 1.2 torchtext 0.4\uff0ctorchaudio 0.3 \u548c torchvision 0.4","text":"<p>\u53d1\u5e03: 2019\u5e748\u67088\u65e5</p> <p>\u8bd1\u8005\uff1a@\u7247\u523b</p> <p>\u539f\u6587: PyTorch</p> <p>\u7ffb\u8bd1: ApacheCN</p> <p>\u81eaPyTorch 1.0\u53d1\u5e03\u4ee5\u6765\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u793e\u533a\u6269\u5c55\u5230\u6dfb\u52a0\u65b0\u5de5\u5177\uff0c\u4e3aPyTorch Hub\u4e2d\u53ef\u7528\u7684\u8d8a\u6765\u8d8a\u591a\u7684\u6a21\u578b\u505a\u51fa\u8d21\u732e\uff0c\u5e76\u4e0d\u65ad\u589e\u52a0\u7814\u7a76\u548c\u751f\u4ea7\u4e2d\u7684\u4f7f\u7528\u3002</p> <p>\u4ece\u6838\u5fc3\u89d2\u5ea6\u6765\u770b\uff0cPyTorch\u4e0d\u65ad\u6dfb\u52a0\u529f\u80fd\u4ee5\u652f\u6301\u7814\u7a76\u548c\u751f\u4ea7\u4f7f\u7528\uff0c\u5305\u62ec\u901a\u8fc7TorchScript\u8fde\u63a5\u8fd9\u4e24\u4e2a\u4e16\u754c\u7684\u80fd\u529b\u3002\u4eca\u5929\uff0c\u6211\u4eec\u5f88\u9ad8\u5174\u5730\u5ba3\u5e03\u6211\u4eec\u6709\u56db\u4e2a\u65b0\u7248\u672c\uff0c\u5305\u62ecPyTorch 1.2\uff0ctorchvision 0.4\uff0ctorchaudio 0.3\u548ctorchtext 0.4\u3002\u60a8\u73b0\u5728\u53ef\u4ee5\u5728pytorch.org\u4e0a\u5f00\u59cb\u4f7f\u7528\u8fd9\u4e9b\u7248\u672c\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#pytorch-12","title":"PyTorch 1.2","text":"<p>\u4f7f\u7528PyTorch 1.2\uff0c\u5f00\u6e90ML\u6846\u67b6\u5728\u751f\u4ea7\u4f7f\u7528\u65b9\u9762\u5411\u524d\u8fc8\u51fa\u4e86\u4e00\u5927\u6b65\uff0c\u589e\u52a0\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\uff0c\u66f4\u52a0\u5b8c\u5584\u7684TorchScript\u73af\u5883\u3002\u8fd9\u4e9b\u6539\u8fdb\u4f7f\u5f97\u66f4\u5bb9\u6613\u53d1\u5e03\u751f\u4ea7\u6a21\u578b\uff0c\u6269\u5c55\u5bf9\u5bfc\u51faONNX\u683c\u5f0f\u6a21\u578b\u7684\u652f\u6301\uff0c\u5e76\u589e\u5f3a\u5bf9 Transformers \u7684\u6a21\u5757\u7ea7\u652f\u6301\u3002\u9664\u4e86\u8fd9\u4e9b\u65b0\u529f\u80fd\u4e4b\u5916\uff0cTensorBoard \u73b0\u5728\u4e0d\u518d\u5177\u6709\u5b9e\u9a8c\u6027 - \u60a8\u53ea\u9700\u952e\u5165<code>from torch.utils.tensorboard import SummaryWriter</code>\u5373\u53ef\u5f00\u59cb\u4f7f\u7528\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#torchscript","title":"TORCHSCRIPT \u6539\u8fdb","text":"<p>\u81ea\u4ece\u5728PyTorch 1.0\u4e2d\u53d1\u5e03\u4ee5\u6765\uff0cTorchScript\u5df2\u7ecf\u4e3a\u70ed\u5207\u7684PyTorch\u6a21\u578b\u63d0\u4f9b\u4e86\u751f\u4ea7\u9014\u5f84\u3002TorchScript\u7f16\u8bd1\u5668\u5c06PyTorch\u6a21\u578b\u8f6c\u6362\u4e3a\u9759\u6001\u7c7b\u578b\u7684\u56fe\u5f62\u8868\u793a\uff0c\u4e3aPython\u4e0d\u53ef\u7528\u7684\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4f18\u5316\u548c\u6267\u884c\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u60a8\u53ef\u4ee5\u5c06\u6a21\u578b\u9010\u6b65\u8f6c\u6362\u4e3aTorchScript\uff0c\u5c06\u7f16\u8bd1\u540e\u7684\u4ee3\u7801\u4e0ePython\u65e0\u7f1d\u6df7\u5408\u3002</p> <p>PyTorch 1.2\u663e\u7740\u6269\u5c55\u4e86TorchScript\u5bf9PyTorch\u6a21\u578b\u4e2d\u4f7f\u7528\u7684Python\u5b50\u96c6\u7684\u652f\u6301\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\uff0c\u66f4\u6613\u4e8e\u4f7f\u7528\u7684API\uff0c\u7528\u4e8e\u5c06\u6a21\u578b\u7f16\u8bd1\u4e3aTorchScript\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u8fc1\u79fb\u6307\u5357\u00a0\u4ee5\u4e0b\u662f\u65b0API\u7684\u793a\u4f8b\u7528\u6cd5\uff1a</p> <pre><code>import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\n# Compile the model code to a static representation\nmy_script_module = torch.jit.script(MyModule(3, 4))\n\n# Save the compiled code and model data so it can be loaded elsewhere\nmy_script_module.save(\"my_script_module.pt\")\n</code></pre> <p>\u8981\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u6211\u4eec\u7684TorchScript\u7b80\u4ecb\u548c\u5728C ++\u4e2d\u52a0\u8f7dPyTorch\u6a21\u578b\u6559\u7a0b\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#onnx-export","title":"\u6269\u5c55\u4e86ONNX EXPORT","text":"<p>\u8be5ONNX\u793e\u4f1a\u7ee7\u7eed\u4ee5\u5f00\u653e\u7684\u6210\u957f\u6cbb\u7406\u7ed3\u6784\u548c\u989d\u5916\u7684\u6307\u5bfc\u59d4\u5458\u4f1a\u6210\u5458\uff0c\u7279\u6b8a\u5174\u8da3\u5c0f\u7ec4(SIG\uff09\u548c\u5de5\u4f5c\u7ec4(WGS\uff09\u3002\u4e0eMicrosoft\u5408\u4f5c\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u5bf9\u5bfc\u51faONNX Opset\u7248\u672c7(v1.2)\uff0c8(v1.3)\uff0c9(v1.4)\u548c10(v1.5)\u7684\u5168\u9762\u652f\u6301\u3002\u6211\u4eec\u8fd8\u589e\u5f3a\u4e86\u5e38\u91cf\u6298\u53e0\u4f20\u9012\uff0c\u4ee5\u652f\u6301\u6700\u65b0\u7248\u672c\u7684ONNX Opset 10\u3002ScriptModule\u4e5f\u5f97\u5230\u4e86\u6539\u8fdb\uff0c\u5305\u62ec\u652f\u6301\u591a\u8f93\u51fa\uff0c\u5f20\u91cf\u5de5\u5382\u548c\u5143\u7ec4\u4f5c\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u6ce8\u518c\u81ea\u5df1\u7684\u7b26\u53f7\u6765\u5bfc\u51fa\u81ea\u5b9a\u4e49\u64cd\u4f5c\uff0c\u5e76\u5728\u5bfc\u51fa\u671f\u95f4\u6307\u5b9a\u8f93\u5165\u7684\u52a8\u6001\u5c3a\u5bf8\u3002\u4ee5\u4e0b\u662f\u6240\u6709\u4e3b\u8981\u6539\u8fdb\u7684\u6458\u8981\uff1a</p> <ul> <li>\u652f\u6301\u591a\u79cdOpset\uff0c\u5305\u62ec\u5728Opset 10\u4e2d\u5bfc\u51fa\u4e22\u5931\uff0c\u5207\u7247\uff0c\u7ffb\u8f6c\u548c\u63d2\u503c\u7684\u529f\u80fd\u3002</li> <li>ScriptModule\u7684\u6539\u8fdb\uff0c\u5305\u62ec\u652f\u6301\u591a\u8f93\u51fa\uff0c\u5f20\u91cf\u5de5\u5382\u548c\u5143\u7ec4\u4f5c\u4e3a\u8f93\u5165\u548c\u8f93\u51fa\u3002</li> <li>\u652f\u6301\u4e86\u5341\u51e0\u4e2a\u989d\u5916\u7684PyTorch\u8fd0\u8425\u5546\uff0c\u5305\u62ec\u5bfc\u51fa\u81ea\u5b9a\u4e49\u8fd0\u8425\u5546\u7684\u80fd\u529b\u3002</li> <li>\u8bb8\u591a\u91cd\u5927\u4fee\u590d\u548c\u6d4b\u8bd5\u57fa\u7840\u6539\u8fdb\u3002</li> </ul> <p>\u60a8\u53ef\u4ee5\u5728\u8fd9\u91cc\u8bd5\u7528\u6700\u65b0\u7684\u6559\u7a0b\uff0c\u7531@ lara-hdr\u5728Microsoft\u63d0\u4f9b\u3002\u975e\u5e38\u611f\u8c22\u6574\u4e2aMicrosoft\u56e2\u961f\u4e3a\u5b8c\u6210\u6b64\u7248\u672c\u6240\u505a\u7684\u6240\u6709\u52aa\u529b\uff01</p>"},{"location":"LatestChanges/PyTorch_V1.2/#nntransformer","title":"NN.TRANSFORMER","text":"<p>\u5728PyTorch 1.2\u4e2d\uff0c\u6211\u4eec\u73b0\u5728\u5305\u542b\u4e00\u4e2a\u6807\u51c6\u7684nn.Transformer\u6a21\u5757\uff0c\u57fa\u4e8e\u201c\u00a0\u6ce8\u610f\u5c31\u662f\u4f60\u6240\u9700\u8981\u7684\u00a0\u201d\u00a0\u8fd9\u7bc7\u8bba\u6587\u3002\u8be5<code>nn.Transformer</code>\u6a21\u5757\u5b8c\u5168\u4f9d\u8d56\u4e8e\u6ce8\u610f\u673a\u5236\u6765\u7ed8\u5236\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002<code>nn.Transformer</code>\u6a21\u5757\u7684\u5404\u4e2a\u7ec4\u4ef6\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u72ec\u7acb\u4f7f\u7528\u3002\u4f8b\u5982\uff0cnn.TransformerEncoder\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff0c\u4e0d\u9700\u8981\u66f4\u5927<code>nn.Transformer</code>\u3002\u65b0API\u5305\u62ec\uff1a</p> <ul> <li><code>nn.Transformer</code></li> <li><code>nn.TransformerEncoder</code>\u00a0\u548c\u00a0<code>nn.TransformerEncoderLayer</code></li> <li><code>nn.TransformerDecoder</code>\u00a0\u548c\u00a0<code>nn.TransformerDecoderLayer</code></li> </ul> <p></p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605Transformer Layers\u6587\u6863\u3002\u6709\u5173\u5b8c\u6574\u7684PyTorch 1.2\u53d1\u884c\u8bf4\u660e\uff0c\u8bf7\u53c2\u89c1\u6b64\u5904\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#api","title":"\u57dfAPI\u5e93\u66f4\u65b0","text":"<p>PyTorch\u57df\u5e93(\u5982torchvision\uff0ctorchtext\u548ctorchaudio\uff09\u63d0\u4f9b\u4e86\u5bf9\u5e38\u7528\u6570\u636e\u96c6\uff0c\u6a21\u578b\u548c\u53d8\u6362\u7684\u4fbf\u6377\u8bbf\u95ee\uff0c\u53ef\u7528\u4e8e\u5feb\u901f\u521b\u5efa\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u8fd8\u63d0\u4f9b\u4e86\u5e38\u89c1\u7684\u62bd\u8c61\uff0c\u4ee5\u51cf\u5c11\u7528\u6237\u53ef\u80fd\u4e0d\u5f97\u4e0d\u91cd\u590d\u5199\u5165\u7684\u6837\u677f\u4ee3\u7801\u3002\u7531\u4e8e\u7814\u7a76\u9886\u57df\u6709\u4e0d\u540c\u7684\u8981\u6c42\uff0c\u56f4\u7ed5PyTorch\u51fa\u73b0\u4e86\u4e00\u4e2a\u79f0\u4e3a\u57dfAPI(DAPI\uff09\u7684\u4e13\u4e1a\u5e93\u751f\u6001\u7cfb\u7edf\uff0c\u4ee5\u7b80\u5316\u8bb8\u591a\u9886\u57df\u4e2d\u65b0\u7b97\u6cd5\u548c\u73b0\u6709\u7b97\u6cd5\u7684\u5f00\u53d1\u3002\u6211\u4eec\u5f88\u9ad8\u5174\u53d1\u5e03\u4e09\u4e2a\u66f4\u65b0\u7684DAPI\u5e93\uff0c\u7528\u4e8e\u652f\u6301PyTorch 1.2\u6838\u5fc3\u7248\u672c\u7684\u6587\u672c\uff0c\u97f3\u9891\u548c\u89c6\u89c9\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#torchaudio-03kaldi","title":"TORCHAUDIO 0.3\u4e0eKALDI\u517c\u5bb9\u6027\uff0c\u65b0\u53d8\u6362","text":"<p>Torchaudio\u4e13\u6ce8\u4e8e\u673a\u5668\u7406\u89e3\u97f3\u9891\u6ce2\u5f62\u3002\u5b83\u662f\u4e00\u4e2aML\u5e93\uff0c\u63d0\u4f9b\u76f8\u5173\u7684\u4fe1\u53f7\u5904\u7406\u529f\u80fd(\u4f46\u4e0d\u662f\u4e00\u822c\u7684\u4fe1\u53f7\u5904\u7406\u5e93\uff09\u3002\u5b83\u5229\u7528PyTorch\u7684GPU\u652f\u6301\u4e3a\u6ce2\u5f62\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u548c\u8f6c\u6362\uff0c\u4f7f\u6570\u636e\u52a0\u8f7d\u548c\u6807\u51c6\u5316\u66f4\u5bb9\u6613\uff0c\u66f4\u6613\u8bfb\u3002\u4f8b\u5982\uff0c\u5b83\u4e3a\u4f7f\u7528sox\u7684\u6ce2\u5f62\u63d0\u4f9b\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5e76\u4e3a\u9891\u8c31\u56fe\uff0c\u91cd\u91c7\u6837\u548cmu-law\u7f16\u7801\u548c\u89e3\u7801\u7b49\u8f6c\u6362\u63d0\u4f9b\u6570\u636e\u52a0\u8f7d\u5668\u3002</p> <p>\u6211\u4eec\u5f88\u9ad8\u5174\u5730\u5ba3\u5e03torchaudio 0.3.0\u7684\u53ef\u7528\u6027\uff0c\u91cd\u70b9\u662f\u6807\u51c6\u5316\u548c\u590d\u6570\uff0c\u8f6c\u6362(\u91cd\u65b0\u91c7\u6837\uff09\u548c\u4e24\u4e2a\u65b0\u7684\u529f\u80fd(phase_vocoder\uff0cISTFT\uff09\uff0cKaldi\u517c\u5bb9\u6027\u548c\u65b0\u6559\u7a0b\u3002Torchaudio\u7ecf\u8fc7\u91cd\u65b0\u8bbe\u8ba1\uff0c\u662fPyTorch\u7684\u6269\u5c55\uff0c\u4e5f\u662f\u57dfAPI(DAPI\uff09\u751f\u6001\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#_1","title":"\u6807\u51c6\u5316","text":"<p>\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u91cd\u8981\u5de5\u4f5c\u662f\u6570\u636e\u51c6\u5907\u3002\u5728\u8fd9\u4e2a\u65b0\u7248\u672c\u4e2d\uff0c\u6211\u4eec\u66f4\u65b0\u4e86torchaudio\u7684\u8f6c\u6362\u63a5\u53e3\uff0c\u4ee5\u4fbf\u56f4\u7ed5\u4ee5\u4e0b\u8bcd\u6c47\u548c\u7ea6\u5b9a\u8fdb\u884c\u6807\u51c6\u5316\u3002</p> <p>\u5047\u8bbe\u5f20\u91cf\u5177\u6709\u901a\u9053\u4f5c\u4e3a\u7b2c\u4e00\u7ef4\u5ea6\uff0c\u65f6\u95f4\u4f5c\u4e3a\u6700\u540e\u7ef4\u5ea6(\u9002\u7528\u65f6\uff09\u3002\u8fd9\u4f7f\u5f97\u5b83\u4e0ePyTorch\u7684\u5c3a\u5bf8\u4e00\u81f4\u3002\u5bf9\u4e8e\u5927\u5c0f\u540d\u79f0\uff0c\u4f7f\u7528\u524d\u7f00<code>n_</code>(\u4f8b\u5982\u201c\u5927\u5c0f(<code>n_freq</code>\uff0c<code>n_mel</code>\uff09\u7684\u5f20\u91cf\u201d\uff09\uff0c\u800c\u7ef4\u5ea6\u540d\u79f0\u4e0d\u5177\u6709\u8be5\u524d\u7f00(\u4f8b\u5982\u201c\u7ef4\u5ea6\u5f20\u91cf(\u901a\u9053\uff0c\u65f6\u95f4\uff09\u201d\uff09\u3002\u6240\u6709\u53d8\u6362\u548c\u51fd\u6570\u7684\u8f93\u5165\u73b0\u5728\u9996\u5148\u5047\u5b9a\u901a\u9053\u3002\u8fd9\u6837\u505a\u662f\u4e3a\u4e86\u4e0ePyTorch\u4fdd\u6301\u4e00\u81f4\uff0cPyTorch\u5177\u6709\u901a\u9053\uff0c\u540e\u8ddf\u6837\u672c\u6570\u91cf\u3002\u73b0\u5728\u4e0d\u63a8\u8350\u4f7f\u7528\u6240\u6709\u8f6c\u6362\u548c\u51fd\u6570\u7684\u901a\u9053\u53c2\u6570\u3002</p> <p>\u8f93\u51fa<code>STFT</code>\u662f(\u901a\u9053\uff0c\u9891\u7387\uff0c\u65f6\u95f4\uff0c2\uff09\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u901a\u9053\u800c\u8a00\uff0c\u5217\u662f\u7279\u5b9a\u7a97\u53e3\u7684\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u56e0\u6b64\u5f53\u6211\u4eec\u6c34\u5e73\u79fb\u52a8\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6bcf\u5217(\u5085\u91cc\u53f6\u53d8\u6362\u6ce2\u5f62\uff09\u968f\u65f6\u95f4\u53d8\u5316\u3002\u8fd9\u7b26\u5408librosa\u7684\u8f93\u51fa\uff0c\u4f7f\u6211\u4eec\u4e0d\u518d\u9700\u8981\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u6bd4\u8f83\uff0c\u8f6c\u7528<code>Spectrogram</code>\uff0c<code>MelScale</code>\uff0c<code>MelSpectrogram</code>\uff0c\u548c<code>MFCC</code>\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u8fd9\u4e9b\u65b0\u7684\u60ef\u4f8b\uff0c\u6211\u4eec\u5f03\u7528<code>LC2CL</code>\u5e76\u4e14<code>BLC2CBL</code>\u7528\u4e8e\u4ece\u4e00\u79cd\u4fe1\u53f7\u5f62\u72b6\u8f6c\u6362\u5230\u53e6\u4e00\u79cd\u5f62\u72b6\u3002</p> <p>\u4f5c\u4e3a\u6b64\u7248\u672c\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u8fd8\u901a\u8fc7\u5c3a\u5bf8\u5f20\u91cf(...\uff0c2\uff09\u5f15\u5165\u5bf9\u590d\u6570\u7684\u652f\u6301\uff0c\u5e76\u63d0\u4f9b<code>magphase</code>\u5c06\u8fd9\u6837\u7684\u5f20\u91cf\u8f6c\u6362\u4e3a\u5176\u5e45\u5ea6\u548c\u76f8\u4f4d\uff0c\u4ee5\u53ca\u7c7b\u4f3c<code>complex_norm</code>\u548c<code>angle</code>\u3002</p> <p>README\u4e2d\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#kaldi","title":"\u529f\u80fd\uff0c\u8f6c\u6362\u548cKaldi\u517c\u5bb9\u6027","text":"<p>\u5728\u6807\u51c6\u5316\u4e4b\u524d\uff0c\u6211\u4eec\u5c06\u72b6\u6001\u548c\u8ba1\u7b97\u5206\u6210\u4e86<code>torchaudio.transforms</code>\u548c<code>torchaudio.functional</code>\u3002</p> <p>\u4f5c\u4e3a\u8f6c\u6362\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u57280.3.0\u4e2d\u6dfb\u52a0\u4e86\u4e00\u4e2a\u65b0\u7684\u8f6c\u6362\uff1a<code>Resample</code>\u3002<code>Resample</code>\u53ef\u4ee5\u5c06\u6ce2\u5f62\u4e0a\u91c7\u6837\u6216\u4e0b\u91c7\u6837\u5230\u4e0d\u540c\u7684\u9891\u7387\u3002</p> <p>\u4f5c\u4e3a\u529f\u200b\u200b\u80fd\u7684\u4e00\u90e8\u5206\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\uff1a<code>phase_vocoder</code>\u4e00\u4e2a\u76f8\u4f4d\u58f0\u7801\u5668\uff0c\u7528\u4e8e\u6539\u53d8\u6ce2\u5f62\u7684\u901f\u5ea6\u800c\u4e0d\u6539\u53d8\u5176\u97f3\u8c03\uff0c\u5e76\u4e14<code>ISTFT</code>\u53cd\u5411<code>STFT</code>\u5b9e\u73b0\u4e0ePyTorch\u63d0\u4f9b\u7684STFT\u517c\u5bb9\u3002\u8fd9\u79cd\u5206\u79bb\u5141\u8bb8\u6211\u4eec\u4f7f\u51fd\u6570\u5f31\u811a\u672c\u5316\u5e76\u57280.3.0\u4e2d\u4f7f\u7528JIT\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6709\u4ee5\u4e0b\u7684\u8f6c\u6362JIT\u548cCUDA\u652f\u6301\uff1a<code>Spectrogram</code>\uff0c<code>AmplitudeToDB</code>(\u539f\u540d<code>SpectrogramToDB</code>\uff09<code>MelScale</code>\uff0c\u00a0<code>MelSpectrogram</code>\uff0c<code>MFCC</code>\uff0c<code>MuLawEncoding</code>\uff0c<code>MuLawDecoding</code>(\u539f\u540d<code>MuLawExpanding</code>\uff09\u3002</p> <p>\u6211\u4eec\u73b0\u5728\u8fd8\u63d0\u4f9b\u4e0eKaldi\u7684\u517c\u5bb9\u63a5\u53e3\uff0c\u4ee5\u7b80\u5316\u5165\u95e8\u5e76\u51cf\u5c11\u7528\u6237\u5bf9Kaldi\u7684\u4ee3\u7801\u4f9d\u8d56\u6027\u3002\u6211\u4eec\u73b0\u5728\u6709\u4e00\u4e2a\u63a5\u53e3<code>spectrogram</code>\uff0c<code>fbank</code>\u548c<code>resample_waveform</code>\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#_2","title":"\u65b0\u6559\u7a0b","text":"<p>\u4e3a\u4e86\u5c55\u793a\u65b0\u7684\u7ea6\u5b9a\u548c\u8f6c\u6362\uff0c\u6211\u4eec\u6709\u4e00\u4e2a\u65b0\u7684\u6559\u7a0b\uff0c\u6f14\u793a\u5982\u4f55\u4f7f\u7528torchaudio\u9884\u5904\u7406\u6ce2\u5f62\u3002\u672c\u6559\u7a0b\u5c06\u4ecb\u7ecd\u52a0\u8f7d\u6ce2\u5f62\u5e76\u5bf9\u5176\u5e94\u7528\u4e00\u4e9b\u53ef\u7528\u8f6c\u6362\u7684\u793a\u4f8b\u3002</p> <p>\u6211\u4eec\u5f88\u9ad8\u5174\u770b\u5230torchaudio\u5468\u56f4\u7684\u6d3b\u8dc3\u793e\u533a\uff0c\u5e76\u6e34\u671b\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u652f\u6301\u5b83\u3002\u6211\u4eec\u9f13\u52b1\u60a8\u7ee7\u7eed\u4f7f\u7528\u672c\u6559\u7a0b\u548c\u53ef\u7528\u7684\u4e24\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff1aVCTK\u548cYESNO\uff01\u4ed6\u4eec\u6709\u4e00\u4e2a\u754c\u9762\u6765\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4ee5\u65b9\u4fbf\u7684\u683c\u5f0f\u9884\u5904\u7406\u5b83\u4eec\u3002\u60a8\u53ef\u4ee5\u5728\u6b64\u5904\u7684\u53d1\u884c\u8bf4\u660e\u4e2d\u627e\u5230\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#torchtext-04","title":"\u5e26\u6709\u76d1\u7763\u5b66\u4e60\u6570\u636e\u96c6\u7684TORCHTEXT 0.4","text":"<p>torchtext\u7684\u4e00\u4e2a\u5173\u952e\u91cd\u70b9\u9886\u57df\u662f\u63d0\u4f9b\u6709\u52a9\u4e8e\u52a0\u901fNLP\u7814\u7a76\u7684\u57fa\u672c\u8981\u7d20\u3002\u8fd9\u5305\u62ec\u8f7b\u677e\u8bbf\u95ee\u5e38\u7528\u6570\u636e\u96c6\u548c\u57fa\u672c\u9884\u5904\u7406\u7ba1\u9053\uff0c\u4ee5\u5904\u7406\u57fa\u4e8e\u539f\u59cb\u6587\u672c\u7684\u6570\u636e\u3002torchtext 0.4.0\u7248\u672c\u5305\u62ec\u51e0\u4e2a\u53d7\u6b22\u8fce\u7684\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\uff0c\u5e26\u6709\u201c\u4e00\u4e2a\u547d\u4ee4\u201d\u7684\u6570\u636e\u52a0\u8f7d\u3002\u5305\u542b\u4e00\u4e2a\u6559\u7a0b\uff0c\u4ee5\u8bf4\u660e\u5982\u4f55\u4f7f\u7528\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u5206\u6790\u3002\u6211\u4eec\u8fd8\u6dfb\u52a0\u5e76\u6539\u8fdb\u4e86\u4e00\u4e9b\u51fd\u6570\uff0c\u4f8b\u5982get_tokenizer\u548cbuild_vocab_from_iterator\uff0c\u4ee5\u4fbf\u66f4\u5bb9\u6613\u5b9e\u73b0\u672a\u6765\u7684\u6570\u636e\u96c6\u3002\u5176\u4ed6\u793a\u4f8b\u53ef\u4ee5\u5728\u8fd9\u91cc\u627e\u5230\u3002</p> <p>\u6587\u672c\u5206\u7c7b\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u9879\u91cd\u8981\u4efb\u52a1\uff0c\u5177\u6709\u8bb8\u591a\u5e94\u7528\uff0c\u4f8b\u5982\u60c5\u611f\u5206\u6790\u3002\u65b0\u7248\u672c\u5305\u62ec\u51e0\u4e2a\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u6d41\u884c\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5305\u62ec\uff1a</p> <ul> <li>AG_NEWS</li> <li>SogouNews</li> <li>DBpedia\u4e2d</li> <li>YelpReviewPolarity</li> <li>YelpReviewFull</li> <li>\u96c5\u864e\u77e5\u8bc6\u5802</li> <li>AmazonReviewPolarity</li> <li>AmazonReviewFull</li> </ul> <p>\u6bcf\u4e2a\u6570\u636e\u96c6\u90fd\u6709\u4e24\u4e2a\u90e8\u5206(\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\uff09\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u5355\u4e2a\u547d\u4ee4\u8f7b\u677e\u52a0\u8f7d\u3002\u6570\u636e\u96c6\u8fd8\u652f\u6301ngrams\u529f\u80fd\uff0c\u4ee5\u6355\u83b7\u6709\u5173\u672c\u5730\u5b57\u987a\u5e8f\u7684\u90e8\u5206\u4fe1\u606f\u3002\u8bf7\u67e5\u770b\u6b64\u5904\u7684\u6559\u7a0b\uff0c\u4ee5\u4e86\u89e3\u6709\u5173\u5982\u4f55\u5c06\u65b0\u6570\u636e\u96c6\u7528\u4e8e\u76d1\u7763\u95ee\u9898(\u5982\u6587\u672c\u5206\u7c7b\u5206\u6790\uff09\u7684\u66f4\u591a\u4fe1\u606f\u3002</p> <pre><code>from torchtext.datasets.text_classification import DATASETS\ntrain_dataset, test_dataset = DATASETS['AG_NEWS'](ngrams=2)\n</code></pre> <p>\u9664\u4e86\u57df\u5e93\u4e4b\u5916\uff0cPyTorch\u8fd8\u63d0\u4f9b\u4e86\u8bb8\u591a\u5de5\u5177\u6765\u7b80\u5316\u6570\u636e\u52a0\u8f7d\u3002\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e9b\u652f\u6301\u826f\u597d\u7684\u5de5\u5177\u52a0\u8f7d\u548c\u9884\u5904\u7406\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u4f8b\u5982torch.utils.data.DataLoader\u548ctorch.utils.data.IterableDataset\u3002\u4ee5\u4e0b\u662f\u4f7f\u7528DataLoader\u5305\u88c5\u6570\u636e\u7684\u51e0\u884c\u4ee3\u7801\u3002\u66f4\u591a\u4f8b\u5b50\u53ef\u4ee5\u5728\u8fd9\u91cc\u627e\u5230\u3002</p> <pre><code>from torch.utils.data import DataLoader\ndata = DataLoader(train_dataset, collate_fn=generate_batch)\n</code></pre> <p>\u67e5\u770b\u6b64\u5904\u7684\u53d1\u884c\u8bf4\u660e\u4ee5\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u5e76\u5728\u6b64\u5904\u8bd5\u7528\u672c\u6559\u7a0b\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.2/#torchvision-04","title":"TORCHVISION 0.4\u652f\u6301\u89c6\u9891","text":"<p>\u89c6\u9891\u73b0\u5728\u662ftorchvision\u7684\u4e00\u6d41\u516c\u6c11\uff0c\u652f\u6301\u6570\u636e\u52a0\u8f7d\uff0c\u6570\u636e\u96c6\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u53d8\u6362\u3002Torch \u76840.4\u7248\u672c\u5305\u62ec\uff1a</p> <ul> <li>\u7528\u4e8e\u8bfb/\u5199\u89c6\u9891\u6587\u4ef6(\u5305\u62ec\u97f3\u9891\uff09\u7684\u9ad8\u6548IO\u539f\u8bed\uff0c\u652f\u6301\u4efb\u610f\u7f16\u7801\u548c\u683c\u5f0f\u3002</li> <li>\u6807\u51c6\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4e0e<code>torch.utils.data.Dataset</code>\u548c\u517c\u5bb9<code>torch.utils.data.DataLoader</code>\u3002</li> <li>\u57fa\u4e8eKinetics-400\u6570\u636e\u96c6\u6784\u5efa\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891(\u5305\u62ec\u8bad\u7ec3\u811a\u672c\uff09\u7684\u52a8\u4f5c\u5206\u7c7b\u3002</li> <li>\u7528\u4e8e\u8bad\u7ec3\u60a8\u81ea\u5df1\u7684\u89c6\u9891\u6a21\u578b\u7684\u53c2\u8003\u8bad\u7ec3\u811a\u672c\u3002</li> </ul> <p>\u6211\u4eec\u5e0c\u671b\u5728PyTorch\u4e2d\u5904\u7406\u89c6\u9891\u6570\u636e\u5c3d\u53ef\u80fd\u7b80\u5355\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u907f\u514d\u4e86\u9700\u8981\u4e8b\u5148\u91cd\u65b0\u7f16\u7801\u89c6\u9891\u7684\u6b65\u9aa4\uff0c\u56e0\u4e3a\u5b83\u4f1a\u6d89\u53ca\uff1a</p> <ul> <li>\u4e00\u4e2a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5b83\u590d\u5236\u6570\u636e\u96c6\u4ee5\u91cd\u65b0\u7f16\u7801\u5b83\u3002</li> <li>\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u5f00\u9500\uff0c\u56e0\u4e3a\u8fd9\u79cd\u91cd\u65b0\u7f16\u7801\u975e\u5e38\u8017\u65f6\u3002</li> <li>\u901a\u5e38\uff0c\u5e94\u4f7f\u7528\u5916\u90e8\u811a\u672c\u6765\u6267\u884c\u91cd\u65b0\u7f16\u7801\u3002</li> </ul> <p>\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u8bf8\u5982\u5b9e\u7528\u7a0b\u5e8f\u7c7b\u4e4b\u7c7b\u7684API\uff0c<code>VideoClips</code>\u901a\u8fc7\u521b\u5efa\u4e00\u7ec4\u89c6\u9891\u4e2d\u6240\u6709\u526a\u8f91\u7684\u7d22\u5f15\uff0c\u7b80\u5316\u4e86\u5728\u89c6\u9891\u6587\u4ef6\u5217\u8868\u4e2d\u679a\u4e3e\u56fa\u5b9a\u5927\u5c0f\u7684\u6240\u6709\u53ef\u80fd\u526a\u8f91\u7684\u4efb\u52a1\u3002\u5b83\u8fd8\u5141\u8bb8\u60a8\u4e3a\u89c6\u9891\u6307\u5b9a\u56fa\u5b9a\u7684\u5e27\u901f\u7387\u3002\u4e0b\u9762\u63d0\u4f9b\u4e86API\u7684\u793a\u4f8b\uff1a</p> <pre><code>from torchvision.datasets.video_utils import VideoClips\n\nclass MyVideoDataset(object):\n    def __init__(self, video_paths):\n        self.video_clips = VideoClips(video_paths,\n                                      clip_length_in_frames=16,\n                                      frames_between_clips=1,\n                                      frame_rate=15)\n\n    def __getitem__(self, idx):\n        video, audio, info, video_idx = self.video_clips.get_clip(idx)\n        return video, audio\n\n    def __len__(self):\n        return self.video_clips.num_clips()\n</code></pre> <p>\u5927\u591a\u6570\u9762\u5411\u7528\u6237\u7684API\u90fd\u5728Python\u4e2d\uff0c\u7c7b\u4f3c\u4e8ePyTorch\uff0c\u8fd9\u4f7f\u5f97\u5b83\u6613\u4e8e\u6269\u5c55\u3002\u6b64\u5916\uff0c\u5e95\u5c42\u5b9e\u73b0\u5f88\u5feb - torchvision\u5c3d\u53ef\u80fd\u5c11\u5730\u4ece\u89c6\u9891\u4e2d\u89e3\u7801\uff0c\u4ee5\u4fbf\u4ece\u89c6\u9891\u4e2d\u8fd4\u56de\u526a\u8f91\u3002</p> <p>\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u67e5\u770b\u6b64\u5904\u7684torchvision 0.4\u00a0\u53d1\u884c\u8bf4\u660e\u3002</p> <p>\u968f\u7740\u6211\u4eec\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u6269\u5c55PyTorch\u6df1\u5ea6\u5b66\u4e60\u5e73\u53f0\uff0c\u6211\u4eec\u671f\u5f85\u7ee7\u7eed\u4e0e\u793e\u533a\u5408\u4f5c\u5e76\u542c\u53d6\u60a8\u7684\u53cd\u9988\u610f\u89c1\u3002</p> <p>\u6211\u4eec\u8981\u611f\u8c22\u6574\u4e2aPyTorch\u56e2\u961f\u548c\u793e\u533a\u5bf9\u8fd9\u9879\u5de5\u4f5c\u7684\u6240\u6709\u8d21\u732e\uff01</p>"},{"location":"LatestChanges/PyTorch_V1.3/","title":"\u65b0\u7248\u672c: PyTorch 1.3 \u6dfb\u52a0 mobile, privacy, quantization \u548c named tensors","text":"<p>\u53d1\u5e03: 2019\u5e7410\u670810\u65e5</p> <p>\u8bd1\u8005\uff1a@\u7247\u523b</p> <p>\u539f\u6587: PyTorch</p> <p>\u7ffb\u8bd1: ApacheCN</p> <p>PyTorch\u7ee7\u7eed\u83b7\u5f97\u52a8\u529b\uff0c\u8fd9\u662f\u56e0\u4e3a\u5176\u4e13\u6ce8\u4e8e\u6ee1\u8db3\u7814\u7a76\u4eba\u5458\u7684\u9700\u6c42\uff0c\u5176\u7b80\u5316\u7684\u751f\u4ea7\u4f7f\u7528\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u4e14\u6700\u91cd\u8981\u7684\u662f\uff0c\u7531\u4e8e\u5b83\u5f97\u5230\u4e86AI\u793e\u533a\u7684\u70ed\u60c5\u652f\u6301\u3002\u6b63\u5982O'Reilly\u6240\u6307\u51fa\u7684\u90a3\u6837\uff0c\u4ec5\u57282019\u5e74\u4e0a\u534a\u5e74\uff0cPyTorch\u5728ArXiv\u4e0a\u7684\u5f15\u7528\u5c31\u589e\u957f\u4e86194\uff05\u00a0\uff0c\u8be5\u5e73\u53f0\u7684\u8d21\u732e\u8005\u6570\u91cf\u5728\u53bb\u5e74\u589e\u957f\u4e8650\uff05\u4ee5\u4e0a\uff0c\u8fbe\u5230\u8fd11200\u4e2a\u3002Facebook\uff0cMicrosoft\uff0cUber\u548c\u5176\u4ed6\u884c\u4e1a\u7684\u7ec4\u7ec7\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u5176\u7528\u4f5c\u6700\u91cd\u8981\u7684\u673a\u5668\u5b66\u4e60(ML\uff09\u7814\u7a76\u548c\u751f\u4ea7\u5de5\u4f5c\u8d1f\u8f7d\u7684\u57fa\u7840\u3002</p> <p>\u6211\u4eec\u73b0\u5728\u901a\u8fc7PyTorch 1.3\u7684\u53d1\u5e03\u8fdb\u4e00\u6b65\u63a8\u8fdb\u8be5\u5e73\u53f0\u7684\u53d1\u5c55\uff0c\u8be5\u7248\u672c\u5305\u62ec\u5bf9\u529f\u80fd\u7684\u5b9e\u9a8c\u6027\u652f\u6301\uff0c\u4f8b\u5982\u65e0\u7f1d\u6a21\u578b\u5230\u79fb\u52a8\u8bbe\u5907\u7684\u90e8\u7f72\uff0c\u6a21\u578b\u91cf\u5316\u4ee5\u5728\u63a8\u7406\u65f6\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u524d\u7aef\u6539\u8fdb(\u4f8b\u5982\u547d\u540d\u5f20\u91cf\uff09\u5e76\u521b\u5efa\u66f4\u6e05\u6670\u7684\u4ee3\u7801\uff0c\u800c\u4e0d\u9700\u8981\u5185\u8054\u6ce8\u91ca\u3002\u6211\u4eec\u8fd8\u5c06\u542f\u52a8\u8bb8\u591a\u5176\u4ed6\u5de5\u5177\u548c\u5e93\uff0c\u4ee5\u652f\u6301\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5c06\u591a\u6a21\u5f0f\u7814\u7a76\u6295\u5165\u751f\u4ea7\u3002</p> <p>\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e0eGoogle\u548cSalesforce\u5408\u4f5c\uff0c\u4e3aCloud Tensor\u5904\u7406\u5355\u5143\u589e\u52a0\u4e86\u5e7f\u6cdb\u7684\u652f\u6301\uff0c\u4e3a\u8bad\u7ec3\u5927\u578b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u663e\u7740\u52a0\u901f\u7684\u9009\u62e9\u3002\u963f\u91cc\u4e91\u8fd8\u52a0\u5165\u4e86Amazon Web Services\uff0cMicrosoft Azure\u548cGoogle Cloud\uff0c\u4e3aPyTorch\u7528\u6237\u63d0\u4f9b\u4e86\u53d7\u652f\u6301\u7684\u4e91\u5e73\u53f0\u3002\u60a8\u73b0\u5728\u53ef\u4ee5\u5728pytorch.org \u4e0a\u5f00\u59cb\u4f7f\u7528\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#pytorch-13","title":"PyTorch 1.3","text":"<p>PyTorch\u76841.3\u7248\u672c\u5e26\u6765\u4e86\u91cd\u8981\u7684\u65b0\u529f\u80fd\uff0c\u5305\u62ec\u5bf9\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72\u7684\u5b9e\u9a8c\u6027\u652f\u6301\uff0c8\u4f4d\u6574\u6570\u7684\u5feb\u901f\u6a21\u5f0f\u91cf\u5316\u4ee5\u53ca\u5f20\u91cf\u547d\u540d\u80fd\u529b\u3002\u901a\u8fc7\u8fd9\u4e9b\u589e\u5f3a\u529f\u80fd\uff0c\u6211\u4eec\u671f\u5f85PyTorch\u793e\u533a\u505a\u51fa\u66f4\u591a\u8d21\u732e\u548c\u6539\u8fdb\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#named-tensors","title":"NAMED TENSORS(\u5b9e\u9a8c\uff09","text":"<p>\u5eb7\u5948\u5c14\u5927\u5b66\u7684Sasha Rush\u8ba4\u4e3a\uff0c\u5c3d\u7ba1\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u4f20\u7edf\u7684\u5f20\u91cf\u5b9e\u73b0\u4ecd\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u4f8b\u5982\u66b4\u9732\u79c1\u6709\u7ef4\u5ea6\uff0c\u57fa\u4e8e\u7edd\u5bf9\u4f4d\u7f6e\u8fdb\u884c\u5e7f\u64ad\u4ee5\u53ca\u5c06\u7c7b\u578b\u4fe1\u606f\u4fdd\u7559\u5728\u6587\u6863\u4e2d\u3002\u4ed6\u63d0\u8bae\u5c06\u5f20\u91cf\u547d\u540d\u4e3a\u66ff\u4ee3\u65b9\u6cd5\u3002</p> <p>\u4eca\u5929\uff0c\u6211\u4eec\u901a\u8fc7\u8bc4\u8bba\u6765\u547d\u540d\u548c\u8bbf\u95ee\u7ef4\u5ea6\uff1a</p> <pre><code># Tensor[N, C, H, W]\nimages = torch.randn(32, 3, 56, 56)\nimages.sum(dim=1)\nimages.select(dim=1, index=0)\n</code></pre> <p>\u4f46\u662f\u547d\u540d\u663e\u5f0f\u5730\u5bfc\u81f4\u4e86\u66f4\u5177\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7684\u4ee3\u7801\uff1a</p> <pre><code>NCHW = ['N', 'C', 'H', 'W']\nimages = torch.randn(32, 3, 56, 56, names=NCHW)\nimages.sum('C')\nimages.select('C', index=0)\n</code></pre>"},{"location":"LatestChanges/PyTorch_V1.3/#quantization","title":"QUANTIZATION(\u5b9e\u9a8c\uff09","text":"<p>\u5f00\u53d1ML\u5e94\u7528\u7a0b\u5e8f\u65f6\uff0c\u6709\u6548\u5229\u7528\u670d\u52a1\u5668\u7aef\u548c\u8bbe\u5907\u4e0a\u7684\u8ba1\u7b97\u8d44\u6e90\u975e\u5e38\u91cd\u8981\u3002\u4e3a\u4e86\u652f\u6301\u5728\u670d\u52a1\u5668\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u66f4\u6709\u6548\u7684\u90e8\u7f72\uff0cPyTorch 1.3\u73b0\u5728\u4f7f\u7528\u719f\u6089\u7684\u6025\u5207\u6a21\u5f0fPython API\u652f\u63018\u4f4d\u6a21\u578b\u91cf\u5316\u3002\u91cf\u5316\u662f\u6307\u7528\u4e8e\u4ee5\u964d\u4f4e\u7684\u7cbe\u5ea6\u6267\u884c\u8ba1\u7b97\u548c\u5b58\u50a8\u7684\u6280\u672f\uff0c\u4f8b\u59828\u4f4d\u6574\u6570\u3002\u5f53\u524d\u5904\u4e8e\u5b9e\u9a8c\u72b6\u6001\u7684\u529f\u80fd\u5305\u62ec\u5bf9\u8bad\u7ec3\u540e\u91cf\u5316\uff0c\u52a8\u6001\u91cf\u5316\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7684\u652f\u6301\u3002\u5b83\u5206\u522b\u9488\u5bf9x86\u548cARM CPU\u00a0\u5229\u7528FBGEMM\u548cQNNPACK\u6700\u65b0\u7684\u91cf\u5316\u5185\u6838\u540e\u7aef\uff0c\u8fd9\u4e9b\u540e\u7aef\u4e0ePyTorch\u96c6\u6210\u5728\u4e00\u8d77\uff0c\u5e76\u4e14\u73b0\u5728\u5171\u4eab\u4e00\u4e2a\u901a\u7528API\u3002</p> <p>\u8981\u4e86\u89e3\u6709\u5173\u8bbe\u8ba1\u548c\u67b6\u6784\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u5728\u6b64\u5904\u67e5\u770bAPI\u6587\u6863\uff0c\u5e76\u4f7f\u7528\u6b64\u5904\u63d0\u4f9b\u7684\u6559\u7a0b\u5f00\u59cb\u4f7f\u7528\u4efb\u4f55\u53d7\u652f\u6301\u7684\u6280\u672f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#pytorch-mobile","title":"PYTORCH MOBILE(\u5b9e\u9a8c\u6027\uff09","text":"<p>\u968f\u7740\u5e94\u7528\u7a0b\u5e8f\u7ee7\u7eed\u8981\u6c42\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884cML\u7684\u91cd\u8981\u6027\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5b83\u4e5f\u662f\u8bf8\u5982\u8054\u5408\u5b66\u4e60\u4e4b\u7c7b\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u57fa\u7840\u8981\u7d20\u3002\u4e3a\u4e86\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bbe\u5907\u4e0aML\uff0cPyTorch 1.3\u73b0\u5728\u652f\u6301\u4ecePython\u5230\u5728iOS\u548cAndroid\u4e0a\u90e8\u7f72\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u3002</p> <p>\u8fd9\u662f\u4e00\u4e2a\u65e9\u671f\u7684\u5b9e\u9a8c\u7248\u672c\uff0c\u9488\u5bf9\u7aef\u5230\u7aef\u5f00\u53d1\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u5373\u5c06\u53d1\u5e03\u7684\u7248\u672c\u5c06\u4fa7\u91cd\u4e8e\uff1a</p> <ul> <li>\u5927\u5c0f\u4f18\u5316\uff1a\u6839\u636e\u7528\u6237\u5e94\u7528\u7a0b\u5e8f\u6240\u9700\u7684\u8fd0\u7b97\u7b26\uff0c\u6784\u5efa\u7ea7\u522b\u7684\u4f18\u5316\u548c\u9009\u62e9\u6027\u7f16\u8bd1(\u5373\uff0c\u4ec5\u4e3a\u6240\u9700\u7684\u8fd0\u7b97\u7b26\u652f\u4ed8\u4e8c\u8fdb\u5236\u5927\u5c0f\uff09</li> <li>\u6027\u80fd\uff1a\u8fdb\u4e00\u6b65\u6539\u5584\u4e86\u79fb\u52a8CPU\u548cGPU\u7684\u6027\u80fd\u548c\u8986\u76d6\u8303\u56f4</li> <li>\u9ad8\u7ea7API\uff1a\u6269\u5c55\u79fb\u52a8\u672c\u673aAPI\uff0c\u4ee5\u6db5\u76d6\u5c06ML\u96c6\u6210\u5230\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6240\u9700\u7684\u5e38\u89c4\u9884\u5904\u7406\u548c\u96c6\u6210\u4efb\u52a1\u3002\u4f8b\u5982\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406</li> </ul> <p>\u5728\u6b64\u5904\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u6216\u5f00\u59cb\u4f7f\u7528Android\u6216iOS\u00a0\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#_1","title":"\u7528\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u9690\u79c1\u6027\u7684\u65b0\u5de5\u5177","text":""},{"location":"LatestChanges/PyTorch_V1.3/#_2","title":"\u8d44\u672c","text":"<p>\u968f\u7740\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5f00\u53d1\u7528\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u79cd\u9700\u6c42\uff0c\u6211\u4eec\u6b63\u5728\u542f\u52a8Captum\uff0c\u8be5\u5de5\u5177\u53ef\u5e2e\u52a9\u5728PyTorch\u4e2d\u5de5\u4f5c\u7684\u5f00\u53d1\u4eba\u5458\u4e86\u89e3\u4e3a\u4ec0\u4e48\u4ed6\u4eec\u7684\u6a21\u578b\u751f\u6210\u7279\u5b9a\u8f93\u51fa\u3002Captum\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u5de5\u5177\u6765\u4e86\u89e3\u7279\u5b9a\u795e\u7ecf\u5143\u548c\u5c42\u7684\u91cd\u8981\u6027\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u505a\u51fa\u7684\u9884\u6d4b\u3002Captum\u7684\u7b97\u6cd5\u5305\u62ec\u79ef\u5206\u68af\u5ea6\uff0c\u7535\u5bfc\uff0cSmoothGrad\u548cVarGrad\u4ee5\u53caDeepLift\u3002</p> <p>\u4e0b\u4f8b\u663e\u793a\u4e86\u5982\u4f55\u5728\u9884\u5148\u8bad\u7ec3\u7684ResNet\u6a21\u578b\u4e0a\u5e94\u7528\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7b97\u6cd5\uff0c\u7136\u540e\u901a\u8fc7\u5c06\u6bcf\u4e2a\u50cf\u7d20\u7684\u5c5e\u6027\u53e0\u52a0\u5728\u56fe\u50cf\u4e0a\u6765\u4f7f\u5176\u53ef\u89c6\u5316\u3002</p> <pre><code>noise_tunnel = NoiseTunnel(integrated_gradients)\n\nattributions_ig_nt, delta = noise_tunnel.attribute(input, n_samples=10, nt_type='smoothgrad_sq', target=pred_label_idx)\n_ = viz.visualize_image_attr_multiple([\"original_image\", \"heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      cmap=default_cmap,\n                                      show_colorbar=True)\n\n</code></pre> <p></p> <p></p> <p>\u5728captum.ai\u4e0a\u4e86\u89e3\u6709\u5173Captum\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#crypten","title":"CRYPTEN","text":"<p>\u901a\u8fc7\u57fa\u4e8e\u4e91\u6216\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1(MLaaS\uff09\u5e73\u53f0\u7684ML\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5b89\u5168\u548c\u9690\u79c1\u6311\u6218\u3002\u7279\u522b\u662f\uff0c\u8fd9\u4e9b\u5e73\u53f0\u7684\u7528\u6237\u53ef\u80fd\u4e0d\u5e0c\u671b\u6216\u65e0\u6cd5\u5171\u4eab\u672a\u52a0\u5bc6\u7684\u6570\u636e\uff0c\u8fd9\u4f7f\u4ed6\u4eec\u65e0\u6cd5\u5145\u5206\u5229\u7528ML\u5de5\u5177\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u673a\u5668\u5b66\u4e60\u793e\u533a\u6b63\u5728\u63a2\u7d22\u5404\u79cd\u6210\u719f\u5ea6\u4e0d\u540c\u7684\u6280\u672f\u65b9\u6cd5\u3002\u8fd9\u4e9b\u5305\u62ec\u540c\u6001\u52a0\u5bc6\uff0c\u5b89\u5168\u7684\u591a\u65b9\u8ba1\u7b97\uff0c\u53d7\u4fe1\u4efb\u7684\u6267\u884c\u73af\u5883\uff0c\u8bbe\u5907\u4e0a\u8ba1\u7b97\u548c\u5dee\u5f02\u9690\u79c1\u3002</p> <p>\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u5982\u4f55\u5e94\u7528\u5176\u4e2d\u7684\u67d0\u4e9b\u6280\u672f\uff0c\u6211\u4eec\u53d1\u5e03\u4e86CrypTen\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u793e\u533a\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u7528\u4e8e\u63a8\u52a8\u9690\u79c1\u4fdd\u62a4ML\u9886\u57df\u7684\u53d1\u5c55\u3002\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173CrypTen\u7684\u66f4\u591a\u4fe1\u606f\u3002\u5b83\u53ef\u4ee5\u5728GitHub\u4e0a\u8fd9\u91cc\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#ai","title":"\u591a\u6a21\u5f0fAI\u7cfb\u7edf\u7684\u5de5\u5177","text":"<p>\u6570\u5b57\u5185\u5bb9\u901a\u5e38\u7531\u51e0\u79cd\u5f62\u5f0f\u7ec4\u6210\uff0c\u4f8b\u5982\u6587\u672c\uff0c\u56fe\u50cf\uff0c\u97f3\u9891\u548c\u89c6\u9891\u3002\u4f8b\u5982\uff0c\u4e00\u4e2a\u516c\u5171\u5e16\u5b50\u53ef\u80fd\u5305\u542b\u56fe\u50cf\uff0c\u6b63\u6587\uff0c\u6807\u9898\uff0c\u89c6\u9891\u548c\u767b\u5f55\u9875\u9762\u3002\u751a\u81f3\u4e00\u4e2a\u7279\u5b9a\u7684\u7ec4\u4ef6\u4e5f\u53ef\u80fd\u5177\u6709\u4e0d\u6b62\u4e00\u79cd\u5f62\u5f0f\uff0c\u4f8b\u5982\u65e2\u5305\u542b\u89c6\u89c9\u4fe1\u53f7\u53c8\u5305\u542b\u97f3\u9891\u4fe1\u53f7\u7684\u89c6\u9891\uff0c\u6216\u8005\u5305\u542b\u56fe\u50cf\uff0c\u6587\u672c\u548cHTML\u6e90\u7684\u767b\u5f55\u9875\u9762\u3002</p> <p>\u4e0ePyTorch\u914d\u5408\u4f7f\u7528\u7684\u5de5\u5177\u548c\u5e93\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u6784\u5efa\u591a\u6a21\u5f0fML\u7cfb\u7edf\u7684\u6311\u6218\u3002\u4ee5\u4e0b\u662f\u4eca\u5929\u542f\u52a8\u7684\u4e00\u4e9b\u6700\u65b0\u5e93\uff1a</p>"},{"location":"LatestChanges/PyTorch_V1.3/#detectron2","title":"DETECTRON2","text":"<p>\u5bf9\u8c61\u68c0\u6d4b\u548c\u5206\u5272\u7528\u4e8e\u4ece\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5230\u5185\u5bb9\u7406\u89e3(\u5e73\u53f0\u5b8c\u6574\u6027\uff09\u7b49\u4efb\u52a1\u3002\u4e3a\u4e86\u63a8\u8fdb\u8fd9\u9879\u5de5\u4f5c\uff0cFacebook AI Research(FAIR\uff09\u53d1\u5e03\u4e86Detectron2\uff0c\u8fd9\u662f\u4e00\u79cd\u5728PyTorch\u4e2d\u5b9e\u73b0\u7684\u5bf9\u8c61\u68c0\u6d4b\u5e93\u3002Detectron2\u63d0\u4f9b\u5bf9\u6700\u65b0\u6a21\u578b\u548c\u4efb\u52a1\u7684\u652f\u6301\uff0c\u589e\u5f3a\u7684\u7075\u6d3b\u6027\u4ee5\u5e2e\u52a9\u8fdb\u884c\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\uff0c\u5e76\u6539\u5584\u4e86\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u4f38\u7f29\u6027\u4ee5\u652f\u6301\u751f\u4ea7\u7528\u4f8b\u3002</p> <p>Detectron2\u00a0\u5728\u8fd9\u91cc\u53ef\u7528\uff0c\u60a8\u53ef\u4ee5\u5728\u8fd9\u91cc\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#fairseq","title":"\u8bed\u97f3\u6269\u5c55\u5230 FAIRSEQ","text":"<p>\u8bed\u8a00\u7ffb\u8bd1\u548c\u97f3\u9891\u5904\u7406\u662f\u7cfb\u7edf\u548c\u5e94\u7528\u7a0b\u5e8f(\u4f8b\u5982\u641c\u7d22\uff0c\u7ffb\u8bd1\uff0c\u8bed\u97f3\u548c\u52a9\u624b\uff09\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6\u3002\u7531\u4e8e\u53d8\u538b\u5668\u7b49\u65b0\u67b6\u6784\u7684\u53d1\u5c55\u4ee5\u53ca\u5927\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u6700\u8fd1\u5728\u8fd9\u4e9b\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u7684\u8fdb\u6b65\u3002\u6211\u4eec\u5df2\u7ecf\u6269\u5c55\u4e86Fairseq(\u8bed\u8a00\u7ffb\u8bd1\u7b49\u5e8f\u5217\u5230\u5e8f\u5217\u5e94\u7528\u7a0b\u5e8f\u7684\u6846\u67b6\uff09\uff0c\u4ee5\u5305\u62ec\u5bf9\u8bed\u97f3\u548c\u97f3\u9891\u8bc6\u522b\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u652f\u6301.fairseq\u7684\u8fd9\u4e9b\u6269\u5c55\u53ef\u4ee5\u52a0\u5feb\u5bf9\u65b0\u8bed\u97f3\u7814\u7a76\u7684\u63a2\u7d22\u548c\u539f\u578b\u5f00\u53d1\u3002\u63d0\u4f9b\u60f3\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u6e05\u6670\u7684\u751f\u4ea7\u8def\u5f84\u3002</p> <p>\u5728\u6b64\u5904\u5f00\u59cb\u4f7f\u7528fairseq\u00a0\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.3/#_3","title":"\u4e91\u63d0\u4f9b\u5546\u548c\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u652f\u6301","text":"<p>\u8bf8\u5982Amazon Web Services\uff0cMicrosoft Azure\u548cGoogle Cloud\u4e4b\u7c7b\u7684\u4e91\u63d0\u4f9b\u5546\u4e3a\u5e0c\u671b\u5728PyTorch\u4e0a\u5f00\u53d1ML\u5e76\u5728\u751f\u4ea7\u4e2d\u8fdb\u884c\u90e8\u7f72\u7684\u4efb\u4f55\u4eba\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u652f\u6301\u3002\u6211\u4eec\u5f88\u9ad8\u5174\u5206\u4eabGoogle Cloud TPU\u652f\u6301\u7684\u5168\u9762\u53ef\u7528\u6027\u4ee5\u53ca\u4e0e\u963f\u91cc\u4e91\u7684\u65b0\u63a8\u51fa\u7684\u96c6\u6210\u3002\u6211\u4eec\u8fd8\u5c06\u6269\u5c55\u5bf9\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u652f\u6301\u3002</p> <ul> <li>Google Cloud TPU\u652f\u6301\u73b0\u5df2\u5e7f\u6cdb\u53ef\u7528\u3002\u4e3a\u4e86\u52a0\u901f\u5f53\u4eca\u90e8\u7f72\u7684\u6700\u5927\u89c4\u6a21\u7684\u673a\u5668\u5b66\u4e60(ML\uff09\u5e94\u7528\u5e76\u5b9e\u73b0\u660e\u5929\u7684ML\u5e94\u7528\u7684\u5feb\u901f\u53d1\u5c55\uff0cGoogle\u521b\u5efa\u4e86\u79f0\u4e3aTensor Processing Units(TPU\uff09\u7684\u5b9a\u5236\u7845\u82af\u7247\u3002\u5c06\u8fd9\u4e9bTPU\u00a0\u7ec4\u88c5\u5230\u79f0\u4e3aCloud TPU Pods\u7684\u591a\u673a\u67b6ML\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e2d\u540e\uff0c\u5b83\u4eec\u53ef\u4ee5\u5728\u51e0\u5206\u949f\u6216\u51e0\u5c0f\u65f6\u5185\u5b8c\u6210ML\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800c\u4ee5\u524d\u5728\u5176\u4ed6\u7cfb\u7edf\u4e0a\u8981\u82b1\u8d39\u51e0\u5929\u6216\u51e0\u5468\u3002\u6765\u81eaFacebook\uff0cGoogle\u548cSalesforce\u7684\u5de5\u7a0b\u5e08\u5171\u540c\u52aa\u529b\uff0c\u5728PyTorch\u4e2d\u542f\u7528\u5e76\u8bd5\u7528\u4e86Cloud TPU\u652f\u6301\uff0c\u5305\u62ec\u5bf9Cloud TPU Pods\u7684\u5b9e\u9a8c\u6027\u652f\u6301\u3002Colab\u8fd8\u63d0\u4f9b\u4e86\u5bf9Cloud TPU\u7684PyTorch\u652f\u6301\u3002\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173\u5982\u4f55\u5f00\u59cb\u4f7f\u7528PyTorch on Cloud TPU\u7684\u66f4\u591a\u4fe1\u606f\u3002</li> <li>\u963f\u91cc\u5df4\u5df4\u5728\u963f\u91cc\u4e91\u4e2d\u6dfb\u52a0\u4e86\u5bf9PyTorch\u7684\u652f\u6301\u3002\u6700\u521d\u7684\u96c6\u6210\u6d89\u53caPyTorch 1.x\u7684\u4e00\u952e\u5f0f\u89e3\u51b3\u65b9\u6848\uff0cData Science Workshop\u7b14\u8bb0\u672c\u670d\u52a1\uff0c\u4f7f\u7528Gloo / NCCL\u8fdb\u884c\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4ee5\u53ca\u4e0e\u963f\u91cc\u5df4\u5df4IaaS(\u4f8b\u5982OSS\uff0cODPS\u548cNAS\uff09\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u6211\u4eec\u671f\u5f85\u4e0e\u963f\u91cc\u5df4\u5df4\u63d0\u4f9b\u7684\u5de5\u5177\u94fe\u4e00\u8d77\uff0c\u5927\u5e45\u964d\u4f4e\u91c7\u7528\u8be5\u7cfb\u7edf\u6240\u9700\u7684\u5f00\u9500\uff0c\u5e76\u5e2e\u52a9\u963f\u91cc\u4e91\u7684\u5168\u7403\u5ba2\u6237\u7fa4\u5229\u7528PyTorch\u5f00\u53d1\u65b0\u7684AI\u5e94\u7528\u7a0b\u5e8f\u3002</li> <li>ML\u786c\u4ef6\u751f\u6001\u7cfb\u7edf\u5f97\u4ee5\u6269\u5c55\u3002\u9664\u4e86\u4e3b\u8981\u7684GPU\u548cCPU\u5408\u4f5c\u4f19\u4f34\u4e4b\u5916\uff0cPyTorch\u751f\u6001\u7cfb\u7edf\u8fd8\u652f\u200b\u200b\u6301\u4e13\u7528\u7684ML\u52a0\u901f\u5668\u3002\u82f1\u7279\u5c14\u548cHabana\u7684\u66f4\u65b0\u5c55\u793a\u4e86PyTorch\u5982\u4f55\u8fde\u63a5\u5230Glow\u4f18\u5316\u7f16\u8bd1\u5668\uff0c\u4ece\u800c\u4f7f\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u5229\u7528\u8fd9\u4e9b\u9488\u5bf9\u7279\u5b9a\u5e02\u573a\u7684\u89e3\u51b3\u65b9\u6848\u3002</li> </ul>"},{"location":"LatestChanges/PyTorch_V1.3/#pytorch","title":"PyTorch\u793e\u533a\u7684\u6210\u957f","text":"<p>\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u7684\uff0c\u793e\u533a\u9a71\u52a8\u7684\u9879\u76ee\uff0cPyTorch\u53d7\u76ca\u4e8e\u4e3a\u751f\u6001\u7cfb\u7edf\u5e26\u6765\u65b0\u529f\u80fd\u7684\u4f17\u591a\u8d21\u732e\u8005\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u6700\u8fd1\u7684\u793a\u4f8b\uff1a</p> <ul> <li>Mila SpeechBrain\u65e8\u5728\u63d0\u4f9b\u57fa\u4e8ePyTorch\u7684\u5f00\u6e90\uff0c\u591a\u5408\u4e00\u8bed\u97f3\u5de5\u5177\u5305\u3002\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u4e2a\u5355\u4e00\u7684\uff0c\u7075\u6d3b\u7684\uff0c\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\u5305\uff0c\u8be5\u5de5\u5177\u5305\u53ef\u7528\u4e8e\u8f7b\u677e\u5f00\u53d1\u8bed\u97f3\u8bc6\u522b(\u7aef\u5230\u7aef\u548cHMM-DNN\uff09\uff0c\u8bf4\u8bdd\u8005\u8bc6\u522b\uff0c\u8bed\u97f3\u5206\u79bb\uff0c\u591a\u8bed\u8a00\u529f\u80fd\u7684\u6700\u65b0\u7cfb\u7edf-\u9ea6\u514b\u98ce\u4fe1\u53f7\u5904\u7406(\u4f8b\u5982\uff0c\u6ce2\u675f\u6210\u5f62\uff09\uff0c\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u4ee5\u53ca\u8bb8\u591a\u5176\u4ed6\u529f\u80fd\u3002\u4e86\u89e3\u66f4\u591a</li> <li>SpaCy\u662f\u4e00\u4e2a\u65b0\u7684\u5305\u88c5\u5e93\uff0c\u5177\u6709\u5bf9\u591a\u4e2a\u6a21\u578b\u7684\u4e00\u81f4\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u754c\u9762\uff0c\u4ee5\u4fbf\u63d0\u53d6\u529f\u80fd\u4ee5\u652f\u6301NLP\u7ba1\u9053\u3002\u901a\u8fc7spaCy\u7684\u6807\u51c6\u8bad\u7ec3API\u63d0\u4f9b\u652f\u6301\u3002\u8be5\u5e93\u8fd8\u8ba1\u7b97\u5bf9\u9f50\u65b9\u5f0f\uff0c\u4ee5\u4fbf\u53ef\u4ee5\u5c06\u53d8\u538b\u5668\u529f\u80fd\u90e8\u4ef6\u4e0e\u5b9e\u9645\u5355\u8bcd\u76f8\u5173\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5355\u8bcd\u3002\u4e86\u89e3\u66f4\u591a</li> <li>HuggingFace PyTorch-Transformers(\u4ee5\u524d\u79f0\u4e3apytorch-pretrained-bert\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP\uff09\u7684\u6700\u65b0\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e93\u3002\u8be5\u5e93\u5f53\u524d\u5305\u542bPyTorch\u5b9e\u73b0\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\uff0c\u7528\u6cd5\u811a\u672c\u548c\u8f6c\u6362\u5b9e\u7528\u7a0b\u5e8f\u8f66\u578b\u5982BERT\uff0cGPT-2\uff0c\u7f57\u4f2f\u5854\u548cDistilBERT\uff0c\u5b83\u4e5f\u5728\u589e\u957f\u8fc5\u901f\uff0c\u62e5\u6709\u8d85\u8fc713,000 GitHub\u7684\u661f\u7ea7\u548c\u5e7f\u6cdb\u7684\u7528\u6237\u3002\u4e86\u89e3\u66f4\u591a</li> <li>PyTorch Lightning\u662fPyTorch\u7684\u7c7b\u4f3cKeras\u7684ML\u5e93\u3002\u5b83\u5c06\u6838\u5fc3\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\u903b\u8f91\u7559\u7ed9\u60a8\uff0c\u5e76\u81ea\u52a8\u5b8c\u6210\u5176\u4f59\u7684\u5de5\u4f5c\u3002\u53ef\u91cd\u590d\u6027\u662f\u8bb8\u591a\u7814\u7a76\u9886\u57df(\u5305\u62ec\u57fa\u4e8eML\u6280\u672f\u7684\u9886\u57df\uff09\u7684\u5173\u952e\u8981\u6c42\u3002\u968f\u7740\u63d0\u4ea4\u7ed9arXiv\u548c\u4f1a\u8bae\u7684\u7814\u7a76\u8bba\u6587\u6570\u91cf\u6fc0\u589e\u81f3\u6570\u4ee5\u4e07\u8ba1\uff0c\u7f29\u653e\u53ef\u91cd\u590d\u6027\u53d8\u5f97\u56f0\u96be\u3002\u4e86\u89e3\u66f4\u591a\u3002</li> </ul> <p>\u6211\u4eec\u6700\u8fd1\u4e3e\u884c\u4e86\u9996\u6b21\u5728\u7ebf\u5168\u7403PyTorch\u590f\u5b63\u9ed1\u5ba2\u9a6c\u62c9\u677e\uff0c\u9080\u8bf7\u4e86\u4e16\u754c\u5404\u5730\u7684\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u4eba\u5458\u4e0ePyTorch\u5efa\u7acb\u521b\u65b0\u7684\u65b0\u9879\u76ee\u3002\u8fd11500\u540d\u5f00\u53d1\u4eba\u5458\u53c2\u52a0\u4e86\u8be5\u9879\u76ee\uff0c\u63d0\u4ea4\u4e86\u4ece\u7272\u755c\u75be\u75c5\u68c0\u6d4b\u5230\u4ee5AI\u4e3a\u52a8\u529b\u7684\u8d22\u52a1\u52a9\u624b\u7b49\u9879\u76ee\u3002\u83b7\u5956\u9879\u76ee\u662f\uff1a</p> <ul> <li>Torchmeta\uff0c\u5b83\u63d0\u4f9b\u4e86PyTorch\u7684\u6269\u5c55\uff0c\u4ee5\u7b80\u5316PyTorch\u4e2d\u5143\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\u3002\u5b83\u5177\u6709\u53d7TorchVision\u542f\u53d1\u7684\u7edf\u4e00\u754c\u9762\uff0c\u53ef\u89e3\u51b3\u5c11\u6570\u955c\u5934\u5206\u7c7b\u548c\u56de\u5f52\u95ee\u9898\uff0c\u4ece\u800c\u53ef\u4ee5\u8f7b\u677e\u5730\u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u63d0\u9ad8\u518d\u73b0\u6027\u3002</li> <li>Open-Unmix\uff0c\u4f7f\u7528PyTorch\u8fdb\u884c\u7aef\u5230\u7aef\u97f3\u4e50\u6df7\u5408\u7684\u7cfb\u7edf\u3002\u6df7\u5408\u5c06\u5355\u4e2a\u4e50\u5668\u6216\u4eba\u58f0\u8f68\u4e0e\u4efb\u4f55\u7acb\u4f53\u58f0\u5f55\u97f3\u5206\u5f00\u3002</li> <li>Endless AI\u4ea7\u751f\u7684Tees\uff0c\u8fd9\u5bb6\u5546\u5e97\u91c7\u7528AI\u4ea7\u751f\u7684T\u6064\u8bbe\u8ba1\uff0c\u53ef\u5728\u5168\u7403\u8303\u56f4\u5185\u8d2d\u4e70\u548c\u4ea4\u4ed8\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b(StyleGAN\uff09\uff0c\u8be5\u6a21\u578b\u7531PyTorch\u6784\u5efa\uff0c\u7136\u540e\u63a5\u53d7\u73b0\u4ee3\u827a\u672f\u8bad\u7ec3\u3002</li> </ul> <p>\u8bbf\u95eepytorch.org\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u5f00\u59cb\u4f7f\u7528PyTorch 1.3\u4ee5\u53ca\u6700\u65b0\u7684\u5e93\u548c\u751f\u6001\u7cfb\u7edf\u9879\u76ee\u3002\u6211\u4eec\u671f\u5f85\u793e\u533a\u901a\u8fc7PyTorch\u6784\u5efa\u7684\u8d21\u732e\uff0c\u4ee4\u4eba\u5174\u594b\u7684\u7814\u7a76\u8fdb\u5c55\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002</p> <p>\u6211\u4eec\u8981\u611f\u8c22\u6574\u4e2aPyTorch\u56e2\u961f\u548c\u793e\u533a\u4e3a\u8fd9\u9879\u5de5\u4f5c\u505a\u51fa\u7684\u6240\u6709\u8d21\u732e\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/","title":"\u65b0\u7248\u672c: PyTorch 1.4\u53d1\u5e03\uff0c\u57df\u5e93\u5df2\u66f4\u65b0","text":"<p>\u53d1\u5e03: 2020\u5e7401\u670815\u65e5</p> <p>\u8bd1\u8005\uff1a@\u7247\u523b</p> <p>\u539f\u6587: PyTorch</p> <p>\u7ffb\u8bd1: ApacheCN</p> <p>\u901a\u8fc7PyTorch\u56e2\u961f</p> <p>\u4eca\u5929\uff0c\u6211\u4eec\u5ba3\u5e03PyTorch 1.4\u7684\u53ef\u7528\u6027\u4ee5\u53caPyTorch\u57df\u5e93\u7684\u66f4\u65b0\u3002\u8fd9\u4e9b\u7248\u672c\u4ee5NeurIPS 2019\u7684\u516c\u544a\u4e3a\u57fa\u7840\uff0c\u5728\u6b64\u6211\u4eec\u5171\u4eab\u4e86PyTorch Elastic\u7684\u53ef\u7528\u6027\uff0c\u65b0\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5206\u7c7b\u6846\u67b6\u4ee5\u53caPyTorch\u793e\u533a\u4e2d\u6dfb\u52a0\u4e86Preferred Networks\u3002\u5bf9\u4e8e\u53c2\u52a0NeurIPS\u7814\u8ba8\u4f1a\u7684\u4eba\u5458\uff0c\u8bf7\u5728\u6b64\u5904\u627e\u5230\u5185\u5bb9\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#pytorch-14_1","title":"PYTORCH 1.4","text":"<p>PyTorch 1.4\u7248\u672c\u589e\u52a0\u4e86\u65b0\u529f\u80fd\uff0c\u5305\u62ec\u4e3aPyTorch Mobile\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6784\u5efa\u7ea7\u522b\u81ea\u5b9a\u4e49\u7684\u529f\u80fd\uff0c\u4ee5\u53ca\u65b0\u7684\u5b9e\u9a8c\u6027\u529f\u80fd\uff0c\u5305\u62ec\u5bf9\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u548cJava\u8bed\u8a00\u7ed1\u5b9a\u7684\u652f\u6301\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#pytorch-mobile-","title":"PyTorch Mobile-\u6784\u5efa\u7ea7\u522b\u81ea\u5b9a\u4e49","text":"<p>\u57281.3\u7248\u672c\u7684PyTorch Mobile\u5f00\u6e90\u4e4b\u540e\uff0cPyTorch 1.4\u6dfb\u52a0\u4e86\u66f4\u591a\u7684\u79fb\u52a8\u652f\u6301\uff0c\u5305\u62ec\u4ee5\u7ec6\u7c92\u5ea6\u7ea7\u522b\u81ea\u5b9a\u4e49\u6784\u5efa\u811a\u672c\u7684\u529f\u80fd\u3002\u8fd9\u4f7f\u79fb\u52a8\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u901a\u8fc7\u4ec5\u5305\u62ec\u5176\u6a21\u578b\u6240\u4f7f\u7528\u7684\u8fd0\u7b97\u7b26\u6765\u4f18\u5316\u5e93\u7684\u5927\u5c0f\uff0c\u5e76\u5728\u6b64\u8fc7\u7a0b\u4e2d\u663e\u7740\u51cf\u5c11\u5176\u8bbe\u5907\u5360\u7528\u7684\u7a7a\u95f4\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u4f8b\u5982\uff0c\u5b9a\u5236\u7684MobileNetV2\u6bd4\u9884\u6784\u5efa\u7684PyTorch\u79fb\u52a8\u5e93\u5c0f40\uff05\u81f350\uff05\u3002\u60a8\u53ef\u4ee5\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173\u5982\u4f55\u521b\u5efa\u81ea\u5df1\u7684\u81ea\u5b9a\u4e49\u7248\u672c\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u5e76\u4e14\u4e0e\u5f80\u5e38\u4e00\u6837\uff0c\u8bf7\u5728PyTorch\u8bba\u575b\u4e0a\u4e0e\u793e\u533a\u4e92\u52a8\uff0c\u4ee5\u63d0\u4f9b\u60a8\u7684\u4efb\u4f55\u53cd\u9988\u3002</p> <p>\u7528\u4e8e\u9009\u62e9\u6027\u5730\u4ec5\u7f16\u8bd1MobileNetV2\u6240\u9700\u7684\u8fd0\u7b97\u7b26\u7684\u793a\u4f8b\u4ee3\u7801\u6bb5\uff1a</p> <pre><code># Dump list of operators used by MobileNetV2:\nimport torch, yaml\nmodel = torch.jit.load('MobileNetV2.pt')\nops = torch.jit.export_opnames(model)\nwith open('MobileNetV2.yaml', 'w') as output:\n    yaml.dump(ops, output)\n\n</code></pre> <pre><code># Build PyTorch Android library customized for MobileNetV2:\nSELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh arm64-v8a\n\n# Build PyTorch iOS library customized for MobileNetV2:\nSELECTED_OP_LIST=MobileNetV2.yaml BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 scripts/build_ios.sh\n\n</code></pre>"},{"location":"LatestChanges/PyTorch_V1.4/#_1","title":"\u5206\u5e03\u5f0f\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\uff08\u5b9e\u9a8c\u6027\uff09","text":"<p>\u968f\u7740\u6a21\u578b\u7684\u89c4\u6a21\uff08\u4f8b\u5982RoBERTa\uff09\u4e0d\u65ad\u589e\u52a0\u5230\u6570\u5341\u4ebf\u4e2a\u53c2\u6570\uff0c\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u5bf9\u4e8e\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7a81\u7834\u6781\u9650\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u6b64\u7248\u672c\u63d0\u4f9b\u4e86\u5206\u5e03\u5f0fRPC\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u5206\u5e03\u5f0f\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u3002\u5b83\u5141\u8bb8\u8fdc\u7a0b\u8fd0\u884c\u529f\u80fd\u548c\u5f15\u7528\u8fdc\u7a0b\u5bf9\u8c61\uff0c\u800c\u65e0\u9700\u590d\u5236\u5b9e\u9645\u6570\u636e\uff0c\u5e76\u63d0\u4f9bautograd\u548cOptimizer API\u4ee5\u900f\u660e\u5730\u5411\u540e\u8fd0\u884c\u5e76\u8de8RPC\u8fb9\u754c\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u8981\u4e86\u89e3\u6709\u5173API\u548c\u6b64\u529f\u80fd\u8bbe\u8ba1\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4ee5\u4e0b\u94fe\u63a5\uff1a</p> <ul> <li>API\u6587\u6863</li> <li>\u5206\u5e03\u5f0fAutograd\u8bbe\u8ba1\u6587\u6863</li> <li>\u8fdc\u7a0b\u53c2\u8003\u8bbe\u8ba1\u6587\u6863</li> </ul> <p>\u6709\u5173\u5b8c\u6574\u7684\u6559\u7a0b\uff0c\u8bf7\u53c2\u89c1\u4e0b\u9762\u7684\u94fe\u63a5\uff1a</p> <ul> <li>\u5b8c\u6574\u7684RPC\u6559\u7a0b</li> <li>\u4f7f\u7528\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5e76\u4f7f\u7528LSTM\u7684\u793a\u4f8b</li> </ul> <p>\u4e0e\u5f80\u5e38\u4e00\u6837\uff0c\u60a8\u53ef\u4ee5\u4e0e\u793e\u533a\u6210\u5458\u8054\u7cfb\u5e76\u5728\u8bba\u575b\u4e0a\u8fdb\u884c\u66f4\u591a\u8ba8\u8bba\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#java","title":"Java\u7ed1\u5b9a\uff08\u5b9e\u9a8c\u6027\uff09","text":"<p>\u9664\u4e86\u652f\u6301Python\u548cC ++\uff0c\u6b64\u7248\u672c\u8fd8\u589e\u52a0\u4e86\u5bf9Java\u7ed1\u5b9a\u7684\u5b9e\u9a8c\u6027\u652f\u6301\u3002\u57fa\u4e8ePyTorch Mobile\u4e2d\u4e3aAndroid\u5f00\u53d1\u7684\u754c\u9762\uff0c\u65b0\u7684\u7ed1\u5b9a\u4f7f\u60a8\u53ef\u4ee5\u4ece\u4efb\u4f55Java\u7a0b\u5e8f\u8c03\u7528TorchScript\u6a21\u578b\u3002\u8bf7\u6ce8\u610f\uff0cJava\u7ed1\u5b9a\u4ec5\u53ef\u7528\u4e8e\u6b64\u7248\u672c\u7684Linux\uff0c\u5e76\u4e14\u4ec5\u7528\u4e8e\u63a8\u65ad\u3002\u6211\u4eec\u5e0c\u671b\u5728\u4ee5\u540e\u7684\u7248\u672c\u4e2d\u6269\u5c55\u652f\u6301\u3002\u6709\u5173\u5982\u4f55\u5728Java\u4e2d\u4f7f\u7528PyTorch\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u4e0b\u9762\u7684\u4ee3\u7801\u7247\u6bb5\uff1a</p> <pre><code>Module mod = Module.load(\"demo-model.pt1\");\nTensor data =\n    Tensor.fromBlob(\n        new int[] {1, 2, 3, 4, 5, 6}, // data\n        new long[] {2, 3} // shape\n        );\nIValue result = mod.forward(IValue.from(data), IValue.from(3.0));\nTensor output = result.toTensor();\nSystem.out.println(\"shape: \" + Arrays.toString(output.shape()));\nSystem.out.println(\"data: \" + Arrays.toString(output.getDataAsFloatArray()));\n\n</code></pre> <p>\u4e86\u89e3\u66f4\u591a\u5173\u4e8e\u5982\u4f55\u4f7f\u7528PyTorch\u4eceJava\u00a0\u8fd9\u91cc\uff0c\u770b\u5230\u5b8c\u6574\u7684Javadoc API\u6587\u6863\u5728\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u76841.4\u7248\u672c\u8bf4\u660e\uff0c\u8bf7\u53c2\u89c1\u6b64\u5904\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#_2","title":"\u57df\u5e93","text":"<p>PyTorch\u57df\u5e93\uff08\u4f8b\u5982torchvision\uff0ctorchtext\u548ctorchaudio\uff09\u4f7f\u7528\u5e38\u89c1\u7684\u6570\u636e\u96c6\uff0c\u6a21\u578b\u548c\u8f6c\u6362\u5bf9PyTorch\u8fdb\u884c\u4e86\u8865\u5145\u3002\u6211\u4eec\u5f88\u9ad8\u5174\u4e0ePyTorch 1.4\u6838\u5fc3\u7248\u672c\u4e00\u8d77\u5171\u4eab\u6240\u6709\u4e09\u4e2a\u57df\u5e93\u7684\u65b0\u7248\u672c\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#torchvision-05","title":"torchvision 0.5","text":"<p>torchvision 0.5\u7684\u6539\u8fdb\u4e3b\u8981\u96c6\u4e2d\u5728\u589e\u52a0\u5bf9\u751f\u4ea7\u90e8\u7f72\u7684\u652f\u6301\uff0c\u5305\u62ec\u91cf\u5316\uff0cTorchScript\u548cONNX\u3002\u4e00\u4e9b\u4eae\u70b9\u5305\u62ec\uff1a</p> <ul> <li>\u73b0\u5728\uff0ctorchvision\u4e2d\u7684\u6240\u6709\u6a21\u578b\u90fd\u53ef\u4ee5\u4f7f\u7528torchscript\u7f16\u5199\uff0c\u4ece\u800c\u4f7f\u5176\u66f4\u6613\u4e8e\u79fb\u690d\u5230\u975ePython\u751f\u4ea7\u73af\u5883\u4e2d</li> <li>ResNets\uff0cMobileNet\uff0cShuffleNet\uff0cGoogleNet\u548cInceptionV3\u73b0\u5728\u5df2\u7ecf\u5177\u6709\u7ecf\u8fc7\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u91cf\u5316\u5bf9\u8c61\uff0c\u5e76\u4e14\u8fd8\u5305\u62ec\u7528\u4e8e\u8fdb\u884c\u91cf\u5316\u610f\u8bc6\u8bad\u7ec3\u7684\u811a\u672c\u3002</li> <li>\u4e0eMicrosoft\u56e2\u961f\u5408\u4f5c\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u5bf9\u6240\u6709\u578b\u53f7\u7684ONNX\u652f\u6301\uff0c\u5305\u62ecMask R-CNN\u3002</li> </ul> <p>\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173Torchvision 0.5\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#torchaudio-04","title":"torchaudio 0.4","text":"<p>torchaudio 0.4\u7684\u6539\u8fdb\u96c6\u4e2d\u5728\u589e\u5f3a\u5f53\u524d\u53ef\u7528\u7684\u8f6c\u6362\uff0c\u6570\u636e\u96c6\u548c\u540e\u7aef\u652f\u6301\u4e0a\u3002\u91cd\u70b9\u5305\u62ec\uff1a</p> <ul> <li>SoX\u73b0\u5728\u662f\u53ef\u9009\u7684\uff0c\u5e76\u4e14\u65b0\u7684\u53ef\u6269\u5c55\u540e\u7aef\u5206\u6d3e\u673a\u5236\u516c\u5f00\u4e86SoundFile\u4f5c\u4e3aSoX\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002</li> <li>\u6570\u636e\u96c6\u7684\u754c\u9762\u5df2\u7edf\u4e00\u3002\u8fd9\u6837\u53ef\u4ee5\u6dfb\u52a0\u4e24\u4e2a\u5927\u578b\u6570\u636e\u96c6\uff1aLibriSpeech\u548cCommon Voice\u3002</li> <li>\u73b0\u5728\u63d0\u4f9b\u4e86\u65b0\u6ee4\u6ce2\u5668\uff0c\u4f8b\u5982\u53cc\u4e8c\u9636\u6ee4\u6ce2\u5668\uff0c\u6570\u636e\u589e\u5f3a\uff08\u4f8b\u5982\u65f6\u95f4\u548c\u9891\u7387\u5c4f\u853d\uff09\uff0c\u53d8\u6362\uff08\u4f8b\u5982MFCC\uff09\uff0c\u589e\u76ca\u548c\u6296\u52a8\u4ee5\u53ca\u65b0\u7279\u5f81\u8ba1\u7b97\uff08\u4f8b\u5982\u589e\u91cf\uff09\u3002</li> <li>\u8f6c\u6362\u73b0\u5728\u652f\u6301\u6279\u5904\u7406\u5e76\u4e14\u662fjitable\u3002</li> <li>\u5177\u6709\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\u529f\u80fd\u7684\u4ea4\u4e92\u5f0f\u8bed\u97f3\u8bc6\u522b\u6f14\u793a\u53ef\u7528\u4e8e\u5b9e\u9a8c\u3002</li> </ul> <p>\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173Torchaudio 0.4\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.4/#torchtext-05","title":"torchtext 0.5","text":"<p>torchtext 0.5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5bf9\u6570\u636e\u96c6\u52a0\u8f7d\u5668API\u7684\u6539\u8fdb\uff0c\u5305\u62ec\u4e0e\u6838\u5fc3PyTorch API\u7684\u517c\u5bb9\u6027\uff0c\u800c\u4e14\u8fd8\u589e\u52a0\u4e86\u5bf9\u65e0\u76d1\u7763\u6587\u672c\u6807\u8bb0\u5316\u7684\u652f\u6301\u3002\u91cd\u70b9\u5305\u62ec\uff1a</p> <ul> <li>\u4e3aSentencePiece\u6dfb\u52a0\u4e86\u7ed1\u5b9a\uff0c\u4ee5\u5b9e\u73b0\u65e0\u76d1\u7763\u7684\u6587\u672c\u6807\u8bb0\u5316\u3002</li> <li>\u6dfb\u52a0\u4e86\u4e00\u4e2a\u65b0\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6570\u636e\u96c6-enwik9\u3002</li> <li>\u5bf9PennTreebank\uff0cWikiText103\uff0cWikiText2\uff0cIMDb\u8fdb\u884c\u4e86\u4fee\u8ba2\uff0c\u4ee5\u4f7f\u5176\u4e0etorch.utils.data\u517c\u5bb9\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u4f4d\u4e8e\u5b9e\u9a8c\u6587\u4ef6\u5939\u4e2d\uff0c\u6211\u4eec\u6b22\u8fce\u60a8\u63d0\u4f9b\u53cd\u9988\u3002</li> </ul> <p>\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173torchtext 0.5\u7684\u66f4\u591a\u4fe1\u606f\u3002</p> <p>\u6211\u4eec\u8981\u611f\u8c22\u6574\u4e2aPyTorch\u56e2\u961f\u548c\u793e\u533a\u4e3a\u8fd9\u9879\u5de5\u4f5c\u505a\u51fa\u7684\u6240\u6709\u8d21\u732e\u3002</p> <p>\u5e72\u676f!</p> <p>PyTorch\u56e2\u961f</p>"},{"location":"LatestChanges/PyTorch_V1.5/","title":"\u65b0\u7248\u672c: PyTorch 1.5\u53d1\u5e03\uff0c\u65b0\u7684\u548c\u66f4\u65b0\u7684API\uff0c\u5305\u62ecC++\u524d\u7aefAPI\u4e0ePython\u7684\u5e73\u4ef7","text":"<p>\u53d1\u5e03: 2020\u5e7404\u670821\u65e5</p> <p>\u8bd1\u8005\uff1a@\u7247\u523b</p> <p>\u539f\u6587: PyTorch</p> <p>\u7ffb\u8bd1: ApacheCN</p> <p>\u6765\u81ea PyTorch\u56e2\u961f</p> <p>\u4eca\u5929\uff0c\u6211\u4eec\u5ba3\u5e03PyTorch 1.5\u4ee5\u53ca\u65b0\u7684\u548c\u66f4\u65b0\u7684\u5e93\u7684\u53ef\u7528\u6027\u3002\u6b64\u7248\u672c\u5305\u62ec\u51e0\u4e2a\u4e3b\u8981\u7684\u65b0\u7684API\u6dfb\u52a0\u548c\u6539\u8fdb\u3002PyTorch\u73b0\u5728\u5305\u62ec\u5bf9C++\u524d\u7aef\u7684\u91cd\u5927\u66f4\u65b0\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u201c\u901a\u9053\u6700\u540e\u201d\u5185\u5b58\u683c\u5f0f\uff0c\u4ee5\u53ca\u7528\u4e8e\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u7684\u5206\u5e03\u5f0fRPC\u6846\u67b6\u7684\u7a33\u5b9a\u7248\u672c\u3002\u8be5\u7248\u672c\u8fd8\u4e3ahessians\u548cjacobians\u63d0\u4f9b\u4e86\u65b0\u7684autograd API\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5141\u8bb8\u521b\u5efa\u53d7pybind\u542f\u53d1\u7684\u81ea\u5b9a\u4e49C++\u7c7b\u7684API\u3002</p> <p>\u60a8\u53ef\u4ee5\u5728\u6b64\u5904\u627e\u5230\u8be6\u7ec6\u7684\u53d1\u5e03\u8bf4\u660e\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#capi","title":"C++\u524d\u7aefAPI\uff08\u7a33\u5b9a\uff09","text":"<p>C++\u524d\u7aefAPI\u73b0\u5728\u4e0ePython\u5904\u4e8e\u540c\u7b49\u5730\u4f4d\uff0c\u6574\u4f53\u529f\u80fd\u5df2\u79fb\u81f3\u201c\u7a33\u5b9a\u201d\uff08\u4ee5\u524d\u6807\u8bb0\u4e3a\u5b9e\u9a8c\u6027\uff09\u3002\u4e00\u4e9b\u4e3b\u8981\u4eae\u70b9\u5305\u62ec\uff1a</p> <ul> <li>\u73b0\u5728\uff0c\u6709\u4e86~100%\u7684C++ torch::nn\u6a21\u5757/\u529f\u80fd\u7684\u8986\u76d6\u7387\u548c\u6587\u6863\uff0c\u7528\u6237\u53ef\u4ee5\u8f7b\u677e\u5730\u5c06\u4ed6\u4eec\u7684\u6a21\u578b\u4ecePython API\u8f6c\u6362\u4e3aC++ API\uff0c\u4f7f\u6a21\u578b\u521b\u4f5c\u4f53\u9a8c\u66f4\u52a0\u6d41\u7545\u3002</li> <li>C++\u4e2d\u7684\u4f18\u5316\u5668\u504f\u79bb\u4e86Python\u7684\u7b49\u4ef7\u7269\uff1aC++\u4f18\u5316\u5668\u4e0d\u80fd\u5c06\u53c2\u6570\u7ec4\u4f5c\u4e3a\u8f93\u5165\uff0c\u800cPython\u7684\u4f18\u5316\u5668\u53ef\u4ee5\u3002\u6b64\u5916\uff0c\u6b65\u8fdb\u51fd\u6570\u7684\u5b9e\u73b0\u5e76\u4e0d\u5b8c\u5168\u76f8\u540c\u3002* \u57281.5\u7248\u672c\u4e2d\uff0cC++\u4f18\u5316\u5668\u7684\u884c\u4e3a\u5c06\u59cb\u7ec8\u4e0ePython\u7684\u7b49\u6548\u76f8\u540c\u3002</li> <li>C++\u4e2d\u7f3a\u5c11\u5f20\u91cf\u591a\u7ef4\u7d22\u5f15API\u662f\u4e00\u4e2a\u4f17\u6240\u5468\u77e5\u7684\u95ee\u9898\uff0c\u5e76\u5728PyTorch Github\u95ee\u9898\u8ddf\u8e2a\u5668\u548c\u8bba\u575b\u4e0a\u53d1\u5e03\u4e86\u8bb8\u591a\u5e16\u5b50\u3002\u4e4b\u524d\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u4f7f\u7528narrow / select / index_select / masked_selec \u7684\u7ec4\u5408\uff0c\u4e0ePython \u7b80\u660e\u7684API tensor[:, 0, ..., mask] \u8bed\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u662f\u7b28\u91cd\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u3002\u57281.5\u7248\u672c\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528 tensor.index({Slice(), 0, \"...\", mask}) \u6765\u8fbe\u5230\u540c\u6837\u7684\u76ee\u7684\u3002</li> </ul>"},{"location":"LatestChanges/PyTorch_V1.5/#_1","title":"\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u7684\u201c\u901a\u9053\u6700\u540e\u201d\u5185\u5b58\u683c\u5f0f\uff08\u5b9e\u9a8c\uff09","text":"<p>\u201c\u901a\u9053\u6700\u540e\u201d\u5185\u5b58\u5e03\u5c40\u89e3\u9501\u4e86\u4f7f\u7528\u9ad8\u6027\u80fd\u5377\u79ef\u7b97\u6cd5\u548c\u786c\u4ef6\uff08NVIDIA\u7684Tensor Cores\u3001FBGEMM\u3001QNNPACK\uff09\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5b83\u88ab\u8bbe\u8ba1\u4e3a\u901a\u8fc7\u8fd0\u7b97\u7b26\u81ea\u52a8\u4f20\u64ad\uff0c\u8fd9\u5141\u8bb8\u5728\u5185\u5b58\u5e03\u5c40\u4e4b\u95f4\u8f7b\u677e\u5207\u6362\u3002</p> <p>\u5728\u8fd9\u91cc\u4e86\u89e3\u6709\u5173\u5982\u4f55\u7f16\u5199\u5185\u5b58\u683c\u5f0f\u611f\u77e5\u8fd0\u7b97\u7b26\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#c","title":"\u81ea\u5b9a\u4e49C++\u7c7b\uff08\u5b9e\u9a8c\uff09","text":"<p>\u6b64\u7248\u672c\u6dfb\u52a0\u4e86\u4e00\u4e2a\u65b0\u7684API\uff0ctorch::class_\uff0c\u7528\u4e8e\u5c06\u81ea\u5b9a\u4e49C++\u7c7b\u540c\u65f6\u7ed1\u5b9a\u5230TorchScript\u548cPython\u4e2d\u3002\u8fd9\u4e2aAPI\u5728\u8bed\u6cd5\u4e0a\u51e0\u4e4e\u4e0epybind11\u76f8\u540c\u3002\u5b83\u5141\u8bb8\u7528\u6237\u5c06\u4ed6\u4eec\u7684C++\u7c7b\u53ca\u5176\u65b9\u6cd5\u516c\u5f00\u7ed9TorchScript\u7c7b\u578b\u7cfb\u7edf\u548c\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u8fd9\u6837\u4ed6\u4eec\u5c31\u53ef\u4ee5\u4eceTorchScript\u548cPython\u5b9e\u4f8b\u5316\u548c\u64cd\u4f5c\u4efb\u610fC++\u5bf9\u8c61\u3002C++\u7ed1\u5b9a\u793a\u4f8b\uff1a</p> <pre><code>template &lt;class T&gt;\nstruct MyStackClass : torch::CustomClassHolder {\n  std::vector&lt;T&gt; stack_;\n  MyStackClass(std::vector&lt;T&gt; init) : stack_(std::move(init)) {}\n\n  void push(T x) {\n    stack_.push_back(x);\n  }\n  T pop() {\n    auto val = stack_.back();\n    stack_.pop_back();\n    return val;\n  }\n};\n\nstatic auto testStack =\n  torch::class_&lt;MyStackClass&lt;std::string&gt;&gt;(\"myclasses\", \"MyStackClass\")\n      .def(torch::init&lt;std::vector&lt;std::string&gt;&gt;())\n      .def(\"push\", &amp;MyStackClass&lt;std::string&gt;::push)\n      .def(\"pop\", &amp;MyStackClass&lt;std::string&gt;::pop)\n      .def(\"size\", [](const c10::intrusive_ptr&lt;MyStackClass&gt;&amp; self) {\n        return self-&gt;stack_.size();\n      });\n</code></pre> <p>\u5b83\u516c\u5f00\u4e86\u4e00\u4e2a\u60a8\u53ef\u4ee5\u5728Python\u548cTorchScript\u4e2d\u4f7f\u7528\u7684\u7c7b\uff0c\u5c31\u50cf\u8fd9\u6837\uff1a</p> <pre><code>@torch.jit.script\ndef do_stacks(s : torch.classes.myclasses.MyStackClass):\n    s2 = torch.classes.myclasses.MyStackClass([\"hi\", \"mom\"])\n    print(s2.pop()) # \"mom\"\n    s2.push(\"foobar\")\n    return s2 # [\"hi\", \"foobar\"]\n</code></pre> <p>\u4f60\u53ef\u4ee5\u5728\u8fd9\u91cc\u7684\u6559\u7a0b\u4e2d\u5c1d\u8bd5\u4e00\u4e0b\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#rpcapi","title":"\u5206\u5e03\u5f0fRPC\u6846\u67b6API\uff08\u73b0\u5728\u7a33\u5b9a\uff09","text":"<p>\u5206\u5e03\u5f0fRPC\u6846\u67b6\u57281.4\u7248\u672c\u4e2d\u4f5c\u4e3a\u5b9e\u9a8c\u6027\u63a8\u51fa\uff0c\u5efa\u8bae\u5c06\u5206\u5e03\u5f0fRPC\u6846\u67b6\u6807\u8bb0\u4e3a\u7a33\u5b9a\u4e14\u4e0d\u518d\u662f\u5b9e\u9a8c\u6027\u7684\u3002\u8fd9\u9879\u5de5\u4f5c\u6d89\u53ca\u8bb8\u591a\u589e\u5f3a\u548c\u9519\u8bef\u4fee\u590d\uff0c\u4ee5\u4f7f\u5206\u5e03\u5f0fRPC\u6846\u67b6\u603b\u4f53\u4e0a\u66f4\u52a0\u53ef\u9760\u548c\u5f3a\u5927\uff0c\u5e76\u6dfb\u52a0\u4e86\u4e00\u4e9b\u65b0\u529f\u80fd\uff0c\u5305\u62ec\u5206\u6790\u652f\u6301\uff0c\u5728RPC\u4e2d\u4f7f\u7528TorchScript\u51fd\u6570\uff0c\u4ee5\u53ca\u4e00\u4e9b\u6613\u4e8e\u4f7f\u7528\u7684\u589e\u5f3a\u529f\u80fd\u3002\u4ee5\u4e0b\u662f\u6846\u67b6\u5185\u5404\u79cdAPI\u7684\u6982\u8ff0\uff1a</p>"},{"location":"LatestChanges/PyTorch_V1.5/#rpc-api","title":"RPC API","text":"<p>RPC API\u5141\u8bb8\u7528\u6237\u6307\u5b9a\u8981\u8fd0\u884c\u7684\u51fd\u6570\u548c\u5728\u8fdc\u7a0b\u8282\u70b9\u4e0a\u5b9e\u4f8b\u5316\u7684\u5bf9\u8c61\u3002\u8fd9\u4e9b\u51fd\u6570\u88ab\u900f\u660e\u5730\u8bb0\u5f55\uff0c\u4ee5\u4fbf\u68af\u5ea6\u53ef\u4ee5\u4f7f\u7528\u5206\u5e03\u5f0f\u81ea\u52a8\u901a\u8fc7\u8fdc\u7a0b\u8282\u70b9\u53cd\u5411\u4f20\u64ad\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#autograd","title":"\u5206\u5e03\u5f0fAutograd","text":"<p>Distributed Autograd\u5c06autograd\u56fe\u8fde\u63a5\u5230\u591a\u4e2a\u8282\u70b9\uff0c\u5e76\u5141\u8bb8\u68af\u5ea6\u5728\u5411\u540e\u4f20\u9012\u671f\u95f4\u6d41\u52a8\u3002\u68af\u5ea6\u88ab\u7d2f\u79ef\u5230\u4e0a\u4e0b\u6587\u4e2d\uff08\u4e0eAutograd\u7684.grad\u5b57\u6bb5\u76f8\u53cd\uff09\uff0c\u7528\u6237\u5fc5\u987b\u5728dist_autograd.context()\u7ba1\u7406\u5668\u4e0b\u6307\u5b9a\u5176\u6a21\u578b\u7684\u6b63\u5411\u4f20\u9012\uff0c\u4ee5\u786e\u4fdd\u6b63\u786e\u8bb0\u5f55\u6240\u6709RPC\u901a\u4fe1\u3002\u76ee\u524d\uff0c\u53ea\u5b9e\u73b0\u4e86FAST\u6a21\u5f0f\uff08\u8bf7\u53c2\u9605\u6b64\u5904\u4e86\u89e3FAST\u548cSMART\u6a21\u5f0f\u4e4b\u95f4\u7684\u533a\u522b\uff09\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#_2","title":"\u5206\u5e03\u5f0f\u4f18\u5316\u5668","text":"<p>\u5206\u5e03\u5f0f\u4f18\u5316\u5668\u4e3a\u6bcf\u4e2a\u5de5\u4eba\u521b\u5efaRRefs\uff0c\u5176\u53c2\u6570\u9700\u8981\u68af\u5ea6\uff0c\u7136\u540e\u4f7f\u7528RPC API\u8fdc\u7a0b\u8fd0\u884c\u4f18\u5316\u5668\u3002\u7528\u6237\u5fc5\u987b\u6536\u96c6\u6240\u6709\u8fdc\u7a0b\u53c2\u6570\u5e76\u5c06\u5176\u5305\u88c5\u5728RRef\u4e2d\uff0c\u56e0\u4e3a\u8fd9\u662f\u5206\u5e03\u5f0f\u4f18\u5316\u5668\u6240\u9700\u7684\u8f93\u5165\u3002\u7528\u6237\u8fd8\u5fc5\u987b\u6307\u5b9a\u5206\u5e03\u5f0fautograd context_id\uff0c\u4ee5\u4fbf\u4f18\u5316\u5668\u77e5\u9053\u5728\u54ea\u4e2a\u4e0a\u4e0b\u6587\u4e2d\u67e5\u627e\u6e10\u53d8\u3002</p> <p>\u5728\u6b64\u5904\u4e86\u89e3\u6709\u5173\u5206\u5e03\u5f0fRPC\u6846\u67b6API\u7684\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#autograd-api","title":"\u65b0\u7684\u9ad8\u7ea7AUTOGRAD API\uff08\u5b9e\u9a8c\uff09","text":"<p>PyTorch 1.5\u4e3atorch.autograd.functional\u5b50\u6a21\u5757\u5e26\u6765\u4e86\u5305\u62ecjacobian\u3001hessian\u3001jvp\u3001vjp\u3001hvp\u548cvhp\u5728\u5185\u7684\u65b0\u529f\u80fd\u3002\u6b64\u529f\u80fd\u57fa\u4e8e\u5f53\u524d\u7684API\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u8f7b\u677e\u6267\u884c\u8fd9\u4e9b\u529f\u80fd\u3002</p> <p>\u53ef\u4ee5\u5728GitHub\u4e0a\u7684\u8be6\u7ec6\u8bbe\u8ba1\u8ba8\u8bba\u5728\u8fd9\u91cc\u627e\u5230\u3002</p>"},{"location":"LatestChanges/PyTorch_V1.5/#python-2","title":"\u4e0d\u518d\u652f\u6301PYTHON 2","text":"<p>\u4ecePyTorch 1.5.0\u5f00\u59cb\uff0c\u6211\u4eec\u5c06\u4e0d\u518d\u652f\u6301Python 2\uff0c\u7279\u522b\u662f2.7\u7248\u672c\u3002\u4eca\u540e\u5bf9Python\u7684\u652f\u6301\u5c06\u4ec5\u9650\u4e8ePython 3\uff0c\u7279\u522b\u662fPython 3.5\u30013.6\u30013.7\u548c3.8\uff08\u5728PyTorch 1.4.0\u4e2d\u9996\u6b21\u542f\u7528\uff09\u3002</p> <p>\u6211\u4eec\u8981\u611f\u8c22\u6574\u4e2aPyTorch\u56e2\u961f\u548c\u793e\u533a\u5bf9\u8fd9\u9879\u5de5\u4f5c\u7684\u6240\u6709\u8d21\u732e\u3002</p> <p>Cheers!</p> <p>PyTorch \u56e2\u961f</p>"},{"location":"LatestChanges/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Introduction</li> <li>PyTorch V1.2 \u65b0\u7279\u6027</li> <li>PyTorch V1.3 \u65b0\u7279\u6027</li> <li>PyTorch V1.4 \u65b0\u7279\u6027</li> </ul>"}]}